[
    {
        "query": "Autoimmune disease",
        "paper_list_string": "Panayi 1976:\n\nTitle: Auto-immune disease.\n\nAbstract: Auto-immune disease may result from the interaction of the genetic load of the individual, modification of self-tissue antigens by environmental agents such as virus or drugs and abnormalities of the immunological system itself such as the loss of controlling or suppressor T cells with age. In the majority of people the outcome is tolerance, maintenance of normal tissue architecture and function. In the unfortunate few the outcome is auto-immune disease, that is, failure to recognize \"self\".\n\n==\n\nDavidson 2014:\n\nTitle: General Features of Autoimmune Disease\n\nAbstract: Abstract Autoimmune diseases are common diseases in which loss of tolerance within the immune system results in pathologic immune responses that target either cellular or organ-specific self-antigens. There is a genetic tendency toward autoreactivity in affected individuals, and both innate and adaptive immune activation may contribute to disease. An important recent advance is the identification of genetic polymorphisms that contribute to risk in most autoimmune diseases and are associated with a variety of immune activation pathways; these associations have been informative about disease-specific pathogenesis and have led to the development of successful therapies for some diseases. A deeper understanding of the components of the innate and adaptive immune system has led to highly effective therapeutic targeting of cytokines, cell surface molecules, and intracellular signaling molecules with marked improvements in outcome in several autoimmune diseases. Fertile new avenues for research include mechanisms of regulation by regulatory immune cells, metabolic regulation of immune cell function, and immune activation by commensal bacteria that colonize humans. The goal of regulating autoimmunity without causing excessive immunosuppression remains elusive but is the holy grail of current research efforts.\n\n==\n\nBaker 1997:\n\nTitle: Autoimmune endocrine disease.\n\nAbstract: Autoimmune endocrine diseases are serious disorders that utilize immense health care resources and cause tremendous disability. They include type 1 diabetes mellitus, thyroiditis, Graves disease, Addison disease, and polyglandular syndromes. Analysis of the basis of autoimmune diseases has been aided by the application of new knowledge in immunologic physiology. Recent investigations using these techniques have revealed complicated disorders that have varied pathogenesis and complex genetic predispositions. While the mainstay of treatment for these diverse diseases remains the replacement of hormones produced by the damaged endocrine organ, investigations into the pathogenesis of these disorders provide hope for the development of specific therapeutic measures to block their pathologic basis.\n\n==\n\nKing 1963:\n\nTitle: Autoimmune diseases; pathogenesis, chemistry and therapy.\n\nAbstract: Medical scientists, meeting the challenge of socalled \"autoimmune\" diseases, have created new techniques and achieved new concepts which are altering markedly our outlook in immunology, immunochemistry, and pathology. Burnet, winner of the Nobel prize for medicine, collaborates with Mackay to present an exposition of this field. They discuss a group of diseases \"for which an autoimmune etiology has been established either as a primary or an associated factor, or in which there is some likelihood that autoimmune processes are concerned.\" Analyzed at length are thyroiditis, systemic lupus erythematosis, certain hemolytic conditions, rheumatoid arthritis, glomerulonephritis, and certain liver and nervous system diseases, while numerous other conditions are treated more cursorily. To diagnose an autoimmune state the authors invoke the following criteria: an increase in gamma globulin, demonstrable autoantibodies against some body component, deposition of denatured gamma globulin at certain sites of elections, accumulation of lymphocytes and plasma cells, some degree of\n\n==\n\nSinha 1990:\n\nTitle: Autoimmune diseases: the failure of self tolerance.\n\nAbstract: The ability to discriminate between self and nonself antigens is vital to the functioning of the immune system as a specific defense against invading microorganisms. Failure of the immune system to \"tolerate\" self tissues can result in pathological autoimmune states leading to debilitating illness and sometimes death. The induction of autoimmunity involves genetic and environmental factors that have focused the attention of researchers on the trimolecular complex formed by major histocompatibility complex molecules, antigen, and T cell receptors. Detailed molecular characterization of these components points to potential strategies for disease intervention.\n\n==\n\nAsherson 1967:\n\nTitle: Growing Points: Autoimmune Disease\u2014I\n\nAbstract: In some human diseases antibodies react with human tissue in vitro. These antibodies are called autoantibodies, and are found in rheumatic fever, Hashimoto's thyroiditis, systemic lupus erythematosus, and other diseases. The presence of autoantibodies is compatible with the view that tissues are damaged by an autoimmune process directed against components of the patient's own tissues. For this reason Hashimoto's thyroiditis and systemic lupus erythematosus are called putative autoimmune diseases. For readers wishing to learn more about autoimmunity the recent books by Humphrey and White (1964) and Glynn and Holborow (1965) are recommended. The bibliography for the statements made in this article is given by Asherson (1965).\n\n==\n\nAgarwal 2009:\n\nTitle: Autoimmunity in common variable immunodeficiency\n\nAbstract: Common variable immunodeficiency (CVID) is the most common clinically significant primary immune defect. Although the hallmark of CVID is hypogammaglobulinemia, the intrinsic dysregulation of the immune system leads to defective T-cell activation and proliferation, as well as dendritic cell and cytokine defects. Although 70% to 80% of patients have had recurrent sinopulmonary infections, autoimmunity and inflammatory complications are also common. The most common autoimmune conditions are immune thrombocytopenic purpura and hemolytic anemia, but other autoimmune complications arise, including rheumatoid arthritis, pernicious anemia, primary biliary cirrhosis, thyroiditis, sicca syndrome, systemic lupus, and inflammatory bowel disease. Treatment of autoimmunity includes highdose immunoglobulins, corticosteroids, selected immu no suppressants, and other immune modulators. This review focuses on autoimmune conditions associated with CVID, potential mechanisms of immune dysregulation, and therapeutic strategies.\n\n==\n\nKatsorida 2001:\n\nTitle: Autoimmunity and autoimmune disease.\n\nAbstract: A disease is defined as autoimmune if the tissue damage is caused by an adaptive immune response to self-antigen. Development of the disease by transferring the active component of the immune response to an appropriate recipient is the best proof that the disease is caused by autoimmunity. The body contains large numbers of potentially autoreactive B and T cells, although there are a variety of mechanisms operating to establish self-tolerance during lymphocyte development. This is particularly true for thymic T cells that are not deleted by selfepitopes. The presence of autoreactive B cells in the normal population is well demonstrated by the development of autoantibodies when autoantigens are injected with adjuvants in normal animals (e.g. anti-thyroglobulin). Existence of autoreactive T cells in normal individuals is demonstrated by the production of autoimmune lines of T cells when normal circulating T cells are stimulated by the appropriate autoantigen (e.g. myelin basic protein) and interleukin-2. Autoimmune disease can be classified as organ specific, when the response is primarily against antigens localized to particular organs or as non-organ specific, when the response is directed against widespread antigens. Hashimoto\u2019s thyroiditis and SLE represent the extremes of autoimmune disease spectrum. In organ specific diseases, the thyroid, adrenals, stomach, and pancreas are the usual target organs. The non-organ specific diseases, which include the rheumatic disorders, characteristically involve the skin, kidney, brain, lungs, joints, and muscles w1x.",
        "final_summary": "The papers suggest that autoimmune diseases result from a loss of self-tolerance within the immune system (Panayi 1976). These diseases involve the immune system targeting self-antigens, leading to tissue damage and dysfunction (Davidson 2014). Genetic factors and environmental triggers, such as viruses or drugs, may contribute to the development of autoimmune diseases (Panayi 1976). Autoimmune endocrine diseases, such as type 1 diabetes and thyroiditis, have complex pathogenesis and genetic predispositions (Baker 1997). Autoantibodies and autoreactive immune cells play a role in the development of autoimmune diseases (Sinha 1990; Katsorida 2001). While autoimmune diseases can affect specific organs or be non-organ specific, they can cause significant disability and require hormone replacement therapy (Baker 1997; Katsorida 2001). Autoimmunity can coexist with primary immune defects, such as common variable immunodeficiency, leading to both recurrent infections and autoimmune complications (Agarwal 2009). Understanding the mechanisms of immune dysregulation and developing targeted therapies remain areas of ongoing research (Davidson 2014; Agarwal 2009)."
    },
    {
        "query": "genomic imprinting",
        "paper_list_string": "Falls 1999:\n\nTitle: Genomic imprinting: implications for human disease.\n\nAbstract: Genomic imprinting refers to an epigenetic marking of genes that results in monoallelic expression. This parent-of-origin dependent phenomenon is a notable exception to the laws of Mendelian genetics. Imprinted genes are intricately involved in fetal and behavioral development. Consequently, abnormal expression of these genes results in numerous human genetic disorders including carcinogenesis. This paper reviews genomic imprinting and its role in human disease. Additional information about imprinted genes can be found on the Genomic Imprinting Website at http://www.geneimprint.com.\n\n==\n\nBarlow 2011:\n\nTitle: Genomic imprinting: a mammalian epigenetic discovery model.\n\nAbstract: Genomic imprinting is an epigenetic process leading to parental-specific expression of one to two percent of mammalian genes that offers one of the best model systems for a molecular analysis of epigenetic regulation in development and disease. In the twenty years since the first imprinted gene was identified, this model has had a significant impact on decoding epigenetic information in mammals. So far it has led to the discovery of long-range cis-acting control elements whose epigenetic state regulates small clusters of genes and of unusual macro noncoding RNAs (ncRNAs) that directly repress genes in cis, and critically, it has demonstrated that one biological role of DNA methylation is to allow expression of genes normally repressed by default. This review describes the progress in understanding how imprinted protein-coding genes are silenced; in particular, it focuses on the role of macro ncRNAs that have broad relevance as a potential new layer of regulatory information in the mammalian genome.\n\n==\n\nBartolomei 2009:\n\nTitle: Genomic imprinting: employing and avoiding epigenetic processes.\n\nAbstract: Genomic imprinting refers to an epigenetic mark that distinguishes parental alleles and results in a monoallelic, parental-specific expression pattern in mammals. Few phenomena in nature depend more on epigenetic mechanisms while at the same time evading them. The alleles of imprinted genes are marked epigenetically at discrete elements termed imprinting control regions (ICRs) with their parental origin in gametes through the use of DNA methylation, at the very least. Imprinted gene expression is subsequently maintained using noncoding RNAs, histone modifications, insulators, and higher-order chromatin structure. Avoidance is manifest when imprinted genes evade the genome-wide reprogramming that occurs after fertilization and remain marked with their parental origin. This review summarizes what is known about the establishment and maintenance of imprinting marks and discusses the mechanisms of imprinting in clusters. Additionally, the evolution of imprinted gene clusters is described. While considerable information regarding epigenetic control of imprinting has been obtained recently, much remains to be learned.\n\n==\n\nFerguson-Smith 2011:\n\nTitle: Genomic imprinting: the emergence of an epigenetic paradigm\n\nAbstract: The emerging awareness of the contribution of epigenetic processes to genome function in health and disease is underpinned by decades of research in model systems. In particular, many principles of the epigenetic control of genome function have been uncovered by studies of genomic imprinting. The phenomenon of genomic imprinting, which results in some genes being expressed in a parental--origin-specific manner, is essential for normal mammalian growth and development and exemplifies the regulatory influences of DNA methylation, chromatin structure and non-coding RNA. Setting seminal discoveries in this field alongside recent progress and remaining questions shows how the study of imprinting continues to enhance our understanding of the epigenetic control of genome function in other contexts.\n\n==\n\nHall 1997:\n\nTitle: Genomic imprinting: nature and clinical relevance.\n\nAbstract: Molecular genetic techniques allow investigators to trace chromosomes and genes from parent to child and, in a single individual, from tissue to tissue. These techniques have uncovered a new type of gene control in which the allele from one parent is expressed and the allele from the other parent is not. This differential expression is called genomic imprinting. It may lead to phenotypic differences when inheritance is from the mother versus the father. Genomic imprinting has been observed in a number of disorders having to do with growth, behavior, and abnormal cell growth. It is important to be aware that such a phenomenon exists and to consider it when making diagnoses and determining therapy.\n\n==\n\nReik 2001:\n\nTitle: Genomic imprinting: parental influence on the genome\n\nAbstract: Genomic imprinting affects several dozen mammalian genes and results in the expression of those genes from only one of the two parental chromosomes. This is brought about by epigenetic instructions \u2014 imprints \u2014 that are laid down in the parental germ cells. Imprinting is a particularly important genetic mechanism in mammals, and is thought to influence the transfer of nutrients to the fetus and the newborn from the mother. Consistent with this view is the fact that imprinted genes tend to affect growth in the womb and behaviour after birth. Aberrant imprinting disturbs development and is the cause of various disease syndromes. The study of imprinting also provides new insights into epigenetic gene modification during development.\n\n==\n\nBarlow 2014:\n\nTitle: Genomic imprinting in mammals.\n\nAbstract: Genomic imprinting affects a subset of genes in mammals and results in a monoallelic, parental-specific expression pattern. Most of these genes are located in clusters that are regulated through the use of insulators or long noncoding RNAs (lncRNAs). To distinguish the parental alleles, imprinted genes are epigenetically marked in gametes at imprinting control elements through the use of DNA methylation at the very least. Imprinted gene expression is subsequently conferred through lncRNAs, histone modifications, insulators, and higher-order chromatin structure. Such imprints are maintained after fertilization through these mechanisms despite extensive reprogramming of the mammalian genome. Genomic imprinting is an excellent model for understanding mammalian epigenetic regulation.\n\n==\n\nVerona 2003:\n\nTitle: Genomic imprinting: intricacies of epigenetic regulation in clusters.\n\nAbstract: An intriguing characteristic of imprinted genes is that they often cluster in large chromosomal domains, raising the possibility that gene-specific and domain-specific mechanisms regulate imprinting. Several common features emerged from comparative analysis of four imprinted domains in mice and humans: (a) Certain genes appear to be imprinted by secondary events, possibly indicating a lack of gene-specific imprinting marks; (b) some genes appear to resist silencing, predicting the presence of cis-elements that oppose domain-specific imprinting control; (c) the nature of the imprinting mark remains incompletely understood. In addition, common silencing mechanisms are employed by the various imprinting domains, including silencer elements that nucleate and propagate a silent chromatin state, insulator elements that prevent promoter-enhancer interactions when hypomethylated on one parental allele, and antisense RNAs that function in silencing the overlapping sense gene and more distantly located genes. These commonalities are reminiscent of the behavior of genes subjected to, and the mechanisms employed in, dosage compensation.",
        "final_summary": "Genomic imprinting, an epigenetic process, results in monoallelic expression of genes (Falls 1999). This process is crucial in fetal and behavioral development, with imprinted genes playing a significant role (Falls 1999). Abnormal expression of these genes can lead to various human genetic disorders, including carcinogenesis (Falls 1999). Imprinted genes are marked with parental origin in gametes through DNA methylation and are maintained using noncoding RNAs, histone modifications, and higher-order chromatin structure (Bartolomei 2009). The study of genomic imprinting has enhanced our understanding of the epigenetic control of genome function (Ferguson-Smith 2011). Genomic imprinting can cause phenotypic differences based on whether the allele is inherited from the mother or father, and it has been observed in disorders related to growth, behavior, and abnormal cell growth (Hall 1997). Imprinting affects gene expression from only one parental chromosome and is established in parental germ cells through epigenetic instructions (Reik 2001). Imprinted genes are regulated through insulators, long noncoding RNAs, DNA methylation, histone modifications, and higher-order chromatin structure (Barlow 2014). Imprinted genes often cluster in chromosomal domains, and domain-specific mechanisms may regulate imprinting (Verona 2003)."
    },
    {
        "query": "why does pulmonary edema tend to occur in hilar area rather than peripheral?",
        "paper_list_string": "Fleischner 1967:\n\nTitle: The butterfly pattern of acute pulmonary edema.\n\nAbstract: Abstract Among the factors causing uneven distribution of pulmonary edema, defective lymph drainage favors the accumulation of edematous fluid in areas of previous disease and scars, both pulmonary and pleural. The medulla and cortex of the lung are structurally different, and there are convincing reasons to assume nonuniformity of ventilation between them. Radiologic evidence shows that the cortex participates in the ventilatory play to a greater extent than the medulla; being readily distensible, it provides probably the greatest part of the ventilatory volume changes at rest. Lymphatic drainage, dependent on movements of the adjacent structures, is greatly promoted in the well ventilated parts, and the poorly moving central parts (medulla) seem to have a sluggish lymph flow. This would explain the frequently encountered preference of pulmonary edema for the central part of the lung or individual lobe. The involved central wet portion is occasionally set off abruptly from the dry cortex, because of the sharp morphologic and functional demarcation between medulla and cortex. The appearance of this demarcation is obviously dependent on factors of ventilation, perfusion, and others. It is, therefore, usually transient in cardiovascular edema.\n\n==\n\nGluecker 1999:\n\nTitle: Clinical and radiologic features of pulmonary edema.\n\nAbstract: Pulmonary edema may be classified as increased hydrostatic pressure edema, permeability edema with diffuse alveolar damage (DAD), permeability edema without DAD, or mixed edema. Pulmonary edema has variable manifestations. Postobstructive pulmonary edema typically manifests radiologically as septal lines, peribronchial cuffing, and, in more severe cases, central alveolar edema. Pulmonary edema with chronic pulmonary embolism manifests as sharply demarcated areas of increased ground-glass attenuation. Pulmonary edema with veno-occlusive disease manifests as large pulmonary arteries, diffuse interstitial edema with numerous Kerley lines, peribronchial cuffing, and a dilated right ventricle. Stage 1 near drowning pulmonary edema manifests as Kerley lines, peribronchial cuffing, and patchy, perihilar alveolar areas of airspace consolidation; stage 2 and 3 lesions are radiologically nonspecific. Pulmonary edema following administration of cytokines demonstrates bilateral, symmetric interstitial edema with thickened septal lines. High-altitude pulmonary edema usually manifests as central interstitial edema associated with peribronchial cuffing, ill-defined vessels, and patchy airspace consolidation. Neurogenic pulmonary edema manifests as bilateral, rather homogeneous airspace consolidations that predominate at the apices in about 50% of cases. Reperfusion pulmonary edema usually demonstrates heterogeneous airspace consolidations that predominate in the areas distal to the recanalized vessels. Postreduction pulmonary edema manifests as mild airspace consolidation involving the ipsilateral lung, whereas pulmonary edema due to air embolism initially demonstrates interstitial edema followed by bilateral, peripheral alveolar areas of increased opacity that predominate at the lung bases. Familiarity with the spectrum of radiologic findings in pulmonary edema from various causes will often help narrow the differential diagnosis.\n\n==\n\nB\u00e4rtsch 2005:\n\nTitle: Physiological aspects of high-altitude pulmonary edema.\n\nAbstract: High-altitude pulmonary edema (HAPE) develops in rapidly ascending nonacclimatized healthy individuals at altitudes above 3,000 m. An excessive rise in pulmonary artery pressure (PAP) preceding edema formation is the crucial pathophysiological factor because drugs that lower PAP prevent HAPE. Measurements of nitric oxide (NO) in exhaled air, of nitrites and nitrates in bronchoalveolar lavage (BAL) fluid, and forearm NO-dependent endothelial function all point to a reduced NO availability in hypoxia as a major cause of the excessive hypoxic PAP rise in HAPE-susceptible individuals. Studies using right heart catheterization or BAL in incipient HAPE have demonstrated that edema is caused by an increased microvascular hydrostatic pressure in the presence of normal left atrial pressure, resulting in leakage of large-molecular-weight proteins and erythrocytes across the alveolarcapillary barrier in the absence of any evidence of inflammation. These studies confirm in humans that high capillary pressure induces a high-permeability-type lung edema in the absence of inflammation, a concept first introduced under the term \"stress failure.\" Recent studies using microspheres in swine and magnetic resonance imaging in humans strongly support the concept and primacy of nonuniform hypoxic arteriolar vasoconstriction to explain how hypoxic pulmonary vasoconstriction occurring predominantly at the arteriolar level can cause leakage. This compelling but as yet unproven mechanism predicts that edema occurs in areas of high blood flow due to lesser vasoconstriction. The combination of high flow at higher pressure results in pressures, which exceed the structural and dynamic capacity of the alveolar capillary barrier to maintain normal alveolar fluid balance.\n\n==\n\nDenomme 1988:\n\nTitle: Pulmonary edema.\n\nAbstract: Pulmonary edema is the accumulation of fluids in the interstitium and alveoli of the lung. There are two main basic mechanisms for edema development: increased hydrostatic pressure in the lung capillaries (\u201chigh-pressure edema\u201d) and increase vascular permeability (\u201clow-pressure edema). This classification helps understand the basic pathophysiological differences between the two types of pulmonary edema, but has limitations. Disruption of some or all layers of the alveolar-capillary unit occurs during elevated capillary hydrostatic pressures, a phenomenon termed \u201cpulmonary capillary stress failure\u201d. Pulmonary capillary stress failure represents a process that blurs the distinction between high-pressure and low-pressure pulmonary edema, as the disruption of the alveolar-capillary membrane by high hydrostatic pressures may render it more permeable to fluid and proteins. The resulting edema fluid has a higher concentration of protein than would be expected in conventional high-pressure pulmonary edema. These observations may explain some features seem in high-altitude pulmonary edema and neurogenic pulmonary edema.\n\n==\n\nGleason 1966:\n\nTitle: The lateral roentgenogram in pulmonary edema.\n\nAbstract: 1. Lateral roentgenograms demonstrate that the edema which appears central on the frontal roentgenogram may, in fact, lie in anterior or posterior segments of any lobe of the lung.2. Interstitial and intra-alveolar pulmonary edema accompany each other in most patients irrespective of the cause.3. There are specific signs of interstitial edema, such as septal lines and peribronchial and perivascular cuffing. but others, such as perihilar haze, can be confused with intra-alveolar edema.4. Although gravity probably plays a part in the distribution of edema, it is difficult to obtain conclusive evidene of this in all cases.5. Pulmonary edema appears to shift easily and rapidly from lobe to lobe and from one lung to the other.\n\n==\n\nGurney 1989:\n\nTitle: Pulmonary edema localized in the right upper lobe accompanying mitral regurgitation.\n\nAbstract: Focal patterns of pulmonary edema are confusing and often mistaken for the more common causes of focal lung disease, pneumonia, infarction, or aspiration. The authors report four cases of right upper lobe edema secondary to mitral regurgitation. The pathogenesis believed to be responsible for this condition is the vector of blood flow from the left ventricle to left atrium, which may be targeted at the right superior pulmonary vein, locally accentuating the forces for edema formation in the right upper lobe. Pulmonary edema accompanying mitral regurgitation should be suspected whenever right upper lobe consolidation develops in a patient with known or suspected mitral valve disease. The presence of interstitial edema in the remainder of the lungs can help in the differentiation of this condition from pneumonia and other disorders.\n\n==\n\nWillms 1988:\n\nTitle: Pulmonary edema due to upper airway obstruction in adults.\n\nAbstract: A report of pulmonary edema following acute upper airway obstruction in an adult is presented, and the literature involving 25 additional cases is reviewed. This form of pulmonary edema appears to be related to markedly negative intrathoracic pressure due to forced inspiration against a closed upper airway resulting in transudation of fluid from pulmonary capillaries to the interstitium. Postanesthetic laryngospasm is the most common cause of pulmonary edema in adults (11/26 cases). The edema usually clears rapidly with supportive care. Aggressive diagnostic and therapeutic interventions may be avoided if the syndrome is recognized. Maintenance of oxygenation and a patent airway are the mainstays of treatment.\n\n==\n\nSzidon 1972:\n\nTitle: The alveolar-capillary membrane and pulmonary edema.\n\nAbstract: CONVENTIONAL teaching, based on light microscopy, has dealt with the physiopathology of pulmonary edema in terms of a simple barrier separating pulmonary capillaries from the alveolar spaces. According to this simplistic view, pulmonary edema represents the extravasation of a filtrate of plasma directly from pulmonary capillaries into alveoli as the result of one of two alternative mechanisms: an imbalance of forces operating across a normal alveolar-capillary barrier so that reabsorptive forces cannot cope with the high transcapillary filtration rates; or, despite normal transcapillary forces, excessive passage of plasma filtrate out of the capillary because of a breakdown of the barrier. .\u00a0.\u00a0.",
        "final_summary": "Several factors may explain why pulmonary edema tends to occur in the hilar area rather than the peripheral area. Fleischner 1967 suggests that defective lymph drainage and nonuniform ventilation between the medulla and cortex of the lung may contribute to the preference for central edema. B\u00e4rtsch 2005 focuses on high-altitude pulmonary edema and suggests that nonuniform hypoxic arteriolar vasoconstriction may cause leakage of fluid in areas of high blood flow. Denomme 1988 highlights the disruption of the alveolar-capillary membrane by high hydrostatic pressures, which may render it more permeable to fluid and proteins, leading to edema with a higher protein concentration. Gleason 1966 mentions that pulmonary edema can shift easily and rapidly from lobe to lobe and from one lung to the other. Gurney 1989 discusses focal pulmonary edema in the right upper lobe accompanying mitral regurgitation. Willms 1988 reports pulmonary edema following acute upper airway obstruction, which may be related to negative intrathoracic pressure. Szidon 1972 challenges the conventional teaching of a simple barrier between capillaries and alveoli, suggesting a more complex physiopathology of pulmonary edema."
    },
    {
        "query": "Antioxidant enzymes and Skin Cancer risk",
        "paper_list_string": "He 2010:\n\nTitle: Polymorphisms in genes involved in oxidative stress and their interactions with lifestyle factors on skin cancer risk.\n\nAbstract: Ultraviolet (UV)-induced oxidative stress has been implicated in skin carcinogenesis [1]. Several antioxidant enzymes, such as glutathione peroxidase (GPX) and catalase (CAT), counteract oxidative damage and constitute a primary defense against oxidative stress. GPX is a soluble selenoprotein that reduces H2O2 and organic hydroperoxides to H2O, and GPX1 is the most abundant and ubiquitous intracellular isoform [1]. GPX1 activity is not strongly affected by UV and is considered to be the most important antioxidant enzyme defense mechanism in the skin [2]. CAT is a heme enzyme that neutralizes reactive oxygen species by converting H2O2 to H2O and O2. CAT activity in the skin is significantly reduced after exposure to UV [2], which suggests its effect may be prone to effect modification by environmental factors. \n \nInherited variants in the encoding genes that affect the activity or expression of these antioxidant enzymes are hypothesized to modulate oxidative stress and thus influence skin cancer risk. A polymorphism in the GPX1 gene (Pro198Leu, rs1050450) and a polymorphism in the promoter region of the CAT gene (C-262T, rs1001179) have been shown to be associated with lower enzyme activities of their encoded enzymes [3, 4]. To test our main hypothesis that these two genetic polymorphisms are associated with skin cancer risk, we conducted a nested case-control study of Caucasians (218 melanoma, 285 squamous cell carcinoma (SCC), and 300 basal cell carcinoma (BCC) cases, and 870 age-matched controls) within the Nurses\u2019 Health Study. We further investigated potential gene-environment interactions between these polymorphisms and lifestyle factors such as dietary antioxidant intake and sun exposure related risk factors. A detailed description of the characteristics of cases and controls was published previously [5]. Information on dietary intake was collected prospectively by food-frequency questionnaires, and total-energy-adjusted cumulative average of dietary intake was used to reduce within-person variation and represent long-term dietary intake [6]. \n \nWe genotyped the two single nuclear polymorphisms (SNPs) (rs1050450 and rs1001179) by the 5\u2032 nuclease assay (TaqMan\u00ae) in 384-well format, using the ABI PRISM 7900HT Sequence Detection System (Applied Biosystems, Foster City, CA). The distributions of genotypes for the two SNPs were in Hardy-Weinberg equilibrium among controls (p=0.94, 0.83, respectively). We compared the cases of each type of skin cancer to the common control series. We used unconditional multivariate logistic regression to model the association between genetic polymorphisms and skin cancer risk and to estimate multivariate Odds Ratios (ORs) and 95% Confidence Intervals (CIs). To test statistical significance of gene-environmental interactions, we used dominant model for genotypes and dichotomized environmental exposures as low versus high based on median values among controls. We tested the statistical significance of a single multiplicative interaction term. \n \nIn the main effect analysis (Table 1), we observed that the GPX1 198 Leu/Leu genotype was significantly associated with a two-fold increased risk of melanoma (OR, 2.14; 95% CI, 1.22\u20133.72), after adjustment for age and other covariates. No association was found between this polymorphism and SCC or BCC risk, which was consistent with one previous study [7]. This polymorphism has been shown to be associated with lung cancer [8] and breast cancer [3] previously. We did not observe significant association between the CAT C-262T polymorphism and the risk of any type of skin cancer. \n \n \n \nTable 1 \n \nAssociation between GPX Pro198Leu and CAT C-262T genetic polymorphisms and skin cancer risk a \n \n \n \nAs exploratory analyses, we further tested gene-environment interactions between the genetic variants and lifestyle factors that modulate oxidative stress. We found the association between the CAT C-262T polymorphism and melanoma risk was significantly modified by history of severe sunburns (p for interaction, 0.008, Table 2), a variable combining exposure intensity and biological response to sun exposure. The positive association between history of severe sunburns and melanoma risk was restricted to T carriers (OR, 1.73; 95% CI, 1.02\u20132.92), compared to women with CC genotype (OR, 1.03; 95% CI, 0.63\u20131.69). We also observed a significant gene-diet interaction between the CAT C-262T polymorphism and total carotenoid intake on melanoma risk (p for interaction, 0.01). The inverse association of total carotenoid intake with melanoma risk was limited among women with CC genotype (OR, 0.63; 95% CI, 0.41\u20130.97), whereas no association was observed among T carriers (OR, 1.23; 95% CI, 0.77\u20131.97). Inconsistent results were reported on the relationship between dietary carotenoid intake and melanoma risk in several previous case-control studies. An inverse association between the intake and the risk of melanoma was observed in some studies [9], but not in other studies [10]. Our results suggest that the inconsistency in the literature may reflect a potential gene-diet interaction. As we tested different genetic polymorphisms, multiple environmental exposures and dietary factors, and three types of skin cancer, multiple testing in our study may lead to false positive results. Replications in independent studies are needed to validate these results. No significant interactions were observed between the GPX1 polymorphism and these lifestyle factors on melanoma risk. We did not observe any significant interaction between these genetic variants and environmental exposures on the risk of SCC or BCC. \n \n \n \nTable 2 \n \nInteraction between the CAT C-262T genetic polymorphism and history of severe sunburns and total carotenoid intake on melanoma risk \n \n \n \nIn summary, we first observed the GPX1 198 Leu/Leu genotype was significantly associated with a two-fold increased risk of melanoma, and the association between the CAT C-262T polymorphism and melanoma risk was significantly modified by history of severe sunburns and total carotenoid intake. Further research is needed to confirm these possible associations and illustrate the underlying molecular mechanisms.\n\n==\n\nHeinen 2007:\n\nTitle: Intake of antioxidant nutrients and the risk of skin cancer.\n\nAbstract: To investigate the associations between intake of antioxidant nutrients and risk of basal cell (BCC) and squamous cell carcinomas (SCC) of the skin, we carried out a prospective study among 1001 randomly selected adults living in an Australian community. Intake of antioxidants was estimated in 1996. Incident, histologically-confirmed BCC and SCC were recorded between 1996 and 2004. High dietary intake of lutein and zeaxanthin was associated with a reduced incidence of SCC in persons who had a history of skin cancer at baseline (highest versus lowest tertile, multivariable adjusted relative risk (RR)=0.47, 95% confidence interval (CI): 0.25-0.89; P for trend=0.02). In persons without a history of skin cancer at baseline, development of BCC was positively associated with intake of vitamins C and E from foods plus supplements (RR=3.1, 95% CI: 1.1-8.6; P for trend=0.03 and RR=2.6, 95% CI: 1.1-6.3; P for trend=0.02, respectively). In those with a skin cancer history at baseline, dietary intake in the second tertile for beta-carotene (multivariable adjusted RR=2.2, 95% CI: 1.2-4.1) and for vitamin E (multivariable adjusted RR=2.1, 95% CI: 1.1-3.9) was associated with increased BCC risk, with no trend, and similar results were seen in those with a specific history of BCC. These data suggest quite different associations between antioxidant intake and SCC compared with BCC, consistent with other evidence of their different causal pathways.\n\n==\n\nReiners 1991:\n\nTitle: Assessment of the antioxidant/prooxidant status of murine skin following topical treatment with 12-O-tetradecanoylphorbol-13-acetate and throughout the ontogeny of skin cancer. Part I: Quantitation of superoxide dismutase, catalase, glutathione peroxidase and xanthine oxidase.\n\nAbstract: The activities of several enzymes involved in reactive oxygen production and detoxification were quantified in murine skin during the ontogeny of chemically induced skin cancer. Relative to solvent-treated controls, the specific activities of epidermal superoxide dismutase (SOD), catalase (CAT) and glutathione peroxidase (GPX) were reduced approximately 45, approximately 60 and approximately 24% respectively, 24 h after the fourth or tenth topical application of 1 microgram of 12-O-tetradecanoylphorbol-13-acetate (TPA) to the dorsal skin of SENCAR mice. The specific activity of epidermal xanthine oxidase (XO) increased approximately 350% during the same period. SOD and CAT specific activities in papillomas and carcinomas generated in an initiation-promotion protocol were approximately 15 and approximately 40% respectively of the activities measured in age-matched, non-treated mice. CAT and SOD activities were also significantly suppressed in the skin adjacent to the papillomas for several weeks following the cessation of TPA promotion, but eventually recovered to the levels measured in age-matched controls. XO specific activities in papillomas and squamous cell carcinomas (SCC) were approximately 85-350% greater than the activities determined in skin adjacent to the tumors. The increases in XO and the decreases in SOD and CAT activities measured in the tumors were independent of continued treatment with TPA, and thus characteristic of the tumor phenotype. GPX activities in papillomas were comparable to normal, untreated skin, but reduced approximately 22-41% in SCC. Collectively, these studies demonstrate that TPA orchestrates changes in the activities of several enzymes involved in reactive oxygen metabolism that are characteristic of the papilloma and SCC phenotype.\n\n==\n\nFuchs 1989:\n\nTitle: Impairment of enzymic and nonenzymic antioxidants in skin by UVB irradiation.\n\nAbstract: Antioxidants may play a significant role in ameliorating or preventing photobiologic damage in skin that could lead to cutaneous disorders such as cancer and premature aging. The objective of this study was to assess the acute cutaneous enzymic and nonenzymic antioxidant response to a single exposure of large fluence (300 mJ/cm2) ultraviolet radiation (greater than 280 nm) in hairless mice. This treatment caused an immediate and statistically significant inhibition of glutathione reductase and catalase activity. Glutathione peroxidase and superoxide dismutase were not affected. Glutathione levels decreased and, conversely glutathione disulfide concentrations increased. A slight depletion of the total glutathione was observed, while the content of total ascorbic acid did not change. The lipophilic antioxidants alpha-tocopherol, ubiquinol 9 and ubiquinone 9 also decreased significantly, and the concentration of malondialdehyde remained constant. The free radical scavenging activity of epidermis, as assessed by reduction of the stable, cationic nitroxide radical [2,2,6,6-tetramethyl-1-piperidinoxy-4-(2',4',6'-trimethyl) methylpyridinium perchlorate] was considerably inhibited. The study indicates that immediately after exposure to a large fluence of ultraviolet radiation the enzymic and nonenzymic antioxidant capacity of skin decreases significantly.\n\n==\n\nPrabasheela 2011:\n\nTitle: Association between Antioxidant Enzymes and Breast Cancer\n\nAbstract: The exact antioxidant status in breast cancer patient is still not clear. So present study was focused on enzymic antioxidants such as Superoxide dismutase, Catalase, Glutathione-S-transferase, Glutathione peroxidase and Glutathione reductase in the serum of 25 histopathologically proven breast cancer patients. When the data were analyzed with age matched control the antioxidant \u00c2\u00a0 levels were found to decrease indicating enhanced \u00c2\u00a0 free radical activity in breast cancer patients while the antioxidant defense mechanism is weakened. However further elaborate clinical studies are required to evaluate the role of such antioxidant enzymes in breast cancer management.\n\n==\n\nTd 1997:\n\nTitle: Antioxidant enzyme levels in cancer.\n\nAbstract: : Normal cells are protected by antioxidant enzymes from the toxic effects of high concentrations of reactive oxygen species generated during cellular metabolism. Even though cancer cells generate reactive oxygen species, it has been demonstrated biochemically that antioxidant enzyme levels are low in most animal and human cancers. However, a few cancer types have been found to have elevated levels of antioxidant enzymes, particularly manganese superoxide dismutase. Morphologic studies of animal and human cancer have confirmed that although the majority of tumor cell types from several organ systems have low antioxidant enzymes, adenocarcinomas may have elevated manganese superoxide dismutase and catalase levels. However, all cancers examined to date have some imbalance in antioxidant enzyme levels compared with the cell of origin. Antioxidant enzyme importance in cancer genesis has been difficult to evaluate in early cancerous lesions using biochemical techniques because such lesions are small and therefore below the level of detection. Using immunohistochemical techniques, early lesions of human and animal cancers were demonstrated to have low antioxidant enzymes, thus suggesting a role for these enzymes both in the genesis of cancer and the malignant phenotype. All but one human cancer cell type (the granular cell variant of human renal adenocarcinoma) examined showed both low catalase and glutathione peroxidase levels, suggesting that most cancer cell types cannot detoxify hydrogen peroxide. Our results to date are used to propose new cancer therapies based on modulation of cellular redox state.\n\n==\n\nOberley 1997:\n\nTitle: Antioxidant enzyme levels in cancer.\n\nAbstract: Normal cells are protected by antioxidant enzymes from the toxic effects of high concentrations of reactive oxygen species generated during cellular metabolism. Even though cancer cells generate reactive oxygen species, it has been demonstrated biochemically that antioxidant enzyme levels are low in most animal and human cancers. However, a few cancer types have been found to have elevated levels of antioxidant enzymes, particularly manganese superoxide dismutase. Morphologic studies of animal and human cancer have confirmed that although the majority of tumor cell types from several organ systems have low antioxidant enzymes, adenocarcinomas may have elevated manganese superoxide dismutase and catalase levels. However, all cancers examined to date have some imbalance in antioxidant enzyme levels compared with the cell of origin. Antioxidant enzyme importance in cancer genesis has been difficult to evaluate in early cancerous lesions using biochemical techniques because such lesions are small and therefore below the level of detection. Using immunohistochemical techniques, early lesions of human and animal cancers were demonstrated to have low antioxidant enzymes, thus suggesting a role for these enzymes both in the genesis of cancer and the malignant phenotype. All but one human cancer cell type (the granular cell variant of human renal adenocarcinoma) examined showed both low catalase and glutathione peroxidase levels, suggesting that most cancer cell types cannot detoxify hydrogen peroxide. Our results to date are used to propose new cancer therapies based on modulation of cellular redox state.\n\n==\n\nGodic 2014:\n\nTitle: The Role of Antioxidants in Skin Cancer Prevention and Treatment\n\nAbstract: Skin cells are constantly exposed to reactive oxygen species (ROS) and oxidative stress from exogenous and endogenous sources. UV radiation is the most important environmental factor in the development of skin cancer and skin aging. The primary products caused by UV exposure are generally direct DNA oxidation or generation of free radicals which form and decompose extremely quickly but can produce effects that can last for hours, days, or even years. UV-induced generation of ROS in the skin develops oxidative stress when their formation exceeds the antioxidant defense ability. The reduction of oxidative stress can be achieved on two levels: by lowering exposure to UVR and/or by increasing levels of antioxidant defense in order to scavenge ROS. The only endogenous protection of our skin is melanin and enzymatic antioxidants. Melanin, the pigment deposited by melanocytes, is the first line of defense against DNA damage at the surface of the skin, but it cannot totally prevent skin damage. A second category of defense is repair processes, which remove the damaged biomolecules before they can accumulate and before their presence results in altered cell metabolism. Additional UV protection includes avoidance of sun exposure, usage of sunscreens, protective clothes, and antioxidant supplements.",
        "final_summary": "The papers suggest that antioxidant enzymes play a role in skin cancer risk. He 2010 found that genetic variants in antioxidant enzyme genes may interact with lifestyle factors to influence skin cancer risk. Heinen 2007 found that high dietary intake of lutein and zeaxanthin was associated with reduced risk of squamous cell carcinoma (SCC) in individuals with a history of skin cancer, while intake of vitamins C and E was associated with increased risk of basal cell carcinoma (BCC) in those without a history of skin cancer. Reiners 1991 demonstrated changes in antioxidant enzyme activities during the development of chemically induced skin cancer in mice. Fuchs 1989 showed that UVB irradiation caused a decrease in enzymic and nonenzymic antioxidants in the skin of hairless mice. Prabasheela 2011 found decreased antioxidant enzyme levels in the serum of breast cancer patients. Td 1997 and Oberley 1997 both highlighted the low levels of antioxidant enzymes in most animal and human cancers, suggesting a potential role in cancer genesis. Godic 2014 emphasized the importance of antioxidant defense in protecting against skin cancer and the need for both UV protection and increased antioxidant levels."
    },
    {
        "query": "high throughput genetic screen are a powerful tool in identifying individual gene function and interaction",
        "paper_list_string": "Shalem 2015:\n\nTitle: High-throughput functional genomics using CRISPR\u2013Cas9\n\nAbstract: Forward genetic screens are powerful tools for the discovery and functional annotation of genetic elements. Recently, the RNA-guided CRISPR (clustered regularly interspaced short palindromic repeat)-associated Cas9 nuclease has been combined with genome-scale guide RNA libraries for unbiased, phenotypic screening. In this Review, we describe recent advances using Cas9 for genome-scale screens, including knockout approaches that inactivate genomic loci and strategies that modulate transcriptional activity. We discuss practical aspects of screen design, provide comparisons with RNA interference (RNAi) screening, and outline future applications and challenges.\n\n==\n\nKweon 2018:\n\nTitle: High-throughput genetic screens using CRISPR\u2013Cas9 system\n\nAbstract: The CRISPR\u2013Cas9 system is a powerful tool for genome engineering, and its programmability and simplicity have enabled various types of gene manipulation such as gene disruption and transcriptional and epigenetic perturbation. Particularly, CRISPR-based pooled libraries facilitate high-throughput screening for functional regulatory elements in the human genome. In this review, we describe recent advances in CRISPR\u2013Cas9 technology and its use in high-throughput genetic screening. We also discuss its potential for drug target discovery and current challenges of this technique in biomedical research.\n\n==\n\nKelley 2005:\n\nTitle: Systematic interpretation of genetic interactions using protein networks\n\nAbstract: Genetic interaction analysis,in which two mutations have a combined effect not exhibited by either mutation alone, is a powerful and widespread tool for establishing functional linkages between genes. In the yeast Saccharomyces cerevisiae, ongoing screens have generated >4,800 such genetic interaction data. We demonstrate that by combining these data with information on protein-protein, prote in-DNA or metabolic networks, it is possible to uncover physical mechanisms behind many of the observed genetic effects. Using a probabilistic model, we found that 1,922 genetic interactions are significantly associated with either between- or within-pathway explanations encoded in the physical networks, covering \u223c40% of known genetic interactions. These models predict new functions for 343 proteins and suggest that between-pathway explanations are better than within-pathway explanations at interpreting genetic interactions identified in systematic screens. This study provides a road map for how genetic and physical interactions can be integrated to reveal pathway organization and function.\n\n==\n\nMohr 2010:\n\nTitle: Genomic screening with RNAi: results and challenges.\n\nAbstract: RNA interference (RNAi) is an effective tool for genome-scale, high-throughput analysis of gene function. In the past five years, a number of genome-scale RNAi high-throughput screens (HTSs) have been done in both Drosophila and mammalian cultured cells to study diverse biological processes, including signal transduction, cancer biology, and host cell responses to infection. Results from these screens have led to the identification of new components of these processes and, importantly, have also provided insights into the complexity of biological systems, forcing new and innovative approaches to understanding functional networks in cells. Here, we review the main findings that have emerged from RNAi HTS and discuss technical issues that remain to be improved, in particular the verification of RNAi results and validation of their biological relevance. Furthermore, we discuss the importance of multiplexed and integrated experimental data analysis pipelines to RNAi HTS.\n\n==\n\nFriedman 2004:\n\nTitle: Genome-wide high-throughput screens in functional genomics.\n\nAbstract: The availability of complete genome sequences from many organisms has yielded the ability to perform high-throughput, genome-wide screens of gene function. Within the past year, rapid advances have been made towards this goal in many major model systems, including yeast, worms, flies, and mammals. Yeast genome-wide screens have taken advantage of libraries of deletion strains, but RNA-interference has been used in other organisms to knockdown gene function. Examples of recent large-scale functional genetic screens include drug-target identification in yeast, regulators of fat accumulation in worms, growth and viability in flies, and proteasome-mediated degradation in mammalian cells. Within the next five years, such screens are likely to lead to annotation of function of most genes across multiple organisms. Integration of such data with other genomic approaches will extend our understanding of cellular networks.\n\n==\n\nCarpenter 2004:\n\nTitle: Systematic genome-wide screens of gene function\n\nAbstract: By using genome information to create tools for perturbing gene function, it is now possible to undertake systematic genome-wide functional screens that examine the contribution of every gene to a biological process. The directed nature of these experiments contrasts with traditional methods, in which random mutations are induced and the resulting mutants are screened for various phenotypes. The first genome-wide functional screens in Caenorhabditis elegans and Drosophila melanogaster have recently been published, and screens in human cells will soon follow. These high-throughput techniques promise the rapid annotation of genomes with high-quality information about the biological function of each gene.\n\n==\n\nTo 2000:\n\nTitle: Identification of differential gene expression by high throughput analysis.\n\nAbstract: High throughput analysis of differential gene expression is a powerful tool that can be applied to many areas in molecular cell biology, including differentiation, development, physiology, and pharmacology. In recent years, a variety of techniques have been developed to analyze differential gene expression, including comparative expressed sequence tag sequencing, differential display, representational difference analysis, cDNA or oligonucleotide arrays, and serial analysis of gene expression. This review explains the technologies, their scopes, impact on science, as well as their costs and possible limitations. The application of differential display is presented as a tool to identify genes induced by darkness or yellowing process in rice leaves.\n\n==\n\nCipriani 2011:\n\nTitle: RNAi methods and screening: RNAi based high-throughput genetic interaction screening.\n\nAbstract: Expanding on decades of mutational analyses, numerous genome-scale RNAi screens have now been performed in C. elegans, leading to estimates that the majority of genes with essential functions that can be revealed by single-gene perturbations have already been identified in this organism. To build on this basic foundation and uncover condition-dependent or combinatorial effects of non-essential genes will require even higher-scale screening. Here we describe a method for performing high-throughput RNAi-based screens in C. elegans in liquid in 96-well plates, and we explain how to systematically test for enhancement and suppression of temperature-sensitive mutations. This chapter covers our entire set of protocols, from setting up the experiment and screening schedule, to scoring the results. The rapid acquisition of high-quality images of each experiment allows the management of a large number of samples per screening cycle and opens up new possibilities for quantitative scoring, computerized image analysis, and the ability to review results independent of the time constraints that are associated with large-scale screening.",
        "final_summary": "High-throughput genetic screens, particularly those using CRISPR-Cas9, are effective in identifying gene function and interaction (Shalem 2015; Kweon 2018). These screens allow for unbiased phenotypic screening and have potential for drug target discovery (Kweon 2018). The integration of genetic interaction data with protein networks can uncover the physical mechanisms behind genetic effects (Kelley 2005). RNAi high-throughput screens have provided insights into the complexity of biological systems and identified new components (Mohr 2010). Genome-wide screens have the potential to annotate gene function across multiple organisms (Friedman 2004). Systematic genome-wide functional screens offer a directed approach to understanding the contribution of each gene to a biological process (Carpenter 2004). Techniques for analyzing differential gene expression can further contribute to understanding gene function (To 2000). High-throughput RNAi-based screens in C. elegans can identify condition-dependent or combinatorial effects of non-essential genes (Cipriani 2011)."
    },
    {
        "query": "competition public transport accessibility inequality poverty",
        "paper_list_string": "Sun 2021:\n\nTitle: Public transport availability inequalities and transport poverty risk across England\n\nAbstract: The general transit feed specification is becoming a popular data format for the publication of public transport schedules, making possible the collection of a nation-wide public transport schedule dataset, which enables monitoring of transit supply at an up-to-date and more precise level across a country than previously possible. In this paper, we use general transit feed specification data to measure local-scale public transport availability across England based on service frequency and spatial proximity to public transport stops/stations. Moreover, to demonstrate the usefulness of public transport availability measures, we examine inequalities of public transport provision and identify areas at risk of transport poverty across England. Furthermore, we estimate population (number of households) who are likely to suffer from transport poverty, accounting for public transport availability, time-based job accessibility by public transport or walking, household income and car ownership levels. Based on the criteria, we have used to identify public transport risk, we find that investment in the development of public transport services should prioritise West Midlands, East of England, South East and South West as those regions have more households who are likely to suffer from transport poverty. This paper contributes by (1) defining more comprehensive transit availability measures than existing measures at a variety of geography levels and (2) integrating fours aspects (i.e. public transport availability, job accessibility by public transport or walking, household income and car availability) to analyse transport poverty comprehensively.\n\n==\n\nTiznado-Aitken 2021:\n\nTitle: Public transport accessibility accounting for level of service and competition for urban opportunities: An equity analysis for education in Santiago de Chile\n\nAbstract: Abstract Several cities around the world have changed their transportation planning paradigm, understanding that the prime goal is to provide access to opportunities for everyone. To address this goal, public transport plays a fundamental role and, therefore, it is key for developing a sustainable and equitable city. This paper proposes a methodology to analyze access to opportunities through public transport incorporating the user's valuation of attributes that impact the level of service on his/her trip and the competitiveness for urban opportunities. Using data from Santiago, Chile, we applied the proposed methodology to analyze accessibility to higher-quality public primary schools. We compare total travel time (TTT) with a proposed measure of total generalized travel time (TGTT) using simple potential and competitive accessibility indicators, accounting for the subjective valuation of walking time, travel time, waiting time, comfort and transfers, and translating them into in-vehicle time units. We find that the inclusion of competition has a more substantial impact than including the subjective valuation of the level of service in the accessibility to educational opportunities. Using competitive measures with TGTT, we found that around 20% of the zones in Santiago have at least a 50% deficit of higher-quality public education, and 71% of them are in peripheral areas. Furthermore, these zones, where medium and low-income population usually lives, can experience, on average, 1\u20132 transfers, 4\u20135 passengers per square meter, and 15-min waiting. We conclude that the proposed methodology provides a more comprehensive way to understand accessibility by incorporating the traveling experience, allowing to determine how and where to intervene to effectively improve accessibility, with a focus on urban equity.\n\n==\n\nPathak 2017:\n\nTitle: Public transit access and the changing spatial distribution of poverty\n\nAbstract: Abstract This article examines whether access to public transportation plays a significant role in determining the spatial distribution of poverty in a metropolitan area. Our empirical strategy relies on long-term changes in poverty and access to bus transit at the neighborhood level in the Atlanta metropolitan area. We estimate the effect of bus transit access on poverty using fixed effects models to control for time-invariant unobservable characteristics. Furthermore, we undertake several robustness checks using a combination of instrumental variable regression, subsample analysis, and propensity score matching. Our results indicate that, on average, after controlling for neighborhood characteristics, census tracts with better access to public bus transportation have a higher proportion of low-income households \u2013 in both the central city and the suburbs. Thus, policies that improve access to transit in underserved areas can plausibly expand residential opportunities for the poor and reduce spatial inequities in urban centers.\n\n==\n\nMart\u00ednez 2017:\n\nTitle: Creating inequality in accessibility: The relationships between public transport and social housing policy in deprived areas of Santiago de Chile\n\nAbstract: This paper identifies the very limited connectivity provided by the current public transport system to the most deprived groups of Santiago de Chile, and explores the territorial aspects of transport and social housing policies that have contributed to the creation of unequal public transport schemes. To achieve those aims, we present a review of public policies in Chile and the results of an original quantitative analysis that measures the travel times required to access the opportunities and activities located in the city. The results show that housing policies put people at a disadvantage by increasing the distance between them and the opportunities of the city. Three decades after the implementation of housing policies, transport still fails to mitigate these distances and instead of alleviating the patterns of segregation, it may have reinforced them. The travel times required increase towards the periphery (even though densities do not decrease) and are higher than the averages of the city in social housing estates.\n\n==\n\nKelobonye 2020:\n\nTitle: Measuring the accessibility and spatial equity of urban services under competition using the cumulative opportunities measure\n\nAbstract: Abstract As accessibility becomes an increasingly relevant concept in the analysis of sustainable transport and urban development, the accuracy of accessibility measures becomes increasingly vital. While more complex measures are gradually gaining popularity with increasing data and computational resources, policy makers and planners are still prone to opt for less complex methods that are easy to use and interpret. The cumulative opportunities measure is the most widely applied accessibility measure in planning practice, but it is also among the least accurate due to its lack of consideration of the impact of competition for those opportunities. This study seeks to highlight the impact of addressing competition for different urban services in the cumulative opportunities measure. A competition component is added to the measure, which is applied to a case study of three types of urban services in the Perth metropolitan area; jobs, primary/secondary education and shopping. The results show that considering competition changes the spatial patterns of accessibility and its equity. Since this approach reveals demand-supply imbalances, it can more accurately determine spatial inequalities in accessibility, and hence increases the utility of the cumulative opportunities measure. We also find that the three services had varying levels and spatial patterns of accessibility and spatial equity, thus relying on any single one of them for assessing spatial structural performance can be misleading.\n\n==\n\nScheurer 2017:\n\nTitle: Spatial accessibility of public transport in Australian cities: Does it relieve or entrench social and economic inequality?\n\nAbstract: City planning in Australian cities has seen a gradual shift in approach, away from planning to facilitate mobility by car in the post-war period toward planning for land-use/public transport integration. By assessing the supply of public transport for city accessibility, a considerable variation within each city can be seen. Of interest is the extent to which there is a relationship between the quality of public transport accessibility and the spatial distribution of socioeconomic advantage and disadvantage. This paper examines this issue by mapping spatial data on socioeconomic disadvantage and advantage against indicators of public transport accessibility. The findings show that Australian cities are characterized by a significant level of spatially manifested socioeconomic inequality exacerbated by transport disadvantage. It is argued that a coincidence of public transport infrastructure and service improvements as well as urban intensification and housing affordability policies are required to counteract these trends.\n\n==\n\nHern\u00e1ndez 2017:\n\nTitle: Uneven mobilities, uneven opportunities: Social distribution of public transport accessibility to jobs and education in Montevideo\n\nAbstract: Even though mobility is a requirement for participation in \u201cmodern life\u201d, the truth is that mobility is unevenly distributed and it constitutes a field of contestation and dispute among social classes. Mobility does not derive from individual decisions or free choices. On the contrary, it is the result of the interaction between individual attributes and social structure. To grasp this interaction, it is necessary to go well beyond observed mobility. This paper explores the unequal access to urban opportunities among different social classes in Montevideo. It does so by computing potential public transport accessibility to two types of crucial opportunities: jobs and education. The paper findings show an unequal distribution of potential mobility, especially for jobs and upper level public education. Primary public schools are an exception, revealing the spatial footprint of the mature Uruguayan social protection network at this level. This paper's approach allows to identify and describe various fields of contestation such as urban form, transit network, the state provision of public goods (in the case of education), through examining the effects of uneven mobility on social goods access and, as a result, on social equity.\n\n==\n\nMohammadi 2018:\n\nTitle: Optimising public transport for reducing employment barriers and fighting poverty\n\nAbstract: Alleviating poverty in low-income and developing nations is integral to social stability, attracting investments and generating employment opportunities which in turn elevate people\u2019s well-being. Employment could be encouraged through a combination of direct (travel time and training) and indirect measures (social institutions like childcare and elder care). Other factors such as legal reform and infrastructure services could help as well. Transit is perhaps the first element (but not the only one) required to provide better access to the labor market, to health and educational facilities and to social institutions. Transit should be cheap, fast, safe, and secure to reach to most travelers within the influence area. The main objective of this research is to propose a method to fight poverty through better access to employment by a proactive cost-effective planning of investments in existing and future public transit systems. A decision-making system is developed to assess the current employment situation in different geographical regions considering unemployment rate, access to jobs and public transportation systems. Real data from a case study of the Costa Rica metropolitan area is used to illustrate the applicability of the proposed approach. The results show that the proposed model can lead governments to a cost-effective solution that decreases the employment barrier index by more than 50% during the first 5 years. The proposed model will be beneficial for transit agencies in charge of BRT, Tramway, and suburban trains.",
        "final_summary": "Public transport accessibility is linked to inequality and poverty, with certain regions facing higher risks of transport poverty due to inadequate public transport services (Sun 2021). The level of service and competition significantly impact accessibility to urban opportunities such as education (Tiznado-Aitken 2021). Neighborhoods with better access to public transportation tend to have a higher proportion of low-income households, indicating a potential spatial distribution of poverty (Pathak 2017). In Santiago de Chile, the limited connectivity of the public transport system to deprived groups exacerbates spatial inequalities (Mart\u00ednez 2017). The consideration of competition in measuring accessibility and spatial equity of urban services is crucial (Kelobonye 2020). Australian cities show a significant level of spatially manifested socioeconomic inequality, worsened by transport disadvantage (Scheurer 2017). In Montevideo, Uruguay, there is an unequal distribution of public transport accessibility to jobs and education, raising social equity concerns (Hern\u00e1ndez 2017). Optimizing public transport can help reduce employment barriers and combat poverty (Mohammadi 2018)."
    },
    {
        "query": "which classifications are suitable for support vector machines?",
        "paper_list_string": "Mayoraz 1999:\n\nTitle: Support Vector Machines for Multi-class Classification\n\nAbstract: Support vector machines (SVMs) are primarily designed for 2-class classification problems. Although in several papers it is mentioned that the combination of K SVMs can be used to solve a K-class classification problem, such a procedure requires some care. In this paper, the scaling problem of different SVMs is highlighted. Various normalization methods are proposed to cope with this problem and their efficiencies are measured empirically. This simple way of ssing SVMs to learn a K-class classification problem consists in choosing the maximum applied to the outputs of K SVMs solving a one-per-class decomposition of the general problem. In the second part of this paper, more sophisticated techniques are suggested. On the one hand, a stacking of the K SVMs with other classification techniques is proposed. On the other end, the one-per-class decomposition scheme is replaced by more elaborated schemes based on error-correcting codes. An incremental algorithm for the elaboration of pertinent decomposition schemes is mentioned, which exploits the properties of SVMs for an efficient computation.\n\n==\n\nYing 2001:\n\nTitle: The Mechanism of Classification for Support Vector Machines\n\nAbstract: The support vector machine is a novel type of learning technique, based on statistical learning theory, which uses Mercer kernels for efficiently performing computations in high dimensional spaces. In pattern recognition, the support vector algorithm constructs nonlinear decision functions by training a classifier to perform a linear separation in some high dimensional space which is nonlinearly related to input space.\n\n==\n\nAwad 2015:\n\nTitle: Support Vector Machines for Classification\n\nAbstract: This chapter covers details of the support vector machine (SVM) technique, a sparse kernel decision machine that avoids computing posterior probabilities when building its learning model. SVM offers a principled approach to problems because of its mathematical foundation in statistical learning theory. SVM constructs its solution in terms of a subset of the training input. SVM has been extensively used for classification, regression, novelty detection tasks, and feature reduction. This chapter focuses on SVM for supervised classification tasks only, providing SVM formulations for when the input space is linearly separable or linearly nonseparable and when the data are unbalanced, along with examples. The chapter also presents recent improvements to and extensions of the original SVM formulation. A case study concludes the chapter.\n\n==\n\nJunli 2000:\n\nTitle: Classification mechanism of support vector machines\n\nAbstract: The purpose of this paper is to provide an introductory tutorial on the basic ideas behind support vector machines (SVM). The paper starts with an overview of structural risk minimization (SRM) principle, and describes the mechanism of how to construct SVM. For a two-class pattern recognition problem, we discuss in detail the classification mechanism of SVM in three cases of linearly separable, linearly nonseparable and nonlinear. Finally, for nonlinear case, we give a new function mapping technique: By choosing an appropriate kernel function, the SVM can map the low-dimensional input space into the high dimensional feature space, and construct an optimal separating hyperplane with maximum margin in the feature space.\n\n==\n\nDeisenroth 2020:\n\nTitle: Classification with Support Vector Machines\n\nAbstract: Support Vector Machines (SVMs) perform pattern recognition between two points classes by finding a decision surface determined by certain points the training set, termed Support Vectors (SV).\n\n==\n\nAbe 2010:\n\nTitle: Support Vector Machines for Pattern Classification\n\nAbstract: A guide on the use of SVMs in pattern classification, including a rigorous performance comparison of classifiers and regressors. The book presents architectures for multiclass classification and function approximation problems, as well as evaluation criteria for classifiers and regressors. Features: Clarifies the characteristics of two-class SVMs; Discusses kernel methods for improving the generalization ability of neural networks and fuzzy systems; Contains ample illustrations and examples; Includes performance evaluation using publicly available data sets; Examines Mahalanobis kernels, empirical feature space, and the effect of model selection by cross-validation; Covers sparse SVMs, learning using privileged information, semi-supervised learning, multiple classifier systems, and multiple kernel learning; Explores incremental training based batch training and active-set training methods, and decomposition techniques for linear programming SVMs; Discusses variable selection for support vector regressors.\n\n==\n\nGunn 1998:\n\nTitle: Support Vector Machines for Classification and Regression\n\nAbstract: The foundations of Support Vector Machines (SVM) have been developed by Vapnik and are gaining popularity due to many attractive features, and promising empirical performance. The formulation embodies the Structural Risk Minimisation (SRM) principle, which in our work has been shown to be superior to traditional Empirical Risk Minimisation (ERM) principle employed by conventional neural networks. SRM minimises an upper bound on the VC dimension (generalisation error), as opposed to ERM which minimises the error on the training data. It is this difference which equips SVMs with a greater ability to generalise, which is our goal in statistical learning. SVMs were developed to solve the classification problem, but recently they have been extended to the domain of regression problems.\n\n==\n\nHermes 2000:\n\nTitle: Feature selection for support vector machines\n\nAbstract: In the context of support vector machines (SVM), high dimensional input vectors often reduce the computational efficiency and significantly slow down the classification process. In this paper, we propose a strategy to rank individual components according to their influence on the class assignments. This ranking is used to select an appropriate subset of the features. It replaces the original feature set without significant loss in classification accuracy. Often, the generalization ability of the classifier even increases due to the implicit regularization achieved by feature pruning.",
        "final_summary": "Support Vector Machines (SVMs) are suitable for various classifications. Mayoraz 1999 discusses the use of SVMs for K-class classification problems, emphasizing the need for scaling and normalization. Ying 2001 and Junli 2000 both highlight that SVMs construct nonlinear decision functions in high-dimensional spaces, making them suitable for complex classifications. Awad 2015 focuses on SVMs for supervised classification tasks, addressing both linearly separable and nonseparable input spaces, and unbalanced data. Deisenroth 2020 and Abe 2010 discuss SVMs in the context of two-class pattern recognition and pattern classification respectively, emphasizing the role of support vectors and multiclass classification. Gunn 1998 underscores the advantage of SVMs in generalization due to the Structural Risk Minimization principle. Lastly, Hermes 2000 proposes a feature selection strategy to improve computational efficiency in SVMs, indicating their suitability for high-dimensional classifications."
    },
    {
        "query": "tramsport fare prices environmental impacts climate change",
        "paper_list_string": "Cano 2021:\n\nTitle: Sustainable tramway, techno-economic analysis and environmental effects in an urban public transport. A comparative study\n\nAbstract: Abstract The problem of climate change because greenhouse gas emissions is aggravating, especially in public transport, which encourages the development of new technologies and clean energy control methods for the propulsion of vehicles such as tramways. A new energy control for a real tramway has been proposed in this paper, combining renewable sources, supercapacitors and lithium ion batteries, both components will absorb the energy from the regenerative braking of the tramway. The system has been modeled in Matlab considering certain restrictions in each component in order to supply the load on the round trip. Finally, a techno-economic and environmental analysis has been done identifying new patterns with respect to existing tramway systems. The annual energy required by the tramway is 867.62 MWh/year. The power variations are mainly supplied by the supercapacitor and the lithium ion battery functions as a backup. In this regard, the proposed system saves $ 2205,724 by supplying energy to the tramway and selling the excess energy to the grid for 20 years. Finally, the renewable system will have avoided 8,445.4 tCO2/MWh.\n\n==\n\nWang 2019:\n\nTitle: Green Tramp Shipping Routing and Scheduling: Effects of Market-Based Measures on CO2 Reduction\n\nAbstract: In this chapter we examine, from a tramp ship operator\u2019s point of view, how potential CO2 emission reduction measures impact the operational decisions and their economic and environmental consequences. Two market-based measures (MBMs) are discussed, the bunker levy scheme and the emission trading scheme, and we show that both can be incorporated in a similar way into a typical tramp ship routing and scheduling model. We also demonstrate with a computational study the environmental benefits of these CO2 reduction schemes.\n\n==\n\nChoia 2022:\n\nTitle: Calculating the Environmental Benefits of Trams\n\nAbstract: The Net-Zero standard was proposed by Science Based Target (SBTi) to bring the net greenhouse gas emissions to zero as global warming intensifies, and this policy is spreading worldwide. Public transportation plays an important role in eco-friendly transportation and establishing a railway-oriented public transportation system is important. Among modes of railway traffic, trams are easy to access compared with subways (a representative modes of railway transportation) and are economical because of their low construction and operation costs. If a priority signal is given to the tram operation, the scheduled speed increases; The efficiency can be further improved. The purpose of this study was to analyse how the conversion of modes to public transportation caused by tram construction can affect the atmosphere and to study how much the increase in physical activity caused by the increase in public transportation affects the reduction of disease. Dongtan New Town in Korea, where trams are scheduled to be introduced, was set as the study area, and the effect of the conversion of modes of transportation resulting from tram construction was analysed through the modal split process of the four-stage transportation demand prediction model. The analysis shows that trams will generate a 54,700 trips/d conversion to public transportation within the affected area. The benefit from air pollution reduction is 25.13\n\n==\n\nProost 2009:\n\nTitle: Will a radical transport pricing reform jeopardize the ambitious EU climate change objectives\n\nAbstract: This paper examines the effects of replacing current fuel taxes by a system of taxes that account better for all the different external costs of the different transport modes. One of the important implications of this reform is that current fuel taxes are decreased to a level of 80 euro/ton of CO2 but that the mileage related taxes on car and truck use increase. Using the TREMOVE model for the transport sector of 31 European countries, one finds that the volume of transport will decrease because current taxes on transport are too low compared to overall external costs. Overall CO2 emissions will decrease slightly. Using the MARKAL-TIMES model for the Belgian energy sector, putting all sectors and technologies on equal footing shows that a fuel tax reform makes that it is not cost efficient to require large CO2 emission reductions in the transport sector and that traditional car technologies will continue to dominate the car market in 2020-2030.\n\n==\n\nZilka 2021:\n\nTitle: Sustainability evaluation of the use of cargo-trams for mixed municipal waste transport in Prague.\n\nAbstract: The use of trams for the transport of waste from urban areas to waste incineration facilities - does it make sense, would it be technically feasible, economically sustainable and environmentally beneficial? These are the questions which we attempted to answer in this comprehensive study. The analysis is performed for a specific potential implementation of this system in Prague and adjacent municipalities. In this work, we compare the current state, where mixed municipal waste is transported to an incineration plant directly by garbage trucks, with variants if mixed municipal waste were taken to transfer stations and from there transported over a longer distance in a large volume by means of tram or truck. Our results show that use of trams results in an overall cost level of \u20ac16.41 per ton of waste transported, which represents a slight saving against the existing system at a cost of \u20ac17.19 per ton. From the purely economic perspective, however, this does not compete with transportation by truck at \u20ac12.28 per ton - above all due to high initial investment into new cargo trams. From the environmental viewpoint, deploying trams brings benefits largely on the local level, where emissions would be reduced by about 50% against the current state. In the global view, assessed through the Life Cycle Assessment method, it appears however only to transfer emissions to the point of energy production, and from this perspective the use of trams is beneficial only if linked with a notable shift from fossil fuel energy to nuclear or renewable sources.\n\n==\n\nKoetse 2009:\n\nTitle: The impact of climate change and weather on transport: An overview of empirical findings\n\nAbstract: This paper presents a survey of the empirical literature on the effects of climate change and weather conditions on the transport sector. Despite mixed evidence on many issues, several patterns can be observed. On a global scale especially shifts in tourism and agricultural production due to increased temperatures may lead to shifts in passenger and freight transport. The predicted rise in sea levels and the associated increase in frequency and intensity of storm surges and flooding incidences may furthermore be some of the most worrying consequences of climate change, especially for coastal areas. Climate change related shifts in weather patterns might also cause infrastructure disruptions. Clear patterns are that precipitation affects road safety by increasing accident frequency but decreasing severity. Precipitation also increases congestion, especially during peak hours. Furthermore, an increased frequency of low water levels may considerably increase costs of inland waterway transport. Despite these insights, the net impact of climate change on generalised costs of the various transport modes are uncertain and ambiguous, with a possible exception for inland waterway transport.\n\n==\n\nNanaki 2017:\n\nTitle: Environmental assessment of 9 European public bus transportation systems\n\nAbstract: The transportation sector is one of the largest sources of EU\u2019s greenhouse gas emissions. In 2011, transportation represented approximately 25 percent of total EU\u2019s greenhouse gas emissions. Urban mobility accounts for 40% of all CO2 emissions of road transport and up to 70% of other pollutants from transport. As, transportation and mobility play a crucial part both in urban economics and the quality of life, it is of great significance to ensure a low carbon transportation sector, so as to deal with the threat that climate change poses to urban areas. This study examines the factors that affect the production of carbon dioxide (CO2) as well as of air pollutants, in 9 major European cities, aiming to provide a comprehensive overview of the actual knowledge on the atmospheric pollution from public transportation systems. CO2 emissions as well as air pollutants, such as CO, HC, PM, NOx are calculated for the diesel and CNG bus fleets of the European cities under study. Finally the environmental benefits, in terms of CO2 and CO, HC, PM, NOx emissions reductions, from the penetration of different biodiesel blends (from B10 to B100) to the bus fleets are estimated.\n\n==\n\nBorken-Kleefeld 2010:\n\nTitle: Specific climate impact of passenger and freight transport.\n\nAbstract: Emissions of short-lived species contribute significantly to the climate impact of transportation. The magnitude of the effects varies over time for each transport mode. This paper compares first the absolute climate impacts of current passenger and freight transportation. Second, the impacts are normalized with the transport work performed and modes are compared. Calculations are performed for the integrated radiative forcing and mean temperature change, for different time horizons and various measures of transport work. An unambiguous ranking of the specific climate impact can be established for freight transportation, with shipping and rail having lowest and light trucks and air transport having highest specific impact for all cases calculated. Passenger travel with rail, coach or two- and three-wheelers has on average the lowest specific climate impact also on short time horizons. Air travel has the highest specific impact on short-term warming, while on long-term warming car travel has an equal or higher impact per passenger-kilometer.",
        "final_summary": "Tram transport fare prices can have environmental impacts related to climate change. Cano 2021 suggests a sustainable energy control system for trams, reducing CO2 emissions by 8,445.4 tCO2/MWh. Choia 2022 indicates that tram construction can lead to a conversion to public transportation, reducing air pollution by 25.13. Zilka 2021's study on waste transport by trams reveals a potential 50% reduction in local emissions, but also highlights the economic challenge of high initial investment into new cargo trams. Proost 2009, however, raises concerns about the potential impact of radical transport pricing reforms on achieving EU climate change objectives. The impact of tram transport fare prices on climate change is complex and requires further investigation."
    },
    {
        "query": "How does inflammation increase the risk of cardiovascular disease in rheumatoid arthritis?",
        "paper_list_string": "Troelsen 2006:\n\nTitle: [Chronic inflammation increases the risk of cardiovascular disease in patients with rheumatoid arthritis].\n\nAbstract: Rheumatoid arthritis is associated with increased cardiovascular morbidity and mortality due to atherosclerosis. This cannot be explained by an increased presence of traditional risk factors but seems to depend on inflammatory mechanisms. The association of inflammatory pathways with atherosclerosis is complex, and more research is required to optimise preventative measures against cardiovascular complications in inflammatory rheumatic diseases.\n\n==\n\nChoy 2014:\n\nTitle: Cardiovascular risk in rheumatoid arthritis: recent advances in the understanding of the pivotal role of inflammation, risk predictors and the impact of treatment\n\nAbstract: Risk of cardiovascular (CV) disease is increased among RA patients. High inflammatory burden associated with RA appears to be a key driver of the increased cardiovascular risk. Inflammation is linked with accelerated atherosclerosis and associated with a paradoxical inversion of the relationship between CV risk and lipid levels in patients with untreated RA, recently coined the lipid paradox. Furthermore, the inflammatory burden is also associated with qualitative as well as quantitative changes in lipoproteins, with the anti-inflammatory and atheroprotective roles associated with high-density lipoprotein cholesterol significantly altered. RA therapies can increase lipid levels, which may reflect the normalization of lipids due to their inflammatory-dampening effects. However, these confounding influences of inflammation and RA therapies on lipid profiles pose challenges for assessing CV risk in RA patients and interpretation of traditional CV risk scores. In this review we examine the relationship between the increased inflammatory burden in RA and CV risk, exploring how inflammation influences lipid profiles, the impact of RA therapies and strategies for identifying and monitoring CV risk in RA patients aimed at improving CV outcomes.\n\n==\n\nL\u00f3pez-Mej\u00edas 2023:\n\nTitle: The genetic contribution of inflammation to the increased risk of cardiovascular disease in rheumatoid arthritis: comment on the article by Yuan et al\n\nAbstract: To the Editor: We read with great interest the study conducted by Dr. Yuan et al on the genetic contribution of rheumatoid arthritis (RA) in relation to risk of coronary artery disease and stroke (1). The authors showed that in addition to possible involvement of HLA genes, the genetic component that influences inflammation may explain the increased risk of cardiovascular disease observed in RA. In this sense, Yuan et al reported that C-reactive protein (CRP) mediates the association between the risk of RA and coronary artery disease. Although we fully agree that chronic inflammation has a role in the development of cardiovascular disease in RA based on clinical evidence in prospective follow-up studies showing that mean CRP level is associated with both subclinical atherosclerosis (2) and cardiovascular events (3), we were unable to find a role for the genetic polymorphisms that are directly implicated in elevated CRP serum levels in the increased cardiovascular risk found in these patients (4). Our study including the most relevant polymorphisms related to increased serum CRP level did not support an influence of such genetic variations on the risk of cardiovascular disease in patients with RA (4). In particular, 3 tag polymorphisms within the CRP gene, covering all its variability, and 9 variants within the HNF1A, LEPR, GCKR, NLRP3, IL1F10, PPP1R3B, ASCL1, HNF4A, and SALL1 genes were evaluated in this study, and no association of these polymorphisms with the development of subclinical atherosclerosis or cardiovascular events was found in 2,313 Caucasian patients with RA (4). Certainly, RA is a complex polygenic disease, and gene\u2013gene interaction can modulate the inflammatory response, leading to increased expression of proinflammatory cytokines, which in turn, can influence the metabolic changes found in RA (5). These mechanismsmay interact together and explain the greater predisposition to cardiovascular disease in RA. The work of Prof. Gonz alez-Gay is supported by grants from Fondo de Investigaci on Sanitaria (PI06/0024, PS09/00748, PI12/00060, PI15/00525, and PI18/00043), RETICS (RD12/0009 and RD16/0012) (RIER), and RICORS (RD21/0002), and by programs from the Instituto de Salud Carlos III (ISCIII; Health Ministry of Spain) and European Union FEDER. Dr. L opez-Mej\u00edas received fellowship support from the Miguel Servet Type II Program funded by the ISCIII and cofunded by the European Social Fund (\u201cInvesting in your future\u201d) (grant CPII21/00004). Author disclosures are available online at https://onlinelibrary.wiley. com/doi/10.1002/art.42468.\n\n==\n\nSattar 2003:\n\nTitle: Explaining how \"high-grade\" systemic inflammation accelerates vascular risk in rheumatoid arthritis.\n\nAbstract: There is intense interest in mechanisms whereby low-grade inflammation could interact with conventional and novel vascular risk factors to promote the atheromatous lesion. Patients with rheumatoid arthritis (RA), who by definition manifest persistent high levels of inflammation, are at greater risk of developing cardiovascular disease. Mechanisms mediating this enhanced risk are ill defined. On the basis of available evidence, we argue here that the systemic inflammatory response in RA is critical to accelerated atherogenesis operating via accentuation of established and novel risk factor pathways. By implication, long-term suppression of the systemic inflammatory response in RA should be effective in reducing risk of coronary heart disease. Early epidemiological observational and clinical studies are commensurate with this hypothesis. By contrast, risk factor modulation with conventional agents, such as statins, may provide unpredictable clinical benefit in the context of uncontrolled systemic inflammatory parameters. Unraveling such complex relationships in which exaggerated inflammation-risk factor interactions are prevalent may elicit novel insights to effector mechanisms in vascular disease generally.\n\n==\n\nStevens 2005:\n\nTitle: Inflammation and atherosclerosis in rheumatoid arthritis\n\nAbstract: Rheumatoid arthritis (RA) associates with increased cardiovascular mortality. This appears to be predominantly due to ischaemic causes, such as myocardial infarction and congestive heart failure. The higher prevalence of cardiac ischaemia in RA is thought to be due to the accelerated development of atherosclerosis. There are two main reasons for this, which might be inter-related: the systemic inflammatory load, characteristic of RA; and the accumulation in RA of classical risk factors for coronary heart disease, which is reminiscent of the metabolic syndrome. We describe and discuss in the context of RA the involvement of local and systemic inflammatory processes in the development and rupture of atherosclerotic plaques, as well as the role of individual risk factors for coronary heart disease. We also present the challenges facing the clinical and scientific communities addressing this problem, which is receiving increasing attention.\n\n==\n\nNurmohamed 2009:\n\nTitle: Cardiovascular risk in rheumatoid arthritis.\n\nAbstract: The increased mortality in rheumatoid arthritis (RA) is mainly due to (atherosclerotic) cardiovascular disease. The cardiovascular morbidity is also increased in comparison with the general population. This increased cardiovascular burden could be caused by 1) an enhanced prevalence of cardiovascular risk factors 2) under treatment of cardiovascular risk factors or 3) RA itself, particularly due to its chronic inflammatory component. Cardiovascular risk factors only partially explain the increased cardiovascular risk and it is becoming increasingly acknowledged that the underlying inflammation in RA plays an essential role. This is probably related to the fact that atherosclerosis also has an inflammatory etiology that is accelerated by RA. Similarly, it can be expected that effective suppression of this inflammatory process by disease modifying antirheumatic drugs and/or biologicals lowers the cardiovascular risk. Altogether, there is accumulating evidence that the increased cardiovascular risk in RA is comparable to that of type 2 diabetes and actually RA should be seen as a new, independent, cardiovascular risk factor for which cardiovascular risk management is essential.\n\n==\n\nKitas 2010:\n\nTitle: Cardiovascular disease in rheumatoid arthritis: state of the art and future perspectives\n\nAbstract: Rheumatoid arthritis is associated with an increased risk for cardiovascular events, such as myocardial infarction and stroke. Epidemiological evidence suggests that classic cardiovascular risk factors, such as hypertension, dyslipidaemia, insulin resistance and body composition alterations are important but not sufficient to explain all of the excess risk. High-grade systemic inflammation and its interplay with classic risk factors may also contribute. Some associations between classic risk factors and cardiovascular risk in people with rheumatoid arthritis appear counterintuitive but may be explained on the basis of biological alterations. More research is necessary to uncover the exact mechanisms responsible for this phenomenon, develop accurate systems used to identify patients at high risk, design and assess prevention strategies specific to this population of patients.\n\n==\n\nMyasoedova 2011:\n\nTitle: Lipid paradox in rheumatoid arthritis: the impact of serum lipid measures and systemic inflammation on the risk of cardiovascular disease\n\nAbstract: Objective To examine the impact of systemic inflammation and serum lipids on cardiovascular disease (CVD) in rheumatoid arthritis (RA). Methods In a population-based RA incident cohort (1987 American College of Rheumatology criteria first met between 1988 and 2007), details were collected of serum lipid measures, erythrocyte sedimentation rates (ESRs), C-reactive protein (CRP) measures and cardiovascular events, including ischaemic heart disease and heart failure. Cox models were used to examine the association of lipids and inflammation with the risk of CVD and mortality, adjusting for age, sex and year of RA incidence. Results The study included 651 patients with RA (mean age 55.8 years, 69% female); 67% were rheumatoid factor positive. ESR was associated with the risk of CVD (HR=1.2 per 10 mm/h increase, 95% CI 1.1 to 1.3). Similar findings, although not statistically significant, were seen with CRP (p=0.07). A significant non-linear association for total cholesterol (TCh) with risk of CVD was found, with 3.3-fold increased risk for TCh <4 mmol/l (95% CI 1.5 to 7.2) and no increased risk of CVD for TCh \u22654 mmol/l (p=0.57). Low low-density lipoprotein cholesterol (LDL <2 mmol/l) was associated with marginally increased risk of CVD (p=0.10); there was no increased risk for LDL \u22652 mmol/l (p=0.76). Conclusion Inflammatory measures (particularly, ESR) are significantly associated with the risk of CVD in RA. Lipids may have paradoxical associations with the risk of CVD in RA, whereby lower TCh and LDL levels are associated with increased cardiovascular risk.",
        "final_summary": "The papers suggest that chronic inflammation plays a crucial role in increasing the risk of cardiovascular disease in patients with rheumatoid arthritis (RA). Troelsen 2006 highlights that inflammatory mechanisms, rather than traditional risk factors, contribute to the elevated cardiovascular morbidity and mortality in RA. Choy 2014 emphasizes the link between inflammation and accelerated atherosclerosis, as well as the impact of RA therapies on lipid profiles. Sattar 2003 argues that the systemic inflammatory response in RA accentuates established and novel risk factor pathways, supporting the notion that long-term suppression of inflammation could reduce coronary heart disease risk. Additionally, Stevens 2005 discusses the involvement of local and systemic inflammation in atherosclerotic plaque development and rupture. Nurmohamed 2009 suggests that RA itself, particularly its chronic inflammatory component, contributes to the increased cardiovascular risk, comparable to type 2 diabetes. Kitas 2010 acknowledges the role of classic risk factors but highlights the need for further research to uncover the exact mechanisms and develop tailored prevention strategies. Myasoedova 2011 adds to the complexity by noting a lipid paradox in RA, where lower total cholesterol and LDL levels may be associated with increased cardiovascular risk. Overall, chronic inflammation in RA appears to be a key factor in promoting cardiovascular disease, necessitating comprehensive management of both inflammation and traditional risk factors. (Troelsen 2006; Choy 2014; Sattar 2003; Stevens 2005; Nurmohamed 2009; Kitas 2010; Myasoedova 2011)"
    },
    {
        "query": "In project management research, time horizon is to the duration of a study's data collection period or the length of time over which data is analyzed (Wong et al., 2021). Different time horizons can be employed when studying employee engagement and organizational performance.",
        "paper_list_string": "Ancona 2001:\n\nTitle: Time: A New Research Lens\n\nAbstract: The article discusses the issue of time as it pertains to organizational research. The author believes that looking at research in terms of time is a powerful tool in assessing organizational phenomena. According to the author, temporal research allows researchers to gain more perspective when looking at organizational issues such as decision making, group performance and organizational transformation. The author notes that the field of temporal research is translated into concepts including pacing, timing and sequencing.\n\n==\n\nSonnentag 2012:\n\nTitle: Time in organizational research: Catching up on a long neglected topic in order to improve theory\n\nAbstract: Time should be an important aspect of organizational theory, but it has been neglected for decades. In this essay, I describe four different ways in which time can be conceptualized in organizational research. I give examples from recent theoretical work that include a temporal perspective. I also specify some directions about how future theory development and refinement should incorporate time.\n\n==\n\nBasu 2019:\n\nTitle: Choosing a Time Horizon in Cost and Cost-effectiveness Analyses.\n\nAbstract: When designing a comparative outcomes or a cost-effectiveness analysis, the time horizon defining the duration of time for outcomes assessment must be carefully considered. The time horizon must be long enough to capture the intended and unintended benefits and harms of the intervention(s).1,2 In some instances, the time horizon should extend beyond the duration of a clinical trial when a specific end point is measured, whereas in other instances modeling outcomes over a longer period is unnecessary. Using a longer time horizon than is necessary may add unnecessary cost and complexity to the cost-effectiveness analysis model. In the May 2017 issue of JAMA Ophthalmology, Wittenborn et al3 examined costs and effectiveness of home-based macular degeneration monitoring systems using a lifetime horizon in a cost-effectiveness analysis and a 10-year horizon in a budget impact analysis. The rationale for selection of time horizons and their implications for interpreting the research is reviewed in this JAMA Guide to Statistics and Methods article.\n\n==\n\nBergh 1993:\n\nTitle: Watch the Time Carefully: The Use and Misuse of Time Effects in Management Research\n\nAbstract: This article assesses how management researchers have used, and in some cases misused, time effects in their research designs, data analyses, and interpretations. Drawing from a content analysis of studies appearing in the Academy of Management Journal and Administrative Science Quarterly over the years 1988-1992, five different approaches for defining and operationalizing time effects are identified. Evaluation within and across these five approaches reveals three recurring patterns of \u201ctime series errors. \u201d The implications of these \u201cerrors \u201cfor studies with, and on time are provided.\n\n==\n\nRuneson 2014:\n\nTitle: Get the cogs in synch: time horizon aspects of industry--academia collaboration\n\nAbstract: In industry--academia collaboration projects, there are many issues related to different time horizons in industry and academia. If not adressed upfront, they may hinder collaboration in such projects. We analyze our experiences from a 10 year industry--academia collaboration program, the EASE Industrial Excellence Center in Sweden, and identify issues and feasible practices to overcome the hurdles of different time horizons. Specifically, we identify issues related to contracts, goals, results, organization (in)stability, and work practices. We identify several areas where the time horizon is different, and conclude that mutual awareness of these differences and management commitment to the collaboration are the key means to overcome the differences. The launch of a mediating institute may also be part of the solution.\n\n==\n\nLee 1999:\n\nTitle: Time in Organizational Studies: Towards a New Research Direction\n\nAbstract: While there is much `time-related research', there is little `research on time'. This is striking since time is a key point in understanding organizations, their actions, culture, efficacy, etc. Most studies of time in management and organizational theory take time for granted. While there are numerous studies that address temporal issues, they are widely dispersed and unsystematic. This paper provides a classification of temporal studies of organizations and management. The scheme is built around two criteria: concepts of time and the role of time in research design. In the former, there are two contrasting concepts of time: clock time and social time. In the latter, time plays the roles of independent or dependent variables. By intersecting the two criteria, four notions of temporality (`deciding', `working', `varying' and `changing' times) are introduced to account for a variety of studies of time. The resulting classification not only reveals the current situation of studies about time, but it also indicates a direction which further research effort should take. We conclude by showing that temporally sensitive approaches will benefit research on organizations.\n\n==\n\nReilly 2016:\n\nTitle: Time Horizon of Investments in the Resource Allocation Process\n\nAbstract: Corporate investment inherently relies on time horizon, as profits result from acquiring assets or developing capabilities that yield future benefits that exceed upfront costs. Despite the importance of time horizon to understanding resource allocation, knowledge about the subject has accumulated slowly. Our review therefore encompasses insights from several research streams that partially address the subject even though time horizon is not the central construct in any of them. We aim to clarify key constructs related to time horizon, organize prior research about the antecedents of time horizon, explain the implications of several theoretical traditions for time horizon, and detail the range of measures that have been used to capture time horizon empirically. By focusing narrowly on this topic but searching broadly for references, we provide integrative summaries of existing research and identify opportunities for new and unique research.\n\n==\n\nStritch 2017:\n\nTitle: Minding the Time: A Critical Look at Longitudinal Design and Data Analysis in Quantitative Public Management Research\n\nAbstract: Public management scholars are looking to longitudinal research designs and data to help overcome the many limitations associated with cross-sectional research. However, far less attention has been given to time itself as a research lens for scholars to consider. This article seeks to integrate time as a construct of theoretical importance into a discussion of longitudinal design, data, and public management research. First, I discuss the relative advantages of longitudinal design and data, but also the challenges, limitations, and issues researchers need to consider. Second, I consider the importance of time as a theoretical construct of interest in the pursuit of longitudinal public management research. Third, I offer a brief look at the use of longitudinal design and panel data analyses in the current public management literature. The overview demonstrates a notable absence of public management research considering the attitudes, motives, perceptions, and experiences of individual public employees and managers. Finally, I consider why there are so few longitudinal studies of public employees and point out the issues public management researchers interested in individual employee-level phenomena need to consider when advancing their own longitudinal research designs.",
        "final_summary": "The papers suggest that time horizon is an important consideration in project management research, particularly when studying employee engagement and organizational performance. Ancona (2001) emphasizes the power of temporal research in assessing organizational phenomena such as decision making and transformation. Sonnentag (2012) highlights the need to incorporate time into organizational theory and provides examples of recent work that includes a temporal perspective. Basu (2019) discusses the careful selection of a time horizon in cost and cost-effectiveness analyses, considering the duration necessary to capture intended outcomes. Bergh (1993) examines the use and misuse of time effects in management research, identifying different approaches and recurring errors. Runeson (2014) explores the challenges of different time horizons in industry-academia collaboration projects and suggests mutual awareness and management commitment as key means to overcome differences. Lee (1999) calls for more systematic research on time in organizational studies, providing a classification of temporal studies and suggesting temporally sensitive approaches. Reilly (2016) reviews the importance of time horizon in corporate investments and offers insights from various research streams. Stritch (2017) discusses the integration of time as a theoretical construct in longitudinal public management research, highlighting the advantages, challenges, and limitations of longitudinal design and data. Overall, these papers emphasize the significance of considering time horizon in project management research and its impact on understanding organizational phenomena, decision making, collaboration, and resource allocation."
    },
    {
        "query": "What is the codex alimentarius recommendation for second-age infant flour?",
        "paper_list_string": "Koletzko 2012:\n\nTitle: Compositional Requirements of Follow-Up Formula for Use in Infancy: Recommendations of an International Expert Group Coordinated by the Early Nutrition Academy\n\nAbstract: The follow-up formula (FUF) standard of Codex Alimentarius adopted in 1987 does not correspond to the recently updated Codex infant formula (IF) standard and current scientific knowledge. New Zealand proposed a revision of the FUF Codex standard and asked the non-profit Early Nutrition Academy, in collaboration with the Federation of International Societies for Paediatric Gastroenterology, Hepatology, and Nutrition (FISPGHAN), for a consultation with paediatric nutrition experts to provide scientific guidance. This global expert group strongly supports breastfeeding. FUF are considered dispensable because IF can substitute for breastfeeding throughout infancy, but FUF are widely used and thus the outdated current FUF standard should be revised. Like IF, FUF serve as breast milk substitutes; hence their marketing should respect appropriate standards. The compositional requirements for FUF for infants from 6 months onwards presented here were unanimously agreed upon. For some nutrients, the compositional requirements for FUF differ from those of IF due to differing needs with infant maturation as well as a rising contribution of an increasingly diversified diet with advancing age. FUF should be fed with adequate complementary feeding that is also appropriate for partially breastfed infants. FUF could be fed also after the age of 1 year without safety concerns, but different compositional requirements should be applied for optimal, age-adapted milk-based formulations for young children used only after the age of 1 year. This has not been considered as part of this review and should be the subject of further consideration.\n\n==\n\nKoletzko 2010:\n\nTitle: Global standard for the composition of infant formula: recommendations of an ESPGHAN coordinated international expert group.\n\nAbstract: The Codex Alimentarius Commission of the Food and Agriculture Organization of the United Nations (FAO) and the World Health Organization (WHO) develops food standards, guidelines and related texts for protecting consumer health and ensuring fair trade practices globally. The major part of the world's population lives in more than 160 countries that are members of the Codex Alimentarius. The Codex Standard on Infant Formula was adopted in 1981 based on scientific knowledge available in the 1970s and is currently being revised. As part of this process, the Codex Committee on Nutrition and Foods for Special Dietary Uses asked the ESPGHAN Committee on Nutrition to initiate a consultation process with the international scientific community to provide a proposal on nutrient levels in infant formulae, based on scientific analysis and taking into account existing scientific reports on the subject. ESPGHAN accepted the request and, in collaboration with its sister societies in the Federation of International Societies on Pediatric Gastroenterology, Hepatology and Nutrition, invited highly qualified experts in the area of infant nutrition to form an International Expert Group (IEG) to review the issues raised. The group arrived at recommendations on the compositional requirements for a global infant formula standard which are reported here.\n\n==\n\nZlotkin 2010:\n\nTitle: The Role of the Codex Alimentarius Process in Support of New Products to Enhance the Nutritional Health of Infants and Young Children\n\nAbstract: The Codex Alimentarius is a collection of internationally recognized standards, codes of practice, guidelines, and other recommendations relating to foods, food production, and food safety. Among other functions, it is responsible for setting international standards for safety and hygiene. Codex food standards and guidelines directed at foods produced primarily for young infants and children have important implications for maintaining nutritional status and health, especially given the positioning of these products as components of established World Health Organization (WHO)/UNICEF-recommended feeding strategies. Recently, new products targeted at this age group (e.g., lipid-based nutrient supplements and micronutrient powders) have been produced and used, but these are not totally covered under existing Codex guidelines or standards. The objective of this paper is to review the role of the Codex process and specifically to suggest revisions to existing Codex guidelines on formulated complementary foods (Guidelines for Formulated Supplementary Foods for Older Infants and Young Children, CAC/GL 08\u20131991) to encompass this new category of fortified complementary foods and home fortificants. In reviewing the existing guidelines, potential areas for revision included the sections on the recommended nutrients in these foods and their intended use. Updating the Codex guidelines provides the opportunity to encourage production and use of new products for children and help ensure that such foods, when used as directed, do not interfere with breastfeeding. The revised guidelines would help governments develop national regulations covering all forms of formulated complementary foods. They would also lessen impediments to international trade by providing clear guidance for foods used in feeding programs and for young children, particularly in developing countries.\n\n==\n\nDossa 2011:\n\nTitle: Evaluation of the suitability and acceptability of a newly designed infant flour for infant feeding in the district of Bopa in south of Benin\n\nAbstract: Infant feeding practices do not always fit with quantity and quality requirements,\u00a0leading to low expression of growth potential. In Benin, 43.1% of children less than6\u00a0months old are exclusively breastfed with 68% of children aged 6-8 months receiving\u00a0complementary food. The study aimed to produce infant flour from raw food\u00a0ingredients available in Bopa district and to test its acceptability by 6-12 months old children. In a first step of the study, formulation and determination of nutritional\u00a0characteristics of the infant flour occurred. A second step concerned acceptability\u00a0tests of gruel made from formulated infant flour. The study sample was composed of\u00a0sixty five mothers and their children. Children\ufffds acceptability test took place in the\u00a0morning for three consecutive days. The gruel was consumed ad libitum. Mothers\ufffd acceptability test consisted of appreciation of organoleptic characteristics of the gruel\u00a0and the infant flour processing.The infant flour was made of maize (65 %), bean (20\u00a0%) and peanut (15 %) and was manually processed. Chemical analysesshowed that it\u00a0contains 4.3% of moisture, 69.3% of carbohydrates, 15.1% of proteins, 10.7% of\u00a0lipids, less than 5% of crude fibres and 1.9% of ash. Its energy density (433.9 kcal/100g) was significantly greater than Codex Alimentarius standards (p<0.05).The\u00a0infant flour contained microbial germs up to 4.8log CFU/g which was closed to\u00a0maximum standard values. Total coliforms (1.7log CFU/g) were significantly lower\u00a0than standard values. The flour was yeast, mould and pathogen (Escherichia coli)\u00a0free. Hundred grams gruel was made from 40g of infant flour, 6g of malted maize and\u00a0250ml of stock of boiled greens leaves (Solanummacrocarpum). Dry matter content of\u00a0gruel was 19.2% and its energy density was 81.5 kcal/100g. Basedon the ratio of\u00a0intake and amount served, 83.3% of children accepted the gruel. However based on\u00a0the ratio of the amount of porridge consumed during the testto the amount usually\u00a0consumed by the children, 65.2% of the childrenaccepted the gruel. Mothers\ufffd appreciation of the gruel ranged from unpleasant to very pleasant with 40% as\u00a0pleasant. Sixty percent of mothers judged the infant flour processing as easy and\u00a0feasible. All mothers expressed their desire to feed their children with the gruel.\u00a0Improving nutritional status of their children motivated their decision. It is concluded\u00a0that integrating this infant flour in nutrition and counselling package targeted to\u00a0mothers may be of a great benefit to the children.\n\n==\n\nMacLean 2010:\n\nTitle: Upper levels of nutrients in infant formulas: Comparison of analytical data with the revised Codex infant formula standard\n\nAbstract: Abstract The Codex Alimentarius Commission adopted a revised standard for infant formula in 2007. This standard provides a regulatory framework for infant formula, including provisions for its essential composition. The recommendations for the essential composition specify minimum levels and either maximum values (MVs) or guidance upper levels (GULs) for 31 nutrients. As part of the revision process, the first cooperative survey of levels of nutrients in infant formulas was conducted by several global manufacturers. Whereas formulas met proposed minimum levels of all nutrients, 15 nutrients were identified whose levels were likely to exceed the proposed MV or GUL: vitamins A and K, thiamine, riboflavin, niacin, vitamin B 6 , folic acid, vitamin B 12 , vitamin C, iron, copper, manganese, potassium and iodine. Analytical data were collected for those nutrients from 21,385 batches of milk-based infant formula and 9070 batches of soy-based infant formula, whose total volumes were sufficient to feed more than 33 million infants for periods of three months. The number of batches analyzed ranged from 440 (vitamin K) to 27,920 (vitamin C). Of nutrients with an MV, only levels of vitamin A in some batches exceeded the maximum; no batch contained levels previously reported in the literature to be associated with adverse effects. There were several nutrients with GULs for which there were batches that exceeded the suggested upper limit. Data for some nutrients showed considerable variability, which related to form (liquid vs. powder), inherent levels of nutrients in formula ingredients, protein source, nutrient stability, analytical variability and effects of process, package and container size.\n\n==\n\nFao 2003:\n\nTitle: Codex alimentarius: food hygiene basic texts.\n\nAbstract: This publication contains guidance on the development and application of international food hygiene standards, which covers practices from primary production through to final consumption, highlighting key hygiene controls at each stage. It also contains guidance on the use and application of the Hazard Analysis and Critical Control Point (HACCP) system to promote food safety, as well as principles for the establishment and application of microbiological criteria for foods and the conduct of microbiological assessment.\n\n==\n\nMiz\u00e9houn-Adissoda 2022:\n\nTitle: Household production and energy content of infant flours for children aged 6 to 11 months in two rural settings in southern Benin\n\nAbstract: Background: Homemade complementary foods which are prepared from staples by the mothers or caregivers are mostly used in Africa including Benin. These foods are not adequately enriched and hygienic conditions are sometimes poor. Aims: The aim was to describe household production methods of infant flours and to estimate their macronutrient content in Benin. Methods: From August to October 2020, 20 mothers of children aged 6 to 11 months were selected in the municipalities of Cov\u00e8 and Djakotomey to examine the process of domestic production of infant flours and porridges. Data were collected using a touch screen questionnaire and the energy content of the flours produced by the mothers was calculated and evaluated using the FAO/INFOODS food composition table for West Africa. Results: The main process units for the production of the infant flours were milling:100%, sorting: cereals 60% and legumes: 33%, roasting: cereals 70%, and legumes: 91.7%. Mothers had poor hygienic practices regarding Material: 20%, Manipulator: 20%, and Milieu: 35% during the preparation of infant porridges. Four of the 20 formulas produced had an acceptable energy content, two had a standard fat content (10 - 25 g/100 g flour) and 12 had a standard protein content (15 g/100 g flour). Conclusions: This study showed that the main production methods of infant flours need to be improved. More, the overall energy content from these flours was lower than the recommended macronutrients contents by the standard FAO/WHO/UNICEF and the standards references. It is therefore important to maintain and strengthen nutritional interventions in order to improve the nutritional quality of homemade infant flours.\nKeywords: Infant flour, household production, nutrition, hygiene.\n\n==\n\nMikiel-Kostyra 2000:\n\nTitle: [Foodstuffs intended for special dietary uses for infants and young children in the light of the directives of the European Union and standards of the Codex Alimentarius FAO/WHO].\n\nAbstract: Food products for infants and young children are included in the group of foodstuffs intended for special dietary uses. The classification of these products comprises the following groups: newborns infant formulas, infant formulas, processed cereal foods and other baby foods for infants and young children and gluten-free products. In the European Union countries the detailed requirements concerning their qualitative and quantitative composition, labeling, advertising and release on the market as well as the health quality of these products have been laid down in the Council Directive of 3 May 1989 on the approximation of the laws of the Member States relating to foodstuffs intended for particular nutritional uses (89/398/EEC with later amendments) and in the detailed directives. Presently, work is being continued on preparing standards of the Codex Alimentarius FAO/WHO for infant formulas for processed cereal foods for infants and young children and for gluten-free foods. In the successive sessions of the Codex Committee on Nutrition and Foods for Special Dietary Uses drafts of these standards are discussed. In the last session of the Committee in Berlin (19-23.06.2000) it was not possible to reach an agreement of views of the various delegations on the standards or their formulation. The work on them and trials at establishing international recommendations has been deferred to the next meeting.",
        "final_summary": "The Codex Alimentarius does not currently have a specific recommendation for second-age infant flour. The papers reviewed discuss the need for revisions to the Codex standards for infant formula and follow-up formula (FUF). Koletzko 2012 suggests that the current FUF standard is outdated and should be revised to align with the updated Codex infant formula standard. The compositional requirements for FUF differ from those of infant formula due to the changing needs of infants as they mature and introduce complementary foods. However, the specific recommendation for second-age infant flour is not addressed in these papers."
    },
    {
        "query": "what are the effects of resistance exercise on the skeletal muscle proteome?",
        "paper_list_string": "Son 2012:\n\nTitle: THE EFFECT OF RESISTANCE AND ENDURANCE EXERCISE TRAINING ON MUSCLE PROTEOME EXPRESSION IN HUMAN SKELETAL MUSCLE\n\nAbstract: To investigate the effect of resistance and endurance training on muscle proteome expression, samples of vastus lateralis from 10 physically active young men were analysed by 2-dimensional electrophoresis (2-DE) and matrix-assisted laser desorption ionization time-of-flight mass spectrometry (MALDI-TOF MS). Differential patterns of protein expression were determined after 4 weeks of endurance or resistance exercise training. Following endurance exercise training, carbonic anhydrase III immunoglobulin heavy chain, myosin heavy chain 1, titin, chromosome 12, and fructose-1,6-bisphosphatase 2 were up-regulated while pyruvate kinase 3 isoform, ubiquitin carboxyl-terminal hydrolase, and phosphoglucomutase were down-regulated. After the 4 weeks of resistance exercise training, five proteins, apolipoprotein A-IV precursor, microtubule-actin cross linking factor 1, myosin light chain, growth hormone inducible transmembrane protein, and an unknown protein were up-regulated and pyruvate kinase 3 isoform, human albumin, and enolase 3 were down-regulated. We conclude that endurance and resistance exercise training differently alter the expression of individual muscle proteins, and that the response of muscle protein expression may be associated with specific myofibre adaptations to exercise training. Proteomic studies represent one of the developing techniques of metabolism which may substantially contribute to new insights into muscle and exercise physiology.\n\n==\n\nPetriz 2017:\n\nTitle: The Effects of Acute and Chronic Exercise on Skeletal Muscle Proteome\n\nAbstract: Skeletal muscle plasticity and its adaptation to exercise is a topic that is widely discussed and investigated due to its primary role in the field of exercise performance and health promotion. Repetitive muscle contraction through exercise stimuli leads to improved cardiovascular output and the regulation of endothelial dysfunction and metabolic disorders such as insulin resistance and obesity. Considerable improvements in proteomic tools and data analysis have broth some new perspectives in the study of the molecular mechanisms underlying skeletal muscle adaptation in response to physical activity. In this sense, this review updates the main relevant studies concerning muscle proteome adaptation to acute and chronic exercise, from aerobic to resistance training, as well as the proteomic profile of natural inbred high running capacity animal models. Also, some promising prospects in the muscle secretome field are presented, in order to better understand the role of physical activity in the release of extracellular microvesicles and myokines activity. Thus, the present review aims to update the fast\u2010growing exercise\u2010proteomic scenario, leading to some new perspectives about the molecular events under skeletal muscle plasticity in response to physical activity. J. Cell. Physiol. 232: 257\u2013269, 2017. \u00a9 2016 Wiley Periodicals, Inc.\n\n==\n\nPadr\u00e3o 2016:\n\nTitle: Uncovering the exercise\u2010related proteome signature in skeletal muscle\n\nAbstract: Exercise training has been recommended as a nonpharmacological strategy for the prevention and attenuation of skeletal muscle atrophy in distinct pathophysiological conditions. Despite the well\u2010established phenotypic alterations, the molecular mechanisms underlying exercise\u2010induced skeletal muscle remodeling are poorly characterized. Proteomics based on mass spectrometry have been successfully applied for the characterization of skeletal muscle proteome, representing a pivotal approach for the wide characterization of the molecular networks that lead to skeletal muscle remodeling. Nevertheless, few studies were performed to characterize the exercise\u2010induced proteome remodeling of skeletal muscle, with only six research papers focused on the cross\u2010talk between exercise and pathophysiological conditions. In order to add new insights on the impact of distinct exercise programs on skeletal muscle proteome, molecular network analysis was performed with bioinformatics tools. This analysis highlighted an exercise\u2010related proteome signature characterized by the up\u2010regulation of the capacity for ATP generation, oxygen delivery, antioxidant capacity and regulation of mitochondrial protein synthesis. Chronic endurance training up\u2010regulates the tricarboxylic acid cycle and oxidative phosphorylation system, whereas the release of calcium ion into cytosol and amino acid metabolism are the biological processes up\u2010regulated by a single bout of exercise. Other issues as exercise intensity, load, mode and regimen as well as muscle type also influence the exercise\u2010induced proteome signature. The comprehensive analysis of the molecular networks modulated by exercise training in health and disease, taking in consideration all these variables, might not only support the therapeutic effect of exercise but also highlight novel targets for the development of enhanced pharmacological strategies.\n\n==\n\nBuford 2009:\n\nTitle: Resistance exercise-induced changes of inflammatory gene expression within human skeletal muscle\n\nAbstract: Aberrant local inflammatory signaling within skeletal muscle is now considered a contributing factor to the development of sarcopenia. Recent evidence indicates that chronic resistance training contributes to the control of locally derived inflammation via adaptations to repeated, acute increases in pro-inflammatory mRNA within muscle. However, only a limited number of gene transcripts related to the inflammatory process have been examined in the literature. The present study utilized an acute bout to examine the effects of resistance exercise on several inflammatory-related genes in 24 physically active, post-menopausal women not currently undergoing hormone replacement therapy. Following a standard warm-up, participants completed a lower-body resistance exercise bout consisting of 3 sets of 10 repetitions on machine squat, leg press, and leg extension exercises (80% intensity). Muscle biopsies were obtained from the vastus lateralis of the dominant leg at baseline and 3\u00a0h following exercise. Significant (p\u00a0<\u00a00.05) up-regulation in mRNA content was observed for TNF\u03b1, IL1\u03b2, IL6, IL8, SOCS2, COX2, SAA1, SAA2, IKKB, cfos, and junB. Muscle mRNA content was not significantly altered at the 0.05 level for IL2, IL5, IL10, or IL12 (p35). Venous blood samples were also obtained at baseline as well as at 3, 24, and 48\u00a0h post-exercise. Serum was analyzed for circulating TNF\u03b1, IL1\u03b2, IL6, IL8, COX2, and SAA with no significant changes observed. These results indicate that resistance exercise is capable of up-regulating transcription of numerous inflammatory mediators within skeletal muscle, and these appear to be worthy of future examination in chronic studies.\n\n==\n\nYuan 2013:\n\nTitle: Proteomic Analysis of Skeletal Muscle in Insulin-Resistant Mice: Response to 6-Week Aerobic Exercise\n\nAbstract: Aerobic exercise has beneficial effects on both weight control and skeletal muscle insulin sensitivity through a number of specific signaling proteins. To investigate the targets by which exercise exerts its effects on insulin resistance, an approach of proteomic screen was applied to detect the potential different protein expressions from skeletal muscle of insulin-resistant mice after prolonged aerobic exercise training and their sedentary controls. Eighteen C57BL/6 mice were divided into two groups: 6 mice were fed normal chow (NC) and 12 mice were fed high-fat diet (HFD) for 10 weeks to produce an IR model. The model group was then subdivided into HFD sedentary control (HC, n\u200a=\u200a6) and HFD exercise groups (HE, n\u200a=\u200a6). Mice in HE group underwent 6 weeks of treadmill running. After 6 weeks, mice were sacrificed and skeletal muscle was dissected. Total protein (n\u200a=\u200a6, each group) was extracted and followed by citrate synthase, 2D proteome profile analysis and immunoblot. Fifteen protein spots were altered between the NC and HC groups and 23 protein spots were changed between the HC and HE groups significantly. The results provided an array of changes in protein abundance in exercise-trained skeletal muscle and also provided the basis for a new hypothesis regarding the mechanism of exercise ameliorating insulin resistance.\n\n==\n\nBurniston 2008:\n\nTitle: Changes in the rat skeletal muscle proteome induced by moderate-intensity endurance exercise.\n\nAbstract: The adaptation of skeletal muscle to endurance exercise has not previously been investigated using proteomic techniques. Such work could improve our understanding and generate novel information regarding the effects of exercise. Plantaris muscles were investigated from rats exercised on treadmills at 70-75% peak oxygen uptake (V O(2)peak) for 30 min, 4 days per week for 5 weeks or sedentary controls. Analysis of 2-D gels matched 187 spots across control and exercised muscles and 80 proteins corresponding to 40 gene products were identified by MALDI-ToF MS. Exercise increased the animals' V O(2)peak by 14% and altered the expression of 15 spots consistent with a shift from glycolysis toward greater fatty-acid oxidation. The majority of differentially expressed gene products were present as multi-spot series of similar M(r) but different pI. Mitochondrial aconitase focused to 5 spots, 2 spots (pI 7.6 and 7.7) decreased (57%) whereas the pI 8.0 spot increased (51%) and was found to contain protein carbonyls. This adaptation may be related to exercise-induced oxidative stress and translocation of aconitase to mitochondrial DNA. In conclusion, proteomic techniques simultaneously demonstrated well-established effects, and identified novel changes not previously associated with the adaptation of muscle to exercise.\n\n==\n\nDeshmukh 2016:\n\nTitle: Proteomics of Skeletal Muscle: Focus on Insulin Resistance and Exercise Biology\n\nAbstract: Skeletal muscle is the largest tissue in the human body and plays an important role in locomotion and whole body metabolism. It accounts for ~80% of insulin stimulated glucose disposal. Skeletal muscle insulin resistance, a primary feature of Type 2 diabetes, is caused by a decreased ability of muscle to respond to circulating insulin. Physical exercise improves insulin sensitivity and whole body metabolism and remains one of the most promising interventions for the prevention of Type 2 diabetes. Insulin resistance and exercise adaptations in skeletal muscle might be a cause, or consequence, of altered protein expressions profiles and/or their posttranslational modifications (PTMs). Mass spectrometry (MS)-based proteomics offer enormous promise for investigating the molecular mechanisms underlying skeletal muscle insulin resistance and exercise-induced adaptation; however, skeletal muscle proteomics are challenging. This review describes the technical limitations of skeletal muscle proteomics as well as emerging developments in proteomics workflow with respect to samples preparation, liquid chromatography (LC), MS and computational analysis. These technologies have not yet been fully exploited in the field of skeletal muscle proteomics. Future studies that involve state-of-the-art proteomics technology will broaden our understanding of exercise-induced adaptations as well as molecular pathogenesis of insulin resistance. This could lead to the identification of new therapeutic targets.\n\n==\n\nPsilander 2003:\n\nTitle: Resistance exercise alters MRF and IGF-I mRNA content in human skeletal muscle.\n\nAbstract: Increasing evidence suggests that the myogenic regulatory factors (MRFs) and IGF-I have important roles in the hypertrophy response observed after mechanical loading. We, therefore, hypothesized that a bout of heavy-resistance training would affect the MRF and IGF-I mRNA levels in human skeletal muscle. Six male subjects completed four sets of 6-12 repetitions on a leg press and knee extensor machine separated by 3 min. Myogenin, MRF4, MyoD, IGF-IEabc (isoforms a, b, and c) and IGF-IEbc (isoform b and c) mRNA levels were determined in the vastus lateralis muscle by RT-PCR before exercise, immediately after, and 1, 2, 6, 24, and 48 h postexercise. Myogenin, MyoD, and MRF4 mRNA levels were elevated (P < 0.005) by 100-400% 0-24 h postexercise. IGF-IEabc mRNA content decreased (P < 0.005) by approximately 44% after 1 and 6 h of recovery. The IGF-IEbc mRNA level was unaffected. The present study shows that myogenin, MyoD, and MRF4 mRNA levels are transiently elevated in human skeletal muscle after a single bout of heavy-resistance training, supporting the idea that the MRFs may be involved in regulating hypertrophy and/or fiber-type transitions. The results also suggest that IGF-IEa expression may be downregulated at the mRNA level during the initial part of recovery from resistance exercise.",
        "final_summary": "Resistance exercise induces significant alterations in the skeletal muscle proteome, affecting various proteins and molecular pathways. Son 2012 found that both endurance and resistance exercise training led to differential expression of specific muscle proteins. Similarly, Petriz 2017 and Padr\u00e3o 2016 highlighted the impact of exercise on the skeletal muscle proteome, with changes in protein expression related to ATP generation, oxygen delivery, and antioxidant capacity. Inflammatory-related genes in skeletal muscle were up-regulated by resistance exercise, as demonstrated by Buford 2009. Yuan 2013 showed that aerobic exercise training altered protein expressions in insulin-resistant mice, while Burniston 2008 provided evidence of novel changes in the skeletal muscle proteome in response to moderate-intensity endurance exercise. Deshmukh 2016 emphasized the potential of proteomics in investigating skeletal muscle insulin resistance and exercise adaptations. Lastly, resistance exercise increased the mRNA levels of myogenic regulatory factors (MRFs) and IGF-I in human skeletal muscle, as revealed by Psilander 2003."
    },
    {
        "query": "What is the relationship between mirror neurons and autism in children?",
        "paper_list_string": "Ruysschaert 2014:\n\nTitle: Exploring the Role of Neural Mirroring in Children with Autism Spectrum Disorder\n\nAbstract: Investigating the underlying neural mechanisms of autism spectrum disorder (ASD) has recently been influenced by the discovery of mirror neurons. These neurons, active during both observation and execution of actions, are thought to play a crucial role in imitation and other social\u2010communicative skills that are often impaired in ASD. In the current electroencephalographic study, we investigated mu suppression, indicating neural mirroring in children with ASD between the ages of 24 and 48 months and age\u2010matched typically developing children, during observation of goal\u2010directed actions and non\u2010goal\u2010directed mimicked hand movements, as well as during action execution. Results revealed no significant group differences with significant central mu suppression in the ASD children and control children during both execution and observation of goal\u2010directed actions and during observation of hand movements. Furthermore, no significant correlations between mu suppression on one hand and quality of imitation, age, and social communication questionnaire scores on the other hand were found. These findings challenge the \u201cbroken mirror\u201d hypothesis of ASD, suggesting that impaired neural mirroring is not a distinctive feature of ASD. Autism Res 2014, 7: 197\u2013 206. \u00a9 2014 International Society for Autism Research, Wiley Periodicals, Inc.\n\n==\n\nRaymaekers 2009:\n\nTitle: EEG study of the mirror neuron system in children with high functioning autism\n\nAbstract: Individuals with Autism Spectrum Disorder (ASD) are characterised by an impaired imitation, thought to be critical for early affective, social and communicative development. One neurological system proposed to underlie this function is the mirror neuron system (MNS) and previous research has suggested a dysfunctional MNS in ASD. The EEG mu frequency, more precisely the reduction of the mu power, is considered to be an index for mirror neuron functioning. In this work, EEG registrations are used to evaluate the mirror neuron functioning of twenty children with high functioning autism (HFA) between 8 and 13 years. Their mu suppression to self-executed and observed movement is compared to typically developing peers and related to age, intelligence and symptom severity. Both groups show significant mu suppression to both self and observed hand movements. No group differences are found in either condition. These results do not support the hypothesis that HFA is associated with a dysfunctional MNS. The discrepancy with previous research is discussed in light of the heterogeneity of the ASD population.\n\n==\n\nDapretto 2006:\n\nTitle: Understanding emotions in others: mirror neuron dysfunction in children with autism spectrum disorders\n\nAbstract: To examine mirror neuron abnormalities in autism, high-functioning children with autism and matched controls underwent fMRI while imitating and observing emotional expressions. Although both groups performed the tasks equally well, children with autism showed no mirror neuron activity in the inferior frontal gyrus (pars opercularis). Notably, activity in this area was inversely related to symptom severity in the social domain, suggesting that a dysfunctional 'mirror neuron system' may underlie the social deficits observed in autism.\n\n==\n\nPerkins 2010:\n\nTitle: Mirror neuron dysfunction in autism spectrum disorders\n\nAbstract: Autism spectrum disorders (ASDs) are developmental conditions characterized by deficits in social interaction, verbal and nonverbal communication and obsessive/stereotyped patterns of behaviour. Although there is no reliable neurophysiological marker associated with ASDs, dysfunction of the parieto-frontal mirror neuron system has been suggested as a disturbance linked to the disorder. Mirror neurons (MNs) are visuomotor neurons which discharge both when performing and observing a goal directed action. Research suggests MNs may have a role in imitation, empathy, theory of mind and language. Although the research base is small, evidence from functional MRI, transcranial magnetic stimulation, and an electroencephalographic component called the mu rhythm suggests MNs are dysfunctional in subjects with ASD. These deficits are more pronounced when ASD subjects complete tasks with social relevance, or that are emotional in nature. Promising research has identified that interventions targeting MN related functions such as imitation can improve social functioning in ASDs. Boosting the function of MNs may improve the prognosis of ASDs, and contribute to diagnostic clarity.\n\n==\n\nEnticott 2012:\n\nTitle: Mirror Neuron Activity Associated with Social Impairments but not Age in Autism Spectrum Disorder\n\nAbstract: BACKGROUND\nThe neurobiology of autism spectrum disorder (ASD) is not particularly well understood, and biomedical treatment approaches are therefore extremely limited. A prominent explanatory model suggests that social-relating symptoms may arise from dysfunction within the mirror neuron system, while a recent neuroimaging study suggests that these impairments in ASD might reduce with age.\n\n\nMETHODS\nParticipants with autism spectrum disorder (i.e., DSM-IV autistic disorder or Asperger's disorder) (n = 34) and matched control subjects (n = 36) completed a transcranial magnetic stimulation study in which corticospinal excitability was assessed during the observation of hand gestures.\n\n\nRESULTS\nRegression analyses revealed that the ASD group presented with significantly reduced corticospinal excitability during the observation of a transitive hand gesture (relative to observation of a static hand) (p < .05), which indicates reduced putative mirror neuron system activity within ventral premotor cortex/inferior frontal gyrus. Among the ASD group, there was also a negative association between putative mirror neuron activity and self-reported social-relating impairments, but there was no indication that mirror neuron impairments in ASD decrease with age.\n\n\nCONCLUSIONS\nThese data provide general support for the mirror neuron hypothesis of autism; researchers now must clarify the precise functional significance of mirror neurons to truly understand their role in the neuropathophysiology of ASD and to determine whether they should be used as targets for the treatment of ASD.\n\n==\n\nWilliams 2001:\n\nTitle: Imitation, mirror neurons and autism\n\nAbstract: Various deficits in the cognitive functioning of people with autism have been documented in recent years but these provide only partial explanations for the condition. We focus instead on an imitative disturbance involving difficulties both in copying actions and in inhibiting more stereotyped mimicking, such as echolalia. A candidate for the neural basis of this disturbance may be found in a recently discovered class of neurons in frontal cortex, 'mirror neurons' (MNs). These neurons show activity in relation both to specific actions performed by self and matching actions performed by others, providing a potential bridge between minds. MN systems exist in primates without imitative and 'theory of mind' abilities and we suggest that in order for them to have become utilized to perform social cognitive functions, sophisticated cortical neuronal systems have evolved in which MNs function as key elements. Early developmental failures of MN systems are likely to result in a consequent cascade of developmental impairments characterised by the clinical syndrome of autism.\n\n==\n\nHamilton 2013:\n\nTitle: Reflecting on the mirror neuron system in autism: A systematic review of current theories\n\nAbstract: There is much interest in the claim that dysfunction of the mirror neuron system in individuals with autism spectrum condition causes difficulties in social interaction and communication. This paper systematically reviews all published studies using neuroscience methods (EEG/MEG/TMS/eyetracking/EMG/fMRI) to examine the integrity of the mirror system in autism. 25 suitable papers are reviewed. The review shows that current data are very mixed and that studies using weakly localised measures of the integrity of the mirror system are hard to interpret. The only well localised measure of mirror system function is fMRI. In fMRI studies, those using emotional stimuli have reported group differences, but studies using non-emotional hand action stimuli do not. Overall, there is little evidence for a global dysfunction of the mirror system in autism. Current data can be better understood under an alternative model in which social top-down response modulation is abnormal in autism. The implications of this model and future research directions are discussed.\n\n==\n\nFan 2010:\n\nTitle: Unbroken mirror neurons in autism spectrum disorders.\n\nAbstract: BACKGROUND\nThe 'broken mirror' theory of autism, which proposes that a dysfunction of the human mirror neuron system (MNS) is responsible for the core social and cognitive deficits in individuals with autism spectrum disorders (ASD), has received considerable attention despite weak empirical evidence.\n\n\nMETHODS\nIn this electroencephalographic study, we examined mu suppression, as an indicator of sensorimotor resonance, concurrent with oculomotor performance while individuals (n = 20) with ASD and control participants (n = 20) either executed hand actions or observed hand actions or a moving dot. No difference in visual attention between groups was found as indicated by fixation duration and normalized fixation number on the presented stimuli.\n\n\nRESULTS\nThe mu suppression over the sensorimotor cortex was significantly affected by experimental conditions, but not by group membership, nor by the interaction between groups and conditions. Individuals with ASD, similar to the controls, exhibited stronger mu suppression when watching hand actions relative to a moving dot. Notably, participants with ASD failed to imitate the observed actions while their mu suppression indicating the MNS activity was intact. In addition, the mu suppression during the observation of hand actions was positively associated with the communication competence of individuals with ASD.\n\n\nCONCLUSION\nOur study clearly challenges the broken mirror theory of autism. The functioning of the mirror neuron system might be preserved in individuals with ASD to a certain degree. Less mu suppression to action observation coupled with more communicational severity can reflect the symptom heterogeneity of ASD. Additional research needs to be done, and more caution should be used when reaching out to the media.",
        "final_summary": "The relationship between mirror neurons and autism in children is still not fully understood based on the papers. Some papers suggest that mirror neuron dysfunction is not a distinctive feature of autism (Ruysschaert 2014), and that there are no group differences in mirror neuron functioning between children with high functioning autism and typically developing children (Raymaekers 2009). However, other papers indicate that children with autism show no mirror neuron activity in certain brain regions (Dapretto 2006), and that reduced mirror neuron activity is associated with social impairments in autism (Enticott 2012). There is also a suggestion that early developmental failures of mirror neuron systems may contribute to the clinical syndrome of autism (Williams 2001). Overall, the findings are mixed and more research is needed to fully understand the relationship between mirror neurons and autism in children."
    },
    {
        "query": "summary the current advance of elastography in obgyn field in the past 3 months",
        "paper_list_string": "Shao 2021:\n\nTitle: Advancements in the Application of Ultrasound Elastography in the Cervix.\n\nAbstract: Ultrasound elastography is a modern imaging technique that has developed rapidly in recent years. It enables objective measurement of tissue stiffness, a physical property intuitive to the human sense of touch. This novel technology has become a hotspot and plays a major role in scientific research and academic practice. Presently, ultrasound elastography has been used in the identification of benign and malignant tumors in superficial organs, such as breast and thyroid, providing clinically accurate diagnosis and treatment. The method has also been widely used for the liver, kidney, prostate, lymph nodes, blood vessels, skin and muscle system. In the application of cervical lesions, ultrasound elastography can distinguish normal cervix from abnormal cervix and differentiate benign from malignant lesions. It can significantly improve the diagnostic specificity for cervical cancer and is also useful for assessing infiltration depth and stage of cervical cancer, as well as predicting chemoradiotherapy treatment response. For cervical evaluation during pregnancy, ultrasound elastography is useful for assessing cervical softening and predicting premature delivery and outcome of induced labor. This article reviews the principles of ultrasound elastography as well as the current status and limitations in its application for cervical lesions and the cervix during pregnancy.\n\n==\n\nCui 2015:\n\nTitle: Endoscopic ultrasound elastography: Current status and future perspectives.\n\nAbstract: Elastography is a new ultrasound modality that provides images and measurements related to tissue stiffness. Endoscopic ultrasound (EUS) has played an important role in the diagnosis and management of numerous abdominal and mediastinal diseases. Elastography by means of EUS examination can assess the elasticity of tumors in the proximity of the digestive tract that are hard to reach with conventional transcutaneous ultrasound probes, such as pancreatic masses and mediastinal or abdominal lymph nodes, thus improving the diagnostic yield of the procedure. Results from previous studies have promised benefits for EUS elastography in the differential diagnosis of lymph nodes, as well as for assessing masses with pancreatic or gastrointestinal (GI) tract locations. It is important to mention that EUS elastography is not considered a modality that can replace biopsy. However, it may be a useful adjunct, improving the accuracy of EUS-fine needle aspiration biopsy (EUS-FNAB) by selecting the most suspicious area to be targeted. Even more, it may be useful for guiding further clinical management when EUS-FNAB is negative or inconclusive. In the present paper we will discuss the current knowledge of EUS elastography, including the technical aspects, along with its applications in the differential diagnosis between benign and malignant solid pancreatic masses and lymph nodes, as well as its aid in the differentiation between normal pancreatic tissues and chronic pancreatitis. Moreover, the emergent indication and future perspectives are summarized, such as the benefit of EUS elastography in EUS-guided fine needle aspiration biopsy, and its uses for characterization of lesions in liver, biliary tract, adrenal glands and GI tract.\n\n==\n\nSigrist 2017:\n\nTitle: Ultrasound Elastography: Review of Techniques and Clinical Applications\n\nAbstract: Elastography-based imaging techniques have received substantial attention in recent years for non-invasive assessment of tissue mechanical properties. These techniques take advantage of changed soft tissue elasticity in various pathologies to yield qualitative and quantitative information that can be used for diagnostic purposes. Measurements are acquired in specialized imaging modes that can detect tissue stiffness in response to an applied mechanical force (compression or shear wave). Ultrasound-based methods are of particular interest due to its many inherent advantages, such as wide availability including at the bedside and relatively low cost. Several ultrasound elastography techniques using different excitation methods have been developed. In general, these can be classified into strain imaging methods that use internal or external compression stimuli, and shear wave imaging that use ultrasound-generated traveling shear wave stimuli. While ultrasound elastography has shown promising results for non-invasive assessment of liver fibrosis, new applications in breast, thyroid, prostate, kidney and lymph node imaging are emerging. Here, we review the basic principles, foundation physics, and limitations of ultrasound elastography and summarize its current clinical use and ongoing developments in various clinical applications.\n\n==\n\nSarvazyan 2011:\n\nTitle: AN OVERVIEW OF ELASTOGRAPHY - AN EMERGING BRANCH OF MEDICAL IMAGING.\n\nAbstract: From times immemorial manual palpation served as a source of information on the state of soft tissues and allowed detection of various diseases accompanied by changes in tissue elasticity. During the last two decades, the ancient art of palpation gained new life due to numerous emerging elasticity imaging (EI) methods. Areas of applications of EI in medical diagnostics and treatment monitoring are steadily expanding. Elasticity imaging methods are emerging as commercial applications, a true testament to the progress and importance of the field.In this paper we present a brief history and theoretical basis of EI, describe various techniques of EI and, analyze their advantages and limitations, and overview main clinical applications. We present a classification of elasticity measurement and imaging techniques based on the methods used for generating a stress in the tissue (external mechanical force, internal ultrasound radiation force, or an internal endogenous force), and measurement of the tissue response. The measurement method can be performed using differing physical principles including magnetic resonance imaging (MRI), ultrasound imaging, X-ray imaging, optical and acoustic signals.Until recently, EI was largely a research method used by a few select institutions having the special equipment needed to perform the studies. Since 2005 however, increasing numbers of mainstream manufacturers have added EI to their ultrasound systems so that today the majority of manufacturers offer some sort of Elastography or tissue stiffness imaging on their clinical systems. Now it is safe to say that some sort of elasticity imaging may be performed on virtually all types of focal and diffuse disease. Most of the new applications are still in the early stages of research, but a few are becoming common applications in clinical practice.\n\n==\n\nDudea-Simon 2020:\n\nTitle: Elastography of the uterine cervix in gynecology: normal appearance, cervical intraepithelial neoplasia and cancer. A systematic review.\n\nAbstract: AIMS\nTo revise the current literature about the usefulness of elastography in cervical cancer (CC) and cervical intraepithelial neoplasia (CIN), from methods and technical limitations, to diagnosis, staging and the ability of predicting the response to oncologic treatment.\n\n\nMETHODS\nAn electronic database search was performed (PubMed, EMBASE, Web of Science) with the data range from January 2000 until May 2020. All studies, fully-available in English, assessing elastography of the uterine cervix in CC and CIN were selected. Studies were reviewed and discussed according to the elastographic technique and to the purpose of the research.\n\n\nRESULTS\nTwenty-three articles were found: 11 articles regarding strain elastography, 4 articles assessing shear wave elastography and 8 papers with matter-related information. Elastography was used in the study of normal variants of the uterine cervix as well as: the positive diagnosis of CC and CIN, clinical staging and the prediction of therapeutic response in CC. Comparison of the elastographic techniques was also performed.\n\n\nCONCLUSIONS\nElastography has multiple applications in the gynecological pathology of the cervix. The methods used to assess the cervix are diverse, and none have become universally accepted. With regard to CC and CIN, elastography is still an ongoing research field.\n\n==\n\nKim 2017:\n\nTitle: Elastographic measurement of the cervix during pregnancy: Current status and future challenges\n\nAbstract: The cervix is a cylindrical structure that is proximally connected to the uterus and distally to the vaginal cavity. The Bishop score has been used to evaluate the cervix during pregnancy. However, alternatives have been evaluated because the Bishop score is uncomfortable for patients, relies on a subjective examination, and lacks internal os data. Elastography has been used to assess the cervix, as it can estimate tissue stiffness. Recent articles on elastography for cervical assessment during pregnancy have focused on its usefulness for prediction of preterm birth and successful labor induction. There is a clinical need for cervical elastography, as an evaluation of biomechanical factors, because cervical length only assesses morphological changes. However, until now, cervical elastography has been studied in the limited field, and not shown a uniformed methodological technique. In this review, the current status, limitations, and future possibility of cervical elastography were discussed. Future studies should focus on overcoming the limitations of cervical elastography. Although the cervical elastography is presently an incompletely defined technique, it needs to be improved and evaluated as a method for use in combination with cervical length.\n\n==\n\nKozma 2021:\n\nTitle: Application of ultrasound elastography in obstetrics and gynecology\n\nAbstract: \u00d6sszefoglal\u00f3. Az ultrahang-elasztogr\u00e1fia az elm\u00falt \u00e9vek sor\u00e1n egyre n\u00f6vekv\u0151 figyelmet kapott a l\u00e1gysz\u00f6vetek elaszticit\u00e1s\u00e1nak vizsg\u00e1lat\u00e1ban. A m\u00f3dszer haszn\u00e1lat\u00e1t az teszi sz\u00fcks\u00e9gess\u00e9, hogy egyes, a mechanikai tulajdons\u00e1gaikban k\u00fcl\u00f6nb\u00f6z\u0151 sz\u00f6vetek hasonl\u00f3 echogenit\u00e1s\u00faak lehetnek, valamint hogy egy adott sz\u00f6vet megv\u00e1ltozott strukt\u00far\u00e1ja vagy mechanikai tulajdons\u00e1ga nem minden esetben j\u00e1r egy\u00fctt a sz\u00f6vet hagyom\u00e1nyos ultrahangk\u00e9p\u00e9nek megv\u00e1ltoz\u00e1s\u00e1val. Az elm\u00falt \u00e9vtizedben a deform\u00e1ci\u00f3s \u00e9s a ny\u00edr\u00e1si ultrahang-elasztogr\u00e1fia v\u00e1lt sz\u00e9les k\u00f6rben el\u00e9rhet\u0151v\u00e9. Ezen \u00faj k\u00e9palkot\u00e1si technika egyre nagyobb szerepet t\u00f6lt be a sz\u00fcl\u00e9szeti-n\u0151gy\u00f3gy\u00e1szati ultrahang-diagnosztik\u00e1ban is. A n\u0151gy\u00f3gy\u00e1szatban szerephez juthat az endometriosis \u00e9s az adenomyosis kimutat\u00e1s\u00e1ban, valamint a benignus \u00e9s a malignus cervicalis \u00e9s ovarialis k\u00e9pletek elk\u00fcl\u00f6n\u00edt\u00e9s\u00e9ben. A n\u0151gy\u00f3gy\u00e1szathoz hasonl\u00f3an a sz\u00fcl\u00e9szetben is jelent\u0151s v\u00e1ltoz\u00e1st hozhat az ultrahang-elasztogr\u00e1fia: alkalmas lehet a sz\u00fcl\u00e9sindukci\u00f3 sikeress\u00e9g\u00e9nek, a korasz\u00fcl\u00e9s bek\u00f6vetkez\u00e9s\u00e9nek \u00e9s a praeeclampsia kialakul\u00e1s\u00e1nak el\u0151rejelz\u00e9s\u00e9re. Orv Hetil. 2021; 162(18): 690-695. Summary. Ultrasound elastography has received significant attention for the assessment and measurement of soft tissue elastic properties in recent years. The advantage of ultrasound elastography lies in the fact that two different tissues can share similar echogenicities but may have other mechanical properties or, on the contrary, mechanical abnormalities of a designated tissue do not necessarily go hand in hand with an altered appearance on a conventional ultrasound image. In the last decade, strain and shear-wave elasticity imaging methods have become the most widely available among commercial ultrasound equipments. The importance of this new method expands rapidly also in the field of obstetrics and gynecology. Ultrasound elastography has a promising role in the diagnosis of endometriosis and adenomyosis and helps to differentiate benign and malignant cervical and ovarian lesions. The use in the prediction of the outcome of labor induction and preterm birth, and in the evaluation of preeclampsia are emerging. Orv Hetil. 2021; 162(18): 690-695.\n\n==\n\nOphir 1991:\n\nTitle: Elastography: A Quantitative Method for Imaging the Elasticity of Biological Tissues\n\nAbstract: We describe a new method for quantitative imaging of strain and elastic modulus distributions in soft tissues. The method is based on external tissue compression, with subsequent computation of the strain profile along the transducer axis, which is derived from cross-correlation analysis of pre- and post-compression A-line pairs. The strain profile can then be converted to an elastic modulus profile by measuring the stresses applied by the compressing device and applying certain corrections for the nonuniform stress field. We report initial results of several phantom and excised animal tissue experiments which demonstrate the ability of this technique to quantitatively image strain and elastic modulus distributions with good resolution, sensitivity and with diminished speckle. We discuss several potential clinical uses of this technique.",
        "final_summary": "Recent advancements in elastography in obstetrics and gynecology have shown promise in various applications. Shao 2021 highlights the use of ultrasound elastography in distinguishing normal and abnormal cervix. Cui 2015 discusses the benefits of endoscopic ultrasound elastography in diagnosing pancreatic masses and lymph nodes. Sigrist 2017 provides an overview of ultrasound elastography techniques and their clinical applications in liver fibrosis, breast, thyroid, prostate, kidney, and lymph node imaging. Sarvazyan 2011 presents the history, techniques, and clinical applications of elasticity imaging, emphasizing its increasing availability in medical diagnostics. Dudea-Simon 2020 reviews the usefulness of elastography in cervical cancer and cervical intraepithelial neoplasia, highlighting its potential for diagnosis, staging, and predicting treatment response. Kim 2017 discusses the current status and challenges of elastographic measurement of the cervix during pregnancy, emphasizing its potential for predicting preterm birth and successful labor induction. Kozma 2021 emphasizes the expanding role of ultrasound elastography in obstetrics and gynecology, including the diagnosis of endometriosis, adenomyosis, and differentiating benign and malignant cervical and ovarian lesions. Lastly, Ophir 1991 introduces the quantitative imaging method of elastography, which allows for the measurement of strain and elastic modulus distributions in soft tissues. These papers collectively demonstrate the growing importance and potential applications of elastography in the field of obstetrics and gynecology."
    },
    {
        "query": "Write about authenticity and credibility when conducting qualitative research.",
        "paper_list_string": "Brink 1993:\n\nTitle: Validity and reliability in qualitative research.\n\nAbstract: Validity and reliability are key aspects of all research. Meticulous attention to these two aspects can make the difference between good research and poor research and can help to assure that fellow scientists accept findings as credible and trustworthy. This is particularly vital in qualitative work, where the researcher\u2019s subjectivity can so readily cloud the interpretation of the data, and where research findings are often questioned or viewed with scepticism by the scientific community.\n\n==\n\nShenton 2004:\n\nTitle: Strategies for ensuring trustworthiness in qualitative research projects\n\nAbstract: Although many critics are reluctant to accept the trustworthiness of qualitative research, frameworks for ensuring rigour in this form of work have been in existence for many years. Guba\u2019s constructs, in particular, have won considerable favour and form the focus of this paper. Here researchers seek to satisfy four criteria. In addressing credibility, investigators attempt to demonstrate that a true picture of the phenomenon under scrutiny is being presented. To allow transferability, they provide sufficient detail of the context of the fieldwork for a reader to be able to decide whether the prevailing environment is similar to another situation with which he or she is familiar and whether the findings can justifiably be applied to the other setting. The meeting of the dependability criterion is difficult in qualitative work, although researchers should at least strive to enable a future investigator to repeat the study. Finally, to achieve confirmability, researchers must take steps to demonstrate that findings emerge from the data and not their own predispositions. The paper concludes by suggesting that it is the responsibility of research methods teachers to ensure that this or a comparable model for ensuring trustworthiness is followed by students undertaking a qualitative inquiry.\n\n==\n\nAmin 2020:\n\nTitle: Establishing trustworthiness and authenticity in qualitative pharmacy research.\n\nAbstract: Spurred by the value it can add, the use of qualitative research methods has been steadily growing by social pharmacy researchers around the globe, either separately or as part of mixed methods research projects. Given this increase, it is important to provide guidance to assist researchers in ensuring quality when employing such methods. This commentary addresses both theoretical fundamentals as well as practical aspects of establishing quality in qualitative social pharmacy research. More specifically, it provides an explanation of each of the criteria of trustworthiness proposed by Lincoln and Guba (credibility, transferability, dependability and confirmability) and different techniques used in establishing them. It also provides a brief overview of authenticity, a more recent and less widely used set of criteria that involve demonstrating fairness, ontological authenticity, educative authenticity, catalytic authenticity, and tactical authenticity. For each of these terms, the commentary provides a definition, how it applies to social pharmacy research, and guidance on when and how to use them. These are accompanied by examples from the pharmacy literature where the criteria have been used. The commentary ends by providing a summary of competing viewpoints of establishing quality in the published literature while inviting the reader to reflect on how the presented criteria would apply to different qualitative research projects.\n\n==\n\nPatton 1999:\n\nTitle: Enhancing the quality and credibility of qualitative analysis.\n\nAbstract: Varying philosophical and theoretical orientations to qualitative inquiry remind us that issues of quality and credibility intersect with audience and intended research purposes. This overview examines ways of enhancing the quality and credibility of qualitative analysis by dealing with three distinct but related inquiry concerns: rigorous techniques and methods for gathering and analyzing qualitative data, including attention to validity, reliability, and triangulation; the credibility, competence, and perceived trustworthiness of the qualitative researcher; and the philosophical beliefs of evaluation users about such paradigm-based preferences as objectivity versus subjectivity, truth versus perspective, and generalizations versus extrapolations. Although this overview examines some general approaches to issues of credibility and data quality in qualitative analysis, it is important to acknowledge that particular philosophical underpinnings, specific paradigms, and special purposes for qualitative inquiry will typically include additional or substitute criteria for assuring and judging quality, validity, and credibility. Moreover, the context for these considerations has evolved. In early literature on evaluation methods the debate between qualitative and quantitative methodologists was often strident. In recent years the debate has softened. A consensus has gradually emerged that the important challenge is to match appropriately the methods to empirical questions and issues, and not to universally advocate any single methodological approach for all problems.\n\n==\n\nJones 2013:\n\nTitle: Authenticity and scientific integrity in qualitative research.\n\nAbstract: Jacqueline Jones, PhD, RN, FRCNA, is an associate professor in the Division of Informatics, Health Systems & Leadership, College of Nursing University of Colorado, Aurora, CO. Scholarship for the care of women, childbearing families, and newborns demands attention to detail, creativity, and purpose in inquiry. A perplexing question is often raised by nurse scientists: Is qualitative research relevant today? A brief survey of a funding search engine, RePORTer, provided a resounding answer in the affirmative (1,470 hits with the term qualitative). Qualitative research makes a contribution to the contemporary development of health-related knowledge for individuals, families, communities, and populations. Qualitative research can help identify facilitators and barriers to intervention and can assist in the development of health care policy (Sandelowski & Leeman, 2012). It helps bring richness, context, and dimension to the study of human beings and their environments.\n\n==\n\nMorrow 2005:\n\nTitle: Quality and trustworthiness in qualitative research in counseling psychology.\n\nAbstract: This article examines concepts of the trustworthiness, or credibility, of qualitative research. Following a \u201cresearcher-as-instrument,\u201d or self-reflective, statement, the paradigmatic underpinnings of various criteria for judging the quality of qualitative research are explored, setting the stage for a discussion of more transcendent standards (those not associated with specific paradigms) for conducting quality research: social validity, subjectivity and reflexivity, adequacy of data, and adequacy of interpretation. Finally, current guidelines for writing and publishing qualitative research are reviewed, and strategies for conducting and writing qualitative research reports are suggested. Qualitative research, ensuing from a variety of disciplines, paradigms, and epistemologies, embraces multiple standards of quality, known variously as validity, credibility, rigor ,o rtrustworthiness. In addition to some standards that may be thought of as somewhat universal across disciplines and paradigms, the \u201cgoodness\u201d (Morrow & Smith, 2000) of qualitative inquiry is assessed on the basis of the paradigmatic underpinnings of the research and the standards of the discipline. Thus, a grounded theory study or a consensual qualitative research investigation in counseling psychology that is rooted in a postpositivist or constructivist/interpretivist paradigm will look quite different from a critical ethnography in education; and the standards appropriate for evaluating these studies will vary accordingly. I begin this article by addressing the paradigmatic underpinnings of trustworthiness or rigor in qualitative research. Next, I discuss central topics related to trustworthiness or validity that span paradigms and may be thought of as relevant across most research designs. I then provide an overview of guidelines that have been suggested for evaluating qualitative research, particularly in psychology. Finally, I offer recommendations for enhancing the quality of qualitative research in counseling psychology and suggest strategies for writing and publishing. First, however, in keeping with the standard of reflexivity as a way for researchers to inform their audiences about their perspectives as well as to manage their subjectivities, I describe my own assumptions about qualitative research methodology and quality.\n\n==\n\nNoble 2015:\n\nTitle: Issues of validity and reliability in qualitative research\n\nAbstract: Evaluating the quality of research is essential if findings are to be utilised in practice and incorporated into care delivery. In a previous article we explored \u2018bias\u2019 across research designs and outlined strategies to minimise bias.1 The aim of this article is to further outline rigour, or the integrity in which a study is conducted, and ensure the credibility of findings in relation to qualitative research. Concepts such as reliability, validity and generalisability typically associated with quantitative research and alternative terminology will be compared in relation to their application to qualitative research. In addition, some of the strategies adopted by qualitative researchers to enhance the credibility of their research are outlined.\n\nAssessing the reliability of study findings requires researchers and health professionals to make judgements about the \u2018soundness\u2019 of the research in relation to \u2026\n\n==\n\nBeck 1993:\n\nTitle: Qualitative Research: The Evaluation of Its Credibility, Fittingness, and Auditability\n\nAbstract: The three criteria of credibility, fittingness, and auditability have been focused on in the hope of facilitating the critique of qualitative research. Once criteria that are appropriate to qualitative methodologies are developed, the scientific merit of these research studies can truly be appreciated. If traditional scientific criteria relevant to quantitative studies are used to critique qualitative methods, the development and acceptance of this paradigm-transcending research will be hindered.",
        "final_summary": "The papers collectively provide insights into authenticity and credibility in qualitative research. Brink (1993) emphasizes the importance of validity and reliability in qualitative research, particularly in addressing the subjectivity of the researcher and ensuring acceptance by the scientific community. Shenton (2004) discusses strategies for ensuring trustworthiness, including credibility, transferability, dependability, and confirmability. Amin (2020) explores the criteria of trustworthiness proposed by Lincoln and Guba, as well as the concept of authenticity in social pharmacy research. Patton (1999) highlights the intersection of quality, credibility, and philosophical beliefs in qualitative analysis. Jones (2013) affirms the relevance of qualitative research in contemporary healthcare and its contribution to understanding human experiences. Morrow (2005) delves into transcendent standards for quality research, such as social validity and reflexivity. Noble (2015) examines the concepts of reliability, validity, and generalizability in relation to qualitative research. Beck (1993) focuses on the criteria of credibility, fittingness, and auditability for evaluating qualitative research. Overall, these papers provide guidance on ensuring authenticity and credibility in qualitative research, addressing various aspects such as validity, trustworthiness, and philosophical underpinnings."
    },
    {
        "query": "Genome mining for bioactive secondary metabolites from microbes",
        "paper_list_string": "Arulprakasam 2021:\n\nTitle: Genome mining of biosynthetic gene clusters intended for secondary metabolites conservation in actinobacteria.\n\nAbstract: Evolution of genome sequencing technology, on the one hand, and advancement of computational genome mining tools, on the other hand, paves way for improvement in predicting secondary metabolites. In past, numerous efforts were made concerning genome mining for recognizing secondary metabolites within the genus, but only a negligible quantity of comparative genomic reports had carried out among species of different genera. In this study, we explored potential of 24 actinobacteria species belonging to the genera, including Streptomyces, Nocardia, Micromonospora, and Saccharomonospora, to traverse diversity and distribution of Biosynthetic Gene Clusters (BGCs). Investigating results obtained from antiSMASH (Antibiotics and Secondary Metabolites Analysis Shell), NaPDoS (Natural Product Domain Seeker), and NP.searcher revealed conservation of genus-specific gene clusters among various species. E.g., NAGGN (n-acetyl glutaminyl glutamine amide) is present in Micromonospora, furan in Nocardia, melanin, and lassopeptide occur in Streptomyces. Bioactive compounds like alkyl-O-dihydro geranyl methoxy hydroquinone, SapB, desferrioxamine E, 2-Methylisoborneol, mayamycin, cyclodipeptide synthase, diisonitrile, salinichelin, hopene, ectoine and isorenieratene are highly conserved among diverse genera. Furthermore, pharmacological activity of actinobacterial derived metabolites against bacterial and fungal pathogens were illustrated. We need to accomplish large-scale analysis of natural products, including various genera of actinobacteria to deliver comprehensive intuition to overcome antibiotic resistance.\n\n==\n\nBlin 2019:\n\nTitle: antiSMASH 5.0: updates to the secondary metabolite genome mining pipeline\n\nAbstract: Abstract Secondary metabolites produced by bacteria and fungi are an important source of antimicrobials and other bioactive compounds. In recent years, genome mining has seen broad applications in identifying and characterizing new compounds as well as in metabolic engineering. Since 2011, the \u2018antibiotics and secondary metabolite analysis shell\u2014antiSMASH\u2019 (https://antismash.secondarymetabolites.org) has assisted researchers in this, both as a web server and a standalone tool. It has established itself as the most widely used tool for identifying and analysing biosynthetic gene clusters (BGCs) in bacterial and fungal genome sequences. Here, we present an entirely redesigned and extended version 5 of antiSMASH. antiSMASH 5 adds detection rules for clusters encoding the biosynthesis of acyl-amino acids, \u03b2-lactones, fungal RiPPs, RaS-RiPPs, polybrominated diphenyl ethers, C-nucleosides, PPY-like ketones and lipolanthines. For type II polyketide synthase-encoding gene clusters, antiSMASH 5 now offers more detailed predictions. The HTML output visualization has been redesigned to improve the navigation and visual representation of annotations. We have again improved the runtime of analysis steps, making it possible to deliver comprehensive annotations for bacterial genomes within a few minutes. A new output file in the standard JavaScript object notation (JSON) format is aimed at downstream tools that process antiSMASH results programmatically.\n\n==\n\nBelknap 2020:\n\nTitle: Genome mining of biosynthetic and chemotherapeutic gene clusters in Streptomyces bacteria\n\nAbstract: Streptomyces bacteria are known for their prolific production of secondary metabolites, many of which have been widely used in human medicine, agriculture and animal health. To guide the effective prioritization of specific biosynthetic gene clusters (BGCs) for drug development and targeting the most prolific producer strains, knowledge about phylogenetic relationships of Streptomyces species, genome-wide diversity and distribution patterns of BGCs is critical. We used genomic and phylogenetic methods to elucidate the diversity of major classes of BGCs in 1,110 publicly available Streptomyces genomes. Genome mining of Streptomyces reveals high diversity of BGCs and variable distribution patterns in the Streptomyces phylogeny, even among very closely related strains. The most common BGCs are non-ribosomal peptide synthetases, type 1 polyketide synthases, terpenes, and lantipeptides. We also found that numerous Streptomyces species harbor BGCs known to encode antitumor compounds. We observed that strains that are considered the same species can vary tremendously in the BGCs they carry, suggesting that strain-level genome sequencing can uncover high levels of BGC diversity and potentially useful derivatives of any one compound. These findings suggest that a strain-level strategy for exploring secondary metabolites for clinical use provides an alternative or complementary approach to discovering novel pharmaceutical compounds from microbes.\n\n==\n\nKhoshakhlagh 2022:\n\nTitle: Investigation of diverse biosynthetic secondary metabolites gene clusters using genome mining of indigenous Streptomyces strains isolated from saline soils in Iran\n\nAbstract: Background and Objectives: Bioactive secondary metabolites are the products of microbial communities adapting to environmental challenges, which have yet remained anonymous. As a result of demands in the pharmaceutical, agricultural, and food industries, microbial metabolites should be investigated. The most substantial sources of secondary metabolites are Streptomyces strains and are potential candidates for bioactive compound production. So, we used genome mining and bioinformatics to predict the isolates secondary metabolites, biosynthesis, and potential pharmaceuticals. Materials and Methods: This is a bioinformatics part of our previous experimental research. Here, we aimed to inspect the underlying secondary metabolite properties of 20 phylogenetically diverse Streptomyces species of saline soil by a rationalized computational workflow by several software tools. We examined the Metabolites\u2019 cytotoxicity and antibacterial effects using the MTT assay and plate count technique, respectively. Results: Among Streptomyces species, three were selected for genome mining and predicted novel secondary metabolites and potential drug abilities. All 11 metabolites were cytotoxic to A549, but ectoine (p\u22640.5) and geosmin (p\u22640.001) significantly operated as an anti-cancer drug. Metabolites of oxytetracycline and phosphinothricin (p\u22640.001), 4Z-annimycin and geosmin (p\u22640.01), and ectoine (p\u22640.5) revealed significant antibacterial activity. Conclusion: Of all the 11 compounds investigated, annimycin, geosmin, phosphinothricin, and ectoine had antimicrobial properties, but geosmin also showed very significant anti-cancer properties.\n\n==\n\nZiemert 2016:\n\nTitle: The evolution of genome mining in microbes - a review.\n\nAbstract: Covering: 2006 to 2016The computational mining of genomes has become an important part in the discovery of novel natural products as drug leads. Thousands of bacterial genome sequences are publically available these days containing an even larger number and diversity of secondary metabolite gene clusters that await linkage to their encoded natural products. With the development of high-throughput sequencing methods and the wealth of DNA data available, a variety of genome mining methods and tools have been developed to guide discovery and characterisation of these compounds. This article reviews the development of these computational approaches during the last decade and shows how the revolution of next generation sequencing methods has led to an evolution of various genome mining approaches, techniques and tools. After a short introduction and brief overview of important milestones, this article will focus on the different approaches of mining genomes for secondary metabolites, from detecting biosynthetic genes to resistance based methods and \"evo-mining\" strategies including a short evaluation of the impact of the development of genome mining methods and tools on the field of natural products and microbial ecology.\n\n==\n\nMicallef 2015:\n\nTitle: Genome mining for natural product biosynthetic gene clusters in the Subsection V cyanobacteria\n\nAbstract: BackgroundCyanobacteria are well known for the production of a range of secondary metabolites. Whilst recent genome sequencing projects has led to an increase in the number of publically available cyanobacterial genomes, the secondary metabolite potential of many of these organisms remains elusive. Our study focused on the 11 publically available Subsection V cyanobacterial genomes, together with the draft genomes of Westiella intricata UH strain HT-29-1 and Hapalosiphon welwitschii UH strain IC-52-3, for their genetic potential to produce secondary metabolites. The Subsection V cyanobacterial genomes analysed in this study are reported to produce a diverse range of natural products, including the hapalindole-family of compounds, microcystin, hapalosin, mycosporine-like amino acids and hydrocarbons.ResultsA putative gene cluster for the cyclic depsipeptide hapalosin, known to reverse P-glycoprotein multiple drug resistance, was identified within three Subsection V cyanobacterial genomes, including the producing cyanobacterium H. welwitschii UH strain IC-52-3. A number of orphan NRPS/PKS gene clusters and ribosomally-synthesised and post translationally-modified peptide gene clusters (including cyanobactin, microviridin and bacteriocin gene clusters) were identified. Furthermore, gene clusters encoding the biosynthesis of mycosporine-like amino acids, scytonemin, hydrocarbons and terpenes were also identified and compared.ConclusionsGenome mining has revealed the diversity, abundance and complex nature of the secondary metabolite potential of the Subsection V cyanobacteria. This bioinformatic study has identified novel biosynthetic enzymes which have not been associated with gene clusters of known classes of natural products, suggesting that these cyanobacteria potentially produce structurally novel secondary metabolites.\n\n==\n\nRomsdahl 2019:\n\nTitle: Recent advances in the genome mining of Aspergillus secondary metabolites (covering 2012-2018).\n\nAbstract: Secondary metabolites (SMs) produced by filamentous fungi possess diverse bioactivities that make them excellent drug candidates. Whole genome sequencing has revealed that fungi have the capacity to produce a far greater number of SMs than have been isolated, since many of the genes involved in SM biosynthesis are either silent or expressed at very low levels in standard laboratory conditions. There has been significant effort to activate SM biosynthetic genes and link them to their downstream products, as the SMs produced by these \"cryptic\" pathways offer a promising source for new drug discovery. Further, an understanding of the genes involved in SM biosynthesis facilitates product yield optimization of first-generation molecules and genetic engineering of second-generation analogs. This review covers advances made in genome mining SMs produced by Aspergillus nidulans, Aspergillus fumigatus, Aspergillus niger, and Aspergillus terreus in the past six years (2012-2018). Genetic identification and molecular characterization of SM biosynthetic gene clusters, along with proposed biosynthetic pathways, will be discussed in depth.\n\n==\n\nBok 2006:\n\nTitle: Genomic mining for Aspergillus natural products.\n\nAbstract: The genus Aspergillus is renowned for its ability to produce a myriad of bioactive secondary metabolites. Although the propensity of biosynthetic genes to form contiguous clusters greatly facilitates assignment of putative secondary metabolite genes in the completed Aspergillus genomes, such analysis cannot predict gene expression and, ultimately, product formation. To circumvent this deficiency, we have examined Aspergillus nidulans microarrays for expressed secondary metabolite gene clusters by using the transcriptional regulator LaeA. Deletion or overexpression of laeA clearly identified numerous secondary metabolite clusters. A gene deletion in one of the clusters eliminated the production of the antitumor compound terrequinone A, a metabolite not described, from A. nidulans. In this paper, we highlight that LaeA-based genome mining helps decipher the secondary metabolome of Aspergilli and provides an unparalleled view to assess secondary metabolism gene regulation.",
        "final_summary": "Genome mining is a potent tool for discovering and characterizing bioactive secondary metabolites from microbes. Arulprakasam 2021 demonstrates the ability of actinobacteria species to conserve biosynthetic gene clusters (BGCs), with genus-specific gene clusters found across different species. Blin 2019 introduces enhancements to the antiSMASH pipeline, the most widely used tool for identifying and analyzing BGCs in bacterial and fungal genomes. Belknap 2020 underscores the significance of understanding the diversity and distribution of BGCs in Streptomyces bacteria for effective drug development. Khoshakhlagh 2022 employs genome mining to predict secondary metabolites and potential pharmaceuticals in Streptomyces strains from saline soils. Ziemert 2016 reviews the evolution of genome mining methods and tools in microbes, highlighting their impact on natural product discovery. Micallef 2015 investigates secondary metabolite gene clusters in Subsection V cyanobacteria through genome mining, revealing a potential for diverse compound production. Romsdahl 2019 discusses the activation of cryptic biosynthetic genes in Aspergillus through genome mining, emphasizing its potential for drug discovery. Bok 2006 uses the transcriptional regulator LaeA for genome mining to identify secondary metabolite gene clusters in Aspergillus species. These papers collectively illustrate the power of genome mining in uncovering the biosynthetic potential of microbes for the discovery of bioactive secondary metabolites."
    },
    {
        "query": "Are emotions innate or constructed?",
        "paper_list_string": "Kurth 2019:\n\nTitle: Are Emotions Psychological Constructions?\n\nAbstract: According to psychological constructivism, emotions result from projecting folk emotion concepts onto felt affective episodes. While constructivists acknowledge there is a biological dimension to emotion, they deny that emotions are (or involve) affect programs. So they also deny emotions are natural kinds. However, the essential role that constructivism gives to felt experience and folk concepts leads to an account that is extensionally inadequate and functionally inaccurate. Moreover, biologically oriented proposals that reject these commitments are not similarly encumbered. Recognizing this has two implications: biological mechanisms are more central to emotion than constructivism allows, and the conclusion that emotions are not natural kinds is premature.\n\n==\n\nLindquist 2013:\n\nTitle: The hundred-year emotion war: are emotions natural kinds or psychological constructions? Comment on Lench, Flores, and Bench (2011).\n\nAbstract: For the last century, there has been a continuing debate about the nature of emotion. In the most recent offering in this scientific dialogue, Lench, Flores, and Bench (2011) reported a meta-analysis of emotion induction research and claimed support for the natural kind hypothesis that discrete emotions (e.g., happiness, sadness, anger, and anxiety) elicit specific changes in cognition, judgment, behavior, experience, and physiology. In this article, we point out that Lench et al. (2011) is not the final word on the emotion debate. First, we point out that Lench et al.'s findings do not support their claim that discrete emotions organize cognition, judgment, experience, and physiology because they did not demonstrate emotion-consistent and emotion-specific directional changes in these measurement domains. Second, we point out that Lench et al.'s findings are in fact consistent with the alternative (a psychological constructionist approach to emotion). We close by appealing for a construct validity approach to emotion research, which we hope will lead to greater consensus on the operationalization of the natural kind and psychological construction approaches, as well as the criteria required to finally resolve the emotion debate.\n\n==\n\nAverill 1980:\n\nTitle: A CONSTRUCTIVIST VIEW OF EMOTION\n\nAbstract: ABSTRACT Traditionally, the emotions have been viewed from a biological perspective; that is, the emotions have been seen as genetically determined and relatively invariable responses. The present chapter, by contrast, views the emotions as social constructions. More precisely, the emotions are here defined as socially constituted syndromes or transitory social roles. A role-conception does not deny the contribution of biological systems to emotional syndromes; it does, however, imply that the functional significance of emotional responses is to be found largely within the sociocultural system. With regard to subjective experience, a person interprets his own behavior as emotional in much the same way that an actor interprets a role\u201cwith feeling.\u201d This involves not only the monitoring of behavior (including feedback from physiological arousal, facial expressions, etc.), but also an understanding of how the emotional role fits into a larger\u201cdrama\u201d written by society. Some of the biological, personal, and situational factors that influence emotional behavior are also discussed.\n\n==\n\nBarrett 2016:\n\nTitle: The theory of constructed emotion: an active inference account of interoception and categorization\n\nAbstract: Abstract The science of emotion has been using folk psychology categories derived from philosophy to search for the brain basis of emotion. The last two decades of neuroscience research have brought us to the brink of a paradigm shift in understanding the workings of the brain, however, setting the stage to revolutionize our understanding of what emotions are and how they work. In this article, we begin with the structure and function of the brain, and from there deduce what the biological basis of emotions might be. The answer is a brain-based, computational account called the theory of constructed emotion.\n\n==\n\nOno 2014:\n\nTitle: How are innate emotions evoked?\n\nAbstract: We read with great interest the book and its precis by Rolls (2014a,b). He defines emotions as states in organisms that are elicited by reinforcements (rewards and punishments) and motivation as a state in which organisms are working for a goal to acquire or avoid reinforcements. Based on Darwin\u2019s theory of evolution (1859), Rolls proposes that emotions have an important evolutionary role in specifying a goal (rewards) (e.g., rewards that elicit positive emotions are necessary for the survival of organisms). According to Rolls\u2019 theory, neural mechanisms are hierarchically implemented in cortical systems. Stimuli (objects) are consciously represented in the higher association areas, such as the inferotemporal cortex, and this information is then sent to the amygdala and orbitofrontal cortex that code emotions and the emotional values of the stimuli. Finally, the value information is transferred to other areas, including the anterior cingulate cortex and basal ganglia, in order to select optimal actions required to acquire rewards. These theories elegantly explain human conscious emotions and behaviors. Furthermore, the theories and hypotheses for human higher cognitive and conscious processes proposed in the book can be testable by computer simulation. This method could uncover the neural mechanisms of emotions in humans at the cellular and system levels. We completely agree with the Darwinian account of emotion (Ono & Nishijo, 1992), as Rolls has proposed. In addition to the cortical systems that Rolls says is important for emotion processing, we would like to stress a role of subcortical sensory systems, such as subcortical fear modules (LeDoux, 2012), in unconscious innate emotions, which might also affect behavior and decision making in primates. Rolls has suggested that early stages in sensory processing may evoke emotional responses in non-primates, such as rodents. We similarly suggest that the phylogenetically old subcortical\n\n==\n\nBarrett 2006:\n\nTitle: Are Emotions Natural Kinds?\n\nAbstract: Laypeople and scientists alike believe that they know anger, or sadness, or fear, when they see it. These emotions and a few others are presumed to have specific causal mechanisms in the brain and properties that are observable (on the face, in the voice, in the body, or in experience)\u2014that is, they are assumed to be natural kinds. If a given emotion is a natural kind and can be identified objectively, then it is possible to make discoveries about that emotion. Indeed, the scientific study of emotion is founded on this assumption. In this article, I review the accumulating empirical evidence that is inconsistent with the view that there are kinds of emotion with boundaries that are carved in nature. I then consider what moving beyond a natural-kind view might mean for the scientific understanding of emotion.\n\n==\n\nScherer 2009:\n\nTitle: Emotions are emergent processes: they require a dynamic computational architecture\n\nAbstract: Emotion is a cultural and psychobiological adaptation mechanism which allows each individual to react flexibly and dynamically to environmental contingencies. From this claim flows a description of the elements theoretically needed to construct a virtual agent with the ability to display human-like emotions and to respond appropriately to human emotional expression. This article offers a brief survey of the desirable features of emotion theories that make them ideal blueprints for agent models. In particular, the component process model of emotion is described, a theory which postulates emotion-antecedent appraisal on different levels of processing that drive response system patterning predictions. In conclusion, investing seriously in emergent computational modelling of emotion using a nonlinear dynamic systems approach is suggested.\n\n==\n\nGriffiths 1997:\n\nTitle: What emotions really are : the problem of psychological categories\n\nAbstract: This study argues that much research of the emotions has been misguided. It attempts to show that \"emotion\" encompasses psychological states of very different, and thus not comparable, kinds. Some emotions, such as a brief flaring up of anger in response to some experience, are evolutionary ancient, reflex-like responses which appear insensitive to culture. Others, like moral guilt, differ importantly across cultures, despite their long history in humans, and affinity to behaviour seen in other species. Yet other emotions appear to be the acting-out of today's psychological myths, as ghost possession acted out the metaphysical myths of past centuries. These three kinds of responses have different evolutionary origins, different adaptive functions, different biological bases, and different roles in human psychology. The concept that binds them together, emotion, plays no useful role, since there is no object of scientific knowledge that corresponds to it. A detailed overview of the relevant theoretical approaches is provided in this text, assessing the relative merits of three main theoretical approaches: affect programme theory, evolutionary psychology, and social constructionism.",
        "final_summary": "Kurth (2019) argues against the constructivist view, asserting that biological mechanisms play a more central role in emotions than constructivism suggests. Lindquist (2013), however, finds the constructivist approach consistent with the findings of a meta-analysis of emotion induction research. Averill (1980) also supports the constructivist view, defining emotions as socially constituted syndromes. Barrett (2016) proposes a brain-based, computational account called the theory of constructed emotion. Ono (2014) emphasizes the role of subcortical sensory systems in evoking innate emotions. Barrett (2006) questions the assumption that emotions are natural kinds, suggesting that this view may not be consistent with empirical evidence. Scherer (2009) describes emotions as emergent processes requiring a dynamic computational architecture. Griffiths (1997) argues that the concept of emotion is too diverse to be useful in scientific knowledge, suggesting that it encompasses different psychological states that are not comparable."
    },
    {
        "query": "police violence and child victimization OR adverse childhood experinces",
        "paper_list_string": "Finkelhor 2012:\n\nTitle: Child and Youth Victimization Known to Police, School, and Medical Authorities. National Survey of Children's Exposure to Violence. Juvenile Justice Bulletin.\n\nAbstract: Considerable efforts have been made during the last generation to encour\u00ad age children and their families to report victimization to authorities. Nonetheless, concern persists that most childhood victimization remains hidden. The 2008 inventory of childhood victimization the National Study of Children s Exposure to Violence (NatSCEV) (see History of the National Survey of Children s Exposure to Violence, p. 4) allowed an assess\u00ad ment of whether authorities, including police, school, and medical authorities, are identifying victimizations.\n\n==\n\nFinkelhor 2010:\n\nTitle: Trends in childhood violence and abuse exposure: evidence from 2 national surveys.\n\nAbstract: OBJECTIVE\nTo assess trends in children's exposure to abuse, violence, and crime victimizations.\n\n\nDESIGN\nAn analysis based on a comparison of 2 cross-sectional national telephone surveys using identical questions conducted in 2003 and 2008.\n\n\nSETTING\nTelephone interview.\n\n\nPARTICIPANTS\nExperiences of children aged 2 to 17 years (2030 children in 2003 and 4046 children in 2008) were assessed through interviews with their caretakers and the children themselves. Outcome Measure Responses to the Juvenile Victimization Questionnaire.\n\n\nRESULTS\nSeveral types of child victimization were reported significantly less often in 2008 than in 2003: physical assaults, sexual assaults, and peer and sibling victimizations, including physical bullying. There were also significant declines in psychological and emotional abuse by caregivers, exposure to community violence, and the crime of theft. Physical abuse and neglect by caregivers did not decline, and witnessing the abuse of a sibling increased.\n\n\nCONCLUSION\nThe declines apparent in this analysis parallel evidence from other sources, including police data, child welfare data, and the National Crime Victimization Survey, suggesting reductions in various types of childhood victimization in recent years.\n\n==\n\nEdje 2021:\n\nTitle: The Case for Expanding Adverse Childhood Experiences to Include Police Violence\n\nAbstract: \u201cAll our phrasing race relations, racial chasm, racial justice, racial profiling, white privilege, even white supremacy serves to obscure that racism is a visceral experience, that it dislodges brains, blocks airway, rips muscle, extracts organs, cracks bones, breaks teeth. You must never look away from this. You must always remember that the sociology, the history, the economics, the graphs, the charts, the regressions all land, with great violence on the body.\u201d Ta-Nehisi Coates\n\n==\n\nDuke 2010:\n\nTitle: Adolescent Violence Perpetration: Associations With Multiple Types of Adverse Childhood Experiences\n\nAbstract: OBJECTIVE: Adverse childhood experiences are associated with significant functional impairment and life lost in adolescence and adulthood. This study identified relationships between multiple types of adverse events and distinct categories of adolescent violence perpetration. METHODS: Data are from 136 549 students in the 6th, 9th, and 12th grades who responded to the 2007 Minnesota Student Survey, an anonymous, self-report survey examining youth health behaviors and perceptions, characteristics of primary socializing domains, and youth engagement. Linear and logistic regression models were used to determine if 6 types of adverse experiences including physical abuse, sexual abuse by family and/or other persons, witnessing abuse, and household dysfunction caused by family alcohol and/or drug use were significantly associated with risk of adolescent violence perpetration after adjustment for demographic covariates. An adverse-events score was entered into regression models to test for a dose-response relationship between the event score and violence outcomes. All analyses were stratified according to gender. RESULTS: More than 1 in 4 youth (28.9%) reported at least 1 adverse childhood experience. The most commonly reported adverse experience was alcohol abuse by a household family member that caused problems. Each type of adverse childhood experience was significantly associated with adolescent interpersonal violence perpetration (delinquency, bullying, physical fighting, dating violence, weapon-carrying on school property) and self-directed violence (self-mutilatory behavior, suicidal ideation, and suicide attempt). For each additional type of adverse event reported by youth, the risk of violence perpetration increased 35% to 144%. CONCLUSIONS: Multiple types of adverse childhood experiences should be considered as risk factors for a spectrum of violence-related outcomes during adolescence. Providers and advocates should be aware of the interrelatedness and cumulative impact of adverse-event types. Study findings support broadening the current discourse on types of adverse events when considering pathways from child maltreatment to adolescent perpetration of delinquent and violent outcomes.\n\n==\n\nRivera 1990:\n\nTitle: Childhood Victimization and Violent Offending\n\nAbstract: The relationship between childhood victimization and violent offending is examined using a prospective cohorts design. Official criminal histories for a large sample of substantiated and validated cases of physical and sexual abuse and neglect (N = 908) from the years 1967 through 1971 were compared to those of a matched control group (N = 667) of individuals with no official record of abuse or neglect. Sex-specific and race-specific effects of childhood victimization and other characteristics of violent offending (chronicity, age of onset, temporal patterns, and continuity) are assessed. Childhood victimization increased overall risk for violent offending and particularly increased risk for males and blacks. In comparison to controls, abused and neglected children began delinquent careers earlier. Temporal patterns of violent offending were examined and childhood victims did not differ in age of arrest for first violent offense, nor were they more likely to continue offending. The findings and their limitations are discussed, as well as directions for future research.\n\n==\n\nGeller 2021:\n\nTitle: Youth\u2012Police Contact: Burdens and Inequities in an Adverse Childhood Experience, 2014\u20122017.\n\nAbstract: Objectives. To assess police contact as a potential adverse childhood experience by measuring its prevalence, nature, and distribution among urban adolescents.Methods. Detailed US population-based data on youth\u2012police contact were collected in the Fragile Families and Child Wellbeing Study (n\u2009=\u20092478) from 2014 to 2017. Using regression modeling, I assessed adolescents' police exposure and the magnitude and robustness of racial disparities in police contact. Sensitivity analyses examined disparities by behavior and socioeconomic context.Results. Urban youths are heavily policed, beginning in preadolescence. Exposure to policing is unevenly distributed, with non-White adolescents-particularly Black boys-reporting more, and more aggressive, contact than their White counterparts. Hispanic\u2012White differences and disparities in girls' experiences were less pronounced but present, particularly in how intrusive stops were. Intrusion disparities were robust to most behavioral controls, but not observed among youths with higher socioeconomic status.Conclusions. Given extant literature documenting adverse health consequences of police encounters, findings implicate policing as a driver of health disparities in adolescence and throughout the life course. Public health infrastructure dedicated to the prevention and treatment of adverse childhood experiences is well suited for mitigating these harms and inequities. (Am J Public Health. Published online ahead of print May 20, 2021: e1-e9. https://doi.org/10.2105/AJPH.2021.306259).\n\n==\n\nFinkelhor 2013:\n\nTitle: Violence, crime, and abuse exposure in a national sample of children and youth: an update.\n\nAbstract: IMPORTANCE\nBecause exposure to violence, crime, and abuse has been shown to have serious consequences on child development, physicians and policymakers need to know the kinds of exposure that occur at various developmental stages.\n\n\nOBJECTIVES\nTo provide updated estimates of and trends for childhood exposure to a broad range of violence, crime, and abuse victimizations.\n\n\nDESIGN\nThe National Survey of Children's Exposure to Violence was based on a cross-sectional, US national telephone survey conducted in 2011.\n\n\nSETTING\nInterviews by telephone.\n\n\nPARTICIPANTS\nThe experiences of 4503 children and youth aged 1 month to 17 years were assessed by interviews with caregivers and with youth in the case of those aged 10 to 17 years.\n\n\nRESULTS\nTwo-fifths (41.2%) of children and youth experienced a physical assault in the last year, and 1 in 10 (10.1%) experienced an assault-related injury. Two percent experienced sexual assault or sexual abuse in the last year, but the rate was 10.7% for girls aged 14 to 17 years. More than 1 in 10 (13.7%) experienced maltreatment by a caregiver, including 3.7% who experienced physical abuse. Few significant changes could be detected in rates since an equivalent survey in 2008, but declines were documented in peer flashing, school bomb threats, juvenile sibling assault, and robbery and total property victimization.\n\n\nCONCLUSIONS AND RELEVANCE\nThe variety and scope of children's exposure to violence, crime, and abuse suggest the need for better and more comprehensive tools in clinical and research settings for identifying these experiences and their effects.\n\n==\n\nFinkelhor 2015:\n\nTitle: Prevalence of Childhood Exposure to Violence, Crime, and Abuse: Results From the National Survey of Children's Exposure to Violence.\n\nAbstract: IMPORTANCE\nIt is important to estimate the burden of and trends for violence, crime, and abuse in the lives of children.\n\n\nOBJECTIVE\nTo provide health care professionals, policy makers, and parents with current estimates of exposure to violence, crime, and abuse across childhood and at different developmental stages.\n\n\nDESIGN, SETTING, AND PARTICIPANTS\nThe National Survey of Children's Exposure to Violence (NatSCEV) includes a representative sample of US telephone numbers from August 28, 2013, to April 30, 2014. Via telephone interviews, information was obtained on 4000 children 0 to 17 years old, with information about exposure to violence, crime, and abuse provided by youth 10 to 17 years old and by caregivers for children 0 to 9 years old.\n\n\nMAIN OUTCOME AND MEASURE\nExposure to violence, crime, and abuse using the Juvenile Victimization Questionnaire.\n\n\nRESULTS\nIn total, 37.3% of youth experienced a physical assault in the study year, and 9.3% of youth experienced an assault-related injury. Two percent of girls experienced sexual assault or sexual abuse in the study year, while the rate was 4.6% for girls 14 to 17 years old. Overall, 15.2% of children and youth experienced maltreatment by a caregiver, including 5.0% who experienced physical abuse. In total, 5.8% witnessed an assault between parents. Only 2 significant rate changes could be detected compared with the last survey in 2011, namely, declines in past-year exposure to dating violence and lifetime exposure to household theft.\n\n\nCONCLUSIONS AND RELEVANCE\nChildren and youth are exposed to violence, abuse, and crime in varied and extensive ways, which justifies continued monitoring and prevention efforts.",
        "final_summary": "The papers collectively indicate a significant prevalence of child victimization and exposure to violence, crime, and abuse. Finkelhor (2012) found that much childhood victimization remains hidden despite efforts to encourage reporting. Finkelhor (2010) reported declines in various types of childhood victimization, but not in physical abuse and neglect by caregivers. Duke (2010) found that multiple types of adverse childhood experiences, including abuse and witnessing abuse, were significantly associated with adolescent violence perpetration. Rivera (1990) found that childhood victimization increased the overall risk of violent offending, particularly for males and Blacks. Geller (2021) highlighted the heavy policing of urban youths, with non-White adolescents, particularly Black boys, reporting more contact with police. Finkelhor (2013) emphasized the need for better tools to identify children's exposure to violence, crime, and abuse. Lastly, Finkelhor (2015) highlighted the extensive and varied ways in which children and youth are exposed to violence, abuse, and crime."
    },
    {
        "query": "cucurbitins",
        "paper_list_string": "Kupchan 1978:\n\nTitle: New cucurbitacins from Phormium tenax and Marah oreganus\n\nAbstract: Abstract Cucurbitacins I and D and two new cucurbitacins, isocucurbitacin D and 3-epi-isocucurbitacin D, were isolated from Phormium tenax . A new cucurbitacin, dihydroisocucurbitacin B, was isolated from Marah oreganus . The acid sensitivity of the 2\u03b2-hydroxy-3-keto system found in cucurbitacin D was demonstrated.\n\n==\n\nMir\u00f3 1995:\n\nTitle: Cucurbitacins and their pharmacological effects\n\nAbstract: The cucurbitacins are triterpenic plant principles with varied pharmacological activities, found in several botanical families. This review includes the structures described in the literature, the plant\u2010containing cucurbitacins and their main pharmacological effects.\n\n==\n\nChen 2012:\n\nTitle: Biological activities and potential molecular targets of cucurbitacins: a focus on cancer\n\nAbstract: Cucurbitacin and its derivatives (cucurbitacins) are a class of highly oxidized tetracyclic triterpenoids. They are widely distributed in the plant kingdom, where they act as heterologous chemical pheromones that protect plants from external biological insults. Their bioactivities first attracted attention in the 1960s. Documented data demonstrate that cucurbitacins possess strong pharmacological properties, such as antitumor, anti-inflammatory, and hepatoprotective effects, etc. Several molecular targets for cucurbitacins have been discovered, such as fibrous-actin, signal transducer and activator of transcription 3, cyclooxygenase-2, etc. The present study summarizes the achievements of the 50 years of research on cucurbitacins. The aim was to systematically analyze their bioactivities with an emphasis on their anticancer effects. Research and development has shed new insight into the beneficial properties of these compounds.\n\n==\n\nLavie 1971:\n\nTitle: The cucurbitanes, a group of tetracyclic triterpenes.\n\nAbstract: The potent physiological activity of plants belonging to the Cucurbitaceae family has been known since antiquity. They were feared on account of their high toxicity (Elisha\u2019s Miracle)*, and vet valued because of the medicinal properties ascribed to them (40). Greeks and Romans used them, the doctors of the Middle Ages praised their virtues and some were still described in the British Pharmacopoeia of 1914.\n\n==\n\nChen 2005:\n\nTitle: Cucurbitacins and cucurbitane glycosides: structures and biological activities.\n\nAbstract: The natural cucurbitacins constitute a group of triterpenoid substances which are well-known for their bitterness and toxicity. Structurally, they are characterized by the tetracyclic cucurbitane nucleus skeleton, namely, 19-(10-->9beta)-abeo-10alpha-lanost-5-ene (also known as 9beta-methyl-19-norlanosta-5-ene), with a variety of oxygen substitutions at different positions. According to the characteristics of their structures, cucurbitacins are divided into twelve categories. The biological effects of the cucurbitacins are also covered.\n\n==\n\nWalker 2011:\n\nTitle: The potential of cucurbit[n]urils in drug delivery\n\nAbstract: In this paper we review cucurbit[n]urils (CB[n]), a relatively new family of macrocycles that has shown potential in improving drug delivery. Encapsulation of drugs within the homologues CB[6], CB[7], or CB[8] can impart enhanced chemical and physical stability, improve drug solubility, and control drug release. The formulation of CB[n] into a dosage form suitable for clinical use is a non-trivial task, because the free macrocycle and its host-drug complex generally exhibit pseudo-polymorphism in the solid state. Despite this, cucurbiturils have been included in tablets for oral delivery and inserts for nasal delivery. Here we examine the potential use of cucurbiturils in drug delivery in the context of getting a new drug into clinical trials and discuss what further research is needed in this area.\n\n==\n\nR\u00edos 2005:\n\nTitle: New insights into the bioactivity of cucurbitacins\n\nAbstract: The cucurbitacins are a group of tetracyclic triterpenoids derived from the cucurbitane skeleton and found primarily in the Cucurbitaceae family. These triterpenoids, present in free or glycosidic form, are generally responsible for the bitter taste of the plants that contain them and are probably the principal cause of the antifeedant effects observed for such plants. Several plants used in traditional medicine to treat both inflammatory diseases as well as various types of tumors are rich in cucurbitacins, a fact which has given rise to several studies concerning their potential use as anti-inflammatory and anticancer agents. Nevertheless, since many cucurbitacins are extremely toxic, relatively few papers have dealt with their pharmacological activity. Recently, however, the relationship between the toxicity of a compound and its chemical pattern of substitution has been established, thus allowing for a more in-depth understanding of this class of triterpenes. In the present review, we provide a compilation of all the studies published in the last ten years on the pharmacological and biological effects of cucurbitacins, focusing principally on their pharmacological properties, especially their anti-inflammatory and anticancer effects.\n\n==\n\nMetcalf 1980:\n\nTitle: Cucurbitacins as kairomones for diabroticite beetles.\n\nAbstract: The characteristic bitter substances of the Cucurbitaceae act as kairomones for a large group of diabroticite beetles (Chrysomelidae, Galerucinae, Luperini), promoting host selection and compulsive feeding behavior. These beetles (e.g., Diabrotica undecimpunctata howardi) respond to as little as 1 ng of cucurbitacin (Cuc) B on thin-layer plates by arrest and compulsive feeding. Six species of diabroticite beetles were about 10 times more responsive to Cuc B than to Cuc E and less responsive to Cuc D, I, and L. Chloroform extracts of 18 species of Cucurbita were developed on thin-layer chromatograms and exposed to diabroticite beetles. The feeding patterns showed pronounced beetle responses to three general Cuc distribution patterns: Cuc B and D as in Cucurbita andreana and C. ecuadorensis; Cuc E and I as in C. okeechobeensis and C. martinezii; and Cuc E glycoside in C. texana. All the diabroticites responded in exactly the same feeding patterns. The results demonstrate a coevolutionary association between the Cucurbitaceae and the Luperini, during which the intensely bitter and toxic Cucs that arose to repel herbivores and protect the plants from attack became specific kairomone feeding stimulants for the beetles.",
        "final_summary": "The papers collectively provide insights into cucurbitins. Cucurbitins are triterpenic plant compounds with varied pharmacological activities (Mir\u00f3 1995). They possess strong pharmacological properties, including antitumor, anti-inflammatory, and hepatoprotective effects (Chen 2012). Cucurbitins have been found in various plant families, such as Cucurbitaceae, and are known for their bitterness and toxicity (Lavie 1971, Chen 2005). They have been studied for their potential use in cancer treatment, with several molecular targets identified (Chen 2012). Additionally, cucurbitins act as kairomones for diabroticite beetles, promoting host selection and feeding behavior (Metcalf 1980). The review of cucurbiturils, a related compound, suggests their potential in drug delivery due to enhanced stability and solubility (Walker 2011). Overall, cucurbitins have diverse biological activities and potential therapeutic applications, but their toxicity should be considered (R\u00edos 2005)."
    },
    {
        "query": "what is flexible work policy",
        "paper_list_string": "Rubery 2016:\n\nTitle: Flexibility bites back: the multiple and hidden costs of flexible employment policies\n\nAbstract: Flexible labour markets are increasingly regarded as the answer to a wide spectrum of labour market and societal challenges from creating jobs to reducing segmentation and welfare dependency, improving public finances and supporting workforce diversity and innovation. The contention is that, contrary to these claims, flexible labour markets generate fundamental contradictions and unsustainable long-term trends. The jobs miracle is exaggerated and based on low productivity jobs, outsiders often lose most from competition, claimants must work flexibly but still secure a full-time wage, low-wage employment is shrinking the fiscal base, jobs are not being adjusted to accommodate workers' changing needs and capacities and the disposable labour model is undermining long-term productivity.\n\n==\n\nOorschot 2004:\n\nTitle: Flexible work and flexicurity policies in the Netherlands. Trends and experiences\n\nAbstract: Increasing the flexibility of work and working life has been high on the agenda of Dutch public debate for at least 15 years. Resulting policies have been guided by the aspiration of combining flexibility and security, or of achieving adequate \u2018flexicurity\u2019, as the combination of these goals has come to be known. This article describes and analyses Dutch flexicurity policies of recent years, as they have been adopted in the fields of part-time work, social security, labour law and the work-care combination. It shows that the government has made it easier for employers and employees to choose part-time work as a strategy for increasing flexibility. In the field of social security there are numerous problems, especially for \u2018flex\u2019 -workers (not for part-time workers as such), but little substantial improvements have been implemented. In labour law important flexibility and security measures have been adopted, but here government has been rather slow in taking the lead. As regards the work-care combination, new policies have improved conditions, but the Netherlands still lags behind other European countries. L\u2019 augmentation de la flexibilit\u00e9 du travail et de la vie active figure parmi les priorit\u00e9s \u00e0 l\u2019 ordre du jour des d\u00e9bats publics aux Pays-Bas depuis au moins 15 ans. Les politiques d\u00e9velopp\u00e9es se sont efforc\u00e9es de combiner flexibilit\u00e9 et s\u00e9curit\u00e9, ou de mettre en oeuvre une \u00abflexicurit\u00e9\u00bb ad\u00e9quate, terme choisi pour d\u00e9signer cette combinaison. Cet article d\u00e9crit et analyse les politiques de flexicurit\u00e9 aux Pays-Bas ces derni\u00e8res ann\u00e9es, qui ont \u00e9t\u00e9 adopt\u00e9es dans le domaine du travail \u00e0 temps partiel, de la s\u00e9curit\u00e9 sociale, de la l\u00e9gislation du travail et de la conciliation de la vie professionnelle et familiale. Il montre que le gouvernement a permis aux employeurs et aux travailleurs de choisir le travail \u00e0 temps partiel comme strat\u00e9gie pour accro\u00eetre la flexibilit\u00e9. Dans le domaine de la s\u00e9curit\u00e9 sociale, il y a de nombreux probl\u00e8mes, particuli\u00e8rement pour les \u00ab flex \u00bb-travailleurs (mais pas pour les travailleurs \u00e0 temps partiel en tant que tels), mais de petites am\u00e9liorations substantielles ont \u00e9t\u00e9 mises en oeuvre. Dans la l\u00e9gislation du travail, d\u2019 importantes mesures de flexibilit\u00e9 et de s\u00e9curit\u00e9 ont \u00e9t\u00e9 adopt\u00e9es, m\u011bme si ici le gouvernement a \u00e9t\u00e9 plut\u00f4t lent \u00e0 lancer l'initiative. En ce qui concerne la conciliation de la vie professionnelle et familiale, les nouvelles politiques ont am\u00e9lior\u00e9 les conditions, mais les Pays-Bas sont toujours \u00e0 la tra\u00eene des autres pays europ\u00e9ens. Seit mindestens 15 Jahren steht die Erh\u00f6hung der Flexibilit\u00e4t der Arbeit und des Arbeitslebens in den Niederlanden im Mittelpunkt der \u00f6ffentlichen Debatte. Die daraus resultierenden Ma\u03b2nahmen wurden von dem Streben geleitet, Flexibilit\u00e4t und Sicherheit miteinander zu verkn\u00fcpfen, oder zu einer angemessenen \u201cFlexicurity\u201d, wie diese Verkn\u00fcpfung der beiden Ziele bezeichnet wird, zu gelangen. Dieser Beitrag beschreibt und analysiert die niederl\u00e4ndischen Ma\u03b2nahmen, die in den letzten Jahren in den Bereichen Teilzeitarbeit, soziale Sicherheit, Arbeitsrecht und Vereinbarkeit von Arbeit und Betreuungspflichten ergriffen wurden, um das Ziel der Flexicurity zu erreichen. Der Beitrag zeigt, dass die Regierung es aus strategischen Gr\u00fcnden Arbeitgebern und Arbeitnehmern leichter macht, sich f\u00fcr Teilzeitarbeit zu entscheiden, um die Flexibilit\u00e4t zu erh\u00f6hen. Im Bereich der sozialen Sicherheit gibt es zahlreiche Probleme, besonders f\u00fcr \u201cFlex\u201c-Arbeitnehmer (nicht f\u00fcr Teilzeitbesch\u00e4ftigte an sich), aber hier wurden nur geringe substanzielle Verbesserungen erzielt. Im Arbeitsrecht wurden zwar bedeutende Ma\u03b2nahmen zur F\u00f6rderung der Flexibilit\u00e4t und der Sicherheit ergriffen, aber die Regierung hat in diesem Bereich nur mit Verz\u00f6gerung die Initiative ergriffen. Was die Vereinbarkeit von Arbeit und Betreuungspflichten angeht, haben sich die Bedingungen zwar aufgrund neuer Ma\u03b2nahmen verbessert, aber die Niederlande bleiben dennoch nach wie vor hinter anderen europ\u00e4ischen L\u00e4ndern zur\u00fcck.\n\n==\n\nBeatson 1994:\n\nTitle: Labour market flexibility\n\nAbstract: The overarching labour market policy of the British government is to aim for a competitive, efficient and flexible labour market. This study aims to assess whether the British labour market has indeed become more flexible.\n\n==\n\nLiechty 2007:\n\nTitle: Flexible Workplace Policies: Lessons From the Federal Alternative Work Schedules Act\n\nAbstract: Abstract: This case study uses a feminist framework to examine the 7-year process by which the Federal Alternative Work Schedules Act (1978 \u2013 1985) became law and the reasons for reenergized implementation in the 1990s. We analyze the legislative discourse for rationale in support of and opposition to this policy, connect findings to current flexible work research, and identify criteria for evaluating the impact of flextime policies with attention to intersections of gender and socioeconomic status. Unexpectedly, traditional views on hierarchical manager-worker relations figured more prominently than gender in the Congressional debates.\n\n==\n\nHalpern 2005:\n\nTitle: How time\u2010flexible work policies can reduce stress, improve health, and save money\n\nAbstract: Data from the US National Study of the Changing Workforce (a nationally representative sample of working adults) were used to test the hypothesis that employees with time-flexible work policies reported less stress, higher levels of commitment to their employer, and reduced costs to the organization because of fewer absences, fewer days late, and fewer missed deadlines. The model provides persuasive findings for the hypothesized relationship and offers important suggestions to employers who can translate reduced illness into savings and increased commitment into better employees. Contrary to expectations, there were no gender differences in how employees responded to flexible work policies, showing that gender-neutral work policies make financial sense. By showing that time-flexible work policies provide employer benefits, we can hasten the change to a new worker model\u2014one that is family and employer friendly. The business case for family-friendly work policies may prove to be the best tool we have in changing how we live and work. Copyright \u00a9 2005 John Wiley & Sons, Ltd.\n\n==\n\nBrewster 1997:\n\nTitle: Flexible Working in Europe: A Review of the Evidence\n\nAbstract: Introduction(1) The concept of flexible working practices(2), the extent of such practices and the implications for practitioners and policy makers in the area, have been much discussed. These are critical issues for employers, trade unions and governments. Recent opinions from the European Court of Justice have raised the political profile of the subject and the European Commission is committed to further action on this issue in 1997. This paper addresses these issues in a comparative European context. The paper presents evidence on developments in flexible working from organisations across Europe. Flexible working here covers only working time and contractual variations (temporary contracts, outsourcing etc.).(3) The paper summarise the debates on the topic; briefly outlines the research that was done; shows the extent and growth of flexibility; explores the reasons for the massive growth in flexible working in Europe and considers whether this is evidence of convergence of European labour markets. This is followed by a wide-ranging examination of the implications of the findings. Theories of Labour Flexibility The concept of \"labour flexibility\" remains, both in theoretical and practical terms, highly problematic. Despite the huge volume of literature devoted to the so-called \"flexibility debate\" (see bibliography in Brewster et al. 1996a), relatively little progress has been made in resolving many of the problems associated with the concept. In the literature, the term \"flexibility\" is applied to a series of quite distinct (if related) theories. There are those which have been labelled \"post-Fordist\": a category which covers a range of variants, but is characterised by the work of Piore and Sabel (1984), Mathews (1989a, 1989b, 1990, 1992), Lash and Urry (1987), Katz (1985), Kern and Schumann (1987), Tolliday and Zeitlin (1986), and Streeck (1987). For these writers, who generally concentrate on manufacturing industry, new technology is the key to a more flexible form of production, more responsive to increasingly rapid changes in the market. Whilst such developments may depend for their success upon a more skilled, motivated, and flexible workforce, the focus of this stream of writing is on production systems rather than employment. A more critical, \"neo-Marxian\" (Clegg 1990) or \"neo-Fordist\" (Wood 1989b, p. 21) group of writers is also concerned with flexible production, though taking a more negative view of its likely effect on individuals and including discussion of the impact on labour markets (Bramble 1988; Bramble/Fieldes 1989, 1992; Harvey 1989, 1991). An alternative conception of flexibility is provided by researchers in the operational management area. There is also an important set of literature labelled by some as \"managerialist\" (Bagguley 1991, p. 164) or \"neomanagerialist\" (Clegg 1990, p. 210) and typified by the work of Atkinson (1984, 1985a, 1985b, 1986, 1987; Atkinson/Gregory 1986; Atkinson/Meager 1986). His work has been subjected to critiques which have attempted to demonstrate the limited utility and lack of theoretical robustness of his work or even, in the early debates, to deny that the labour market had changed substantially, rather than attempting to build upon the insights which it provides or to develop a more comprehensive theoretical framework based on it (see for example: Pollert 1988a, 1988b). Nonetheless, Atkinson's work has been extremely influential. His vision of flexibility has influenced policy debates internationally (OECD 1986a, 1989). These changes and the development of a more flexible labour market have been controversial. They are seen by some to have taken our societies back towards the early years of the industrial revolution, with the creation of a significant underclass of underprivileged and \"vulnerable\" workers. They are seen by others as evidence of an uncaring and irresponsible approach to employees by employers -- sometimes associated with a desire to \"de-unionise\" the workforce. \u2026\n\n==\n\nGolden 2001:\n\nTitle: Flexible Work Schedules\n\nAbstract: More than 27% of the U.S. workforce now reports having an ability to alter their daily starting and ending times of work. Yet, provision of flexibility in the timing of work is not keeping pace with demand. Moreover, there is much disparity in access to schedule flexibility by workers' demographic, work, and job characteristics. Probit estimation finds that the probability that a worker has such flexibility is reduced by being female, non-White, and less educated. The likelihood is increased by being self-employed, in college, married, part-time, in certain occupations and industries, and working 50 or more hours per week. Flexibility is reduced for those working a standard day shift or 40-hour workweek. Workers thus sacrifice either leisure time or income to gain better access to flexibility in the scheduling of work, or they endure the costs of job mobility. Public policy should focus on delivering more flexible schedules to the excluded 73%.\n\n==\n\nBrewster 1997:\n\nTitle: Flexible working in Europe\n\nAbstract: This research paper analyzes and reports upon the current practice of flexible working amongst organizations in Europe: focussing on current developments in the use, by employing organizations, of parttime workers and a range of contractual variations (temporary work; fixed-term contracts etc). The analysis draws heavily, though not exclusively, upon a substantial, independent database of organizational level questionnaire responses covering all sectors of the economy in 14 European countries. We provide evidence that across Europe there is a substantial amount of flexible working and that there has been a continuing increase in its use. It is argued that these developments have considerable, as yet little understood, implications for policy makers, individuals and employers.",
        "final_summary": "Flexible work policy, as discussed in these papers, is a complex and multifaceted concept with varying outcomes. Rubery (2016) critiques flexible labor markets, highlighting their hidden costs and contradictions. Oorschot (2004) provides an analysis of Dutch flexicurity policies, noting that while part-time work has been facilitated, social security for flexible workers remains a challenge. Liechty (2007) offers a case study on the Federal Alternative Work Schedules Act, emphasizing the importance of hierarchical manager-worker relations in the legislative discourse. Halpern (2005) presents data from the US National Study of the Changing Workforce, demonstrating that time-flexible work policies can reduce stress, improve health, and save money for employers. Brewster (1997) discusses the growth of flexible working in Europe and its implications. Golden (2001) highlights disparities in access to schedule flexibility in the US workforce based on demographic and job characteristics. The papers collectively underscore the complexities associated with flexible work policies. (Rubery 2016; Oorschot 2004; Liechty 2007; Halpern 2005; Brewster 1997; Golden 2001)"
    },
    {
        "query": "organizational attention diversion",
        "paper_list_string": "Yaniv 2011:\n\nTitle: ORGANIZATIONAL ATTENTION: A METAPHOR FOR A CORE COGNITIVE PROCESS\n\nAbstract: Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen\u0219s domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.\n\n==\n\nYaniv 2011:\n\nTitle: Organizational Attention\n\nAbstract: INTRODUCTION Attention is a term commonly used in education, psychiatry, and psychology. Attention can be defined as an internal cognitive process by which one actively selects environmental information (i.e., sensation) or actively processes information from internal sources (i.e., stored memories and thoughts; Sternberg, 1996). In more general terms, attention can be defined as an ability to focus and maintain interest in a given task or idea, including managing distractions. Attention is selective by its nature. According to Pashler (1998, p. 37), \" The process of selecting from among the many potentially available stimuli is the clearest manifestation of selective attention. \" Why do firms respond to certain events or stimuli in their environment while neglecting others? It seems that organizations, just like individuals , have limited attention capacity. Hence, they must select from among the many potentially available stimuli and respond to these selected stimuli only. Organizational attention is defined as the socially structured pattern of attention by decision makers within the organization (Ocasio, 1997). Organizational attention, like human attention , is a limited resource: \" Attentional limits filter or screen incoming information such that a great deal of data pertinent to strategic decision may never get processed \" (2003) show that the extent to which CEOs (chief executive officers) are selective in their attention to sectors of the environment is a significant predictor of performance. Knowledge management (KM) models and process theories, almost without exception, incorporate a stage or phase in which a given knowledge item is brought to bear on a current decision or action. This stage, referred to alternatively as, is of crucial importance in any knowledge-management cycle. The flow of knowledge in and out of an awareness stage is not merely a function of the universe of available organizational memory or the technological tools available to filter and identify such knowledge. It is influenced to a large degree by organizational attention. The second area in which organizational attention is key is knowledge acquisition and creation as discussed by Ocasio (1997), and Yaniv and Elizur (2003). Successful knowledge management requires attention. Davenport and Volpel (2001) argues that attention is the currency of the information age. Knowledge consumers must pay attention to knowledge and become actively involved in the knowledge-transfer processes. This is particularly important when the knowledge to be received is tacit (Nonaka, 1994). Knowledge can be part of the organization's repository, however, if it does not \u2026\n\n==\n\nMay 2007:\n\nTitle: Organizing Attention: Responses of the Bureaucracy to Agenda Disruption\n\nAbstract: Federal agencies are routinely confronted with requests from policymakers that they must address in some manner. These range from routine directives to cut through red tape to exceptional demands to alter policy priorities. We theorize that how attention is organized by public bureaucracies affects their responses. We draw on a variety of scholarship about public bureaucracies to develop a theory about the bureaucratic organization of attention and its consequences. In illustrating these notions, we trace federal agency attention to the threat of terrorism as it gained prominence on the national policy agenda over the 1980s to 1990s and became a prominent issue after the terrorist attacks of 2001. The consequences of the Department of Homeland Security's centralized attention to the terrorism threat suggest a paradox of issue attention. Though concentration of authority at the top of the organization holds the prospect of control over the substance and speed of policymaking, this control is highly circumscribed by the limits of attention faced by all organizations.\n\n==\n\nG\u00f3mez 2018:\n\nTitle: Just Paying Attention: Communication for Organizational Attention\n\nAbstract: The main premises in this article are that organizational attention is inherently communicative, and can be nurtured through communication interventions. Two communication practices that reflect organizational attention\u2014information allocation and dialogue\u2014can be nurtured through organizational structures and interventions. Increasing opportunities for dialogue across organizational functions is critical to improve collective attention. Prior research and empirical data are presented to assert that a long-term orientation is also imperative to develop attention through communication practices such as information allocation and dialogue.\n\n==\n\nRamos 2019:\n\nTitle: The Impact of using IT artefacts on Organizational Attention: the Case of a City Hall\n\nAbstract: Using the concept of organizational attention proposed by William Ocasio (1997), which states that formal, informal and unofficial communication engages collective attention in issues and action alternatives relevant for decision, our study investigates the role played by IT artefacts in shaping attentional engagement. Based on a case study of the City Planning Division of the Porto City Council in Portugal, our findings reveal that IT artefacts channel attentional engagement, expanding or hindering the ability to understand situations. When shifts in attention are promoted by a changing strategy (the dominant pattern of attention) without the corresponding change in the IT infrastructure, serious difficulties may arise in sharing perceptions that strongly bind collective cognition and action to the achievement of organizational goals (collective attentional engagement). In order to circumvent arising problems, unofficial IT-supported communicative practices tend to emerge. Our exploratory research extends the literature on organizational attention and information systems by providing insights on the impact of IT use on organizational attention.\n\n==\n\nFerreira 2010:\n\nTitle: Attention Mosaics: Studies of Organizational Attention\n\nAbstract: textabstractOrganizational studies emphasizing the role of attention in organizational behavior depart from the idea that organizations, like individuals, have limited capacity to attend to environmental stimuli. The bounded capacity of the organizations to respond to stimuli is conditioned by the limited cognitions of individuals and by the limited capability of organizations to distribute, coordinate and integrate those cognitions. The cross-level nature of organizational attention, its dual character as both a process and an output, means that theories of attention afford interesting insights to explain organizational behavior.\nThis dissertation presents one conceptual and two empirical studies about organizational attention. In the conceptual study entitled \u201cAttention span: expanding the attention-based view to team, organizational and social movements levels\u201d, it is argued that attentional processes have functional equivalence at the team, organizational and social movements level. The study entitled \u201cWhen a thousand words are (not) enough: an empirical study of the relationship between firm performance and attention to shareholders\u201d, tests the power of the attention-based view combined with resource dependence theory to explain the relationship between financial performance and attention to shareholders. Finally, the study \u201cSense and sensibility: testing the effects of attention structures and organizational attention on financial performance\u201d tests the process model of situated attention by examining the effects of attention structures and the allocation of attention on organizational social responses and performance/\nTogether, these studies deepen and expand attentional perspectives on organizational behavior. Moreover, they renew scholars\u2019 interest in organizational attention, indicating some of the strengths and limitations of theories of attention and also revealing a prolific research stream.\n\n==\n\nDessein 2013:\n\nTitle: Rational Inattention and Organizational Focus\n\nAbstract: This paper studies optimal communication flows in organizations. A production process can be coordinated ex ante, by letting agents stick to a prespecified plan of action. Alternatively, agents may adapt to task-specific shocks, in which case tasks must be coordinated ex post, using communication. When attention is scarce, an optimal organization coordinates only a few tasks ex post. Those tasks are higher performing, more adaptive to the environment, and influential. Hence, scarce attention requires setting priorities, not just local optimization. Our results provide microfoundations for a central idea in the management literature that firms should focus on a limited set of core competencies.\n\n==\n\nForster 2016:\n\nTitle: Establishing the Attention-Distractibility Trait\n\nAbstract: Failures to focus attention will affect any task engagement (e.g., at work, in the classroom, when driving). At the clinical end, distractibility is a diagnostic criterion of attention-deficit/hyperactivity disorder (ADHD). In this study, we examined whether the inability to maintain attentional focus varies in the overall population in the form of an attention-distractibility trait. To test this idea, we administered an ADHD diagnostic tool to a sample of healthy participants and assessed the relationship between ADHD symptoms and task distraction. ADHD symptom summary scores were significantly positively associated with distractor interference in letter-search and name-classification tasks (as measured by reaction time), as long as the distractors were irrelevant (cartoon images) rather than relevant (i.e., compatible or incompatible with target names). Higher perceptual load during a task eliminated distraction irrespective of ADHD score. These findings suggest the existence of an attention-distractibility trait that confers vulnerability to irrelevant distraction, which can be remedied by increasing the level of perceptual load during the task.",
        "final_summary": "Organizational attention is a cognitive process that plays a crucial role in various organizational phenomena, with its relevance to selective and divided attention highlighted by Yaniv (2011). May (2007) theorizes that the organization of attention within public bureaucracies influences their responses to policy demands. G\u00f3mez (2018) posits that organizational attention is inherently communicative and can be nurtured through communication practices such as information allocation and dialogue. Ramos (2019) investigates the impact of IT artifacts on organizational attention, emphasizing the role of IT use in shaping attentional engagement. Ferreira (2010) presents studies on organizational attention, demonstrating its cross-level nature and its effects on firm performance. Dessein (2013) examines optimal communication flows in organizations, highlighting the need to set priorities and focus on a limited set of core competencies when attention is scarce. Lastly, Forster (2016) explores the attention-distractibility trait, suggesting that failures to focus attention can vary in the overall population and are associated with vulnerability to irrelevant distraction."
    },
    {
        "query": "organizational attention definition Ocasio",
        "paper_list_string": "Yaniv 2011:\n\nTitle: Organizational Attention\n\nAbstract: INTRODUCTION Attention is a term commonly used in education, psychiatry, and psychology. Attention can be defined as an internal cognitive process by which one actively selects environmental information (i.e., sensation) or actively processes information from internal sources (i.e., stored memories and thoughts; Sternberg, 1996). In more general terms, attention can be defined as an ability to focus and maintain interest in a given task or idea, including managing distractions. Attention is selective by its nature. According to Pashler (1998, p. 37), \" The process of selecting from among the many potentially available stimuli is the clearest manifestation of selective attention. \" Why do firms respond to certain events or stimuli in their environment while neglecting others? It seems that organizations, just like individuals , have limited attention capacity. Hence, they must select from among the many potentially available stimuli and respond to these selected stimuli only. Organizational attention is defined as the socially structured pattern of attention by decision makers within the organization (Ocasio, 1997). Organizational attention, like human attention , is a limited resource: \" Attentional limits filter or screen incoming information such that a great deal of data pertinent to strategic decision may never get processed \" (2003) show that the extent to which CEOs (chief executive officers) are selective in their attention to sectors of the environment is a significant predictor of performance. Knowledge management (KM) models and process theories, almost without exception, incorporate a stage or phase in which a given knowledge item is brought to bear on a current decision or action. This stage, referred to alternatively as, is of crucial importance in any knowledge-management cycle. The flow of knowledge in and out of an awareness stage is not merely a function of the universe of available organizational memory or the technological tools available to filter and identify such knowledge. It is influenced to a large degree by organizational attention. The second area in which organizational attention is key is knowledge acquisition and creation as discussed by Ocasio (1997), and Yaniv and Elizur (2003). Successful knowledge management requires attention. Davenport and Volpel (2001) argues that attention is the currency of the information age. Knowledge consumers must pay attention to knowledge and become actively involved in the knowledge-transfer processes. This is particularly important when the knowledge to be received is tacit (Nonaka, 1994). Knowledge can be part of the organization's repository, however, if it does not \u2026\n\n==\n\nRamos 2019:\n\nTitle: The Impact of using IT artefacts on Organizational Attention: the Case of a City Hall\n\nAbstract: Using the concept of organizational attention proposed by William Ocasio (1997), which states that formal, informal and unofficial communication engages collective attention in issues and action alternatives relevant for decision, our study investigates the role played by IT artefacts in shaping attentional engagement. Based on a case study of the City Planning Division of the Porto City Council in Portugal, our findings reveal that IT artefacts channel attentional engagement, expanding or hindering the ability to understand situations. When shifts in attention are promoted by a changing strategy (the dominant pattern of attention) without the corresponding change in the IT infrastructure, serious difficulties may arise in sharing perceptions that strongly bind collective cognition and action to the achievement of organizational goals (collective attentional engagement). In order to circumvent arising problems, unofficial IT-supported communicative practices tend to emerge. Our exploratory research extends the literature on organizational attention and information systems by providing insights on the impact of IT use on organizational attention.\n\n==\n\nYaniv 2011:\n\nTitle: ORGANIZATIONAL ATTENTION: A METAPHOR FOR A CORE COGNITIVE PROCESS\n\nAbstract: Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen\u0219s domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.\n\n==\n\nEberhard 2013:\n\nTitle: Unpacking the Conceptual Linkages Between Organizational Attention and Sensemaking\n\nAbstract: \u201cFurther examining the relationship between sensemaking and attention may provide additional insights because sensemaking is particularly consequential when rare or critical events make top-down attention insufficient.\u201d (Ocasio, 2011) The purpose of the symposium is to openly debate the relatively unexplored but critical relationship between organizational attention and sensemaking (Ocasio, 1997; Weick, 1995) and to describe contexts and approaches for empirical work most likely to yield further insights. Challenges to this conversation include bridging the multiplicity of perspectives and language in the field of attention (Ocasio, 2011), crossing levels of analysis between attention and sensemaking approaches (Maitlis & Sonenshein, 2010; Weber & Glynn, 2006), and defining where attention ends and sensemaking begins (or vice versa) and whether the two constructs intersect, interact, or operate in parallel. We present three case studies of rare events that cross levels of analysis and temporal contexts, a...\n\n==\n\nG\u00f3mez 2018:\n\nTitle: Just Paying Attention: Communication for Organizational Attention\n\nAbstract: The main premises in this article are that organizational attention is inherently communicative, and can be nurtured through communication interventions. Two communication practices that reflect organizational attention\u2014information allocation and dialogue\u2014can be nurtured through organizational structures and interventions. Increasing opportunities for dialogue across organizational functions is critical to improve collective attention. Prior research and empirical data are presented to assert that a long-term orientation is also imperative to develop attention through communication practices such as information allocation and dialogue.\n\n==\n\nFerreira 2010:\n\nTitle: Attention Mosaics: Studies of Organizational Attention\n\nAbstract: textabstractOrganizational studies emphasizing the role of attention in organizational behavior depart from the idea that organizations, like individuals, have limited capacity to attend to environmental stimuli. The bounded capacity of the organizations to respond to stimuli is conditioned by the limited cognitions of individuals and by the limited capability of organizations to distribute, coordinate and integrate those cognitions. The cross-level nature of organizational attention, its dual character as both a process and an output, means that theories of attention afford interesting insights to explain organizational behavior.\nThis dissertation presents one conceptual and two empirical studies about organizational attention. In the conceptual study entitled \u201cAttention span: expanding the attention-based view to team, organizational and social movements levels\u201d, it is argued that attentional processes have functional equivalence at the team, organizational and social movements level. The study entitled \u201cWhen a thousand words are (not) enough: an empirical study of the relationship between firm performance and attention to shareholders\u201d, tests the power of the attention-based view combined with resource dependence theory to explain the relationship between financial performance and attention to shareholders. Finally, the study \u201cSense and sensibility: testing the effects of attention structures and organizational attention on financial performance\u201d tests the process model of situated attention by examining the effects of attention structures and the allocation of attention on organizational social responses and performance/\nTogether, these studies deepen and expand attentional perspectives on organizational behavior. Moreover, they renew scholars\u2019 interest in organizational attention, indicating some of the strengths and limitations of theories of attention and also revealing a prolific research stream.\n\n==\n\nMarkovska-Simoska 2013:\n\nTitle: \u041erganizational Attention Deficit as Sustainability Indicator: Assessment and Management\n\nAbstract: The aim of this paper is to contribute towards dealing with the at- tention indicator in a standard Pressure-State-Response (PSR) framework of sustainable development. Hence, the daily information overflow is identified as a pressure variable adversely affecting the attention. This is manifested by an increased likelihood of missing key information when making decisions, diminished time for attentive reflection, difficulty in holding others' attention, as well as by decreased ability to focus when necessary, which are symptoms of the so-called organizational attention deficit disorder - OADD. As to the state variable there were combined techniques from cognitive neuroscience and applied sciences in order to objectively measure the incidence of OADD. In particular, the use of neuropsychological procedures for objective assessment of OADD will be summarized. Namely, few attention diagnostic tests of the Amsterdam Neuropsychological Tasks Program (ANT) will be considered for determining the focused, shifted and sustained attention. As intervention measure attention strengthening techniques is applied. In particular, in re- search were proposed the neurofeedback technique as response variable and have shown its effectiveness in coping with the OADD. This approach repre- sents a new perspective in social psychophysiology and expands the potential applications of ANT and neurofeedback methodology. 1 Macedonian Academy of Sciences and Arts, Skopje, Republic of Macedonia,\n\n==\n\nOrgan 1997:\n\nTitle: Organizational Citizenship Behavior: It's Construct Clean-Up Time\n\nAbstract: Accumulated empirical evidence, some telling criticisms, and even the most cursory glance at the business press compel us to rethink the defining character of organizational citizenship behavior (OCB). It no longer seems fruitful to regard OCB as extra-role, beyond the job, or unrewarded by the formal system. A more tenable position is one that defines OCB much along the lines of what Borman and Motowidlo (1993) called contextual performance. Some preliminary suggestions are offered for the repositioning and articulation of the OCB construct as redefined; due attention is given to the problems that nonetheless remain, and will occupy us for some time to come as we reckon with root changes in the very character of organizations.",
        "final_summary": "The papers collectively provide insights into the concept of organizational attention as defined by Ocasio. Yaniv (2011) discusses attention as a cognitive process that involves selecting and processing information from the environment or internal sources. Ramos (2019) explores the role of IT artifacts in shaping attentional engagement within organizations. Ferreira (2010) emphasizes the cross-level nature of organizational attention and its impact on behavior. G\u00f3mez (2018) highlights the communicative nature of organizational attention and the importance of information allocation and dialogue. Markovska-Simoska (2013) examines organizational attention deficit as an indicator of sustainability. Overall, these papers contribute to our understanding of organizational attention and its various dimensions."
    },
    {
        "query": "Amylase-Producing Bacteria",
        "paper_list_string": "Ekka 2018:\n\nTitle: Screening , Isolation and Characterization of Amylase Producing Bacteria and optimization for Production of Amylase\n\nAbstract: Amylase is (E.C.3.2.1.1-1,4-alpha D-glucanohydrolase) an extracellular enzyme, which is involved in the starch processing industries where it breaks starch into simple sugar constituents.Amylase has extensive application in starch processing, brewing and sugar production, in textile industries and in detergent manufacturing processes. Interestingly, the first enzyme produced industrially was an amylase.In the present study, amylase producing bacteria were isolated from rice field, sugarcane field and sugarcane dump area and characterized for their morphological and biochemical properties. Then amylase activity of isolated bacterial cultures were determined and it was concluded that 3 (NN1, NN2, NN5)out of 6 bacterial colonies(NN1, NN2, NN3, NN4, NN5, NN6) were potent and their enzyme activity was more than other colonies. The potent colonies were also optimized for enzyme activity under certain conditions like different carbon sources, nitrogen sources, pH, incubation time and chlorides.Agro-industrial wastes were used as substrate for amylase production by Solid-State FermentationSSF) and we have found that wheat bran was the suitable substrate for amylase production.\n\n==\n\nGarba 2021:\n\nTitle: Preliminary Investigation of Amylase Producing-Bacteria from Soil in Gombe Metropolis\n\nAbstract: Amylases are enzymes that are able to hydrolyse starch or glycogen molecules into polymers of glucose units. They have great potential applications in various industrial processes like in pharmaceutical, fermentation and food industries. Research on starch degrading enzymes has resulted into increased applications of amylases in different industrial processes. These enzymes occupy a greater space in the current biotechnological processes such as detergent, starch degradation, pharmaceutical, foodstuff, textile, and paper manufacturing. In fact, amylases constitute nearly 25% of the total sale of global enzymes. Amylases have been screened and identified from various sources, both eukaryotic and prokaryotic organisms such as animals, plants, fungi and bacteria, respectively. To further isolate novel amylases with enhanced desirable properties for such diverse industrial application, more organisms need to be screened. In this study, a total of 27 bacterial isolates were isolated from soil samples in Gombe metropolis. The bacteria were screened for amylase production using plate screening method. Each isolate was streaked onto a 1% starch agar plate and incubated for 24h at 37 \u00c2\u00b0C. The plates were covered with iodine solution and observed for positive amylase isolates based on the formation of clearing zones against the blue black background. The results confirmed eight (8) isolates of amylase-producing bacteria which include Bacillus subtilis, Escherichia coli, Streptococcus spp., Salmonella spp., Pseudomonas spp., Serratia spp., Proteus vulgaris, and Klebsiella spp. In conclusion, bacterial isolates capable of amylase production have been successfully screened and identified. This research may serve as a stepping stone to isolating functional amylase enzymes from these bacteria for promising industrial applications.\n\n==\n\nSamanta 2013:\n\nTitle: Characterization and Optimization of Amylase Producing Bacteria Isolated from Solid Waste\n\nAbstract: Municipal waste is one of the most hazardous components of developing countries. However, enzymes do provide an eco-friendly solution in this case. Amylase is an important enzyme in food, textile and pharmaceutical industry and can be used for bioconversion of waste. From the municipal solid waste we have isolated an amylase producing bacteria that can grow in the irritant municipal waste and help in their bio conversation. The bacteria were identified as Cronobacter sakazakii Jor52 (C2). The optimized media for maximum amylase production after 24 h of incubation, contains 2% starch, 0.6% peptone, 0.01% CaCl2, 0.05% KCl, 0.05% MgSO4 and 0.05% K2HPO4. The crude enzyme activity and stability study revealed that the amylase is stable within the pH 6 - 8 and temperature 30\u00b0C - 40\u00b0C and give maximum activity at 37\u00b0C at pH-8.\n\n==\n\nSyed 2013:\n\nTitle: Isolation of Amylase Producing Bacteria from Solar Salterns of Nellore District, Andhra Pradesh, India\n\nAbstract: \u03b1-Amylases are a class of starch degrading enzymes catalyzing the hydrolysis of internal \u03b1-1,4-O-glycosidic bonds in polysaccharides. The following investigation were carried out to isolate haloalkaliphilic bacteria, a group of organisms with twin extremities of pH and salinity, capable of producing \u03b1-amylases from an artificial solar saltern. A total of 25 discrete colonies were isolated, 21 isolates showed amylase production. Among these 7 isolates produced amylase at extreme conditions such as salt, alkalinity and temperature. The isolates were characterized biochemically and also for other enzymes. From the results it is imperative that these isolates can be further studied to exploit them up to industrial scale.\n\n==\n\nShah 2012:\n\nTitle: Characterization of Amylase Producing Bacterial Isolates\n\nAbstract: Amylases are among the most important industrial enzymes and also have great significance in Biotechnological studies. In this study cultural, morphological, and metabolic characteristics of the bacterial isolates were studied. Total 18 bacterial cultures were isolated from collected soil samples. Among 18 bacterial isolates, 14 isolates showed the amylolytic activity. These 18 isolate was identified according to Bergey\u2019s manual of systemic Bacteriology .These isolates related to Bacillus sp. The optimum pH for the growth of all the cultures was observed at pH 7. Submerged fermentation was carried out for the production of amylase was observed in the range of 0.045-1.35 U/min/mL. The maximum activity of amylase was 1.35 (U/min/mL) after 48 hours was recorded, have great significance.\n\n==\n\nLuang-In 2019:\n\nTitle: Isolation and Identification of Amylase-producing Bacteria from Soil in Nasinuan Community Forest, Maha Sarakham, Thailand\n\nAbstract: This study aimed to isolate and identify bacteria that can produce amylase enzyme from the unexplored Nasinuan Forest, Kantarawichai District, Mahasarakham Province, Thailand. Thirteen bacterial isolates with amylase-producing capacity on 1% starch agar were identified using 16S rRNA sequencing. Twelve bacteria were gram-positive, rod shaped and identified as Bacillus spp. and one bacterium with gram-negative and rod shaped character was Enterobacter cloacae. Their closest relatives were found in India, China, Korea, Indonesia, Argentina, Italy, Israel, USA, Argentina and South Africa. These bacteria were tested for specific amylase activity after 1-3 days enzyme induction with 1% starch at 37\u00b0C. The results showed the highest specific activity at day 2 incubation in the order: Bacillus cereus 3.5AL2 > 3.4AL1 > 1.4AL3 and thus 2-day enzyme induction was chosen for further analysis. Bacillus sp. 3.5AL2 was found to exhibit the highest specific amylase enzyme activity of 1.97 \u00b1 0.41 U/mg protein at the optimal conditions of 60\u00b0C and pH 7.0 after 30 min incubation with 1% starch in 0.05 M PBS buffer. This amylase\u2013producing bacterial strain offers great potential for applications in food and agricultural industries in Thailand.\n\n==\n\nIndriati 2018:\n\nTitle: Potency of Amylase-producing Bacteria and Optimization Amylase Activities\n\nAbstract: Enzymes are capable to act as biocatalyst for a wide variety of chemical reactions. Amylase have potential biotechnological applications in a wide range of industrial processes and account for nearly 30% of the world\u2019s enzyme market. Amylase are extracellular enzymes that catalyze the hydrolysis of internal \u03b1-1,4-glycosidic linkages in starch to dextrin, and other small carbohydrate molecules constituted of glucose units. Although enzymes are produced from animal and plant sources, the microbial sources are generally the most suitable for commercial applications. Bacteria from hot springs is widely used as a source of various enzymes, such as amylase. But the amount of amylase-producing bacteria is still very limited. Therefore it is necessary to search sources of amylase-producing bacteria new, such as from hot springs Pariangan. The purpose of this study was to isolation of amylase-producing bacteria from Pariangan hot spring, West Sumatera and amylase activity optimization. The results were obtained 12 isolates of thermophilic bacteria and 5 isolates of amyalse-producing bacteria with the largest amylolytic index of 3.38 mm. The highest amylase activity was obtained at 50\u00b0C and pH 7.5.\n\n==\n\nShanmugasundaram 2015:\n\nTitle: Screening and Identification of Amylase Producing Bacteria from Marakkanam Saltpan Environment, Tamil Nadu, India.\n\nAbstract: Saltpans are an extreme environments, that inhabits organisms are survive in terribly high level of salinities, high temperatures and endure severe solar radiations. The present study was carried out to isolate, screen and optimize amylase enzyme producing bacteria from saltpan environment at Marakkanam. The results of present study was revealed that a total 25 bacterial strains were isolated of which 12 strains were produced significant amount of starch hydrolysis. Given value of zone of inhibitions are in the range of 1.2 mm to 13.1 mm. The isolated SUS3, strain was identified as Bacillus subtilis using Bergy\u2019s manual 1994. The present study disclosed that this thermo stable amylase producing strain could also be helpful industrial application particularly in food and beverages, animal feed, brewing textiles, detergents and health care",
        "final_summary": "The papers collectively provide information on amylase-producing bacteria. Ekka 2018 isolated and characterized amylase-producing bacteria from different environments and optimized their enzyme production. Garba 2021 screened soil bacteria and confirmed the presence of amylase-producing bacteria. Samanta 2013 isolated amylase-producing bacteria from municipal solid waste and optimized their enzyme production. Syed 2013 isolated haloalkaliphilic bacteria from a solar saltern and found amylase production under extreme conditions. Shah 2012 characterized amylase-producing bacterial isolates from soil samples. Luang-In 2019 isolated and identified amylase-producing bacteria from a community forest. Indriati 2018 isolated amylase-producing bacteria from a hot spring and optimized amylase activity. Shanmugasundaram 2015 screened and identified amylase-producing bacteria from a saltpan environment. These papers collectively demonstrate the diversity of amylase-producing bacteria and their potential for various industrial applications."
    },
    {
        "query": "monetary policy, bank assets",
        "paper_list_string": "Dang 2020:\n\nTitle: Monetary policy, bank leverage and liquidity\n\nAbstract: PurposeThe study explores how banks design their financial structure and asset portfolio in response to monetary policy changes.Design/methodology/approachThe authors conduct the research design for the Vietnamese banking market during 2007\u20132018. To ensure robust findings, the authors employ two econometric models of static and dynamic panels, multiple monetary policy indicators and alternative measures of bank leverage and liquidity.FindingsBanks respond to monetary expansion by raising their financial leverage on the liability side and cutting their liquidity positions on the asset side. Further analysis suggests that larger banks' financial leverage is more responsive to monetary policy changes, while smaller banks strengthen the potency of monetary policy transmission toward bank liquidity. Additionally, the authors document that lower interest rates induce a beneficial effect on the net stable funding ratio (NSFR) under Basel III guidelines, implying that banks appear to modify the composition of liabilities to improve the stability of funding sources.Originality/valueThe study is the first attempt to simultaneously examine the impacts of monetary policy on both sides of bank balance sheets, across various banks of different sizes under a multiple-tool monetary regime. Besides, understanding how banks organize their stable funding sources and illiquid assets amid monetary shocks is an innovation of this study.\n\n==\n\nAikman 2006:\n\nTitle: Bank Capital, Asset Prices and Monetary Policy\n\nAbstract: We study a general equilibrium model in which informational frictions impede entrepreneurs' ability to borrow and banks' ability to intermediate funds. These financial market frictions are embedded in an otherwise-standard dynamic New Keynesian model. We find that exogenous shocks have an amplified and more persistent effect on output and investment, relative to the case of perfect capital markets. The chief contribution of the paper is to analyse how these financial sector imperfections - in particular, those relating to the banking sector - modify our understanding of optimal monetary policy. Our main finding is that optimal monetary policy tolerates only a very small amount of inflation volatility. Given that similar results have been reported for models that abstract from banks, we conclude that assigning a non-trivial role for banks need not materially affect the properties of optimal monetary policy.\n\n==\n\nGambacorta 2018:\n\nTitle: Why Bank Capital Matters for Monetary Policy\n\nAbstract: One aim of post-crisis monetary policy has been to ease credit conditions for borrowers by unlocking bank lending. We find that bank equity is an important determinant of both the bank's funding cost and its lending growth. In a cross-country bank-level study, we find that a 1 percentage point increase in the equity-to-total assets ratio is associated with a 4 basis point reduction in debt financing and with a 0.6 percentage point increase in annual loan growth.\n\n==\n\nMcMillin 1996:\n\nTitle: Monetary policy and bank portfolios\n\nAbstract: Abstract This paper examines the existence of the bank lending channel for monetary policy over the period 1973:1\u20131994:11. The results are consistent with a bank lending channel when the Bernanke-Blinder model is extended to include commercial paper and the spread between the loan and commercial paper rates. The results are robust to alternative monetary policy measures. However, stability tests indicate instability over the nonborrowed reserves operating regime. When the estimates excluded data for this period, there was little evidence of systematic movement in bank loans in the direction predicted by the bank lending channel.\n\n==\n\nHeuvel 2006:\n\nTitle: The Bank Capital Channel of Monetary Policy\n\nAbstract: This paper examines the role of bank lending in the transmission of monetary policy in the presence of capital adequacy regulations. I develop a dynamic model of bank asset and liability management that incorporates risk-based capital requirements and an imperfect market for bank equity. These conditions imply a failure of the Modigliani-Miller theorem for the bank: its lending will depend on the bank\u00e2\u20ac\u2122s financial structure, as well as on lending opportunities and market interest rates. Combined with a maturity mismatch on the bank\u00e2\u20ac\u2122s balance sheet, this gives rise to a \u00e2\u20ac\u02dcbank capital channel\u00e2\u20ac\u2122 by which monetary policy affects bank lending through its impact on bank equity capital. This mechanism does not rely on any particular role of bank reserves and thus falls outside the conventional \u00e2\u20ac\u02dcbank lending channel\u00e2\u20ac\u2122. I analyze the dynamics of the new channel. An important result is that monetary policy effects on bank lending depend on the capital adequacy of the banking sector; lending by banks with low capital has a delayed and then amplified reaction to interest rate shocks, relative to well-capitalized banks. Other implications are that bank capital affects lending even when the regulatory constraint is not momentarily binding, and that shocks to bank profits, such as loan defaults, can have a persistent impact on lending\n\n==\n\nBernanke 2000:\n\nTitle: Monetary Policy and Asset Price Volatility\n\nAbstract: We explore the implications of asset price volatility for the management of monetary policy. We show that it is desirable for central banks to focus on underlying inflationary pressures. Asset prices become relevant only to the extent they may signal potential inflationary or deflationary forces. Rules that directly target asset prices appear to have undesirable side effects. We base our conclusions on (i) simulation of different policy rules in a small scale macro model and (ii) a comparative analysis of recent U.S. and Japanese monetary policy.\n\n==\n\nBianchi 2014:\n\nTitle: Banks, Liquidity Management and Monetary Policy\n\nAbstract: We develop a new framework for studying the implementation of monetary policy through the banking sector. Banks are subject to a maturity mismatch problem leading to precautionary holdings of reserves. Through various instruments, monetary policy alters tradeos banks face between lending, holding reserves, holding deposits and paying dividends. This translates into the real economy via eects on real interests and lending. We study how these instruments interact with shocks to the volatility in the payments system, bank losses, the demand for loans and with capital requirements. We use a calibrated version of the model to answer, quantitatively, why have banks held onto a substantial increase in reserves while not increasing lending since 2008.\n\n==\n\nC\u00fardia 2010:\n\nTitle: The Central-Bank Balance Sheet as an Instrument of Monetary Policy\n\nAbstract: While many analyses of monetary policy consider only the adjustment of a central bank\u2019s target for a short-term nominal interest rate, other dimensions of policy have recently been of greater importance: changes in the supply of bank reserves beyond those required to achieve an interest-rate target, changes in the assets acquired by central banks, and changes in the interest rate paid on reserves. We extend a standard New Keynesian model to allow a role for the central bank\u2019s balance sheet in equilibrium determination, and consider the connections between these alternative dimensions of policy and traditional interest-rate policy. We distinguish between \\quantitative easing\" in the strict sense and targeted asset purchases by a central bank, and argue that while the former is likely be inefiective at all times, the latter dimension of policy can be efiective when flnancial markets are su\u2010ciently disrupted. Neither is a perfect substitute for conventional interest-rate policy, but purchases of illiquid assets are particularly likely to improve welfare when the zero lower bound on the policy rate is reached. We also consider optimal policy with regard to the payment of interest on reserves, and argue that the interest rate on reserves should be kept near the central bank\u2019s target for the policy rate at all times.",
        "final_summary": "Monetary policy impacts bank assets in various ways. Dang (2020) found that banks increase their financial leverage and decrease their liquidity positions in response to monetary expansion. Aikman (2006) emphasized that financial market frictions, especially those related to the banking sector, alter our understanding of optimal monetary policy. Gambacorta (2018) identified bank equity as a crucial factor influencing both the cost of bank funding and the growth of lending. McMillin (1996) provided evidence for a bank lending channel for monetary policy. Heuvel (2006) proposed a separate concept, the bank capital channel, where monetary policy influences bank lending via its impact on bank equity capital. Bernanke (2000) argued that central banks should concentrate on underlying inflationary pressures rather than directly targeting asset prices. Bianchi (2014) examined how monetary policy is implemented through the banking sector, considering the trade-offs banks face between lending, holding reserves, and paying dividends. Lastly, C\u00fardia (2010) discussed the role of the central bank's balance sheet in monetary policy, including the effectiveness of quantitative easing and targeted asset purchases."
    },
    {
        "query": "topological mapping with encoded latent vectors from inverse camera projections",
        "paper_list_string": "Bertrand 2001:\n\nTitle: Topological Map: Minimal Encoding of 3d Segmented Images\n\nAbstract: In this paper we define formally and completely the 3d topological map introduced in BertrandAl00: a model which encodes totally and minimally all the topological information of a 3d image. In particular, we focus on the problem of disconnections induced by the constructive definition based on several levels of maps. This multilevel representation is more or less a graph pyramid in sense that each level can be compute from the previous one in term of merge operations. Furthermore, algorithms extracting these maps from a segmented image have been given in BertrandAl00 and have been implemented and tested in practical applications.\n\n==\n\nKorrapati 2012:\n\nTitle: Topological Mapping with Image Sequence Partitioning\n\nAbstract: Topological maps are vital for fast and accurate localization in large environments. Sparse topological maps can be constructed by partitioning a sequence of images acquired by a robot, according to their appearance. All images in a partition have similar appearance and are represented by a node in a topological map. In this paper, we present a topological mapping framework which makes use of image sequence partitioning (ISP) to produce sparse maps. The framework facilitates coarse loop closure at node level and a finer loop closure at image level. Hierarchical inverted files (HIF) are proposed which are naturally adaptable to our sparse topological mapping framework and enable efficient loop closure. Computational gain attained in loop closure with HIF over sparse topological maps is demonstrated. Experiments are performed on outdoor environments using an omni-directional camera.\n\n==\n\nMontijano 2011:\n\nTitle: Distributed multi-camera visual mapping using topological maps of planar regions\n\nAbstract: This paper presents a multi-agent solution for cooperative visual mapping using planar regions. Each agent is assumed to be equipped with a conventional camera and has limited communication capabilities. Our approach starts building topological maps from independent image sequences where natural landmarks extracted from conventional images are grouped to create a graph of planes. With this approach the features observed in several images belonging to the same planar region are stored only once, reducing the size of the individual maps. In a distributed scenario this is very important because smaller maps can be transmitted faster, which makes our approach better suited for cooperative mapping. The later fusion of the individual maps is obtained via distributed consensus without any initial information about the relations between the different maps. Experiments with real images in complex scenarios show the good performance of our proposal.\n\n==\n\nLiu 2014:\n\nTitle: Topological Mapping and Scene Recognition With Lightweight Color Descriptors for an Omnidirectional Camera\n\nAbstract: Scene recognition problems for mobile robots have been extensively studied. This is important for tasks such as visual topological mapping. Usually, sophisticated key-point-based descriptors are used, which can be computationally expensive. In this paper, we describe a lightweight novel scene recognition method using an adaptive descriptor, which is based on color features and geometric information that are extracted from an uncalibrated omnidirectional camera. The proposed method enables a mobile robot to perform online registration of new scenes onto a topological representation automatically and solve the localization problem to topological regions simultaneously, all in real time. We adopt a Dirichlet process mixture model (DPMM) to describe the online inference process. It is based on an approximation of conditional probabilities of the new measurements given incrementally estimated reference models. It enables online inference speeds of up to 50 Hz for a normal CPU. We compare it with state-of-the-art key-point descriptors and show the advantage of the proposed algorithm in terms of performance and computational efficiency. A real-world experiment is carried out with a mobile robot equipped with an omnidirectional camera. Finally, we show the results on extended datasets.\n\n==\n\nKorrapati 2011:\n\nTitle: Efficient Topological Mapping with Image Sequence Partitioning\n\nAbstract: Topological maps are vital for fast and accurate localization in large environments. Sparse topological maps can be constructed by partitioning a sequence of images acquired by a robot, according to their appearance. All images in a partition have similar appearance and are represented by a node in a topological map. In this paper, we present a topological mapping framework which makes use of image sequence partitioning (ISP) to produce sparse maps. The framework facilitates coarse loop closure at node level and a finer loop closure at image level. Hierarchical inverted files (HIF) are proposed which are naturally adaptable to our sparse topological mapping framework and enable efficient loop closure. Computational gain attained in loop closure with HIF over sparse topological maps is demonstrated. Experiments are performed on outdoor environments using an omnidirectional camera.\n\n==\n\nStimec 2008:\n\nTitle: Unsupervised Learning of a Hierarchy of Topological Maps Using Omnidirectional Images\n\nAbstract: This paper presents a novel appearance-based method for path-based map learning by a mobile robot equipped with an omnidirectional camera. In particular, we focus on an unsupervised construction of topological maps, which provide an abstraction of the environment in terms of visual aspects. An unsupervised clustering algorithm is used to represent the images in multiple subspaces, forming thus a sensory grounded representation of the environment's appearance. By introducing transitional fields between clusters we are able to obtain a partitioning of the image set into distinctive visual aspects. By abstracting the low-level sensory data we are able to efficiently reconstruct the overall topological layout of the covered path. After the high level topology is estimated, we repeat the procedure on the level of visual aspects to obtain local topological maps. We demonstrate how the resulting representation can be used for modeling indoor and outdoor environments, how it successfully detects previously visited locations and how it can be used for the estimation of the current visual aspect and the retrieval of the relative position within the current visual aspect.\n\n==\n\nBertrand 2000:\n\nTitle: Topological Encoding of 3D Segmented Images\n\nAbstract: In this paper we define the 3d topological map and give an optimal algorithm which computes it from a segmented image. This data structure encodes totally all the information given by the segmentation. More, it allows to continue segmentation either algorithmically or interactively. We propose an original approach which uses several levels of maps. This allows us to propose a reasonable and implementable solution where other approaches don't allow suitable solutions. Moreover our solution has been implemented and the theoretical results translate very well in practical applications.\n\n==\n\nLi 2018:\n\nTitle: Topological Map Extraction From Overhead Images\n\nAbstract: We propose a new approach, named PolyMapper, to circumvent the conventional pixel-wise segmentation of (aerial) images and predict objects in a vector representation directly. PolyMapper directly extracts the topological map of a city from overhead images as collections of building footprints and road networks. In order to unify the shape representation for different types of objects, we also propose a novel sequentialization method that reformulates a graph structure as closed polygons. Experiments are conducted on both existing and self-collected large-scale datasets of several cities. Our empirical results demonstrate that our end-to-end learnable model is capable of drawing polygons of building footprints and road networks that very closely approximate the structure of existing online map services, in a fully automated manner. Quantitative and qualitative comparison to the state-of-the-arts also show that our approach achieves good levels of performance. To the best of our knowledge, the automatic extraction of large-scale topological maps is a novel contribution in the remote sensing community that we believe will help develop models with more informed geometrical constraints.",
        "final_summary": "The papers collectively provide insights into the research question of \"topological mapping with encoded latent vectors from inverse camera projections.\" \n\nBertrand (2001) introduces a 3D topological map that encodes all topological information of a 3D image, focusing on the problem of disconnections induced by the constructive definition based on multiple levels of maps.\n\nKorrapati (2012) presents a topological mapping framework that utilizes image sequence partitioning to construct sparse maps, enabling coarse and fine loop closure. Hierarchical inverted files (HIF) are proposed to facilitate efficient loop closure in sparse topological maps.\n\nMontijano (2011) proposes a distributed multi-camera visual mapping approach using topological maps of planar regions. Independent image sequences are used to build individual maps, and later fusion is achieved via distributed consensus, making it suitable for cooperative mapping.\n\nLiu (2014) describes a lightweight scene recognition method using color features and geometric information from an uncalibrated omnidirectional camera. The method enables online registration of new scenes onto a topological representation and solves the localization problem in real-time.\n\nStimec (2008) presents an unsupervised learning method for constructing a hierarchy of topological maps using omnidirectional images. The approach uses unsupervised clustering to represent images in multiple subspaces, allowing efficient reconstruction of the overall topological layout.\n\nBertrand (2000) defines the 3D topological map and provides an optimal algorithm to compute it from a segmented image, encoding all the information given by the segmentation. The approach utilizes multiple levels of maps to offer a practical solution.\n\nLi (2018) proposes PolyMapper, an approach to extract topological maps from overhead images by predicting objects in a vector representation. The method directly extracts building footprints and road networks, achieving good performance compared to existing approaches.\n\nIn summary, the papers collectively discuss various techniques and frameworks for topological mapping using different approaches such as image sequence partitioning, distributed multi-camera mapping, lightweight scene recognition, unsupervised learning, and vector representation extraction from overhead images."
    },
    {
        "query": "History of Nucleic Acid Vaccines",
        "paper_list_string": "Restifo 2000:\n\nTitle: The promise of nucleic acid vaccines\n\nAbstract: Establishing the effective use of \u2018naked\u2019 nucleic acids as vaccines would undoubtedly be one of the most important advances in the history of vaccinology. While nucleic acids show much promise for use as vaccine vectors in experimental animals, not a single naked nucleic acid vector has been approved for use in humans. Indeed, data from human clinical trials is scant: nucleic acid vaccines have not been clearly demonstrated to have any convincing efficacy in the prevention or treatment of infectious disease or cancer. Here we illustrate possible mechanisms underlying effective nucleic acid vaccination. We focus on progress that has been made in the improvement of their function. Additionally, we identify promising new strategies and try to forecast future developments that could lead to the real success of nucleic acid vaccines in the prevention and treatment of human disease.\n\n==\n\nRando 2022:\n\nTitle: The Coming of Age of Nucleic Acid Vaccines during COVID-19\n\nAbstract: In the 21st century, several emergent viruses have posed a global threat. Each pathogen has emphasized the value of rapid and scalable vaccine development programs. The ongoing SARS-CoV-2 pandemic has made the importance of such efforts especially clear. New biotechnological advances in vaccinology allow for recent advances that provide only the nucleic acid building blocks of an antigen, eliminating many safety concerns. During the COVID-19 pandemic, these DNA and RNA vaccines have facilitated the development and deployment of vaccines at an unprecedented pace. This success was attributable at least in part to broader shifts in scientific research relative to prior epidemics; the genome of SARS-CoV-2 was available as early as January 2020, facilitating global efforts in the development of DNA and RNA vaccines within two weeks of the international community becoming aware of the new viral threat. Additionally, these technologies that were previously only theoretical are not only safe but also highly efficacious. Although historically a slow process, the rapid development of vaccines during the COVID-19 crisis reveals a major shift in vaccine technologies. Here, we provide historical context for the emergence of these paradigm-shifting vaccines. We describe several DNA and RNA vaccines and in terms of their efficacy, safety, and approval status. We also discuss patterns in worldwide distribution. The advances made since early 2020 provide an exceptional illustration of how rapidly vaccine development technology has advanced in the last two decades in particular and suggest a new era in vaccines against emerging pathogens.\n\n==\n\nRando 2023:\n\nTitle: The Coming of Age of Nucleic Acid Vaccines during COVID-19\n\nAbstract: The SARS-CoV-2 pandemic has caused untold damage globally, presenting unusual demands on but also unique opportunities for vaccine development. The development, production, and distribution of vaccines are imperative to saving lives, preventing severe illness, and reducing the economic and social burdens caused by the COVID-19 pandemic. ABSTRACT In the 21st century, several emergent viruses have posed a global threat. Each pathogen has emphasized the value of rapid and scalable vaccine development programs. The ongoing severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic has made the importance of such efforts especially clear. New biotechnological advances in vaccinology allow for recent advances that provide only the nucleic acid building blocks of an antigen, eliminating many safety concerns. During the COVID-19 pandemic, these DNA and RNA vaccines have facilitated the development and deployment of vaccines at an unprecedented pace. This success was attributable at least in part to broader shifts in scientific research relative to prior epidemics: the genome of SARS-CoV-2 was available as early as January 2020, facilitating global efforts in the development of DNA and RNA vaccines within 2 weeks of the international community becoming aware of the new viral threat. Additionally, these technologies that were previously only theoretical are not only safe but also highly efficacious. Although historically a slow process, the rapid development of vaccines during the COVID-19 crisis reveals a major shift in vaccine technologies. Here, we provide historical context for the emergence of these paradigm-shifting vaccines. We describe several DNA and RNA vaccines in terms of their efficacy, safety, and approval status. We also discuss patterns in worldwide distribution. The advances made since early 2020 provide an exceptional illustration of how rapidly vaccine development technology has advanced in the last 2 decades in particular and suggest a new era in vaccines against emerging pathogens. IMPORTANCE The SARS-CoV-2 pandemic has caused untold damage globally, presenting unusual demands on but also unique opportunities for vaccine development. The development, production, and distribution of vaccines are imperative to saving lives, preventing severe illness, and reducing the economic and social burdens caused by the COVID-19 pandemic. Although vaccine technologies that provide the DNA or RNA sequence of an antigen had never previously been approved for use in humans, they have played a major role in the management of SARS-CoV-2. In this review, we discuss the history of these vaccines and how they have been applied to SARS-CoV-2. Additionally, given that the evolution of new SARS-CoV-2 variants continues to present a significant challenge in 2022, these vaccines remain an important and evolving tool in the biomedical response to the pandemic.\n\n==\n\nVogel 1995:\n\nTitle: Nucleic acid vaccines\n\nAbstract: The use of nucleic acid-based vaccines is a novel approach to immunization that elicits immune responses similar to those induced by live, attenuated vaccines. Administration of nucleic acid vaccines results in the endogenous generation of viral proteins with native conformation, glycosylation profiles, and other posttranslational modifications that mimic antigen produced during natural viral infection. Nucleic acid vaccines have been shown to elicit both antibody and cytotoxic T-lymphocyte responses to diverse protein antigens. Advantages of nucleic acid-based vaccines include the simplicity of the vector, the ease of delivery, the duration of expression, and, to date, the lack of evidence of integration. Further studies are needed to assess the feasibility, safety, and efficacy of this new and promising technology.\n\n==\n\nHoffman 1995:\n\nTitle: Nucleic Acid Malaria Vaccines\n\nAbstract: During the twentieth century, the primary approach to malaria prevention has been to interfere with transmission of the parasite between the infected mosquito and the human host using physical barriers, insecticides and prophylactic drugs. Despite these measures, it is estimated that there are 300-500 new Plasmodium infections and 1-2 million deaths annually due to malaria. Thus, there have been major efforts to develop malaria vaccines. This is thought to be a feasible approach because immunization with radiation-attenuated Plasmodium spp. sporozoites induces sterile protective immunity against malaria in rodents, primates and humans(reviewed in Ref. 2). The complex life cycle of the Plasmodium spp. parasites and the extensive variability among strains of the same Plasmodium species dictate, however, that an effective malaria vaccine will probably need to induce protective antibodies as well as effector CD4+ and CD8+ T lymphocytes specific for variants of multiple antigens expressed at different stages of the life cycle. It is our view that nucleic acid vaccines offer an excellent approach to developing a multivalent vaccine that effectively activates all arms of the immune system. Furthermore, such vaccines may overcome the additional problem inherent in subunit vaccine development: major histocompatibility complex (MHC) restriction of the protective immune responses to identified target epitopes. Supporting data and rationale for our view are presented here. The major emphasis of the work thus far has been to use plasmid DNA to induce protective CD8+ cytotoxic T lymphocytes (CTL), but the induction of protective antibody and CD4+ T cell responses is being investigated.\n\n==\n\nHasan 1999:\n\nTitle: Nucleic acid immunization: concepts and techniques associated with third generation vaccines.\n\nAbstract: A radical change in vaccine methodology arrived nine years ago with the advent of nucleic acid immunization. Aspects such as plasmid design, gene selection, the use of immunostimulatory complexes and clinical trials are discussed in this review. Furthermore, concepts and protocols involved in the construction, evaluation and immunization of a DNA vaccine have been examined as new strategies to enhance this technology continues to grow.\n\n==\n\nChavda 2021:\n\nTitle: Nucleic Acid Vaccines for COVID-19: A Paradigm Shift in the Vaccine Development Arena\n\nAbstract: Coronavirus disease, COVID-19, has touched every country globally except five countries (North Korea, Turkmenistan, Tonga, Tuvalu and Nauru). Vaccination is the most effective method to protect against infectious diseases. The objective is to ensure that everyone has access to a COVID-19 vaccine. The conventional vaccine development platforms are complex and time-consuming to obtain desired approved vaccine candidates through rigorous regulatory pathways. These safeguards guarantee that the optimized vaccine product is safe and efficacious for various demographic populations prior to it being approved for general use. Nucleic acid vaccines employ genetic material from a pathogen, such as a virus or bacteria, to induce an immune response against it. Based on the vaccination, the genetic material might be DNA or RNA; as such, it offers instructions for producing a specific pathogen protein that the immune system will perceive as foreign and mount an immune response. Nucleic acid vaccines for multiple antigens might be made in the same facility, lowering costs even more. Most traditional vaccine regimens do not allow for this. Herein, we demonstrate the recent understanding and advances in nucleic acid vaccines (DNA and mRNA based) against COVID-19, specifically those in human clinical trials.\n\n==\n\nBabiuk 2000:\n\nTitle: Nucleic acid vaccines: research tool or commercial reality.\n\nAbstract: Polynucleotide immunization has captured the imagination of numerous researchers and commercial companies around the world as a novel approach for inducing immunity in animals. Clearly, the 'proof-of-principle' has been demonstrated both in rodents and various animal species. However, to date, no commercial veterinary vaccine has been developed, or to our knowledge, is in the licensing phase. The present review summarizes the types of pathogens and host species for which polynucleotide immunization has been tried. We have tried to identify possible barriers to commercialization of this technology and areas that need attention if this promising technology is ever to become a reality in the commercial arena.",
        "final_summary": "The papers suggest that nucleic acid vaccines have shown promise but have not yet been widely approved or established in human clinical trials. Restifo (2000) highlights the potential of nucleic acid vaccines but notes the lack of convincing efficacy in preventing or treating diseases. Rando (2022) and Rando (2023) discuss the significant role of DNA and RNA vaccines in the rapid development and deployment of vaccines during the COVID-19 pandemic. Vogel (1995) emphasizes the advantages of nucleic acid vaccines, including their ability to elicit immune responses similar to live vaccines. Hoffman (1995) focuses on the potential of nucleic acid vaccines for malaria prevention, particularly in inducing multivalent immune responses. Hasan (1999) discusses the concepts and techniques associated with nucleic acid immunization. Chavda (2021) highlights nucleic acid vaccines as a paradigm shift in vaccine development, specifically for COVID-19. Babiuk (2000) acknowledges the proof-of-principle of polynucleotide immunization but notes the lack of commercial veterinary vaccines. Overall, while nucleic acid vaccines show promise, further research and development are needed for their widespread use. (Restifo 2000; Rando 2022, 2023; Vogel 1995; Hoffman 1995; Hasan 1999; Chavda 2021; Babiuk 2000)"
    },
    {
        "query": "\"Environmental uncertainty would moderate the relationship between demand risk and supply chain disruption impact such that the relationship would become stronger when environmental uncertainty is high.\"",
        "paper_list_string": "Fynes 2004:\n\nTitle: Environmental uncertainty, supply chain relationship quality and performance\n\nAbstract: Abstract Environmental uncertainty is a fact of life in today's supply chains. In this paper we develop a model of environmental uncertainty, supply chain (SC) relationship quality and SC performance. We use data from the electronics sector in Ireland to test our model. Our results provide mixed support for the model, with the moderating role of both demand and supply uncertainty being supported, but technological uncertainty not supported. We reflect on these findings and suggest a research agenda based on our results.\n\n==\n\nSreedevi 2017:\n\nTitle: Uncertainty and supply chain risk: The moderating role of supply chain flexibility in risk mitigation\n\nAbstract: Abstract In order to remain competitive in the market, firms are forced to expand their product offerings and offer high levels of customization, bringing about high uncertainty in their supply chain. Firms that face high environmental uncertainty are increasingly facing higher risks in terms of supply disruptions, production and delivery delays that ultimately result in poor operational performance. This study aims at understanding the antecedents of supply chain operational risk faced by firms and the conditions under which such risks can be mitigated. Using Indian data from the sixth edition of International Manufacturing Strategy Survey (IMSS) and structural equation modeling, we investigate the relationships between environmental uncertainty and supply chain risk and the moderating effect of supply chain flexibility. We identify appropriate types of flexibility to mitigate the three major aspects of supply chain risk: supply risk, manufacturing process risk and delivery risk. Our empirical investigation reveals that uncertainty in the supply chain leads to high supply chain risk; and in uncertain environments, supply and manufacturing flexibility help in reducing the supply and manufacturing process risks respectively. However, our results also indicate that, in emerging markets such as India where logistic infrastructure is less developed, internal capabilities alone may not be sufficient in reducing supply chain delivery risk. Our findings not only contribute towards filling certain gaps in the supply chain risk management literature, but also provide practicing managers and researchers a better understanding of the types of flexibility that can mitigate supply chain risk in different business environments.\n\n==\n\nFord 2015:\n\nTitle: Supply Chain Quality Management and Environmental Uncertainty: A Contingency Perspective\n\nAbstract: This article extends contingency perspectives of quality management to the interorganizational quality context. Supply chain quality management (SCQM) is conceptualized as a mechanism for governing quality of resource flows between interdependent organizations. Higher environmental uncertainty is proposed to intensify SCQM practices among supply chain members. However, SCQM practices are likely to assume different orientations depending on whether volume or technological uncertainty is present. High asset specificities among exchange partners are proposed to moderate the relationship between SCQM and environmental uncertainty by elevating quality practice intensities as environments become more turbulent. Disequilibrium environments should discourage SCQM, as organizations lack bureaucratic resources to manage the quality of interorganizational resource flows via bureaucratic means. Implications of these propositions on research and practice are discussed.\n\n==\n\nSingh 2020:\n\nTitle: Managing environmental uncertainty for improved firm financial performance: the moderating role of supply chain risk management practices on managerial decision making\n\nAbstract: ABSTRACT With increasing global exposure, organisations have started to witness supply chain risks that they traditionally were not exposed to. This article therefore attempts to answer the research question of how environmental uncertainty within a firm\u2019s supply chain, impacts firm financial performance. We further evaluate the role of supply chain risk management practices in mitigating the negative impact of such disruption events. To answer the research question, we conduct a literature review and develop the corresponding hypothesis. We test our hypothesis using both primary and secondary data. The results show that environmental uncertainty has a negative impact on firm financial performance, with entrepreneurial managerial capitalism mediating the impact. However, organisations that adopt macro and network supply chain risk management practices are able to improve managerial decision making frame, and mitigate the negative impact of environmental uncertainty on firm financial performance. The article concludes with our findings, along with managerial and practitioner implications of the research.\n\n==\n\nBoon-itt 2008:\n\nTitle: Moderating Effects of Environmental Uncertainty on Supply Chain Integration and Product Quality: An Empirical Study of Thai Automotive Industry\n\nAbstract: Although effective and efficient supply chain management requires integrated business processes that go beyond purchasing and logistics activities, some doubts are expressed concerning the applicability of supply chain integration practices. Specifically, a careful literature search reveals that no studies have included the effect of environmental uncertainty on the relationship between sup- ply chain integration and product quality. This study, therefore, represents an attempt to provide the contribution in the field by developing a model to explore the relationships and to fulfill the gap between the literature on supply chain integration and environmental uncertainty. The findings indi- cate that the effects of supply chain integration on product quality are moderated by environmental uncertainty as demonstrated by Chow tests. The results also provide managerial insights about envi- ronmental uncertainty-supply chain integration practices connection.\n\n==\n\nGanbold 2017:\n\nTitle: IMPACT OF ENVIRONMENTAL UNCERTAINTY ON SUPPLY CHAIN INTEGRATION\n\nAbstract: Today\u2019s ever-changing business environment is often described to be highly competitive, dynamic and complex. Customers are demanding more variability, better quality, higher reliability and faster delivery. Organizations are being faced with more uncertainties from its task environment than before. In order to respond to the uncertainties, organizations are internalizing fewer resources and capabilities, while increasing their integration with partners in the supply chain. Drawing on the resource-dependence theory, this study aims to examine the impact of environmental uncertainty on supply chain integration initiatives. Environmental uncertainty is considered in terms of three types, namely, supply uncertainty, demand or customer uncertainty, and technology uncertainty, based on its sources. Supply chain integration is comprised of internal integration, customer integration, and supplier integration. Based on the empirical study with 108 Japanese manufacturing firms, this study makes significant contributions to the knowledge base and provides theoretical and practical implications.\n\n==\n\nInman 2021:\n\nTitle: Environmental uncertainty and supply chain performance: the effect of agility\n\nAbstract: PurposeToday's businesses are facing a world that is more complex, turbulent and unpredictable than in the past with increasing levels of environmental complexity. Rather than proposing environmental uncertainty as a mediator/moderator of the relationship between agility and performance as others have done, the authors offer an alternative view where supply chain agility is seen as mediating the relationship between environmental uncertainty and supply chain performance.Design/methodology/approachThe authors propose that supply chain agility is a response to the effects of environmental uncertainty and, as such, environmental uncertainty should be seen as a driver of supply chain agility. Few studies test the direct relationship between uncertainty and supply chain performance, and none simultaneously test for agility's mediation and moderation effect between environmental uncertainty and agility.FindingsThe model was statistically assessed using partial-least-squares structural equation modeling (PLS/SEM) by analyzing survey data from manufacturing managers in 136 US firms. The study results did not indicate a significant relationship between environmental uncertainty and supply chain performance. However, the authors did find a significant positive relationship between agile manufacturing and supply chain performance using measures that were primarily operations-centered rather than financial. Additionally, the authors found that agile manufacturing fully mediates the relationship between environmental uncertainty and supply chain performance.Originality/valueThe authors\u2019 model, though simple, provides a base for future research for them and other researchers who can incorporate other impacting variables into the model. The study results show that uncertainty can be a force for good and that utilizing agile manufacturing can be a new source of opportunity.\n\n==\n\nXu 2010:\n\nTitle: The effect of environmental uncertainty on supply chain integration in Chinese manufacturing industry\n\nAbstract: Supply chain integration nowadays is considered as an important approach to building and sustaining advantages. Many previous empirical researches focus on the effect of supply chain integration on the firms' performance, yet, the antecedence of supply chain integration is still largely unknown. This paper investigates the relationship between environmental uncertainty (including demand uncertainty, supply uncertainty and technology uncertainty) and supply chain integration (including customer integration, supplier integration and internal integration) with 139 samples from Chinese manufacturing industry. The results reveal both supply uncertainty and technology uncertainty significantly influence supply chain integration; the effect of supply uncertainty is negative, while that of technology uncertainty is positive. However, demand uncertainty just has a significant effect on internal integration. The relationship between demand uncertainty and external integration (including customer integration and supplier integration) is mediated by the internal integration.",
        "final_summary": "Environmental uncertainty can moderate the relationship between demand risk and supply chain disruption impact. Fynes (2004) found mixed support for the moderating role of demand uncertainty in the relationship between environmental uncertainty and supply chain performance. Sreedevi (2017) found that high environmental uncertainty leads to higher supply chain operational risk, but supply chain flexibility can mitigate these risks. Ford (2015) proposed that higher environmental uncertainty intensifies supply chain quality management practices, particularly in the presence of volume or technological uncertainty. Singh (2020) found that environmental uncertainty negatively impacts firm financial performance, but supply chain risk management practices can mitigate this impact. Boon-itt (2008) demonstrated that environmental uncertainty moderates the relationship between supply chain integration and product quality. Inman (2021) proposed that supply chain agility mediates the relationship between environmental uncertainty and supply chain performance. Xu (2010) revealed that supply uncertainty and technology uncertainty significantly influence supply chain integration."
    },
    {
        "query": "phylogeny reconstruction in breast cancer",
        "paper_list_string": "Pennington 2006:\n\nTitle: Expectation-maximization method for reconstructing tumor phylogenies from single-cell data.\n\nAbstract: Recent studies of gene expression in cancerous tumors have revealed that cancers presenting indistinguishable symptoms in the clinic can represent substantially different entities at the molecular level. The ability to distinguish between these different cancers makes possible more accurate prognoses and more finely targeted therapeutics. Making full use of this knowledge, however, requires characterizing commonly occurring cancer sub-types and the specific molecular abnormalities that produce them. Computational approaches to this problem to date have been hindered by the fact that tumors are highly heterogeneous masses typically containing cells at multiple stages of progression from healthy to aggressively malignant. We present a computational approach for taking advantage of tumor heterogeneity when characterizing tumor progression pathways by inferring those pathways from single-cell assays. Our approach uses phylogenetic algorithms to infer likely evolutionary sequences producing cell populations in single tumors, which are in turn used to create a profile of commonly used pathways across the patient population. This approach is combined with expectation maximization to infer unknown parameters used in the phylogeny construction. We demonstrate the approach on a set of fluorescent in situ hybridization (FISH) data measuring cell-by-cell gene and chromosome copy numbers in a large sample of breast cancers. The results validate the proposed computational methods by showing consistency with several previous findings on these cancers. They also provide novel insights into the mechanisms of tumor progression in these patients.\n\n==\n\nPennington 2007:\n\nTitle: Reconstructing Tumor phylogenies from Heterogeneous Single-Cell Data\n\nAbstract: Studies of gene expression in cancerous tumors have revealed that tumors presenting indistinguishable symptoms in the clinic can be substantially different entities at the molecular level. The ability to distinguish between these genetically distinct cancers will make possible more accurate prognoses and more finely targeted therapeutics, provided we can characterize commonly occurring cancer sub-types and the specific molecular abnormalities that produce them. We develop a new method for identifying these common tumor progression pathways by applying phylogeny inference algorithms to single-cell assays, taking advantage of information on tumor heterogeneity lost to prior microarray-based approaches. We combine this approach with expectation maximization to infer unknown parameters used in the phylogeny construction. We further develop new algorithms to merge inferred trees across different assays. We validate the expectation maximization method on simulated data and demonstrate the combined approach on a set of fluorescent in situ hybridization (FISH) data measuring cell-by-cell gene and chromosome copy numbers in a large sample of breast cancers. The results further validate the proposed computational methods by showing consistency with several previous findings on these cancers and provide novel insights into the mechanisms of tumor progression in these patients.\n\n==\n\nShackney 1995:\n\nTitle: Preferred genetic evolutionary sequences in human breast cancer: a case study.\n\nAbstract: Multiparameter flow cytometry studies were performed on the cells of an aggressive human breast cancer at the time of diagnosis and at relapse. The aneuploid cells that overexpressed large amounts of both HER-2/neu and ras survived intensive chemotherapy and were responsible for tumor relapse. At relapse, these cells were shown to overexpress simultaneously at least five oncogenes: HER-2/neu, ras, EGF receptor, p53 and c-myc. A partial reconstruction of the genetic evolutionary sequence in this tumor indicated that HER-2/neu overexpression was an early step in the sequence. Subsequent HER-2/neu overexpression, EGF receptor overexpression and p53 protein overexpression were each associated with ras overexpression. The data suggest that ploidy and oncogene overexpression cannot be used as independent clinical prognostic factors. The ability to characterize tumors according to the degree of advancement in the genetic evolutionary might serve as a basis for genetic staging for adjuvant therapy.\n\n==\n\nMeaburn 2009:\n\nTitle: Disease-specific gene repositioning in breast cancer\n\nAbstract: The nuclear repositioning of specific genes may be a novel diagnostic strategy to distinguish malignant from normal tissue.\n\n==\n\nTan 2015:\n\nTitle: Genomic landscapes of breast fibroepithelial tumors\n\nAbstract: Breast fibroepithelial tumors comprise a heterogeneous spectrum of pathological entities, from benign fibroadenomas to malignant phyllodes tumors. Although MED12 mutations have been frequently found in fibroadenomas and phyllodes tumors, the landscapes of genetic alterations across the fibroepithelial tumor spectrum remain unclear. Here, by performing exome sequencing of 22 phyllodes tumors followed by targeted sequencing of 100 breast fibroepithelial tumors, we observed three distinct somatic mutation patterns. First, we frequently observed MED12 and RARA mutations in both fibroadenomas and phyllodes tumors, emphasizing the importance of these mutations in fibroepithelial tumorigenesis. Second, phyllodes tumors exhibited mutations in FLNA, SETD2 and KMT2D, suggesting a role in driving phyllodes tumor development. Third, borderline and malignant phyllodes tumors harbored additional mutations in cancer-associated genes. RARA mutations exhibited clustering in the portion of the gene encoding the ligand-binding domain, functionally suppressed RARA-mediated transcriptional activation and enhanced RARA interactions with transcriptional co-repressors. This study provides insights into the molecular pathogenesis of breast fibroepithelial tumors, with potential clinical implications.\n\n==\n\nDing 2010:\n\nTitle: Genome Remodeling in a Basal-like Breast Cancer Metastasis and Xenograft\n\nAbstract: Massively parallel DNA sequencing technologies provide an unprecedented ability to screen entire genomes for genetic changes associated with tumour progression. Here we describe the genomic analyses of four DNA samples from an African-American patient with basal-like breast cancer: peripheral blood, the primary tumour, a brain metastasis and a xenograft derived from the primary tumour. The metastasis contained two de novo mutations and a large deletion not present in the primary tumour, and was significantly enriched for 20 shared mutations. The xenograft retained all primary tumour mutations and displayed a mutation enrichment pattern that resembled the metastasis. Two overlapping large deletions, encompassing CTNNA1, were present in all three tumour samples. The differential mutation frequencies and structural variation patterns in metastasis and xenograft compared with the primary tumour indicate that secondary tumours may arise from a minority of cells within the primary tumour.\n\n==\n\nPrat 2011:\n\nTitle: Deconstructing the molecular portraits of breast cancer\n\nAbstract: Breast cancer is a heterogeneous disease in terms of histology, therapeutic response, dissemination patterns to distant sites, and patient outcomes. Global gene expression analyses using high\u2010throughput technologies have helped to explain much of this heterogeneity and provided important new classifications of cancer patients. In the last decade, genomic studies have established five breast cancer intrinsic subtypes (Luminal A, Luminal B, HER2\u2010enriched, Claudin\u2010low, Basal\u2010like) and a Normal Breast\u2010like group. In this review, we dissect the most recent data on this genomic classification of breast cancer with a special focus on the Claudin\u2010low subtype, which appears enriched for mesenchymal and stem cell features. In addition, we discuss how the combination of standard clinical\u2010pathological markers with the information provided by these genomic entities might help further understand the biological complexity of this disease, increase the efficacy of current and novel therapies, and ultimately improve outcomes for breast cancer patients.\n\n==\n\nRobinson 2011:\n\nTitle: Functionally Recurrent Rearrangements of the MAST Kinase and Notch Gene Families in Breast Cancer\n\nAbstract: Breast cancer is a heterogeneous disease that has a wide range of molecular aberrations and clinical outcomes. Here we used paired-end transcriptome sequencing to explore the landscape of gene fusions in a panel of breast cancer cell lines and tissues. We observed that individual breast cancers have a variety of expressed gene fusions. We identified two classes of recurrent gene rearrangements involving genes encoding microtubule-associated serine-threonine kinase (MAST) and members of the Notch family. Both MAST and Notch-family gene fusions have substantial phenotypic effects in breast epithelial cells. Breast cancer cell lines harboring Notch gene rearrangements are uniquely sensitive to inhibition of Notch signaling, and overexpression of MAST1 or MAST2 gene fusions has a proliferative effect both in vitro and in vivo. These findings show that recurrent gene rearrangements have key roles in subsets of carcinomas and suggest that transcriptome sequencing could identify individuals with rare, targetable gene fusions.",
        "final_summary": "Phylogeny reconstruction in breast cancer can elucidate tumor progression and molecular abnormalities, as suggested by Pennington 2006 and Pennington 2007. These studies propose computational approaches using phylogenetic algorithms to infer evolutionary sequences. Shackney 1995 provides a detailed examination of the genetic evolutionary sequence in aggressive breast cancer, specifically noting that the overexpression of oncogenes such as HER-2/neu and ras is a key step in the sequence. Meaburn 2009 introduces the concept of nuclear gene repositioning as a potential diagnostic strategy. Tan 2015 delves into the genomic landscapes of breast fibroepithelial tumors, identifying distinct mutation patterns. Ding 2010 offers an analysis of genomic changes in a basal-like breast cancer metastasis and xenograft, revealing differential mutation frequencies and structural variations. Prat 2011 discusses the molecular classification of breast cancer subtypes, contributing to a deeper understanding of disease complexity. Lastly, Robinson 2011 identifies recurrent gene rearrangements in the MAST and Notch gene families, suggesting their potential as therapeutic targets in breast cancer."
    },
    {
        "query": "Tallinn emerged as a pioneer by introducing free public transport in 2013.",
        "paper_list_string": "Gabald\u00f3n-Estevan 2016:\n\nTitle: Environmental innovation through transport policy. The implementation of the free fare policy on public transport in Tallinn, Estonia\n\nAbstract: Urban areas are of increasing relevance when it comes to sustainability. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 First, about half of the world\u2019s population now lives in cities (increasing to 60% by 2030). \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 Second, cities are nowadays responsible for levels of resource consumption and waste generation that are higher beyond their share on world population. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 Third, cities are more vulnerable to disruptive events that can lead to restrictions on the provision of resources and to changes on the environment caused by climate change. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 And fourth, because they concentrate key resources (political, social, cultural\u2026), cities are seen as strategic scenarios where to experiment and develop solutions to cope with the prevailing sustainability challenges driven by the major social and environmental transformations. Urban agglomerations can be seen as complex innovation systems where human activities are shaped in order to transform societies towards sustainable development. For this paper, we focus on the case of an environmental innovation regarding transport policy, the implementation of the fare-free policy on public transport for all inhabitants of Tallinn, Estonia. Tallinn, with 414,000 inhabitants in 2015, is the capital of Estonia and the largest city in the country. Over the last two decades the share of public transport trips decreased dramatically. After a public opinion poll in 2012, in which over 75% of the participants voted for a fare-free public transportation system (FFPTS) in Tallinn, the new policy was implemented on 1st January 2013. From that date on inhabitants of Tallinn could use all public transport services (busses, trams, trolly-busses) operated by city-run operators for free. Later the fare-free system was implemented also on trains within Tallinn. In this paper we analyze the context, in which this policy was implemented, the main characteristics of its implementation and its actual situation. DOI: http://dx.doi.org/10.4995/CIT2016.2016.3532\n\n==\n\nHess 2017:\n\nTitle: Decrypting fare-free public transport in Tallinn, Estonia\n\nAbstract: Abstract Among many possible interventions in public transport finance and policy designed to enhance the attractiveness of riding public transport, one of the most extreme, which is seldom implemented, is the elimination of passenger fares, effectively making public transport \u201cfree\u201d for riders (with operating costs paid from other funding sources). This article describes a fare-free public transport program in Tallinn, Estonia, launched in 2013, which has exhibited lower-than-expected increases in ridership. Evaluations of Tallinn\u2019s fare-free public transport program are presented and synthesized, with a focus on program goals and how goals are met through program performance. Findings suggest certain flaws limit the program\u2019s potential success since the program design is misaligned with its primary stated goals, and several program goals relating to external effects of fare reform cannot be evaluated. Although it would be valuable for transport managers in other cities to learn about this experience, the Tallinn fare-free public transport program provides scant transferable evidence about how such a program can operate outside of a politicized context, which was crucial to its implementation in Estonia.\n\n==\n\nGaley 2014:\n\nTitle: License to Ride: Free Public Transportation for Residents of Tallinn\n\nAbstract: The City of Tallinn, capital of Estonia, with a population of 420,000, recently became the world\u2019s largest municipality offering free public transportation. Tourists still have to pay to ride the city\u2019s bus, trolley, and tram network, but registered residents\u2014including a large population of Russian-speaking non-citizens\u2014only have to tap their municipal transit cards once onboard. This article presents a qualitative account of the world\u2019s largest free public transporta- tion experiment to date. The results challenge and inform the conventional measures and objectives of transportation experts. The analysis is meant to complement the existing literature surveying free public transportation experiments and evaluating transportation pricing schemes.\n\n==\n\nGabald\u00f3n-Estevan 2019:\n\nTitle: Broader impacts of the fare-free public transportation system in Tallinn\n\nAbstract: ABSTRACT In this paper, we focus on the rationale for implementing the fare-free public transportation system (FFPTS) in Tallinn, Estonia, that took place on 1 January 2013. Through a series of interviews with relevant informants, we identify the main enablers and the FFPTS in Tallinn faced and the potential of such a system to contribute to the sustainable city development. Our analysis shows that the interlinking between local and national politics determines not only the type of initiatives implemented and the support they receive but also the degree of their success and their stability. We conclude that to be even more effective, it should be extended to the all potential users, not just to local registered residents as it has been recently applied in state-run bus travels in rural municipalities in Estonia. Finally, more restrictive private car policies should be considered to fuel a sustainable mobility transition and increase cities life quality.\n\n==\n\nK\u0119b\u0142owski 2019:\n\nTitle: Towards an urban political geography of transport: Unpacking the political and scalar dynamics of fare-free public transport in Tallinn, Estonia\n\nAbstract: In this article, we study the largest existing fare-free public transport (FFPT) programme, launched in 2013 in Tallinn, Estonia. Instead of focusing solely on the rationale and impact of fare-free public transport in terms of finances and travel patterns, we propose to analyse FFPT from the perspective of urban political geography, and to inquire into its political and scalar dynamics. We analyse how Tallinn\u2019s fare-free programme was developed, and demonstrate the politics of its conception and implementation. We observe who has access to free travel and we reveal how FFPT is embedded in Estonia\u2019s place-of-residence-based taxation system. Finally, we identify where lies the impact of territorial competition exacerbated by FFPT. Therefore, we argue that transport policies \u2013 of which FFPT is but an example \u2013 should be understood as much more than strategies dealing with transport issues per se. Instead, we propose to approach them as political and spatial projects, whose processual, cross-sectorial and scalar dimensions help to reveal the embeddedness of transport in inherently urban questions of metropolitan governance, electoral strategies, territorial competition and socio-spatial inequalities.\n\n==\n\nCats 2014:\n\nTitle: Public Transport Pricing Policy\n\nAbstract: Cities worldwide are looking for new policies to attract travelers to shift from cars to public transport. Policies focused on reducing public transport fares are aimed at improving social inclusion and leading to a modal shift. The City of Tallinn, the capital of Estonia, has recently introduced a fare-free public transport (FFPT) service in an effort to improve accessibility and mobility for its residents. The case of Tallinn is a full-scale, real-world experiment that provides a unique opportunity for investigating the impacts of FFPT policy. A macrolevel empirical evaluation of FFPT impacts on service performance, passenger demand, and accessibility for various groups of travelers is presented. In contrast to previous studies, the influence of FFPT on passenger demand was estimated while changes in supply were controlled. The results indicate that the FFPT measure accounts for an increase of 1.2% in passenger demand, with the remaining increase attributed to an extended network of public transport priority lanes and increased service frequency. The relatively small effect could be attributed to the previous price level and public transport share as well as analysis of the short-term impact. The evidence-based policy evaluation in this paper is instrumental in supporting policy making and facilitating the design of public transport pricing strategies.\n\n==\n\nCats 2017:\n\nTitle: The prospects of fare-free public transport: evidence from Tallinn\n\nAbstract: The subsidy level of public transport systems varies considerably among systems worldwide. While limited-scale free-fare public transport (FFPT) services such as limited campaigns and fare evasion for special groups or specific services are prevalent, there is only limited evidence on the consequences of introducing a full-fledged FFPT. The case of Tallinn, Estonia offers a full-scale experiment that provides a unique opportunity to investigate the impacts of FFPT. This study examines travel pattern changes based on individual travel habit survey shortly before and almost 1\u00a0year after the introduction of FFPT policy in Tallinn based on interviews and travel diaries of a random sample of 1500 household. We analyse modal shift effects and whether they are driven by trip generation or trip substitution, travel attitudes and satisfactions as well as impacts on equity, employment prospects, and trip destination choices. Almost a year after the introduction of FFPT, public transport usage increased by 14\u00a0% and there is evidence that the mobility of low-income residents has improved. The effect of FFPT on ridership is substantially lower than those reported in previous studies due to the good level of service provision, high public transport usage and low public transport fees that existed already prior to the FFPT.\n\n==\n\nCats 2017:\n\nTitle: Erratum to: The prospects of fare-free public transport: evidence from Tallinn\n\nAbstract: The prospects of fare-free public transport : evidence from Tallinn (vol 44, pg 1083, 2017)",
        "final_summary": "Tallinn's introduction of free public transport in 2013 led to an increase in public transport usage and improved mobility for low-income residents (Gabald\u00f3n-Estevan 2016, Cats 2017). However, the program's design was flawed and misaligned with its primary stated goals (Hess 2017). Galey 2014 offered a qualitative account of the experiment. The success and stability of such initiatives are influenced by the interlinking of local and national politics (Gabald\u00f3n-Estevan 2019). The fare-free program is embedded in Estonia's taxation system, revealing its political and scalar dynamics (K\u0119b\u0142owski 2019). For a more effective and sustainable mobility transition, further evaluation and consideration of private car policies are suggested."
    },
    {
        "query": "application of algebraic analysis for stochastic partial differential equation",
        "paper_list_string": "Ocone 1988:\n\nTitle: Stochastic calculus of variations for stochastic partial differential equations\n\nAbstract: Abstract This paper develops the stochastic calculus of variations for Hilbert space-valued solutions to stochastic evolution equations whose operators satisfy a coercivity condition. An application is made to the solutions of a class of stochastic pde's which includes the Zakai equation of nonlinear filtering. In particular, a Lie algebraic criterion is presented that implies that all finite-dimensional projections of the solution define random variables which admit a density. This criterion generalizes hypoellipticity-type conditions for existence and regularity of densities for finite-dimensional stochastic differential equations.\n\n==\n\nZhou 1992:\n\nTitle: A duality analysis on stochastic partial differential equations\n\nAbstract: Abstract The duality equations of stochastic partial differential equations are solved in the Sobolev space H m (= W 2 m ( R d )), and the H m -norm estimates of the solutions are obtained. As an application, the H m -norm estimates with negative m for the solutions of stochastic partial differential equations are derived.\n\n==\n\nSerrano 1985:\n\nTitle: Analysis of stochastic groundwater flow problems. Part II: Stochastic partial differential equations in groundwater flow. A functional-analytic approach\n\nAbstract: Abstract Following the scheme and concepts presented in Part I, Part II uses functional-analytic theory to analyze the problem of stochastic partial differential equations of the type appearing in groundwater flow. Equations are treated as abstract stochastic evolution equations for elliptic partial differential operators in an appropriate functional Sobolev space. Explicit forms of solutions are obtained by using a strongly continuous semigroup. The deterministic and the stochastic problem can then be treated under the same theoretical framework. Use of the theory is indicated in an application to the solution of the stochastic analogue of the regional groundwater flow problem studied in Part I. Two cases are solved: The randomly forced and the randomly initiated equation. The solution is obtained by applying the properties of semigroups and expressing the Wiener process as an infinite basis in a Hilbert space composed of independent unidimensional Wiener processes with incremental variance parameters. The first two moments of the solution as well as sample functions for different cases are derived.\n\n==\n\nSee\u03b2elberg 1993:\n\nTitle: Numerical integration of stochastic partial differential equations\n\nAbstract: Abstract The solution of stochastic partial differential equations generally relies on numerical tools. However, conventional numerical procedures are not appropriate to solve such problems. In this paper an algorithm is proposed which allows the numerical treatment of a large class of stochastic partial differential equations. To this end we reduce stochastic partial differential equations to a system of stochastic ordinary differential equations which can be solved numerically by a well-known stochastic Euler-procedure. We apply our algorithm to two stochastic partial differential equations which are special examples because their stationary two-point correlation functions can be determined analytically. Our algorithm proves to work out very well when numerical results are compared with the analytic correlation function.\n\n==\n\nHigham 2001:\n\nTitle: An Algorithmic Introduction to Numerical Simulation of Stochastic Differential Equations\n\nAbstract: A practical and accessible introduction to numerical methods for stochastic differential equations is given. The reader is assumed to be familiar with Euler's method for deterministic differential equations and to have at least an intuitive feel for the concept of a random variable; however, no knowledge of advanced probability theory or stochastic processes is assumed. The article is built around $10$ MATLAB programs, and the topics covered include stochastic integration, the Euler--Maruyama method, Milstein's method, strong and weak convergence, linear stability, and the stochastic chain rule.\n\n==\n\nNunno 2014:\n\nTitle: Approximations of stochastic partial differential equations\n\nAbstract: In this paper we show that solutions of stochastic partial dierential equations driven by Brownian motion can be approximated by stochastic partial dierential equations forced by pure jump noise/random kicks. Applications to stochastic Burgers equations are discussed.\n\n==\n\nSigrist 2012:\n\nTitle: Stochastic partial differential equation based modelling of large space\u2013time data sets\n\nAbstract: Increasingly larger data sets of processes in space and time ask for statistical models and methods that can cope with such data. We show that the solution of a stochastic advection\u2013diffusion partial differential equation provides a flexible model class for spatiotemporal processes which is computationally feasible also for large data sets. The Gaussian process defined through the stochastic partial differential equation has, in general, a non\u2010separable covariance structure. Its parameters can be physically interpreted as explicitly modelling phenomena such as transport and diffusion that occur in many natural processes in diverse fields ranging from environmental sciences to ecology. To obtain computationally efficient statistical algorithms, we use spectral methods to solve the stochastic partial differential equation. This has the advantage that approximation errors do not accumulate over time, and that in the spectral space the computational cost grows linearly with the dimension, the total computational cost of Bayesian or frequentist inference being dominated by the fast Fourier transform. The model proposed is applied to post\u2010processing of precipitation forecasts from a numerical weather prediction model for northern Switzerland. In contrast with the raw forecasts from the numerical model, the post\u2010processed forecasts are calibrated and quantify prediction uncertainty. Moreover, they outperform the raw forecasts, in the sense that they have a lower mean absolute error.\n\n==\n\nLamnabhi-Lagarrigue 1983:\n\nTitle: Algebraic computation of the statistics of the solution of some nonlinear stochastic differential equations\n\nAbstract: This paper presents an algebraic method for computing the statistics of the solution of some stochastic non-linear differential equations by mean of the Volterra functional expansion. The symbolic calculus introduced, based on noncommutative variables and iterated integrals has the advantage of allowing easily the use of symbolic computation systems, like REDUCE or MACSYMA, to perform the manipulations. This becomes necessary as soon as one tries to get high order terms.",
        "final_summary": "Algebraic analysis can be applied to stochastic partial differential equations (SPDEs) in various ways. Ocone (1988) develops the stochastic calculus of variations for SPDEs, applying it to the Zakai equation of nonlinear filtering. Zhou (1992) solves the duality equations of SPDEs and obtains norm estimates for the solutions. Serrano (1985) uses functional-analytic theory to analyze SPDEs in groundwater flow, obtaining explicit solutions. See\u03b2elberg (1993) proposes an algorithm for numerical integration of SPDEs, reducing them to a system of stochastic ordinary differential equations. Nunno (2014) shows that solutions of SPDEs can be approximated by equations forced by pure jump noise. Sigrist (2012) demonstrates that SPDEs can be used for modelling large space-time data sets, with applications in environmental sciences. Lastly, Lamnabhi-Lagarrigue (1983) presents an algebraic method for computing the statistics of the solution of nonlinear stochastic differential equations using Volterra functional expansion."
    },
    {
        "query": "Technology Transfer: COP26 emphasized the importance of technology transfer from developed to developing countries to foster climate adaptation and mitigation. This entails sharing and facilitating access to clean and sustainable technologies that can support low-carbon pathways for development. Unilever, as a global player in the consumer goods industry, has made significant contributions through technology transfer. For instance, the company's Sustainable Living Plan includes initiatives such as sharing renewable energy solutions, water-saving technologies, and sustainable packaging innovations with suppliers and partners across its value chain. Unilever's commitment to technology transfer supports the agenda set at COP26, promoting the equitable distribution of climate-friendly technologies for a more sustainable future.",
        "paper_list_string": "Flamos 2010:\n\nTitle: Technology transfer insights for new climate regime\n\nAbstract: The purpose of technology transfer under the UNFCCC Article 4.5 is to \u201c\u2026promote, facilitate, and finance as appropriate the transfer of, or access to, environmentally sound technologies and know how to other Parties particularly Developing Country parties to enable them to implement the provisions of the Convention.\u201d The key challenge in this respect is that low-carbon sustainable technologies need to be adopted both by developed as well as developing countries. However, this paper focuses on the process of technology transfer to developing countries to allow them to move quickly to environmentally sound and sustainable practices, institutions and technologies. In the above framework, this paper reviews key aspects of technology transfer from a range of perspectives in the literature and discusses insights from this literature for the transfer and innovation process needed to reduce global vulnerability to climate change in the context of current international activities based on the research undertaken by the EU sponsored ENTTRANS project.\n\n==\n\nPopp 2010:\n\nTitle: International Technology Transfer for Climate Policy\n\nAbstract: While the developed world is starting to limit emissions of greenhouse gases, emissions from the developing world are increasing as a result of economic growth. Reducing these emissions while still enabling developing countries to grow requires the use of new technologies. In most cases, these technologies are first created in high-income countries. Thus, the challenge for climate policy is to encourage the transfer of these climate-friendly technologies to the developing world. This policy brief reviews the economic literature on environmental technology transfer. It then discusses the implications of this literature for climate policy, focuing on the Clean Developmenht Mechanism (CDM) ofthe Kyoto Protocol. It concludes by asking whether the current structure of the CDM provides sufficient incentives for technology transfer. Are CDM projects providing real emissions reductions, or are developed countries simply receiving credit for reductions that developing countries could have achieved on their own? What lessons can we learn from recent experience that may guide the development of the CDM (or other similar policy tools) during the next round of international climate policy negotiations?\n\n==\n\nKarakosta 2010:\n\nTitle: Technology transfer through climate change: Setting a sustainable energy pattern\n\nAbstract: Climate change mitigation is considered as a high priority internationally and is placed in the top of the agenda for most politicians and decision makers. The key challenge is that low-carbon sustainable technologies need to be adopted both by developed as well as developing countries, in an effort to avoid past unsustainable practices and being locked into old, less sustainable technologies. Technology transfer (TT), as an important feature of both the United Nations Framework Convention on Climate Change (UNFCCC) and its Kyoto Protocol can play a key role. TT can allow countries to move quickly to environmentally sound and sustainable practices, institutions and technologies. Indeed, the transfer or innovation process must be fast enough, to reduce global vulnerability to climate change. The aim of this paper is to analyse the TT challenges and emerging opportunities under the new climate regime, in terms of the process of innovation into an existing energy system, the related barriers and the supporting diffusion mechanisms. Good practices for renewable energy are also presented and discussed by both the developed and the developing countries in this respect.\n\n==\n\nTawney 2011:\n\nTitle: Innovation and Technology Transfer: Supporting Low Carbon Development with Climate Finance\n\nAbstract: January 2011 OVERVIEW Meeting the ambitious goal of limiting global warming to 2\u00b0 Celsius or less will require significant innovation the improvement of technologies and processes to drive down their cost and improve their performance. Public climate finance is essential to spurring innovation and creating the conditions that attract private investment. Investing in innovation also makes the most efficient use of the limited financial resources available and takes advantage of the developing world's growth to improve technologies. Countries like the UAE have an opportunity to play a pioneering role in this expanded international innovation system.\n\n==\n\nPopp 2011:\n\nTitle: International Technology Transfer, Climate Change, and the Clean Development Mechanism\n\nAbstract: As the developed world begins efforts to limit its emissions of greenhouse gases, economic growth in developing countries is causing increased emissions from the developing world. Reducing these emissions while still enabling developing countries to grow requires the use of climate-friendly technologies in these countries. In most cases, these technologies are first created in high-income countries. Thus, the challenge for climate policy is to encourage the transfer of these climate-friendly technologies to the developing world. This article reviews the economic literature on environmental technology transfer and discusses the implications of this literature for climate policy, focusing on the Clean Development Mechanism (CDM). A key point is that technology diffusion is gradual. Early adoption of policy by developed countries leads to the development of new technologies that make it easier for developing countries to reduce pollution as well. Since clean technologies are first developed in the world's leading economies, international trade and foreign investments provide access to these technologies. Moreover, evidence suggests that some technologies, such as those enhancing energy efficiency, will diffuse to developing countries even without the aid of policy prescriptions, such as the CDM. This is important for assessing the potential emissions reductions of proposed CDM projects.\n\n==\n\nLee 2021:\n\nTitle: Digitalization to Achieve Technology Innovation in Climate Technology Transfer\n\nAbstract: Technology Innovation has the potential to play a strategic role in improving the effectiveness and efficiency of national efforts to address climate change. The United Nations (UN) Climate Technology Centre and Network (CTCN) is mandated to support developing countries\u2019 climate change responses through innovative technologies to achieve the goals of the Paris Agreement. In order to enhance the role of the CTCN as an innovation matchmaker, it is important to explore and leverage the implementation potential of new digital technologies and their transformational impact. Thus, in this research, to engage digitalization as an innovative tool with the environment, we first explored digitalization during the climate technology transfer processes by comprehensively reviewing CTCN Technical Assistance (Digitalization Technical Assistance, D-TA) activities in three climate sectors of risk prediction, policy decision making, and resource optimization. Then, by applying analytical methodologies of in-depth interviews with major digital-climate stakeholders and a staged model for technology innovation, we propose future strategies for enhancing the role of CTCN as an innovation matchmaker in the three digitalization cases of digital collection, digital analysis, and digital diffusion.\n\n==\n\nUeno 2009:\n\nTitle: Technology Transfer to China to Address Climate Change Mitigation\n\nAbstract: This paper analyzes whether and how transfer of climate mitigation technologies to China occurs, by studying cases of seven technologies that are at the stage of deployment or diffusion. Most of these technologies were already transferred to China in terms of both technology adoption and local production. International division of labor of manufacturing and localization policies by the Chinese government facilitated local production by China, which resulted in deep cuts in production costs. Such cost reduction, coupled with technology diffusion policies by the Chinese government, then accelerated deployment and diffusion in China and other emerging economies that import Chinese products.\n\n==\n\nPigato 2020:\n\nTitle: Technology Transfer and Innovation for Low-Carbon Development\n\nAbstract: Technological revolutions have increased the world\u2019s wealth to a level that was once unimaginable. They have done so unevenly, however, and in ways that have accelerated climate change. Technology Transfer and Innovation for Low-Carbon Development argues that most of the emissions reductions required to achieve the Paris Agreement goals can be reached through the global deployment of existing and commercially proven low-carbon technologies (LCTs). Deploying LCTs from high-income countries, which account for 80 percent of all LCT innovations and 70 percent of all LCT exports, to developing countries, where they are needed most, will not be easy; but the evidence is clear that it can be done. Transferring LCTs is not only necessary to meet the climate targets, but it is also an opportunity to achieve development goals. The policies needed to deploy LCTs to developing countries could raise output and employment while yielding welfare benefits, such as reduced air and water pollution. Moreover, adopting LCT offers an opportunity for countries to benefit from participation in global value chains and to become LCT producers and exporters.",
        "final_summary": "Technology transfer is crucial for climate adaptation and mitigation, particularly from developed to developing countries (Flamos 2010). This process faces challenges, such as the need for effective incentives, with the Clean Development Mechanism (CDM) being questioned for its effectiveness in this regard (Popp 2010). Despite these challenges, technology transfer presents opportunities under the new climate regime, including the adoption of low-carbon sustainable practices (Karakosta 2010). Public climate finance is essential in spurring innovation and creating conditions that attract private investment (Tawney 2011). The diffusion of clean technologies is a gradual process, with the potential for significant emissions reductions through proposed CDM projects (Popp 2011). Digitalization could enhance technology transfer and innovation in climate technology (Lee 2021). The case of China shows that international division of labor and localization policies can facilitate technology transfer for climate change mitigation (Ueno 2009). Lastly, the global deployment of existing low-carbon technologies could achieve most emissions reductions, offering developing countries the opportunity to benefit from technology transfer (Pigato 2020)."
    },
    {
        "query": "Female attractiveness evolves compared to male attractiveness because men are more interested in attractiveness?",
        "paper_list_string": "Rozmus-Wrzesinska 2005:\n\nTitle: Men\u2019s ratings of female attractiveness are influenced more by changes in female waist size compared with changes in hip size\n\nAbstract: Women's attractiveness has been found to be negatively correlated with waist-to-hip ratio (WHR) in many studies. Two components of this ratio can, however, carry different signals for a potential mate. Hip size indicates pelvic size and the amount of additional fat storage that can be used as a source of energy. Waist size conveys information such as current reproductive status or health status. To assess which of these two dimensions is more important for men's perception of female attractiveness, we used a series of photographs of a woman with WHR manipulated either by hip or waist changes. Attractiveness was correlated negatively with WHR, when WHR was manipulated by waist size. The relation was inverted-U shape when WHR was changed by hip size. We postulate that in westernized societies with no risk of seasonal lack of food, the waist, conveying information about fecundity and health status, will be more important than hip size for assessing a female's attractiveness.\n\n==\n\nLittle 2001:\n\nTitle: Self-perceived attractiveness influences human female preferences for sexual dimorphism and symmetry in male faces\n\nAbstract: Exaggerated sexual dimorphism and symmetry in human faces have both been linked to potential \u2018good\u2013gene\u2019 benefits and have also been found to influence the attractiveness of male faces. The current study explores how female self\u2013rated attractiveness influences male face preference in females using faces manipulated with computer graphics. The study demonstrates that there is a relatively increased preference for masculinity and an increased preference for symmetry for women who regard themselves as attractive. This finding may reflect a condition\u2013dependent mating strategy analogous to behaviours found in other species. The absence of a preference for proposed markers of good genes may be adaptive in women of low mate value to avoid the costs of decreased parental investment from the owners of such characteristics.\n\n==\n\nBurriss 2011:\n\nTitle: Men\u2019s attractiveness predicts their preference for female facial femininity when judging for short-term, but not long-term, partners.\n\nAbstract: It is well established that women\u2019s preferences for masculinity are contingent on their own market-value and the duration of the sought relationship, but few studies have investigated similar effects in men. Here, we tested whether men\u2019s attractiveness predicts their preferences for feminine face shape in women when judging for long- and short-term relationship partners. We found that attractive men expressed a stronger preference for facial femininity compared to less attractive men. The relationship was evident when men judged women for a short-term, but not for a long-term, relationship. These findings suggest that market-value may influence men\u2019s preferences for feminine characteristics in women\u2019s faces and indicate that men\u2019s preferences may be subject to facultative variation to a greater degree than was previously thought.\n\n==\n\nGottschall 2007:\n\nTitle: Greater Emphasis on Female Attractiveness in Homo Sapiens: A Revised Solution to an Old Evolutionary Riddle\n\nAbstract: Substantial evidence from psychology and cross-cultural anthropology supports a general rule of greater emphasis on female physical attractiveness in Homo sapiens. As sensed by Darwin (1871) and clarified by Trivers (1972), generally higher female parental investment is a key determinant of a common pattern of sexual selection in which male animals are more competitive, more eager sexually and more conspicuous in courtship display, ornamentation, and coloration. Therefore, given the larger minimal and average parental investment of human females, keener physical attractiveness pressure among women has long been considered an evolutionary riddle. This paper briefly surveys previous thinking on the question, before offering a revised explanation for why we should expect humans to sharply depart from general zoological pattern of greater emphasis on male attractiveness. This contribution hinges on the argument that humans have been seen as anomalies mainly because we have been held up to the wrong zoological comparison groups. I argue that humans are a partially sex-role reversed species, and more emphasis on female physical attractiveness is relatively common in such species. This solution to the riddle, like those of other evolutionists, is based on peculiarities in human mating behavior, so this paper is also presented as a refinement of current thinking about the evolution of human mating preferences.\n\n==\n\nScott 2007:\n\nTitle: An evolutionary perspective on male preferences for female body shape\n\nAbstract: Cross-culturally, humans make systematic use of physical attractiveness to discriminate among members of the opposite sex, and physical cues to youth, health, and fertility may be particularly important to men (Buss, 1989). Nevertheless, there is controversy over whether attraction preferences are adaptive, particularly in novel environments, and whether they are universal or flexible depending on cultural circumstances (Singh & Luis, 1995). To date, a good deal of research into somatic (i.e., body) attractiveness has focused on two particular characteristics: waist-to-hip ratio (WHR) and the body mass index (BMI). WHR is calculated as the circumference of the waist divided by circumference of the hips, and provides an index of a woman\u2019s \u2018curvaceousness.\u2019 BMI is calculated as an individual\u2019s weight (kilogrammes) divided by height (metres) squared, and provides an estimate of body fatness.\n\n==\n\nWiederman 1993:\n\nTitle: Evolved gender differences in mate preferences: Evidence from personal advertisements\n\nAbstract: Abstract Evolutionary theorists have posited that contemporary men and women may differ in their specific psychological mechanisms having to do with mate selection because different strategies would have benefitted men versus women in our distant ancestral past. From these theorized gender differences in mating strategies, several hypotheses were generated and subsequently tested in the current study using a large sample of personal advertisements ( N = 1111). The results were generally supportive of evolutionary predictions: men were more likely than women to offer financial resources and honesty/ sincerity, and to seek attractiveness, appealing body shape, and a photograph in selecting a potential mate; women were more likely than men to offer an appealing body shape and to seek financial resources, qualities likely to lead to resource acquisition, and honesty/sincerity in potential mates. Women were also more likely than men to seek male friendship and/or companionship and to offer greater involvement only after the establishment of such friendship, whereas men more frequently than women made explicit requests for a sexual relationship. In general, men sought potential mates who were younger than themselves, a trend which became more pronounced among older advertisers. Women generally sought mates who were older than themselves, a trend which decreased slightly with the age of the advertiser. Results are discussed with regard to implications for hypothesized gender differences in evolved psychological mechanisms.\n\n==\n\nConfer 2010:\n\nTitle: More than just a pretty face: Men's priority shifts toward bodily attractiveness in short-term versus long-term mating contexts\n\nAbstract: Abstract Studies of physical attractiveness have long emphasized the constituent features that make faces and bodies attractive, such as symmetry, skin texture, and waist-to-hip ratio. Few studies, however, have examined the reproductively relevant cues conveyed by faces and bodies as whole units. Based on the premise that fertility cues are more readily assessed from a woman's body than her face, the present study tested the hypothesis that men evaluating a potential short-term mate would give higher priority to information gleaned from her body, relative to her face, than men evaluating a potential long-term mate. Male and female participants ( N =375) were instructed to consider dating an opposite sex individual, whose face was occluded by a \"face box\" and whose body was occluded by a \"body box,\" as a short-term or long-term mate. With the instruction that only one box could be removed to make their decision about their willingness to engage in the designated relationship with the occluded individual, significantly more men assigned to the short-term, compared to the long-term, mating condition removed the body box. Women's face versus body information choice, in contrast, was unaffected by the temporal dimension of the mating condition. These results suggest that men, but not women, have a condition-dependent adaptive proclivity to prioritize facial cues in long-term mating contexts, but shift their priorities toward bodily cues in short-term mating contexts.\n\n==\n\nJohnston 2001:\n\nTitle: Male facial attractiveness: evidence for hormone-mediated adaptive design\n\nAbstract: Abstract Experimenters examining male facial attractiveness have concluded that the attractive male face is (1) an average male face, (2) a masculinized male face, or (3) a feminized male face. Others have proposed that symmetry, hormone markers, and the menstrual phase of the observer are important variables that influence male attractiveness. This study was designed to resolve these issues by examining the facial preferences of 42 female volunteers at two different phases of their menstrual cycle. Preferences were measured using a 40-s QuickTime movie (1200 frames) that was designed to systematically modify a facial image from an extreme male to an extreme female configuration. The results indicate that females exhibit (1) a preference for a male face on the masculine side of average, (2) a shift toward a more masculine male face preference during the high-risk phase of their menstrual cycle, and (3) no shift in other facial preferences. An examination of individual differences revealed that women who scored low on a \"masculinity\" test (1) showed a larger menstrual shift, (2) had lower self-esteem, and (3) differed in their choice of male faces for dominance and short-term mates. The results are interpreted as support for a hormonal theory of facial attractiveness whereby perceived beauty depends on an interaction between displayed hormone markers and the hormonal state of the viewer.",
        "final_summary": "The papers suggest that men's interest in female attractiveness plays a role in the evolution of female attractiveness. Rozmus-Wrzesinska 2005 found that men's ratings of female attractiveness were more influenced by changes in waist size than hip size, indicating that waist size conveys information about reproductive status and health. Little 2001 found that women who perceive themselves as attractive have a stronger preference for masculine and symmetrical male faces, potentially reflecting a condition-dependent mating strategy. Burriss 2011 found that attractive men expressed a stronger preference for feminine facial features in women when judging for short-term relationships. Gottschall 2007 argues that greater emphasis on female physical attractiveness in humans is due to higher female parental investment and the partially sex-role reversed nature of humans. Scott 2007 discusses the importance of physical cues related to youth, health, and fertility in human mate selection. Wiederman 1993 found gender differences in mate preferences, with men prioritizing attractiveness and women prioritizing financial resources and honesty/sincerity. Confer 2010 found that men prioritize bodily attractiveness over facial attractiveness in short-term mating contexts. Johnston 2001 suggests that female preferences for male facial attractiveness are influenced by hormone markers and the menstrual phase of the observer. These papers support the idea that men's interest in attractiveness contributes to the evolution of female attractiveness. (Rozmus-Wrzesinska 2005; Little 2001; Burriss 2011; Gottschall 2007; Scott 2007; Wiederman 1993; Confer 2010; Johnston 2001)"
    },
    {
        "query": "laws and policies governing climate change disinformation or misinformation",
        "paper_list_string": "Treen 2020:\n\nTitle: Online misinformation about climate change\n\nAbstract: Policymakers, scholars, and practitioners have all called attention to the issue of misinformation in the climate change debate. But what is climate change misinformation, who is involved, how does it spread, why does it matter, and what can be done about it? Climate change misinformation is closely linked to climate change skepticism, denial, and contrarianism. A network of actors are involved in financing, producing, and amplifying misinformation. Once in the public domain, characteristics of online social networks, such as homophily, polarization, and echo chambers\u2014characteristics also found in climate change debate\u2014provide fertile ground for misinformation to spread. Underlying belief systems and social norms, as well as psychological heuristics such as confirmation bias, are further factors which contribute to the spread of misinformation. A variety of ways to understand and address misinformation, from a diversity of disciplines, are discussed. These include educational, technological, regulatory, and psychological\u2010based approaches. No single approach addresses all concerns about misinformation, and all have limitations, necessitating an interdisciplinary approach to tackle this multifaceted issue. Key research gaps include understanding the diffusion of climate change misinformation on social media, and examining whether misinformation extends to climate alarmism, as well as climate denial. This article explores the concepts of misinformation and disinformation and defines disinformation to be a subset of misinformation. A diversity of disciplinary and interdisciplinary literature is reviewed to fully interrogate the concept of misinformation\u2014and within this, disinformation\u2014particularly as it pertains to climate change.\n\n==\n\nKa\u1e91ys 2018:\n\nTitle: CLIMATE CHANGE INFORMATION ON INTERNET BY DIFFERENT BALTIC SEA REGION LANGUAGES: RISKS OF DISINFORMATION & MISINTERPRETATION\n\nAbstract: The internet space is the most important and affluent source of climate change related information. Hoverer information content are not always satisfying and threat of fake news and disinformation are very realistic. The analysis included top10 search results of four phrases (Climate change, Global warming, Adaptation to climate change and Climate change policy) using Google search engine. The phrases were searched in 11 Baltic Sea Region (BSR) languages and in the Ukrainian and English languages. The results revealed that climate change disinformation and misinterpretation exists on the internet. Mostly it displayed in indirect forms such as old information, existence of junksites, advertisements, unequal share by main actors (government, mass media, etc.). Moreover, on Eastern BSR languages, internet search results of climate change information are less convenient comparing to western BSR languages. The usage of multilanguage approach in Wikipedia pages could be one of the freshest and most reliable sources of information about climate change.\n\n==\n\nHassan 2023:\n\nTitle: Analysis of climate change disinformation across types, agents and media platforms\n\nAbstract: Disinformation represents one of the major obstacles to meaningful actions against climate change skepticism. This study, therefore, aimed to investigate climate change disinformation across types, agents, and media platforms as reported in selected online newspapers. A total of 124 news articles gathered from Malaysia's The Star and New Straits Times were subjected to content analysis. The articles were collected from the newspapers\u2019 digital archives between August 2015 and October 2021 using \u201cclimate change\u201d and \u201cdisinformation\u201d as keywords. Subsequently, data were analyzed using the Chi-square test and descriptive statistics. The study found that disinformation about climate change is more likely to be spread by politicians, organizations, and anonymous agencies compared with business tycoons, celebrities, and academics. Also, misleading and fabricated contents represent the most common types of disinformation spread by politicians. The findings suggest that politics and social media represent the most critical factors influencing climate change. The study, therefore, calls for the joint efforts of activists, media practitioners, and governments to mitigate disinformation about climate change.\n\n==\n\nMoxnes 2008:\n\nTitle: Misperceptions of global climate change: information policies\n\nAbstract: Previous experimental studies have found that people generally misperceive the basic dynamics of renewable resources, and in particular the accumulation of greenhouse gases (GHGs) in the atmosphere. The purpose of the present laboratory experiment is to find out why people misperceive the dynamics of CO2 accumulation and how misperceptions could be avoided. Using a simulator, 242 subjects were each asked to control total global emissions of CO2 to reach a given target for the stock of CO2 in the atmosphere. Consistent with previous investigations we find a strong tendency for people to overshoot the stated goal. Furthermore, our results point out that people need help to develop proper mental models of CO2 accumulation and they need motivation to reconsider inappropriate decision heuristics. Based on these results and the literature on conceptual change a new information strategy is designed. To motivate, it imposes cognitive conflict; and to facilitate new understanding, it provides simple analogies. A new test shows promising learning effects. The results have important implications for the Intergovernmental Panel on Climate Change (IPCC), governments, and media covering the climatic change issue as well as for general education.\n\n==\n\nMehling 2013:\n\nTitle: Climate Law in the United States: Facing Structural and Procedural Limitations\n\nAbstract: Just 5 years ago, the official position of the White House on the issue of climate change was that there was no such position. President George W. Bush and his administration declined to address whether climate change was even occurring, much less how to mitigate the causes of a phenomenon that had potentially contributed to billion-dollar disasters, thousands of fatalities during Hurricanes Rita and Katrina, and a significant number of displaced U.S. citizens.\n\n==\n\nLinden 2017:\n\nTitle: Inoculating the Public against Misinformation about Climate Change\n\nAbstract: Effectively addressing climate change requires significant changes in individual and collective human behavior and decision\u2010making. Yet, in light of the increasing politicization of (climate) science, and the attempts of vested\u2010interest groups to undermine the scientific consensus on climate change through organized \u201cdisinformation campaigns,\u201d identifying ways to effectively engage with the public about the issue across the political spectrum has proven difficult. A growing body of research suggests that one promising way to counteract the politicization of science is to convey the high level of normative agreement (\u201cconsensus\u201d) among experts about the reality of human\u2010caused climate change. Yet, much prior research examining public opinion dynamics in the context of climate change has done so under conditions with limited external validity. Moreover, no research to date has examined how to protect the public from the spread of influential misinformation about climate change. The current research bridges this divide by exploring how people evaluate and process consensus cues in a polarized information environment. Furthermore, evidence is provided that it is possible to pre\u2010emptively protect (\u201cinoculate\u201d) public attitudes about climate change against real\u2010world misinformation.\n\n==\n\nFarrell 2019:\n\nTitle: The growth of climate change misinformation in US philanthropy: evidence from natural language processing\n\nAbstract: Two of the most consequential developments affecting US politics are (1) the growing influence of private philanthropy, and (2) the large-scale production and diffusion of misinformation. Despite their importance, the links between these two trends have not been scientifically examined. This study employs a sophisticated research design on a large collection of new data, utilizing natural language processing and approximate string matching to examine the relationship between the large-scale climate misinformation movement and US philanthropy. The study finds that over a twenty year period, networks of actors promulgating scientific misinformation about climate change were increasingly integrated into the institution of US philanthropy. The degree of integration is predicted by funding ties to prominent corporate donors. These findings reveal new knowledge about large-scale efforts to distort public understanding of science and sow polarization. The study also contributes a unique computational approach to be applied at this increasingly important, yet methodologically fraught, area of research.\n\n==\n\nWentz 2022:\n\nTitle: LIABILITY FOR PUBLIC DECEPTION: LINKING FOSSIL FUEL DISINFORMATION TO CLIMATE DAMAGES\n\nAbstract: Over two dozen U.S. states and municipalities have filed lawsuits against fossil fuel companies, seeking abatement orders and compensation for climate damages based on theories such as public nuisance, negligence, and failure to warn, and alleging these companies knew about the dangers of their products, intentionally concealed those dangers, created doubt about climate science, and undermined public support for climate action. This Article examines how tort plaintiffs can establish a causal nexus between public deception and damages, drawing from past litigation, particularly claims filed against manufacturers for misleading the public about the risks of tobacco, lead paint, and opioids. A key finding is that courts may infer public reliance on false and misleading statements using multiple lines of evidence, including information about the scope and magnitude of the deceptive communications, defendants\u2019 internal assessments of the efficacy of their disinformation campaigns, acknowledgements of intended reliance made by defendants, expert testimony on the effects of disinformation, public polling data",
        "final_summary": "The papers collectively highlight the presence and impact of climate change disinformation or misinformation in various contexts. Treen 2020 emphasizes the involvement of different actors in financing, producing, and amplifying misinformation, while also discussing the role of online social networks in its spread. Ka\u1e91ys 2018 focuses on the risks of disinformation and misinterpretation in climate change information found on the internet, particularly noting disparities among different languages. Hassan 2023 examines the types and agents of climate change disinformation reported in online newspapers, highlighting the role of politicians, organizations, and anonymous agencies in spreading misleading and fabricated content. Moxnes 2008 explores the misperceptions of global climate change and proposes an information strategy to address them, emphasizing the need for proper mental models and motivation to reconsider decision heuristics. Mehling 2013 discusses the historical context of climate change denial in the United States and the challenges faced in addressing the issue. Linden 2017 suggests that conveying the consensus among experts about human-caused climate change can help counteract the spread of influential misinformation. Farrell 2019 examines the integration of actors promoting climate change misinformation into US philanthropy, particularly through funding ties to corporate donors. Wentz 2022 explores the liability of fossil fuel companies for public deception and its connection to climate damages, drawing from past litigation and the establishment of a causal nexus between disinformation and harm. Overall, these papers shed light on the existence, spread, and consequences of climate change disinformation or misinformation, as well as potential strategies to address this issue."
    },
    {
        "query": "Curry (2005) and Seip (2013) highlight that government-owned businesses often prioritize meeting social obligations over commercial objectives, leading to inefficient resource allocation and financial struggles for these enterprises.",
        "paper_list_string": "Christiansen 2013:\n\nTitle: Balancing Commercial and Non-Commercial Priorities of State-Owned Enterprises\n\nAbstract: The overarching question for the government owners of state-owned enterprises (SOEs) is why these companies need to be owned by the state. The OECD Guidelines on Corporate Governance of State-Owned Enterprises provides a \u201cblueprint\u201d for the corporatisation and commercialisation of such enterprises, but it may be assumed that the reason for continued state ownership is that they are expected to act differently from private companies. A relatively clear case occurs when SOEs are established with the purpose of pursuing mostly non-commercial activities. In many cases, their activities might otherwise be carried out by government institutions; the SOE incorporation has been chosen mostly on efficiency grounds.A number of other rationales for public ownership of enterprises have been offered, including: (i) monopolies in sectors where competition and market regulation is not deemed feasible or efficient; (ii) market incumbency, for instance in sectors where competition has been introduced but a state-owned operator remains responsible for public service obligations; (iii) imperfect contracts, where those public service obligations that SOEs are charged with are too complex or malleable to be laid down in service contracts; (iv) industrial policy or development strategies, where SOEs are being used to overcome obstacles to growth or correct market imperfections...\n\n==\n\nMoizer 2010:\n\nTitle: Strategy making in social enterprise: The role of resource allocation and its effects on organizational sustainability\n\nAbstract: Social enterprises are non-profit organizations that seek to achieve social goals through commercial activity. Because they have both social and commercial objectives, social enterprises are confronted with high levels of complexity in their strategizing and operations; not only do they have to generate sufficient revenue to re-invest in their business operations, they also have to maintain investment in social projects in their community. Under conditions of resource scarcity, this poses severe challenges which can threaten the long-term sustainability of the enterprise. In this paper the tension between allocating resources to commercial activity and social action is explored. A simple causal-loop diagram is constructed which maps out the relationships between resource allocation and a number of other variables thought to influence the sustainability of social enterprises. By tracing through the diagram, a range of generic strategic alternatives available to social enterprises is evaluated and discussed. Copyright \u00a9 2010 John Wiley & Sons, Ltd.\n\n==\n\nJanda 2011:\n\nTitle: Inefficient Credit Rationing and Public Support of Commercial Credit Provision\n\nAbstract: Credit contracting between a lender with market power and a small start-up entrepreneur may lead to the rejection of projects whose expected benefits are higher than their total costs when adverse selection is present. This inefficiency may be eliminated by government support in the form of credit guarantees or interest-rate subsidies. The principal-agent model of this paper compares different forms of government support and concludes that credit guarantees and interest-rate subsidies have an unambiguous positive effect on social efficiency, since they enable the funding of socially efficient projects that would not be financed otherwise.\n\n==\n\nSappington 2004:\n\nTitle: Anticompetitive Behavior by State-Owned Enterprises: Incentives and Capabilities\n\nAbstract: State-owned enterprises (SOEs), also known as public enterprises, are owned by governments rather than private investors and compete directly with private, profit-maximizing enterprises in many important markets. For example, government postal firms typically offer overnight mail and package shipping services in direct competition with private delivery companies. In addition, many public hospitals and educational institutions compete directly with private suppliers of similar services. Production by public enterprises is particularly widespread in developing countries. During the 1980s, for example, public enterprises accounted for approximately 14 percent of gross domestic product in African nations and approximately 11 percent in developing countries as a whole.1 Typically, SOEs are required to pursue goals other than pure profit maximization. One might therefore suspect they would act less aggressively toward their competitors than would private, profit-\n\n==\n\nGroves 1937:\n\nTitle: Government Corporations and Federal Funds\n\nAbstract: The increasing use of the government corporation in the conduct of governmental business enterprises is one of the most significant of recent trends in public administration. Indeed, the corporate device represents one answer to the charge that government is so poorly organized and so beset by red tape that public operation of economic enterprise is inevitably sluggish and inefficient. Most notably during the mobilization crisis of 1917 and the depression years following 1929, when speedy and vigorous action was needed, the federal government has relied heavily upon government corporations which could cut through red tape and \u201cget things done.\u201d Also in more normal times, however, particular enterprises somewhat isolated from regular governmental activities have been carried on in corporate form.\n\n==\n\nVernon-Wortzel 1989:\n\nTitle: Privatization: Not the only answer\n\nAbstract: Abstract Governments in many developing countries created state-owned enterprises (SOEs) to produce goods or provide services the private sector seemed unwilling or unable to offer. In many cases, SOEs have turned out to be problem children\u2014at best, inefficient and, at worst, unable to fulfill their original functions. Now the recommended solution to the problem of state-owned enterprises is privatization. But privatization is no more a solution to the problems of SOEs than SOEs were a solution to the problems they were created to solve. The problem of SOEs is not ownership, but rather a lack of explicit goals and objectives, and an absence of organization cultures and systems that support and encourage fulfillment of those goals and objectives. In some circumstances, privatization may aid in fulfillment of an enterprise's goals and objectives because of the culture and systems it fosters; in other cases not.\n\n==\n\nKodzi 2015:\n\nTitle: The Clash of Missions: Juxtaposing Competing Pressures in South Africa's Social Enterprises\n\nAbstract: Abstract Social enterprises seek resource combinations to provide some assurance of sustainability as they create social value in a defined domain of action. However, this resource-seeking mandate also constitutes a distraction that complicates the operations of any social enterprise. How do social enterprises manage the implicit duality of maintaining a commercial mission in order to achieve their social mission? Using the context of South Africa, this study examines the process trade-offs that enhance or limit social impact under the referenced duality. The study clarifies the nature of these trade-offs, as a basis for appropriating efficiency-enhancing process design in enterprises that create, but do not capture value. The author proposes a unique intersection between the logic of control and the logic of empowerment in the field of social entrepreneurship, and suggests that value chain processes be controlled to the extent that the enterprise acts as a custodian of community empowerment for its target beneficiaries.\n\n==\n\nHua 2021:\n\nTitle: Referee, Sponsor or Coach: How Does the Government Harness the Development of Social Enterprises? A Case Study of Chengdu, China\n\nAbstract: This paper examines how the government harnesses the development of social enterprises. Drawing on an in-depth qualitative study in Chengdu City, China, we find that institution building, financial supporting, incubation developing, systematical cultivating and resource matchmaking are the main strategies employed by the municipal government to promote the development of social enterprises, whereupon we argue that besides the role as referee and sponsor which are the governments\u2019 traditional roles in western countries, the government in China also play a role of coach in harnessing the development of social enterprises. This paper contributes to the literature by theorizing how administrative forces affect the growth of hybrid organizations and also contributes to providing empirical evidence of the institutional environment of social enterprises in China.",
        "final_summary": "Christiansen (2013) discusses the different rationales for public ownership of state-owned enterprises, including non-commercial activities and market incumbency. Moizer (2010) highlights the challenges faced by social enterprises in balancing resource allocation between commercial activity and social action. Janda (2011) argues that government support, such as credit guarantees and interest-rate subsidies, can improve the efficiency of credit provision by addressing adverse selection. Sappington (2004) examines the anticompetitive behavior of state-owned enterprises. Vernon-Wortzel (1989) challenges the notion that privatization is the only solution to the problems of state-owned enterprises, emphasizing the importance of explicit goals and organizational cultures. Kodzi (2015) explores the trade-offs faced by social enterprises in South Africa in maintaining a commercial mission while achieving their social mission. Hua (2021) examines how the government in Chengdu, China, harnesses the development of social enterprises through institution building, financial support, incubation, and resource matchmaking."
    },
    {
        "query": "how does deliberate ignorance affect punishment in the dictator game?",
        "paper_list_string": "Bartling 2013:\n\nTitle: Does Willful Ignorance Deflect Punishment? - An Experimental Study\n\nAbstract: This paper studies whether people can avoid punishment by remaining willfully ignorant about possible negative consequences of their actions for others. We employ a laboratory experiment, using modified dictator games in which a dictator can remain willfully ignorant about the payoff consequences of his decision for a receiver. A third party can punish the dictator after observing the dictator\u2019s decision and the resulting payoffs. On the one hand, willfully ignorant dictators are punished less if their actions lead to unfair outcomes than dictators who reveal the consequences before implementing the same outcome. On the other hand, willfully ignorant dictators are punished more than revealing dictators if their actions do not lead to unfair outcomes. We conclude that willful ignorance can circumvent blame when unfair outcomes result, but that the act of remaining willfully ignorant is itself punished, regardless of the outcome.\n\n==\n\nRuffle 1998:\n\nTitle: More Is Better, But Fair Is Fair: Tipping in Dictator and Ultimatum Games\n\nAbstract: Abstract This paper examines Allocators' willingness to reward and punish their paired Recipients. Recipients only compete in a skill-testing contest, the outcome of which determines the size of the surplus. In the dictator game, Allocators reward skillful Recipients, but punish unskillful ones only modestly. The punishment effect is mitigated by the belief held by some Allocators that effort is the appropriate measure of deservingness. The ultimatum game extension reveals offerers' ability to adapt to the strategic environment. Offers to skillful Recipients in the ultimatum game, however, are shown to be motivated by a taste for fairness, and not strategic considerations. Journal of Economic Literature Classification Numbers: C70, C91, D63.\n\n==\n\nCason 1998:\n\nTitle: Social Influence in the Sequential Dictator Game.\n\nAbstract: This paper introduces the sequential dictator game to study how social influence may affect subjects' choices when making dictator allocations. Subjects made dictator allocations of $40 before and after learning the allocation made by one other subject in the Relevant Information treatment, or the birthday of one other subject in the Irrelevant Information treatment. Subjects on average become more self-regarding in the Irrelevant Information treatment, but observing relevant information constrains some subjects from moving toward more self-regarding choices. We also find that subjects who exhibit more self-regarding behavior on their first decisions are less likely to change choices between their first and second decisions, and the use of the Strategy Method in this setting does not significantly alter choices. The relationships between our findings and the economic and psychological literature regarding how social influence operates are also explored. Copyright 1998 Academic Press.\n\n==\n\nSchulz 2014:\n\nTitle: Affect and fairness: Dictator games under cognitive load\n\nAbstract: We investigate the impact of affect and deliberation on other-regarding decisions. In our laboratory experiment subjects decide on a series of mini-Dictator games while under varying degrees of cognitive load. Cognitive load is intended to decrease deliberation and therefore enhance the influence of affect on behavior. In each game subjects have two options: they can decide between a fair and an unfair allocation. We find that subjects in a high-load condition are more generous \u2013 they more often choose the fair allocation than subjects in a low-load condition. The series of mini-Dictator games also allows us to investigate how subjects react to the games\u2019 varying levels of advantageous inequality. Low-load subjects react considerably more to the degree of advantageous inequality. Our results underscore the importance of affect for basic altruistic behavior and deliberation in adjusting decisions to a given situation.\n\n==\n\nAchtziger 2015:\n\nTitle: Money, depletion, and prosociality in the dictator game\n\nAbstract: We study the effects of ego depletion, a manipulation which consumes self-control resources, on social preferences in a dictator game. Depleted dictators give considerably less than non-depleted dictators and hence exhibit strong preferences for selfish allocation. In contrast to earlier studies, participants were explicitly paid for completing the egodepletion task (with either a flat rate or strictly performance-based payment). We studied the dynamics of decisions by repeating the dictator game 12 times (anonymously). Depleted dictators start with much lower offers than non-depleted ones, but, strikingly, offers decrease in time for both groups, and more rapidly so for non-depleted dictators. We conclude that, while depleted dictators neglect fairness motives from the very first decision on, non-depleted dictators initially resist the tendency to act selfishly, but eventually become depleted or learn to act selfishly. Hence, prosocial behavior may be short-lived, and ego depletion uncovers the default tendencies for selfishness earlier.\n\n==\n\nDana 2006:\n\nTitle: What you don't know won't hurt me: Costly (but quiet) exit in dictator games\n\nAbstract: We used simple economic games to examine pro-social behavior and the lengths that people will take in order to avoid engaging in it. Over two studies, we found that about one third of participants were willing to exit a $10 dictator game and take $9 instead. The exit option left the receiver nothing, but also ensured that the receiver never knew that a dictator game was to be played. Because most social utility models are defined over monetary outcomes, they cannot explain choosing the ($9, $0) exit outcome over the dominating $10 dictator game, since the game includes outcomes of ($10, $0) and ($9, $1). We also studied exiting using a private dictator game. In the private game, the receiver never knew about the game or from where any money was received. Gifts in this game were added innocuously to a payment for a separate task. Almost no dictators exited from the private game, indicating that receivers' beliefs are the key factor in the decision to exit. When, as in the private game, the receivers' beliefs and expectations cannot be manipulated by exit, exit is seldom taken. We conclude that giving often reflects a desire not to violate others' expectations rather than a concern for others' welfare per se. We discuss the implications of our results for understanding ethical decisions and for testing and modeling social preferences. An adequate specification of social preferences should include \"psychological\" payoffs that directly incorporate beliefs about actions into the utility function.\n\n==\n\nEdele 2013:\n\nTitle: Explaining altruistic sharing in the dictator game : The role of affective empathy, cognitive empathy, and justice sensitivity\n\nAbstract: Abstract Experimental games like the dictator game have proven of great value for the study of altruism and sharing behavior. It has been shown that individuals differ substantially in the amount of money they offer to an anonymous receiver. Yet, to date little is known about how personality dispositions shape differences in altruistic sharing. The current study aimed at elucidating the effects of affective empathy in comparison to cognitive empathy and the disposition of justice sensitivity on altruistic sharing in the dictator game. Different aspects of these constructs were assessed multi-methodologically in young adults and related to their offers in the dictator game. Analyses revealed that affective empathy and justice sensitivity explain altruistic sharing behavior, but different dimensions of cognitive empathy do not. The results support the assumption that affective reaction tendencies explain differences in altruistic sharing.\n\n==\n\nG\u0105siorowska 2012:\n\nTitle: Psychological consequences of money and money attitudes in dictator game\n\nAbstract: According to the research conducted by Vohs, Mead, and Goode (2006, 2008), reminders of money cause people to behave self-sufficiently, and especially to reveal a reduced tendency to charitable behaviour. In this study, we wanted to establish if this tendency would be present in the dictator game, and if so, whether money activation would just change behaviour, or whether it would also change people\u2019s evaluation of their own decisions. We assumed that people who revealed symbolic and instrumental money attitudes would react differently to money activation. As expected, money priming caused smaller money transfers compared to the control condition, and this effect was significant among those with symbolic attitudes toward money. Moreover, participants who transferred at least half of the sum after money activation declared stronger negative emotions and lower satisfaction with the decision than those who made similar transfers in the control condition.",
        "final_summary": "The papers suggest that deliberate ignorance can have varying effects on punishment in the dictator game. Bartling 2013 found that willfully ignorant dictators are punished less when their actions lead to unfair outcomes, but punished more when their actions do not lead to unfair outcomes. Ruffle 1998 found that allocators in the dictator game reward skillful recipients but only modestly punish unskillful ones. Cason 1998 found that social influence can affect subjects' choices in making dictator allocations. Schulz 2014 found that cognitive load can influence other-regarding decisions, with subjects under high cognitive load being more generous. Achtziger 2015 found that ego depletion leads to selfish allocation in the dictator game. Dana 2006 found that some participants are willing to exit the dictator game to avoid engaging in pro-social behavior. Edele 2013 found that affective empathy and justice sensitivity can explain differences in altruistic sharing in the dictator game. G\u0105siorowska 2012 found that money priming can lead to smaller money transfers in the dictator game, particularly among those with symbolic attitudes toward money."
    },
    {
        "query": "wheat stem sugar reserve mobilization",
        "paper_list_string": "Ram 2018:\n\nTitle: Stem Reserve Mobilization in Relation to Yield under Different Drought and High Temperature Stress Conditions in Wheat (Triticum aestivum L.) Genotypes\n\nAbstract: Carbon requirements for grain filling in wheat are mainly from current assimilation by photosynthesis and remobilization of reserves from the stems (Yang et al., 2000). Remobilization of assimilates is an active process that involves translocation of stored reserves from stems and sheaths to grains (Gupta et al., 2015). Stem reserves contribute 20 to 40% weight of the grain in non-stressed condition (Vignjevic et al., 2015) and this can be up to 70% under stressed conditions during grain filling (Rebetzke et al., 2008). Drought and high temperature induced earlier mobilization of non-structural reserve carbohydrates from stem and leaf sheaths, which provided a greater proportion of grain International Journal of Current Microbiology and Applied Sciences ISSN: 2319-7706 Volume 7 Number 04 (2018) Journal homepage: http://www.ijcmas.com\n\n==\n\nBlum 1997:\n\nTitle: Improving wheat grain filling under stress by stem reserve mobilisation\n\nAbstract: Stem reserves from pre-anthesis plant assimilation are being increasingly recognised as an important source of carbon for grain filling when current photosynthesis is inhibited by drought, heat or disease stress during this stage. Genotypic and environmental factors affecting reserve accumulation and utilisation for grain filling are reviewed. The genetic improvement of stem reserve storage and utilisation as a potent mechanism for grain filling under stress is discussed, and practical guidelines for selection work are provided.\n\n==\n\nBlum 2004:\n\nTitle: Improving wheat grain filling under stress by stem reserve mobilisation\n\nAbstract: Stem reserves from pre-anthesis plant assimilation are being increasingly recognised as an important source of carbon for grain filling when current photosynthesis is inhibited by drought, heat or disease stress during this stage. Genotypic and environmental factors affecting reserve accumulation and utilisation for grain filling are reviewed. The genetic improvement of stem reserve storage and utilisation as a potent mechanism for grain filling under stress is discussed, and practical guidelines for selection work are provided.\n\n==\n\nDavidson 1992:\n\nTitle: Storage and remobilization of water-soluble carbohydrates in stems of spring wheat\n\nAbstract: Grain yield of wheat (Triticum aestivum L.) depends, in part, on carbohydrate reserves available in the stem. This study was conducted to determine the effects of water deficit during the post-jointing period on quantitative changes in water soluble carbohydrates (WSC; including simple sugars, starch, and fructans) in the stems of spring wheat. Cultivars Edwall and Waverly were planted in 1983 and 1984 at Spillman Agronomy Farm near Pullman, WA, at rates of 94 and 168 kg per ha in rows 30 and 15 cm apart, respectively, in both irrigated and nonirrigated treatments. Beginning at jointing, plants were harvested weekly. Stem material was dried, milled, digested with amyloglucosidase, and analyzed for WSC by iodometric titration. Results were similar for both varieties and both years. Anthesis and peak stem carbohydrate concentration occurred 4 to 7 d earlier in nonirrigated than irrigated plants; and physiological maturity of the grain occurred 6 to 14 days earlier. The concentration of WSC in stems increased to between 250 and 380 mg per g dry wt. at approximately equal to 10 to 14 d after anthesis and then declined to less than 50 mg per g dry wt. by physiological maturity of the grain. From the time of peak WSC content until physiological maturity in 1984, 959 to 1235 mg WSC were lost from the stems of irrigated plants but only 619 to 662 mg WSC were lost from stems of nonirrigated plants. The data indicate that stems are an important temporary storage site for reserve carbohydrates in both irrigated and nonirrigated plants.\n\n==\n\nBlum 1994:\n\nTitle: Stem Reserve Mobilisation Supports Wheat-grain Filling Under Heat Stress\n\nAbstract: The grain filling of wheat (Triticum aestivum L.) is seriously impaired by heat stress due to reductions in current leaf and ear photosynthesis at high temperatures. An alternative source of carbon for grain filling is stored stem reserves. Two spring wheat cultivars (V5 and V2183) of very similar phenology and plant stature, which had previously been found to differ in grain shrivelling under drought and heat stress conditions in the field, were used to evaluate the hypothesis that the mobilisation of stored stem reserves into the growing grain is an important source of carbon for supporting grain filling under heat stress. In two experiments in Israel (1990 and 1991), the rates of stem dry matter (DM) and stem total non-structural carbohydrates (TNC) loss, grain growth and leaf senescence were monitored under optimal (control) and high (stressed) temperatures in the glasshouse (1990) and the growth chamber (1991). Cultivar V5 always sustained a smaller reduction in grain dry weight under heat stress, than V2183. Irrespective of temperature, V5 had a higher stem DM and TNC content at the onset of grain filling, greater depletion of stem dry matter (or TNC) during grain filling, and longer duration of grain filling, than V2183. During grain filling V5 generally exported about two to three times more DM from the stems than V2183, under both non-stressed and stressed conditions. On the other hand, V5 was more heat-susceptible than V2183 in terms of leaf longevity, in vivo chlorophyll stability and grain abortion under heat stress. In a third experiment (1992) five cultivars (including V5 and V2183) were subjected to chemical desiccation (0.3% potassium iodide) of the canopy in the field in order to destroy the photosynthetic source ofthe plant after anthesis. The same cultivars were subjected to heat stress (35/25oC) or non-stressed (25/15oC) conditions after anthesis in the growth chamber. It was found that grain dry weight reduction by chemical desiccation was highly correlated with grain dry weight reduction by heat stress (r2 = 0.89). Therefore, the superior capacity of V5 for grain filling from mobilised stem reserves is a consti- tutive trait which supports grain filling under heat stress which can be tested for by chemical desiccation of plants under non-stressed conditions.\n\n==\n\nSrivastava 2017:\n\nTitle: Effect of stem reserve mobilization on grain filling under drought stress conditions in recombinant inbred population of wheat\n\nAbstract: Pre-anthesis carbon assimilation of stem reserves is considered as an important source for grain filling during post anthesis drought stresses that inhibit photosynthesis. 175 RILs from cross (C518/2*PBW343) along with check cultivars were evaluated for stem reserve mobilization under irrigated and rainfed conditions. These two cultivars belonging to distinct adaptation mechanism, offer several morpho -physiological and biochemical con-trasts. C 518 is tall and adapted to low input rainfed conditions whereas PBW 343 is semi -dwarf and input re-sponsive. Further C 518 is known for better stem reserves on account of larger peduncle and strong commitment to grain filling due to effective stem reserve mobilization. The parents and the RIL population was tested for stem reserve mobilization by defoliation of flag leaf and second leaf at anthesis under irrigated and rainfed environments. Evaluated entries differed significantly (pl0.001) for reduction in 1000 grain weight under defoliation (TGWL). Percent reduction in 1000 grain weight ranged from 4.4 % to 39.6 % under irrigated environment and 3.2 % to 35.0 under rainfed condition. A significant positive correlation (r = +0.357) between stem reserve mobilization and peduncle length was observed under rainfed condition. Tested RILs vary individually for stem reserve mobilization when subjected to removal of flag leaf and second leaf inhibiting the photosynthesis. The genotypes with better stem reserve mobilization based on 1000 grain weight in the absence of photosynthesis may also provide relative tolerance to drought.\n\n==\n\nSharbatkhari 2016:\n\nTitle: Wheat stem reserves and salinity tolerance: molecular dissection of fructan biosynthesis and remobilization to grains\n\nAbstract: AbstractMain conclusionFructan accumulation and remobilization to grains under salinity can decrease dependency of the wheat tolerant cultivar on current photosynthesis and protect it from severe yield loss under salt stress.\n Tolerance of plants to abiotic stresses can be enhanced by accumulation of soluble sugars, such as fructan. The current research sheds light on the role of stem fructan remobilization on yield of bread wheat under salt stress conditions. Fructan accumulation and remobilization as well as relative expression of the major genes of fructan metabolism were investigated in the penultimate internodes of \u2018Bam\u2019 as the salt-tolerant and \u2018Ghods\u2019 as the salt-sensitive wheat cultivars under salt-stressed and controlled conditions and their correlations were analyzed. More fructan production and higher efficiency of fructan remobilization was detected in Bam cultivar under salinity. Up-regulation of sucrose: sucrose 1-fructosyltransferase (1-SST) and sucrose: fructan 6-fructosyltransferase (6-SFT) (fructan biosynthesis genes) at anthesis and up-regulation of fructan exohydrolase (1-FEH) and vacuolar invertase (IVR) genes (contributed to fructan metabolism) during grain filling stage and higher expression of sucrose transporter gene (SUT1) in Bam was in accordance with its induced fructan accumulation and remobilization under salt stress. A significant correlation was observed between weight density, WSCs and gene expression changes under salt stress. Based on the these results, increased fructan production and induced stem reserves remobilization under salinity can decrease dependency of the wheat tolerant cultivar on current photosynthesis and protect it from severe yield loss under salt stress conditions.\n\n==\n\nGupta 2011:\n\nTitle: Stem Reserve Mobilization and Sink Activity in Wheat under Drought Conditions\n\nAbstract: The effect of water deficit on stem reserve mobilization and sink activity in wheat (Triticum aestivum L.) cultivars, viz., C306 (drought tolerant) and PBW343 (drought sensitive) was studied. Drought was maintained in pot raised plants by withholding irrigation at 95 days after sowing (DAS), i.e. just five days before the initiation of anthesis. Drought induced a significant reduction in mean biomass of all the internodes of sensitive cultivar as compared to those of tolerant one. Mobilized dry matter and mobilization efficiency were observed to be higher in the internodes of tolerant cultivar, both under control and stress conditions, which resulted in enhanced translocation of stem reserves to the grains. Water soluble carbohydrates (WSC), which mainly occur as fructans, were observed to be higher in the internodes of tolerant cultivar than those of sensitive one. When drought was applied, fructans were mobilized more effectively from the internodes of tolerant cultivar. A significantly higher sucrose synthase activity in the grains of tolerant cultivar, under drought conditions, increased the sink strength by unloading the assimilates in the sink, thereby increasing further mobilization of assimilates to the grains. Grains of sensitive cultivar attained maturity much earlier as compared to the tolerant one, both under control and stress conditions. The longer duration of grain maturation in tolerant cultivar supported enhanced mobilization of stem reserves, thus restricting heavy decrease in grain yield, under stress conditions, as compared to the sensitive cultivar. It may, therefore, be concluded that certain characteristics viz., enhanced capability of fructan storage, higher mobilization efficiency, stronger sink activity and longer duration of grain maturation might help the drought tolerant cultivar in coping the stress conditions",
        "final_summary": "The papers suggest that wheat stem sugar reserve mobilization plays a crucial role in grain filling under various stress conditions. Stem reserves contribute a significant proportion of grain weight, especially under stress conditions (Ram 2018). Stem reserves are mobilized when current photosynthesis is inhibited by drought, heat, or disease stress, providing an alternative source of carbon for grain filling (Blum 1997, Blum 2004). Drought and high temperature stress can induce earlier mobilization of non-structural reserve carbohydrates from stems and leaf sheaths, increasing their contribution to grain filling (Ram 2018). Stem reserve mobilization is influenced by genotypic and environmental factors, and improving storage and utilization of stem reserves can enhance grain filling under stress (Blum 1997, Blum 2004). Stem reserves, including water-soluble carbohydrates (WSC), mainly fructans, are important temporary storage sites for reserve carbohydrates in both irrigated and non-irrigated plants (Davidson 1992). The mobilization of stem reserves, particularly fructans, can decrease the dependency on current photosynthesis and protect wheat cultivars from severe yield loss under salt stress conditions (Sharbatkhari 2016). Drought-tolerant wheat cultivars exhibit enhanced stem reserve mobilization, higher mobilization efficiency, stronger sink activity, and longer grain maturation duration, contributing to better grain filling under drought conditions (Gupta 2011). Overall, these findings highlight the significance of stem sugar reserve mobilization in wheat grain filling under stress conditions."
    },
    {
        "query": "Health economic benefits of reducing NHS waiting times",
        "paper_list_string": "Cooper 2009:\n\nTitle: Equity, waiting times, and NHS reforms: retrospective study\n\nAbstract: Objective To determine whether observable changes in waiting times occurred for certain key elective procedures between 1997 and 2007 in the English National Health Service and to analyse the distribution of those changes between socioeconomic groups as an indicator of equity. Design Retrospective study of population-wide, patient level data using ordinary least squares regression to investigate the statistical relation between waiting times and patients\u2019 socioeconomic status. Setting English NHS from 1997 to 2007. Participants 427\u2009277 patients who had elective knee replacement, 406\u2009253 who had elective hip replacement, and 2\u2009568\u2009318 who had elective cataract repair. Main outcome measures Days waited from referral for surgery to surgery itself; socioeconomic status based on Carstairs index of deprivation. Results Mean and median waiting times rose initially and then fell steadily over time. By 2007 variation in waiting times across the population tended to be lower. In 1997 waiting times and deprivation tended to be positively related. By 2007 the relation between deprivation and waiting time was less pronounced, and, in some cases, patients from the most deprived fifth were waiting less time than patients from the most advantaged fifth. Conclusions Between 1997 and 2007 waiting times for patients having elective hip replacement, knee replacement, and cataract repair in England went down and the variation in waiting times for those procedures across socioeconomic groups was reduced. Many people feared that the government\u2019s NHS reforms would lead to inequity, but inequity with respect to waiting times did not increase; if anything, it decreased. Although proving that the later stages of those reforms, which included patient choice, provider competition, and expanded capacity, was a catalyst for improvements in equity is impossible, the data show that these reforms, at a minimum, did not harm equity.\n\n==\n\nGiuntella 2015:\n\nTitle: The Effects of Immigration on NHS Waiting Times\n\nAbstract: This paper analyzes the effects of immigration on waiting times for the National Health Service (NHS) in England. Linking administrative records from Hospital Episode Statistics (2003-2012) with immigration data drawn from the UK Labour Force Survey, we find that immigration reduced waiting times for outpatient referrals and did not have significant effects on waiting times in accident and emergency departments (A&E) and elective care. The reduction in outpatient waiting times can be explained by the fact that immigration increases natives' internal mobility and that immigrants tend to be healthier than natives who move to different areas. Finally, we find evidence that immigration increased waiting times for outpatient referrals in more deprived areas outside of London. The increase in average waiting times in more deprived areas is concentrated in the years immediately following the 2004 EU enlargement and disappears in the medium term (e.g., 3-4 years).\n\n==\n\nHarrison 2009:\n\nTitle: Reducing Waiting Times for Hospital Treatment: Lessons from the English NHS\n\nAbstract: In recent years, the English NHS has achieved substantial reductions in waiting times for hospital treatment. This paper considers first whether the data used by the Government provide an accurate description of changes in waiting times and identifies some of the limitations of the measures used. It then attempts to identify how reductions have been achieved. It argues that some features of central government policy have been important - such as the use of targets - others, such as the introduction of new private sector capacity have not. It also shows that changes at local level have been critical to achieving the recorded improvements, but the precise impact of these is hard to identify.\n\n==\n\nMarques 2014:\n\nTitle: Disclosing total waiting times for joint replacement: evidence from the English NHS using linked HES data.\n\nAbstract: For the last decade, stringent monitoring of waiting time performance targets provided English hospitals with incentives to reduce official waiting times for elective surgery. It is less clear whether the total amount of time patients waited in secondary care, from first referral to outpatient clinic until treatment, has also fallen. We used Hospital Episode Statistics inpatient data for patients undergoing total joint replacement during a period of active monitoring of targets (between 2006/7 and 2008/9) and linked it to outpatient data to reconstruct patients' pathway in the 3\u2009years before surgery and provide alternative measurements of waiting times. Our findings suggest that although official waiting times decreased drastically in our study period, total waiting time in secondary care has not declined. Patients with shorter official waits spent a longer time in a 'work-up' period prior to inclusion in the official waiting list, and socio-economic inequities persisted in waiting times for joint replacement. We found no evidence that target policies achieved efficiency gains during our study period.\n\n==\n\nHamilton 1999:\n\nTitle: The Impact of the NHS Reforms on Queues and Surgical Outcomes in England: Evidence From Hip Fracture Patients\n\nAbstract: National Health Service (NHS) reform introduced incentives for efficiency and cost effectiveness, yet little is known about their effectiveness in reducing waiting times for surgery or improving postsurgical outcomes. This paper finds that waiting times for hip fracture surgery declined after the NHS reforms and patients were more likely to be discharged to another provider. However, hospitals have not simply shifted the burden of care to other providers, since lengths of stay ending in a discharge to home fell. The effect of wait time on surgical outcomes is small in magnitude and cannot explain the postreform improvements in outcomes.\n\n==\n\nPropper 2002:\n\nTitle: Waiting times for hospital admissions: the impact of GP fundholding.\n\nAbstract: Waiting times for hospital care are a significant issue in the UK National Health Service (NHS). The reforms of the health service in 1990 gave a subset of family doctors (GP fundholders) both the ability to choose the hospital where their patients were treated and the means to pay for some services. One of the key factors influencing family doctors' choice of hospital was patient waiting time. However, without cash inducements, hospitals would get no direct reward from giving shorter waiting times to a subset of patients. Using a unique dataset, we investigate whether GP fundholders were able to secure shorter waiting times for their patients, whether they were able to do so in cases where they had no financial rewards to offer hospitals, and whether the impact of fundholding spilled over into shorter waiting times for all patients.\n\n==\n\nSiciliani 2005:\n\nTitle: Tackling excessive waiting times for elective surgery: a comparative analysis of policies in 12 OECD countries.\n\nAbstract: This paper compares policies to tackle excessive waiting times for elective surgery in 12 OECD countries. It is found that waiting times may be reduced by acting on the supply of or on the demand for surgery (or both). On the supply side, evidence suggests that both capacity and financial incentives towards productivity can play an important role. On the demand side, inducing a raising of clinical thresholds may reduce waiting times but may also provoke tension between clinicians and policy makers. Preliminary evidence also suggests that an increase in private health insurance coverage may reduce waiting times.\n\n==\n\nJones 2010:\n\nTitle: The four hour target to reduce emergency department \u2018waiting time\u2019: A systematic review of clinical outcomes\n\nAbstract: Governments in Australasia are introducing emergency department length of stay (EDLOS) time targets similar to the UK \u2018four hour rule\u2019. There is debate about whether this rule had beneficial effects on health\u2010care outcomes. We sought to determine what effects the four hour time target for EDLOS had on clinically relevant outcomes in the UK by conducting a systematic search for evidence. Articles were screened by both authors independently and assessed for quality using standard tools. Differences in outcomes measured and how they were measured precluded meta\u2010analysis. There were inconsistencies between target achievement reported by Trusts and that reported in the studies, and empirical evidence that the target might be unattainable. National Health Service spending on ED increased \u00a3820\u2003000\u2003000.00 (1998\u20132007) and emergency admissions rose overall by 35% (2002\u20132006), but not in all hospitals. Time to see a treating clinician and hospital mortality was unchanged. One hospital demonstrated a small reduction in return rate. The impact of the introduction of an ED time target and the associated massive financial investment has not resulted in a consistent improvement in care with markedly varying effects being reported between hospitals. Countries seeking to emulate the UK experience should proceed with caution.",
        "final_summary": "The papers suggest that reducing NHS waiting times can have various effects. Cooper 2009 found that waiting times for elective procedures decreased between 1997 and 2007 in England, with reduced variation across socioeconomic groups. Giuntella 2015 found that immigration reduced waiting times for outpatient referrals in the NHS, but increased waiting times in more deprived areas outside of London. Marques 2014, however, found that while official waiting times decreased, total waiting time in secondary care did not decline for joint replacement patients. The impact of NHS reforms on waiting times and surgical outcomes was explored by Hamilton 1999, which found that waiting times for hip fracture surgery decreased after the reforms, but the effect on outcomes was small. Propper 2002 investigated the impact of GP fundholding on waiting times and found that fundholders were able to secure shorter waiting times for their patients. Siciliani 2005 compared policies in 12 OECD countries and found that both supply-side interventions (increasing capacity and financial incentives) and demand-side interventions (raising clinical thresholds) can reduce waiting times. Finally, Jones 2010 examined the effects of the four-hour target for emergency department waiting times in the UK and found inconsistent improvements in care across hospitals."
    },
    {
        "query": "Online university versus on campus and academic performance differences",
        "paper_list_string": "Xu 2014:\n\nTitle: Performance Gaps Between Online and Face-to-Face Courses: Differences Across Types of Students and Academic Subject Areas\n\nAbstract: Using a dataset containing nearly 500,000 courses taken by over 40,000 community and technical college students in Washington State, this study examines the performance gap between online and face-to-face courses and how the size of that gap differs across student subgroups and academic subject areas. While all types of students in the study suffered decrements in performance in online courses, those with the strongest declines were males, younger students, Black students, and students with lower grade point averages. Online performance gaps were also wider in some academic subject areas than others. After controlling for individual and peer effects, the social sciences and the applied professions (e.g., business, law, and nursing) showed the strongest online performance gaps.\n\n==\n\nXu 2014:\n\nTitle: Performance Gaps between Online and Face-to-Face Courses: Differences across Types of Students and Academic Subject Areas\n\nAbstract: Using a dataset containing nearly 500,000 courses taken by over 40,000 community and technical college students in Washington State, this study examines the performance gap between online and face-to-face courses and how the size of that gap differs across student subgroups and academic subject areas. While all types of students in the study suffered decrements in performance in online courses, those with the strongest declines were males, younger students, Black students, and students with lower grade point averages. Online performance gaps were also wider in some academic subject areas than others. After controlling for individual and peer effects, the social sciences and the applied professions (e.g., business, law, and nursing) showed the strongest online performance gaps.\n\n==\n\nMcPhee 2010:\n\nTitle: Comparison of equated learning for online and on campus postgraduate students on academic achievement\n\nAbstract: This study assesses the effects of study mode on student achievement in two modes of study: on-campus learning and online learning. The University of the West of Scotland has been offering flexible postgraduate programmes in Alcohol and Drugs Studies online since 1999 and uses Blackboard, the Virtual Learning Environment (VLE), to support equated learning. The explicit focus of this continuing longitudinal study (dating originally from 2002) is on student achievement. In this continuing evaluation comparing on-campus and online student grade performance, online study groups have exactly the same module syllabus as their on-campus counterparts. There is equivalence of support in that students on both modes of study are taught on the same traditional 15-week trimesters as students on the on-campus version, have the same learning materials, live interactive lectures using the VLE as a central hub, and the same assessment methods including assignments, projects, and class tests. Most importantly, the online and oncampus modes of study had the same learning outcomes, the same academic module moderator and also the same external examiner to ensure that assessed work by students on each mode of study was marked to the same standard. Statistical analysis of academic outcomes revealed no significant differences in grades (summative marks) between online and on-campus groups. This finding indicates that students are not disadvantaged by selecting to study via online learning and that equated learning is indeed occurring in practice.\n\n==\n\nCampbell 2011:\n\nTitle: Assessment Of Student Performance And Attitudes For Courses Taught Online Versus Onsite\n\nAbstract: This paper assesses the differences in performance and attitudes of students taught online versus onsite. Students completed a course evaluation designed to determine student satisfaction in specific areas. Student performance was measured by means of a comprehensive exam that tested all material covered in the course. Results support the contention that students in online courses learn as much or more than students in traditional onsite courses and are as satisfied with the course and the instruction as their onsite counterparts.\n\n==\n\nCavanaugh 2015:\n\nTitle: A Large Sample Comparison of Grade Based Student Learning Outcomes in Online vs. Face-to-Face Courses.\n\nAbstract: Comparisons of grade based learning outcomes between online and face-to-face course formats have become essential because the number of online courses, online programs and institutional student enrollments have seen rapid growth in recent years. Overall, online education is largely viewed by education professionals as being equivalent to instruction conducted face-to-face. However, the research investigating student performance in online versus face-to-face courses has been mixed and is often hampered by small samples or a lack of demographic and academic controls. This study utilizes a dataset that includes over 5,000 courses taught by over 100 faculty members over a period of ten academic terms at a large, public, four-year university. The unique scale of the dataset facilitates macro level understanding of course formats at an institutional level. Multiple regression was used to account for student demographic and academic corollaries\u2014factors known to bias course format selection and grade based outcomes\u2014to generate a robust test for differences in grade based learning outcomes that could be attributed to course format. The final model identified a statistical difference between course formats that translated into a negligible difference of less than 0.07 GPA points on a 4 point scale. The primary influence on individual course grades was student GPA. Interestingly, a model based interaction between course type and student GPA indicated a cumulative effect whereby students with higher GPAs will perform even better in online courses (or alternatively, struggling students perform worse when taking courses in an online format compared to a face-to-face format). These results indicate that, given the large scale university level, multi course, and student framework of the current study, there is little to no difference in grade based student performance between instructional modes for courses where both modes are applicable.\n\n==\n\nChung 2022:\n\nTitle: Correlates of Academic Performance in Online Higher Education: A Systematic Review\n\nAbstract: The existing steady and continual rise of online learning in higher education has been accelerated by COVID-19 and resulted in a move away from solely on-campus teaching. Prior to the pandemic, online education was providing higher education to students who were returning to study to up-skill, are employed full-time, caring for family members, living rurally or remotely and/or for whom otherwise face-to-face campus learning was not a preference or option. To understand how we can better support online students in their unique circumstances and create an optimal learning environment, we must understand the factors associated with academic achievement within an online setting. This systematic review involved a search of relevant databases published between January 2009 and May 2021 examining factors and constructs related to academic performance in online higher education settings. Across 34 papers, 23 (67.6%) explored factors and constructs related to student characteristics including cognitive and psychological, demographic, university enrolment, and prior academic performance. Twenty-one (61.8%) papers explored learning environment factors including engagement, student experience, course design, and instructor. Our overall synthesis of findings indicates that academic performance in online learning is most strongly associated with motivation (including self-efficacy), and self-regulation. We propose three main implications of our review for online learning stakeholders such as educators and designers. Firstly, we argue that the wellbeing of online learners is important to understand, and future research should explore its impact on students\u2019 experience and success in online higher education. Secondly, we emphasise the importance of developing and designing online courses utilising relevant frameworks and evidence-based principles. Finally, we propose an approach to promoting improved student cognitive and psychosocial experiences (such as self-efficacy, self-regulation, and perceived support) could be achieved by creating and incorporating an online learning orientation module at the commencement of enrolment. Systematic Review Registration: (website), identifier (registration number).\n\n==\n\nDendir 2018:\n\nTitle: Performance differences between face-to-face and online students in economics\n\nAbstract: Abstract The author analyzes differences in student performance in an economics course offered face to face (F2F) and online over a period of four semesters at a comprehensive public university. Apart from mode of delivery, the characteristics of the course stayed nearly identical throughout. Exam and homework assignment scores are used as measures of performance. Although mean comparisons and basic regressions indicate that online students outperform F2F students on exams, once self-selection is addressed, online students no more outperform F2F students. Furthermore, according to assessments that gauge higher levels of learning, there is evidence that online students underperform compared with F2F students.\n\n==\n\nXu 2013:\n\nTitle: The impact of online learning on students\u2019 course outcomes: Evidence from a large community and technical college system\n\nAbstract: Using a large administrative dataset from a statewide system including 34 community and technical colleges, the authors employed an instrumental variable technique to estimate the impact of online versus face-to-face course delivery on student course performance. The travel distance between each student's home and college campus served as an instrument for the likelihood of enrolling in an online section of a given course. In addition, college-by-course fixed effects controlled for within- and between-course selection bias. Analyses yield robust negative estimates for online learning in terms of both course persistence and course grade, contradicting the notion that there is no significant difference between online and face-to-face student outcomes\u2014at least within the community college setting. Accordingly, both two-year and four-year colleges may wish to focus on evaluating and improving the quality of online coursework before engaging in further expansions of online learning.",
        "final_summary": "The papers suggest that there are performance differences between online and on-campus courses. Xu 2014 found that all types of students experienced decreased performance in online courses, with the largest declines observed in males, younger students, Black students, and students with lower GPAs. Performance gaps were also wider in certain academic subject areas. McPhee 2010, however, found no significant differences in grades between online and on-campus groups when equated learning was implemented. Campbell 2011 supported the notion that online students perform as well as, if not better than, on-campus students and are equally satisfied with their courses. Cavanaugh 2015, using a large dataset, found a negligible difference in grade-based learning outcomes between online and face-to-face courses. Chung 2022 highlighted the importance of factors such as motivation and self-regulation in online academic performance. Dendir 2018 found that online students initially outperformed face-to-face students on exams, but once self-selection was considered, the performance difference disappeared. Xu 2013, using an instrumental variable technique, found negative estimates for online learning in terms of course persistence and grade. These findings suggest that while online learning may have performance differences compared to on-campus learning, the extent and direction of these differences vary across studies and student characteristics."
    },
    {
        "query": "ClO/ClO2 data in water analysis",
        "paper_list_string": "Zhang 2021:\n\nTitle: Rapid in situ determination of ClO2 in drinking water by improved solid DPD spectrophotometry\n\nAbstract: This research aims to realize the rapid detection of ClO2 content in drinking water by adopting improved solid DPD. This method is fast and convenient with low cost and less waste liquid. The results show that this method has good precision and sensitivity. The linear correlation coefficients of the cubic regression equation were all greater than 0.999. The detection limit of the method was 0.002mg/L ClO2. The relative standard deviations (RSD) of seven parallel tests were between 1.37% and 8.87%, and the relative errors were small. The recovery rate was 96.67~110%. The method could be used for the direct determination of water samples with a mass concentration of 0.02mg/L~2.00mg/L in drinking water after ClO2 disinfection.\n\n==\n\nJiang 2006:\n\nTitle: Resonance scattering effect of rhodamine dye association nanoparticles and its application to respective determination of trace ClO2 and Cl2.\n\nAbstract: A new resonance scattering method, based on resonance scattering (RS) effect, for the respective determination of ClO2 and Cl2 in water samples was developed. In HCl-NaAc buffer solutions with the pH value of 1.42, chlorine dioxide, or chlorine, oxidizes I- to form 12, which then reacts with the excess I- to form I3-. The resulting 13- would combine, respectively, with four rhodamine(Rh) dyes, including rhodamine B (RhB), butyl rhodamine B (b-RhB), rhodamine 6G (RhG), and rhodamine S (RhS), to form association particles which exhibit a stronger resonance scattering (RS) effect at 420 nm. For four systems of RhB, bRhB, RhG, and RhS, chlorine dioxide was, respectively, determined in the concentration range of 0.0056 to approximately 0.787 mg/L, 0.0034 to approximately 0.396 mg/L, 0.0057 to approximately 0.795 mg/L, and 0.0052 to approximately 0.313 mg/L, with the detection limits of 0.0011 mg/L, 0.006 mg/L, 0.0054 mg/ L, and 0.0023 mg/L ClO2, respectively. At the same experimental conditions as those for the determination of ClO2, chlorine was, respectively, determined in the concentration range of 0.013 to approximately 0.784 mg/L, 0.0136 to approximately 0.522 mg/ L, 0.014 to approximately 0.81 mg/L, and 0.014 to approximately 0.42 mg/L, with the detection limits of 0.0016 mg/L, 0.0104 mg/L, 0.0079 mg/L, and 0.0037 mg/L Cl2, respectively. The total RS value originally from ClO2 and Cl2 was recorded in the buffer solution, while the RS value from ClO2 was obtained by using dimethyl sulfoxide to mask chlorine. Thus the RS value of chlorine was calculated by deducting the RS value of chlorine dioxide from the total RS value. The RhB RS method was chosen for the determination of ClO2 and Cl2 in drinking water, with advantages of high sensitivity, good selectivity, simplicity, rapidity, and convenience.\n\n==\n\nWheeler 1978:\n\nTitle: A rapid microdetermination of chlorine dioxide in the presence of active chlorine compounds\n\nAbstract: Abstract A titrimetric and spectrophotometric procedure has been developed for the determination of ClO2 in water samples. The procedure is rapid, accurate, and free of normal interferences present in water. It is based upon the reaction of ClO2 with substituted halophenol indicators.\n\n==\n\nGan 2020:\n\nTitle: The reactions of chlorine dioxide with inorganic and organic compounds in water treatment: kinetics and mechanisms\n\nAbstract: Chlorine dioxide (ClO2), as an alternative to chlorine, has been widely applied in water treatment. In order to better understand the performance of ClO2 in water treatment, the kinetics and mechanisms of ClO2 reactions with inorganic and organic compounds found in waters are critically reviewed. In the case of inorganic compounds, ClO2 reacts with I\u2212, CN\u2212, NO2\u2212, SO32\u2212, Fe(II) and Mn(II) rapidly at apparent second-order reaction rate constants (kapp) of 102\u2013106 M\u22121 s\u22121 at pH 7.0 and barely reacts with NH4+ and Br\u2212. In the case of organic compounds, ClO2 selectively reacts with compounds with electron-rich moieties, such as phenols (kapp = 103\u2013109 M\u22121 s\u22121), anilines (kapp = 105\u2013108 M\u22121 s\u22121), and thiols (kapp > 108 M\u22121 s\u22121). ClO2 also shows high reactivity towards aliphatic tertiary amines and heterocyclic nitrogenous compounds (i.e., indoles and piperidines) with kapp of 101\u2013106 M\u22121 s\u22121 at pH 7.0, but low reactivity with unsaturated structures (i.e., olefins and aldehydes). The kapp values at pH 7.0 in ClO2 oxidation vary over 14 orders of magnitude. Electron transfer is the dominant pathway for ClO2 reactions. Quantitative structure\u2013activity relationships (QSARs) can be used to predict the species-specific secondary reaction rate constants for ClO2 oxidation of compounds containing phenolic and amine structures. Little modifications are expected on the structure of the parent compounds upon the primary attack of ClO2, but further oxidation generally leads to the formation of quinones, aldehydes and carboxylic acids. Furthermore, the transformation kinetics of inorganic compounds, typical organic compounds and emerging micropollutants are compared and their half-life times under typical water treatment conditions during ClO2 oxidation are calculated.\n\n==\n\nHuber 2005:\n\nTitle: Oxidation of pharmaceuticals during water treatment with chlorine dioxide.\n\nAbstract: The potential of chlorine dioxide (ClO2) for the oxidation of pharmaceuticals during water treatment was assessed by determining second-order rate constants for the reaction with selected environmentally relevant pharmaceuticals. Out of 9 pharmaceuticals only the 4 following compounds showed an appreciable reactivity with ClO2 (in brackets apparent second-order rate constants at pH 7 and T = 20 degrees C): the sulfonamide antibiotic sulfamethoxazole (6.7 x 10(3) M(-1) s(-1)), the macrolide antibiotic roxithromycin (2.2 x 10(2) M(-1) s(-1)), the estrogen 17alpha-ethinylestradiol (approximately 2 x 10(5) M(-1) s(-1)), and the antiphlogistic diclofenac (1.05 x 10(4) M(-1) s(-1)). Experiments performed using natural water showed that ClO2 also reacted fast with other sulfonamides and macrolides, the natural hormones estrone and 17beta-estradiol as well as 3 pyrazolone derivatives (phenazone, propylphenazone, and dimethylaminophenazone). However, many compounds in the study were ClO2 refractive. Experiments with lake water and groundwater that were partly performed at microgram/L to nanogram/L levels proved that the rate constants determined in pure water could be applied to predict the oxidation of pharmaceuticals in natural waters. Compared to ozone, ClO2 reacted more slowly and with fewer compounds. However, it reacted faster with the investigated compounds than chlorine. Overall, the results indicate that ClO2 will only be effective to oxidize certain compound classes such as the investigated classes of sulfonamide and macrolide antibiotics, and estrogens.\n\n==\n\nQuentel 1994:\n\nTitle: Electrochemical determination of low levels of residual chlorine dioxide in tap water\n\nAbstract: The reaction between 1,2-dihydroxyanthraquinone-3-sulphonic acid and chlorine dioxide, in phosphate buffer medium, was studied spectrophotometrically and electrochemically. An electroanalytical method is proposed for the determination of traces of ClO2 with a detection limit of 2 \u03bcg l\u22121. The stoichiometry of the reaction, the effects of various operational parameters and the influence of foreign ions are reported.\n\n==\n\nHoehn 2003:\n\nTitle: ClO2 and by\u2010product persistence in a Drinking Water System\n\nAbstract: This research evaluated a chlorine gas\u2013solid sodium chlorite chlorine dioxide (ClO2) generation system at the Roanoke County (Va.) Spring Hollow Water Treatment Facility and monitored concentrations of the oxidant and its by\u2010products within the facility and distribution system. Project objectives were to (1) document changes in ClO2, chlorite ion (ClO2\u2013), and chlorate ion (ClO3\u2013) concentrations throughout the facility and distribution system following raw water ClO2 pretreatment; (2) evaluate ClO2\u2013 removal by the postfilter granular activated carbon (GAC) contactors; and (3) determine distribution system ClO2 concentrations associated with odor complaints. The average raw water ClO2 dose was 0.4 mg/L, and overall ClO2\u2013 removals by GAC averaged 63% but declined markedly with increasing throughput volume. Distribution system ClO2\u2013 concentrations were generally <0.2 mg/L and decreased with increasing distance from the treatment facility; ClO3\u2013 never exceeded 0.1 mg/L. ClO2 was measured at low concentrations (0.017\u20130.17 mg/L) in the distribution system, and a measured concentration of 0.03 mg/L was found at the home of one customer who complained of odors.\n\n==\n\nKang 2006:\n\nTitle: [A new fluorescence quenching method for the determination of trace ClO2 in water using silver nanoparticles].\n\nAbstract: In pH 9.1 NH4Cl-NH3 x H2O buffer solution, there is a fluorescence peak at 470 nm for silver nanoparticles. A fluorescence quenching takes place when it was oxidized by ClO2. The intensity of fluorescence quenching is linear with the concentration of ClO2 in the range of 0.0011-0.185 microg x mL(-1). The detection limit is 0.004 7 microg x mL(-1). A new fluorescence method has been proposed for the determination of ClO2 in water samples with satisfactory results.",
        "final_summary": "Zhang 2021 presents a method for rapid detection of ClO2 in drinking water using improved solid DPD spectrophotometry, demonstrating good precision and sensitivity. Jiang 2006 develops a resonance scattering method for the determination of trace ClO2 and Cl2 in water samples. Wheeler 1978 offers a rapid titrimetric and spectrophotometric procedure for ClO2 determination in water samples. Gan 2020 critically reviews the kinetics and mechanisms of ClO2 reactions with inorganic and organic compounds in water treatment. Huber 2005 assesses the potential of ClO2 for the oxidation of pharmaceuticals during water treatment, indicating its reactivity with certain compounds. Quentel 1994 proposes an electroanalytical method for the determination of traces of ClO2. Hoehn 2003 evaluates ClO2 and by-product persistence in a drinking water system, monitoring concentrations throughout the facility and distribution system. Lastly, Kang 2006 introduces a fluorescence quenching method using silver nanoparticles for the determination of trace ClO2 in water."
    },
    {
        "query": "polar vortex stretching stratosphere",
        "paper_list_string": "Charlton 2005:\n\nTitle: The Splitting of the Stratospheric Polar Vortex in the Southern Hemisphere, September 2002: Dynamical Evolution\n\nAbstract: Abstract The polar vortex of the Southern Hemisphere (SH) split dramatically during September 2002. The large-scale dynamical effects were manifest throughout the stratosphere and upper troposphere, corresponding to two distinct cyclonic centers in the upper troposphere\u2013stratosphere system. High-resolution (T511) ECMWF analyses, supplemented by analyses from the Met Office, are used to present a detailed dynamical analysis of the event. First, the anomalous evolution of the SH polar vortex is placed in the context of the evolution that is usually witnessed during spring. Then high-resolution fields of potential vorticity (PV) from ECMWF are used to reveal several dynamical features of the split. Vortex fragments are rapidly sheared out into sheets of high (modulus) PV, which subsequently roll up into distinct synoptic-scale vortices. It is proposed that the stratospheric circulation becomes hydrodynamically unstable through a significant depth of the troposphere\u2013stratosphere system as the polar vortex elo...\n\n==\n\nVargin 2015:\n\nTitle: Stratospheric Polar Vortex Splitting in December 2009\n\nAbstract: Abstract The 2009\u201310 Arctic stratospheric winter, in comparison with other recent winters, is mainly characterized by a major Sudden Stratospheric Warming (SSW) in late January associated with planetary wavenumber 1. This event led to a large increase in the temperature of the polar stratosphere and to the reversal of the zonal wind. Unlike other major SSW events in recent winters, after the major SSW in January 2010 the westerlies and polar vortex did not recover to their pre-SSW strength until the springtime transition. As a result, the depletion of the ozone layer inside the polar vortex over the entire winter was relatively small over the past 20 years. The other distinguishing feature of the 2010 winter was the splitting of the stratospheric polar vortex into two lobes in December. The vortex splitting was accompanied by an increase in the temperature of the polar stratosphere and a weakening of the westerlies but with no reversal. The splitting occurred when, in addition to the high-pressure system over northeastern Eurasia and the northern Pacific Ocean, the tropospheric anticyclone over Europe amplified and extended to the lower stratosphere. Analysis of wave activity in the extratropical troposphere revealed that two Rossby wave trains propagated eastward to the North Atlantic several days prior to the vortex splitting. The first wave train propagated from the subtropics and mid-latitudes of the eastern Pacific Ocean over North America and the second one propagated from the northern Pacific Ocean. These wave trains contributed to an intensification of the tropospheric anticyclone over Europe and to the splitting of the stratospheric polar vortex.\n\n==\n\nPalmer 1959:\n\nTitle: The stratospheric polar vortex in winter\n\nAbstract: In winter the polar stratospheric air within the earth's shadow forms the core of an intense \u2018cold low\u2019 which extends from about 10 km to at least 50 km and possibly to the base of the ionosphere. Compared with the tropospheric general circulation, this vortex seems to be remarkably stable, particularly in the Southern Hemisphere. \n \nRecent research work in Canada and the United States on the characteristics of the vortex is reviewed. The chief conclusions are that the vortex is more stable in the Southern than in the Northern Hemisphere, that \u2018explosive warmings\u2019 in the lower stratosphere of the Northern Hemisphere follow the breakdown of the vortex at high levels close to the pole, and that the breakdown extends from above downward over a period of several days. It is suggested that the high-level breakdown is correlated with solar activity.\n\n==\n\nWaugh 1994:\n\nTitle: Transport out of the lower stratospheric Arctic vortex by Rossby wave breaking\n\nAbstract: The fine-scale structure in lower stratospheric tracer transport during the period of the two Arctic Airborne Stratospheric Expeditions (January and February 1989; December 1991 to March 1992) is investigated using contour advection with surgery calculations. These calculations show that Rossby wave breaking is an ongoing occurrence during these periods and that air is ejected from the polar vortex in the form of long filamentary structures. There is good qualitative agreement between these filaments and measurements of chemical tracers taken aboard the NASA ER-2 aircraft. The ejected air generally remains filamentary and is stretched and mixed with midlatitude air as it is wrapped around the vortex. This process transfers vortex air into midlatitudes and also produces a narrow region of fine-scale filaments surrounding the polar vortex. Among other things, this makes it difficult to define a vortex edge. The calculations also show that strong stirring can occur inside as well as outside the vortex.\n\n==\n\nManney 2000:\n\nTitle: Development of the polar vortex in the 1999\u20132000 Arctic winter stratosphere\n\nAbstract: The 1999\u20132000 Arctic stratospheric vortex was unusually cold, especially in the early winter lower stratosphere, with a larger area near polar stratospheric cloud formation temperatures in Dec and Jan, and much lower temperatures averaged over Nov\u2013Jan, than any previously observed Arctic winter. In Nov and early Dec, there was a double jet in the upper stratosphere, with the anticyclone cutoff in a region of cyclonic material. By late Dec, there was a discontinuous vortex, large in the upper stratosphere, small in the lower stratosphere; evolving to a strong, continuous, relatively upright vortex by mid\u2010Jan. This vortex evolution in 1999\u20132000 is typical of that in other cold early winters. Despite unusually low temperatures, the lower stratospheric vortex developed more slowly than in previous unusually cold early winters, and was weaker than average until late Dec.\n\n==\n\nSeviour 2016:\n\nTitle: Stratospheric polar vortex splits and displacements in the high\u2010top CMIP5 climate models\n\nAbstract: Sudden stratospheric warming (SSW) events can occur as either a split or a displacement of the stratospheric polar vortex. Recent observational studies have come to different conclusions about the relative impacts of these two types of SSW upon surface climate. A clearer understanding of their tropospheric impact would be beneficial for medium\u2010range weather forecasts and could improve understanding of the physical mechanism for stratosphere\u2010troposphere coupling. Here we perform the first multimodel comparison of stratospheric polar vortex splits and displacements, analyzing 13 stratosphere\u2010resolving models from the fifth Coupled Model Intercomparison Project (CMIP5) ensemble. We find a wide range of biases among models in both the mean state of the vortex and the frequency of vortex splits and displacements, although these biases are closely related. Consistent with observational results, almost all models show vortex splits to occur barotropically throughout the depth of the stratosphere, while vortex displacements are more baroclinic. Vortex splits show a slightly stronger North Atlantic surface signal in the month following onset. However, the most significant difference in the surface response is that vortex displacements show stronger negative pressure anomalies over Siberia. This region is shown to be colocated with differences in tropopause height, suggestive of a localized response to lower stratospheric potential vorticity anomalies.\n\n==\n\nWaugh 2013:\n\nTitle: Stratospheric Polar Vortices\n\nAbstract: The Stratosphere: Geophysical Mon Copyright 2010 b 10.1029/2009GM The intense cyclonic vortices that form over the winter pole are one of the most prominent features of the stratospheric circulation. The structure and dynamics of these \u201cpolar vortices\u201d play a dominant role in the winter and spring stratospheric circulation and are key to determining distribution of trace gases, in particular ozone, and the couplings between the stratosphere and troposphere. In this chapter, we review the observed structure, dynamical theories, and modeling of these polar vortices. We consider both the zonal mean and three-dimensional potential vorticity perspective and examine the occurrence of extreme events and long-term trends.\n\n==\n\nMitchell 2011:\n\nTitle: Characterizing the Variability and Extremes of the Stratospheric Polar Vortices Using 2D Moment Analysis\n\nAbstract: AbstractThe mean state, variability, and extreme variability of the stratospheric polar vortices, with an emphasis on the Northern Hemisphere (NH) vortex, are examined using two-dimensional moment analysis and extreme value theory (EVT). The use of moments as an analysis tool gives rise to information about the vortex area, centroid latitude, aspect ratio, and kurtosis. The application of EVT to these moment-derived quantities allows the extreme variability of the vortex to be assessed. The data used for this study are 40-yr ECMWF Re-Analysis (ERA-40) potential vorticity fields on interpolated isentropic surfaces that range from 450 to 1450 K.Analyses show that the most extreme vortex variability occurs most commonly in late January and early February, consistent with when most planetary wave driving from the troposphere is observed. Composites around sudden stratospheric warming (SSW) events reveal that the moment diagnostics evolve in statistically different ways between vortex splitting events and vort...",
        "final_summary": "The phenomenon of polar vortex stretching in the stratosphere is characterized by several dynamic processes. Charlton (2005) details the splitting of the Southern Hemisphere polar vortex into distinct synoptic-scale vortices. Vargin (2015) further elaborates on this splitting process in the Arctic stratosphere, noting an associated increase in polar stratospheric temperature and weakening of westerlies. The stability of these vortices varies, with Palmer (1959) noting remarkable stability in the Southern Hemisphere and a tendency for breakdown in the Northern Hemisphere. The transport of air out of the Arctic vortex is facilitated by Rossby wave breaking, leading to the ejection of air in filamentary structures and mixing with midlatitude air (Waugh, 1994). Manney (2000) observes slower development and unusually cold temperatures in the Arctic stratospheric vortex in the 1999-2000 winter. Seviour (2016) identifies biases in models of vortex splits and displacements, with vortex displacements showing stronger negative pressure anomalies over Siberia. Waugh (2013) underscores the role of polar vortices in stratospheric circulation and trace gas distribution. Finally, Mitchell (2011) finds that extreme variability in the stratospheric polar vortices is most common in late January and early February, aligning with planetary wave driving."
    },
    {
        "query": "I need all research papers which used Shotgun sequencing to study gut microbiome of IBS patients in India",
        "paper_list_string": "Dhakan 2019:\n\nTitle: The unique composition of Indian gut microbiome, gene catalogue, and associated fecal metabolome deciphered using multi-omics approaches\n\nAbstract: Abstract Background Metagenomic studies carried out in the past decade have led to an enhanced understanding of the gut microbiome in human health; however, the Indian gut microbiome has not been well explored. We analyzed the gut microbiome of 110 healthy individuals from two distinct locations (North-Central and Southern) in India using multi-omics approaches, including 16S rRNA gene amplicon sequencing, whole-genome shotgun metagenomic sequencing, and metabolomic profiling of fecal and serum samples. Results The gene catalogue established in this study emphasizes the uniqueness of the Indian gut microbiome in comparison to other populations. The gut microbiome of the cohort from North-Central India, which was primarily consuming a plant-based diet, was found to be associated with Prevotella and also showed an enrichment of branched chain amino acid (BCAA) and lipopolysaccharide biosynthesis pathways. In contrast, the gut microbiome of the cohort from Southern India, which was consuming an omnivorous diet, showed associations with Bacteroides, Ruminococcus, and Faecalibacterium and had an enrichment of short chain fatty acid biosynthesis pathway and BCAA transporters. This corroborated well with the metabolomics results, which showed higher concentration of BCAAs in the serum metabolome of the North-Central cohort and an association with Prevotella. In contrast, the concentration of BCAAs was found to be higher in the fecal metabolome of the Southern-India cohort and showed a positive correlation with the higher abundance of BCAA transporters. Conclusions The study reveals the unique composition of the Indian gut microbiome, establishes the Indian gut microbial gene catalogue, and compares it with the gut microbiome of other populations. The functional associations revealed using metagenomic and metabolomic approaches provide novel insights on the gut-microbe-metabolic axis, which will be useful for future epidemiological and translational researches.\n\n==\n\nMitra 2013:\n\nTitle: Analysis of the intestinal microbiota using SOLiD 16S rRNA gene sequencing and SOLiD shotgun sequencing\n\nAbstract: BackgroundMetagenomics seeks to understand microbial communities and assemblages by DNA sequencing. Technological advances in next generation sequencing technologies are fuelling a rapid growth in the number and scope of projects aiming to analyze complex microbial environments such as marine, soil or the gut. Recent improvements in longer read lengths and paired-sequencing allow better resolution in profiling microbial communities. While both 454 sequencing and Illumina sequencing have been used in numerous metagenomic studies, SOLiD sequencing is not commonly used in this area, as it is believed to be more suitable in the context of reference-guided projects.ResultsTo investigate the performance of SOLiD sequencing in a metagenomic context, we compared taxonomic profiles of SOLiD mate-pair sequencing reads with Sanger paired reads and 454 single reads. All sequences were obtained from the bacterial 16S rRNA gene, which was amplified from microbial DNA extracted from a human fecal sample. Additionally, from the same fecal sample, complete genomic microbial DNA was extracted and shotgun sequenced using SOLiD sequencing to study the composition of the intestinal microbiota and the existing microbial metabolism. We found that the microbiota composition of 16S rRNA gene sequences obtained using Sanger, 454 and SOLiD sequencing provide results comparable to the result based on shotgun sequencing. Moreover, with SOLiD sequences we obtained more resolution down to the species level. In addition, the shotgun data allowed us to determine a functional profile using the databases SEED and KEGG.ConclusionsThis study shows that SOLiD mate-pair sequencing is a viable and cost-efficient option for analyzing a complex microbiome. To the best of our knowledge, this is the first time that SOLiD sequencing has been used in a human sample.\n\n==\n\nQiu 2017:\n\nTitle: Targeted Metagenome Based Analyses Show Gut Microbial Diversity of Inflammatory Bowel Disease patients\n\nAbstract: Inflammatory bowel disease (IBD) is a multifactorial disease including both genetic and environmental factors. We compared the diversity of intestinal microbesamong a cohort of IBD patients to study the microbial ecological effects on IBD. Fecal samples from patients were sequenced with next generation sequence technology at 16S rDNA region. With statistical tools, microbial community was investigated at different level. The gut microbial diversity of Crohn\u2019s disease (CD) patients and colonic polyp (CP) patients significantly different from each other. However, the character of ulcerative colitis (UC) patients has of both CD and CP features. The microbial community from IBD patients can be very different (CD patient) or somewhat similar (UC patients) to non-IBD patients. Microbial diversity can be an important etiological factor for IBD clinical phenotype.\n\n==\n\nMalinen 2005:\n\nTitle: Analysis of the Fecal Microbiota of Irritable Bowel Syndrome Patients and Healthy Controls with Real-Time PCR\n\nAbstract: OBJECTIVE:The gut microbiota may contribute to the onset and maintenance of irritable bowel syndrome (IBS). In this study, the microbiotas of patients suffering from IBS were compared with a control group devoid of gastrointestinal (GI) symptoms.METHODS:Fecal microbiota of patients (n = 27) fulfilling the Rome II criteria for IBS was compared with age- and gender-matched control subjects (n = 22). Fecal samples were obtained at 3 months intervals. Total bacterial DNA was analyzed by 20 quantitative real-time PCR assays covering approximately 300 bacterial species.RESULTS:Extensive individual variation was observed in the GI microbiota among both the IBS- and control groups. Sorting of the IBS patients according to the symptom subtypes (diarrhea, constipation, and alternating predominant type) revealed that lower amounts of Lactobacillus spp. were present in the samples of diarrhea predominant IBS patients wheras constipation predominant IBS patients carried increased amounts of Veillonella spp. Average results from three fecal samples suggested differences in the Clostridium coccoides subgroup and Bifidobacterium catenulatum group between IBS patients (n = 21) and controls (n = 15). Of the intestinal pathogens earlier associated with IBS, no indications of Helicobacter spp. or Clostridium difficile were found whereas one case of Campylobacter jejuni was identified by sequencing.CONCLUSIONS:With these real-time PCR assays, quantitative alterations in the GI microbiota of IBS patients were found. Increasing microbial DNA sequence information will further allow designing of new real-time PCR assays for a more extensive analysis of intestinal microbes in IBS.\n\n==\n\nAttri 2018:\n\nTitle: High throughput sequence profiling of gut microbiome in Northern Indian infants during the first four months and its global comparison\n\nAbstract: Abstract The present study characterized the colonization and development of gut microbial communities in healthy Indian infants from North-Western Himalayan region in the province Himachal Pradesh. The diversity and transitions of core genera was assessed targeting the 16S rRNA V3-V4 hypervariable region on an Illumina platform. Analysis of more than 17,000 filtered high quality reads indicated that the diversity was lowest in the month 2 followed by gradual increase towards month 4 (1.24 folds increase in Shannon index). The microbial population in month 1 was dominated by Firmicutes and Proteobacteria followed by dominance of Actinobacteria and Firmicutes in the month 4. The analysis of aggregate microbiota at class level indicated relatively higher abundance of Clostridia, Bacteroides and Actinobacteria in month 1, 3 and 4, respectively. The global comparison of dominance of different phyla with the similar subjects indicated that the Indian microbiome is more similar with studies conducted with Swedish infants, although the differences in DNA extraction protocols, geographical location and sequencing platforms as confounding factors cannot be neglected. The findings in this small cohort study could facilitate future studies exploring various aspects of the human gut microbiome in Indian subcontinent.\n\n==\n\nSaulnier 2011:\n\nTitle: Gastrointestinal microbiome signatures of pediatric patients with irritable bowel syndrome.\n\nAbstract: BACKGROUND & AIMS\nThe intestinal microbiomes of healthy children and pediatric patients with irritable bowel syndrome (IBS) are not well defined. Studies in adults have indicated that the gastrointestinal microbiota could be involved in IBS.\n\n\nMETHODS\nWe analyzed 71 samples from 22 children with IBS (pediatric Rome III criteria) and 22 healthy children, ages 7-12 years, by 16S ribosomal RNA gene sequencing, with an average of 54,287 reads/stool sample (average 454 read length = 503 bases). Data were analyzed using phylogenetic-based clustering (Unifrac), or an operational taxonomic unit (OTU) approach using a supervised machine learning tool (randomForest). Most samples were also hybridized to a microarray that can detect 8741 bacterial taxa (16S rRNA PhyloChip).\n\n\nRESULTS\nMicrobiomes associated with pediatric IBS were characterized by a significantly greater percentage of the class \u03b3-proteobacteria (0.07% vs 0.89% of total bacteria, respectively; P < .05); 1 prominent component of this group was Haemophilus parainfluenzae. Differences highlighted by 454 sequencing were confirmed by high-resolution PhyloChip analysis. Using supervised learning techniques, we were able to classify different subtypes of IBS with a success rate of 98.5%, using limited sets of discriminant bacterial species. A novel Ruminococcus-like microbe was associated with IBS, indicating the potential utility of microbe discovery for gastrointestinal disorders. A greater frequency of pain correlated with an increased abundance of several bacterial taxa from the genus Alistipes.\n\n\nCONCLUSIONS\nUsing 16S metagenomics by PhyloChip DNA hybridization and deep 454 pyrosequencing, we associated specific microbiome signatures with pediatric IBS. These findings indicate the important association between gastrointestinal microbes and IBS in children; these approaches might be used in diagnosis of functional bowel disorders in pediatric patients.\n\n==\n\nZyoud 2021:\n\nTitle: Global research trends in the microbiome related to irritable bowel syndrome: A bibliometric and visualized study\n\nAbstract: BACKGROUND Irritable bowel syndrome (IBS) is a common functional gastrointestinal disorder. Dysregulation of the gut\u2013brain axis plays a central role in the pathophysiology of IBS. It is increasingly clear that the microbiome plays a key role in the development and normal functioning of the gut\u2013brain axis. AIM To facilitate the identification of specific areas of focus that may be of relevance to future research. This study represents a bibliometric analysis of the literature pertaining to the microbiome in IBS to understand the development of this field. METHODS The data used in our bibliometric analysis were retrieved from the Scopus database. The terms related to IBS and microbiome were searched in titles or abstracts within the period of 2000\u20132019. VOSviewer software was used for data visualization. RESULTS A total of 13055 documents related to IBS were retrieved at the global level. There were 1872 scientific publications focused on the microbiome in IBS. There was a strong positive correlation between publication productivity related to IBS in all fields and productivity related to the microbiome in IBS (r = 0.951, P < 0.001). The United States was the most prolific country with 449 (24%) publications, followed by the United Kingdom (n = 176, 9.4%), China (n = 154, 8.2%), and Italy (n = 151, 8.1%). The h-index for all retrieved publications related to the microbiome in IBS was 138. The hot topics were stratified into four clusters: (1) The gut\u2013brain axis related to IBS; (2) Clinical trials related to IBS and the microbiome; (3) Drug-mediated manipulation of the gut microbiome; and (4) The role of the altered composition of intestinal microbiota in IBS prevention. CONCLUSION This is the first study to evaluate and quantify global research productivity pertaining to the microbiome in IBS. The number of publications regarding the gut microbiota in IBS has continuously grown since 2013. This finding suggests that the future outlook for interventions targeting the gut microbiota in IBS remains promising.\n\n==\n\nKrogius-Kurikka 2009:\n\nTitle: Microbial community analysis reveals high level phylogenetic alterations in the overall gastrointestinal microbiota of diarrhoea-predominant irritable bowel syndrome sufferers\n\nAbstract: BackgroundA growing amount of scientific evidence suggests that microbes are involved in the aetiology of irritable bowel syndrome (IBS), and the gastrointestinal (GI) microbiota of individuals suffering from diarrhoea-predominant IBS (IBS-D) is distinguishable from other IBS-subtypes. In our study, the GI microbiota of IBS-D patients was evaluated and compared with healthy controls (HC) by using a high-resolution sequencing method. The method allowed microbial community analysis on all levels of microbial genomic guanine plus cytosine (G+C) content, including high G+C bacteria.MethodsThe collective faecal microbiota composition of ten IBS-D patients was analysed by examining sequences obtained using percent G+C (%G+C) -based profiling and fractioning combined with 16S rRNA gene clone library sequencing of 3267 clones. The IBS-D library was compared with an analogous healthy-control library of 23 subjects. Real-time PCR analysis was used to identify phylotypes belonging to the class Gammaproteobacteria and the order Coriobacteriales.ResultsSignificant differences were found between clone libraries of IBS-D patients and controls. The microbial communities of IBS-D patients were enriched in Proteobacteria and Firmicutes, but reduced in the number of Actinobacteria and Bacteroidetes compared to control. In particular, 16S rDNA sequences belonging to the family Lachnospiraceae within the phylum Firmicutes were in greater abundance in the IBS-D clone library.ConclusionsIn the microbiota of IBS-D sufferers, notable differences were detected among the prominent bacterial phyla (Firmicutes, Actinobacteria, Bacteroidetes, and Proteobacteria) localized within the GI tract.",
        "final_summary": "Dhakan 2019 provides insights into the unique composition of the Indian gut microbiome and its association with dietary patterns. Mitra 2013 uses SOLiD sequencing to study the intestinal microbiota and microbial metabolism in a human fecal sample, but does not specifically mention IBS patients. Qiu 2017 investigates the gut microbial diversity among a cohort of IBD patients, but does not compare different subtypes of IBS patients and non-IBD patients. Malinen 2005 identifies quantitative alterations in the gut microbiota of IBS patients, including differences in specific bacterial taxa. Attri 2018 characterizes the colonization and development of gut microbial communities in healthy Indian infants. Saulnier 2011 associates specific microbiome signatures with pediatric IBS, indicating the importance of gastrointestinal microbes in this condition. Zyoud 2021 provides a bibliometric analysis of the microbiome in IBS, highlighting the growing research interest in this field. Krogius-Kurikka 2009 reveals high-level phylogenetic alterations in the overall gastrointestinal microbiota of IBS-D patients."
    },
    {
        "query": "how pregnancy reshape the mother brain to cope with sleep deprivation",
        "paper_list_string": "Pardo 2016:\n\nTitle: Effects of sleep restriction during pregnancy on the mother and fetuses in rats\n\nAbstract: The present study aimed to analyze the effects of sleep restriction (SR) during pregnancy in rats. The following three groups were studied: home cage (HC pregnant females remained in their home cage), Sham (females were placed in tanks similar to the SR group but with sawdust) and SR (females were submitted to the multiple platform method for 20 h per day from gestational days (GD) 14 to 20). Plasma corticosterone after 6 days of SR was not different among the groups. However, the relative adrenal weight was higher in the SR group compared with the HC group, which suggests possible stress impact. SR during pregnancy reduces the body weight of the female but no changes in liver glycogen, cholesterol and triglycerides, and muscle glycogen were detected. On GD 20, the fetuses of the females submitted to SR exhibited increased brain derived neurotrophic factor (BDNF) in the hippocampus, which indicates that sleep restriction of mothers during the final week of gestation may affect neuronal growth factors in a fetal brain structure, in which active neurogenesis occurs during the deprivation period. However, no changes in the total reactive oxygen species (ROS) in the cortex, hippocampus, or cerebellum of the fetuses were detected. SR females showed no major change in the maternal behavior, and the pups' preference for the mother's odor on postpartum day (PPD) 7 was not altered. On GD 20, the SR females exhibited increased plasma prolactin (PRL) and oxytocin (OT) compared with the HC and Sham groups. The negative outcomes of sleep restriction during delivery could be related, in part, to this hormonal imbalance. Sleep restriction during pregnancy induces different changes compared with the changes described in males and affects both the mother and offspring.\n\n==\n\nBrunton 2008:\n\nTitle: The expectant brain: adapting for motherhood\n\nAbstract: A successful pregnancy requires multiple adaptations of the mother's physiology to optimize fetal growth and development, to protect the fetus from adverse programming, to provide impetus for timely parturition and to ensure that adequate maternal care is provided after parturition. Many of these adaptations are organized by the mother's brain, predominantly through changes in neuroendocrine systems, and these changes are primarily driven by the hormones of pregnancy. By contrast, adaptations in the mother's brain during lactation are maintained by external stimuli from the young. The changes in pregnancy are not necessarily innocuous: they may predispose the mother to post-partum mood disorders.\n\n==\n\nBrunton 2015:\n\nTitle: Maternal Brain Adaptations in Pregnancy\n\nAbstract: During pregnancy multiple physiological adaptations take place in the mother to optimize the chances of a successful pregnancy outcome. These adaptations play a critical role in reproductive physiology, serving to (1) increase the supply of oxygen and nutrients to the placenta and maternal organs supporting the pregnancy; (2) optimize fetal growth and development; (3) prepare expanded oxytocin stores for when they are in demand at parturition and for lactation; (4) protect the pregnancy from premature birth; (5) protect the fetus from adverse programming, e.g., by maternal stress; (6) ensure adequate milk production and delivery at lactation; and (7) ensure that the appropriate brain circuits are primed and behaviors altered to deliver sufficient maternal care after parturition. In this chapter we focus on these pregnancy-related adaptations that are organized by the maternal brain, many of which are induced by the increased levels of pregnancy hormones.\n\n==\n\nPires 2020:\n\nTitle: Effects of sleep modulation during pregnancy in the mother and offspring: Evidences from preclinical research\n\nAbstract: Disturbed sleep during gestation may lead to adverse outcomes for both mother and child. Animal research plays an important role in providing insights into this research field by enabling ethical and methodological requirements that are not possible in humans. Here, we present an overview and discuss the main research findings related to the effects of prenatal sleep deprivation in animal models. Using systematic review approaches, we retrieved 42 articles dealing with some type of sleep alteration. The most frequent research topics in this context were maternal sleep deprivation, maternal behaviour, offspring behaviour, development of sleep\u2013wake cycles in the offspring, hippocampal neurodevelopment, pregnancy viability, renal physiology, hypertension and metabolism. This overview indicates that the number of basic studies in this field is growing, and provides biological plausibility to suggest that sleep disturbances might be detrimental to both mother and offspring by promoting increased risk at the behavioural, hormonal, electrophysiological, metabolic and epigenetic levels. More studies on the effects of maternal sleep deprivation are needed, in light of their major translational perspective.\n\n==\n\nChang 2010:\n\nTitle: Sleep deprivation during pregnancy and maternal and fetal outcomes: is there a relationship?\n\nAbstract: Sleep duration in the population has been declining. Women occupy an increasingly prominent place in the work force without reducing most of their responsibilities at home. Consequently, sleep needs are often pushed to the bottom of women's daily priority list. Prior research has indicated that sleep deprivation is associated with higher levels of pro-inflammatory serum cytokines. This is important because higher plasma concentrations of pro-inflammatory serum cytokine levels are associated with postpartum depression and adverse birth outcomes such as preterm delivery. However, little research has directly examined how sleep deprivation may affect maternal and fetal outcomes. This review summarizes the existing data on the effect of sleep deprivation during pregnancy on maternal and fetal outcomes. We review supporting evidence for the hypotheses that sleep deprivation during pregnancy increases the risk of preterm delivery and postpartum depression, and that systemic inflammation is the causal mechanism in the association. Prior research on sleep in pregnancy has been limited by varying data collection methods, subjective self-reported sleep measures, small and non-representative samples, cross-sectional designs; descriptive or non-hypothesis driven studies. Future research with longitudinal study designs is needed to allow examination of the effect of sleep deprivation on adverse maternal and fetal outcomes.\n\n==\n\nSlattery 2008:\n\nTitle: No stress please! Mechanisms of stress hyporesponsiveness of the maternal brain\n\nAbstract: The time around birth is accompanied by behavioural and physiological adaptations of the maternal brain, which ensure reproductive functions, maternal care and the survival of the offspring. In addition, profound neuroendocrine and neurobiological adaptations have been described with respect to behavioural and neuroendocrine stress responsiveness in rodents and human mothers. Thus, the hormonal response of the hypothalamo\u2010pituitary\u2010adrenal (HPA) axis and the response of the sympathetic nervous system to emotional and physical stressors are severely attenuated. Moreover, anxiety\u2010related behaviour and emotional responsiveness to stressful stimuli are reduced with the result of general calmness. These complex adaptations of the maternal brain are likely to be a consequence of an increased activity of brain systems with inhibitory effects on the HPA axis (such as the oxytocin and prolactin systems) and of a reduced activity of excitatory pathways (noradrenaline (norepinephrine), corticotrophin\u2010releasing factor and opioids). Experimental manipulation of these systems using complementary approaches indeed demonstrates their importance in these maternal brain adaptations. Maternal stress adaptations are not only important for the healthy prenatal development of the offspring by preventing excessive glucocorticoid responses and in the promotion of postnatal maternal behaviour, but are also vital for the well\u2010being of the mother and her mental health.\n\n==\n\nHedman 2002:\n\nTitle: Effects of pregnancy on mothers' sleep.\n\nAbstract: OBJECTIVES\nTo survey the effects of pregnancy on mothers' sleep.\n\n\nMETHODS\nMothers were interviewed during and after pregnancy with a series of five questionnaires to assess alterations in their sleep. The first questionnaire covered the 3 months before becoming pregnant, the next three the trimesters of pregnancy and the last the 3 months after delivery. The study was carried out in a central hospital and the maternity care units in the nearby rural community. Altogether, 325 pregnant women completed all five questionnaires.\n\n\nRESULTS\nThe total amounts of reported sleep and of nocturnal sleep increased significantly during the first trimester of pregnancy, began to decrease thereafter and were shortest during the 3 months after pregnancy. During late pregnancy expectant mothers over 30 years of age reported less sleep than those under 30. During the whole pregnancy, but increasingly toward the end of pregnancy, sleep became more restless and fragmentary and its subjective quality worsened, due at least partly to increased restless legs and nightly awakenings increasing with advancing pregnancy.\n\n\nCONCLUSIONS\nThe subjective quality of sleep is disturbed as early as the first trimester of pregnancy, although total sleeping time increases. The amount of reported sleep begins to decrease in the second trimester. The frequency of reported sleep disturbances, such as restless legs syndrome and nocturnal awakenings, is maximum in the third trimester but is about normal within 3 months after delivery.\n\n==\n\nTikotzky 2009:\n\nTitle: Maternal sleep-related cognitions and infant sleep: a longitudinal study from pregnancy through the 1st year.\n\nAbstract: Infant sleep is a major source of concern for many parents. The aims of this longitudinal study were to assess: (a) the development of sleep patterns among infants, (b) the development of maternal cognitions regarding infant sleep, and (c) the relations between these domains during the 1st year of life. Eighty-five mothers were recruited during pregnancy and completed a questionnaire aimed at assessing maternal sleep-related cognitions. After delivery (at 1, 6, and 12 months) sleep was assessed using actigraphy and sleep logs, and maternal cognitions were reassessed. The findings demonstrated significant predictive and concomitant links between maternal cognitions and infant sleep. Maternal soothing behaviors mediated the relations between these domains.",
        "final_summary": "Pregnancy triggers adaptations in the mother's brain to manage sleep deprivation. Pardo 2016 found that sleep restriction in pregnant rats led to increased brain-derived neurotrophic factor in the fetal hippocampus, suggesting potential effects on neuronal growth. Brunton 2008 and Brunton 2015 discuss how the maternal brain orchestrates physiological adaptations during pregnancy, including neuroendocrine system changes driven by pregnancy hormones. Sleep deprivation during pregnancy can lead to adverse maternal and offspring outcomes, including behavioral, hormonal, metabolic, and epigenetic changes (Pires 2020). Chang 2010 suggests that sleep deprivation during pregnancy may increase the risk of preterm delivery and postpartum depression, potentially through systemic inflammation. Slattery 2008 highlights the maternal brain's stress hyporesponsiveness during pregnancy, which may contribute to reduced anxiety. Sleep quality is disturbed during pregnancy, with increased restlessness and fragmented sleep (Hedman 2002). Finally, maternal sleep-related cognitions and infant sleep patterns are interrelated, suggesting a bidirectional relationship (Tikotzky 2009)."
    },
    {
        "query": "cognitive load theory in prekindergarten education",
        "paper_list_string": "Meissner 2013:\n\nTitle: Towards Cognitive Load Theory as Guideline for Instructional Design in Science Education.\n\nAbstract: We applied cognitive load theory in an heuristic out-of-school science lesson. The lesson comprises experimentsconcerning major attributes of NaCl and was designed for 5th to 8th grade students. Our interest focused on wethercognitive load theory provides sufficient guidelines for instructional design in the field of heuristic science education.We extracted student clusters derived from pre-knowledge and learning success. We characterised students, based oncognitive achievement, mental effort, and instructional efficiency. Cluster analyses revealed three student clusterswith quite satisfying results. Two further clusters showed improvable results, two showed no learning success, whichmay point to difficulties in coping with the learning setting. Motivational characterisation will refine the results, andmay confirm starting points to advance cognitive load theory in heuristic science education.\n\n==\n\nMoos 2014:\n\nTitle: Student teacher challenges: using the cognitive load theory as an explanatory lens\n\nAbstract: Cognitive load theory (CLT) can explain the challenges faced by student teachers. This study, guided by the CLT, included 26 pre-service teachers. Participants completed a cognitive load self-report questionnaire and were interviewed at two points during their student teaching. Results revealed that student teachers decreased mental effort related to monitoring their students\u2019 level of attention, meeting needs of diverse learners, and managing internal and external distractions. Qualitative analysis revealed: (1) student teachers became aware of limited cognitive resources; (2) lesson planning imposes cognitive load during student teaching; and (3) cognitive overload limits the ability to make modifications during teaching.\n\n==\n\nSpringer 2010:\n\nTitle: Cognitive load theory, educational research, and instructional design: some food for thought\n\nAbstract: Cognitive load is a theoretical notion with an increasingly central role in the educational research literature. The basic idea of cognitive load theory is that cognitive capacity in working memory is limited, so that if a learning task requires too much capacity, learning will be hampered. The recommended remedy is to design instructional systems that optimize the use of working memory capacity and avoid cognitive overload. Cognitive load theory has advanced educational research considerably and has been used to explain a large set of experimental findings. This article sets out to explore the open questions and the boundaries of cognitive load theory by identifying a number of prob lematic conceptual, methodological and application-related issues. It concludes by pre senting a research agenda for future studies of cognitive load.\n\n==\n\nJong 2010:\n\nTitle: Cognitive load theory, educational research, and instructional design: some food for thought\n\nAbstract: Cognitive load is a theoretical notion with an increasingly central role in the educational research literature. The basic idea of cognitive load theory is that cognitive capacity in working memory is limited, so that if a learning task requires too much capacity, learning will be hampered. The recommended remedy is to design instructional systems that optimize the use of working memory capacity and avoid cognitive overload. Cognitive load theory has advanced educational research considerably and has been used to explain a large set of experimental findings. This article sets out to explore the open questions and the boundaries of cognitive load theory by identifying a number of problematic conceptual, methodological and application-related issues. It concludes by presenting a research agenda for future studies of cognitive load.\n\n==\n\nKennedy 2021:\n\nTitle: Cognitive Load Theory: An Applied Reintroduction for Special and General Educators\n\nAbstract: There are numerous reasons why students with disabilities struggle in school. A key reason is professionals in the field may not pay enough attention to students\u2019 overwhelmed cognitive capacity. Cognitive load theory explains that all humans have limited capacity at any given time to use their auditory, visual, and tactile inputs (independently or collectively) to acquire new information and store it in long-term memory. When available cognition is overwhelmed \u2013 which can be caused by any number of reasons \u2013 learning cannot occur. In this article, we introduce the key aspects of cognitive load theory and give specific examples of how special educators can use this information to shape their instruction to support students\u2019 unique needs.\n\n==\n\nSweller 2019:\n\nTitle: Cognitive load theory and educational technology\n\nAbstract: Cognitive load theory provides instructional recommendations based on our knowledge of human cognition. Evolutionary psychology is used to assume that knowledge should be divided into biologically primary information that we have specifically evolved to acquire and biologically secondary information that we have not specifically evolved to acquire. Primary knowledge frequently consists of generic-cognitive skills that are important to human survival and cannot be taught because they are acquired unconsciously while secondary knowledge is usually domain-specific in nature and requires explicit instruction in education and training contexts. Secondary knowledge is first processed by a limited capacity, limited duration working memory before being permanently stored in long-term memory from where unlimited amounts of information can be transferred back to working memory to govern action appropriate for the environment. The theory uses this cognitive architecture to design instructional procedures largely relevant to complex information that requires a reduction in working memory load. Many of those instructional procedures can be most readily used with the assistance of educational technology.\n\n==\n\nCooper 1990:\n\nTitle: Cognitive load theory as an aid for instructional design\n\nAbstract: This paper attempts to draw together several recent findings in educational psychology that have led to the development and application of cognitive load theory to the format of instruction (Chandler and Sweller, unpublished manuscript; Sweller, 1988). These findings are directly related to the processes involved in learning, and will ultimately affect the way instructional design is approached. Cognitive load may be viewed as the level of 'mental energy' required to process a given amount of information. As the amount of information to be processed increases, so too does the associated cognitive load. Cognitive load theory suggests that effective instructional material promotes learning by directing cognitive resources towards activities that are relevant to learning rather than to processes that are an adjunct to learning. Cognitive load theory grew out of research investigating the differences between experts and novices. This research will be discussed briefly before returning to the issue of cognitive load theory.\n\n==\n\nMoreno 2010:\n\nTitle: Cognitive Load Theory: Cognitive Load Theory: Historical Development and Relation to Other Theories\n\nAbstract: The goal of this introductory chapter is to provide a historical review of the assumptions underlying Cognitive Load Theory (CLT) and to place the theory into the broader context of the learning sciences. The chapter focuses on the theoretical developments that guided the research on cognitive load and learning for the past twenty years and is organized in the following way. First, we examine the nature of the cognitive load construct and compare it to similar psychological constructs. Second, we present a historical review of the development of CLT's assumptions in the following four stages: (a) extraneous cognitive load in problem solving, (b) intrinsic cognitive load and the first additivity hypothesis, (c) germane cognitive load and the second additivity hypothesis, and (d) the evolutionary interpretation of CLT. Finally, we conclude the chapter by examining the constructs and assumptions of CLT in relation to other theories in psychology and education. THE COGNITIVE LOAD CONSTRUCT CLT is a psychological theory because it attempts to explain psychological or behavioral phenomena resulting from instruction. Psychological theories are concerned with the possible relationships among psychological constructs or between a psychological construct and an observable phenomenon of practical consequence. A psychological construct is an attribute or skill that happens in the human brain. In CLT, the main constructs of interest are cognitive load , hence the name of the theory, and learning . CLT was developed to explain the effects of instructional design on these two constructs.",
        "final_summary": "The papers suggest that cognitive load theory has implications for instructional design in prekindergarten education. Meissner 2013 applied cognitive load theory in a science lesson and found clusters of students with varying levels of learning success and mental effort. Moos 2014, guided by cognitive load theory, found that student teachers faced challenges related to monitoring student attention and managing distractions. Springer 2010 and Jong 2010 discuss the importance of cognitive load theory in educational research and highlight conceptual and methodological issues that need further exploration. Kennedy 2021 emphasizes the relevance of cognitive load theory in understanding the struggles of students with disabilities and suggests instructional strategies for special educators. Sweller 2019 explores the application of cognitive load theory in educational technology. Cooper 1990 discusses cognitive load theory's impact on instructional design, focusing on the level of mental energy required for information processing. Moreno 2010 provides a historical overview of cognitive load theory's development and its relation to other theories in psychology and education."
    },
    {
        "query": "What are the competitive dynamics of a mid sized company in government contracting",
        "paper_list_string": "Stumpf 2000:\n\nTitle: Competitive pressures on middle\u2010market contractors in the UK\n\nAbstract: Medium\u2010sized regional building contractors in the UK are exhibiting poorer performance in the 1980s and the 1990s and are less likely to survive than their larger or smaller counterparts. The market structure of contracting appears to be changing, putting pressure on these intermediate firms. Evidence drawn from the Department of the Environment (DoE) statistical series shows industry composition is changing, in particular the gradual decline over time of the middle market. An analysis of company accounts for a sample of approximately 200 contractors shows that medium\u2010sized firms are also displaying inferior business ratios. Possible explanations are offered, including barriers to entry, such as capitalization, economies of scale (pecuniary and market), along with changes in construction demand.\n\n==\n\nPayne 2009:\n\nTitle: Competitive Dynamics among Service SMEs\n\nAbstract: In an effort to further our understanding of competitive dynamics, the three constructs of firm specialization, environmental munificence, and rivalry intensity are examined in relation to financial performance in service\u2010intensive Small and Medium\u2010Sized Enterprises (SMEs). Using a sample of physician organizations, direct and interaction relationships are empirically examined using multivariate regression analyses. Findings confirm a three\u2010way interaction that exists among these factors in relationship to overall performance; this supports a more complex, configurations approach to competitive dynamics research, particularly among service\u2010intensive SMEs.\n\n==\n\nHefetz 2004:\n\nTitle: Privatization and Its Reverse: Explaining the Dynamics of the Government Contracting Process\n\nAbstract: Empirical evidence shows local government contracting is a dynamic process that includes movements from public delivery to markets and from market contracts back to in-house delivery.This \u2018\u2018reversecontracting\u2019\u2019reflectsthe complexityofpublicserviceprovisionin aworld where market alternatives are used along with public delivery. We develop a methodology to link responses to national surveys and create a longitudinal data set that captures the dynamics of the contracting process. We present a framework that incorporates principal agent problems, government management, monitoring and citizen concerns, and market structure. Our statistical analysis finds government management, monitoring, and principal agent problems to be most important in explaining both new contracting out and contracting back-in. Professional managers recognize the importance of monitoring and the need for public engagement in the service delivery process. The results support the new public service that argues public managers do more than steer a market process; they balance technical and political concerns to secure public value.\n\n==\n\nFlammer 2017:\n\nTitle: Competing for Government Procurement Contracts: The Role of Corporate Social Responsibility\n\nAbstract: Research Summary: This study examines whether corporate social responsibility (CSR) improves firms\u2019 competitiveness in the market for government procurement contracts. To obtain exogenous variation in firms\u2019 social engagement, I exploit a quasi\u2010natural experiment provided by the enactment of state\u2010level constituency statutes, which allow directors to consider stakeholders\u2019 interests when making business decisions. Using constituency statutes as instrumental variable (IV) for CSR, I find that companies with higher CSR receive more procurement contracts. The effect is stronger for more complex contracts and in the early years of the government\u2010company relationship, suggesting that CSR helps mitigate information asymmetries by signaling trustworthiness. Moreover, the effect is stronger in competitive industries, indicating that CSR can serve as a differentiation strategy to compete against other bidders. Managerial Summary: This study examines how companies can strategically improve their competitiveness in the market for government procurement contracts\u2014a market of economic importance (15\u201320% of GDP). It shows that companies with higher social and environmental performance (CSR) receive more procurement contracts. This effect is stronger for more complex contracts, in the early years of the government\u2013company relationship, and in more competitive industries. These findings indicate that firms\u2019 CSR can serve as a signaling and differentiation strategy that influences the purchasing decision of government agencies. Accordingly, managers operating in the business\u2010to\u2010government (B2G) sector could benefit from integrating social and environmental considerations into their strategic decision making.\n\n==\n\nStr\u00f6mb\u00e4ck 2015:\n\nTitle: Contract Size and Small Firm Competition in Public Procurement\n\nAbstract: The European Commission encourages public authorities to split procurement contracts into multiple contracts in order to increase the competiveness of small and medium sized enterprises (SMEs). In ...\n\n==\n\nJing 2012:\n\nTitle: Is Competitive Contracting Really Competitive? Exploring Government\u2013Nonprofit Collaboration in China\n\nAbstract: ABSTRACT Local governments in China have adopted competitive contracting in social service delivery to restructure burgeoning government\u2013nonprofit collaborations. The stated purpose of using competitive contracting is to forge formal, effective, and accountable collaborative relations. By applying institutional, transaction cost, and resource dependence theories to China's context, we reveal the institutional pressures of adopting a competitive contracting policy and identify the challenges of economizing transaction costs and handling mutual resource dependence in its implementation. We find the prevalence of \u201cdecoupling\u201d to meet these competing pressures at the sacrifice of competition: that is, many contracts only appear competitive. We conducted interviews, participant observations, and a questionnaire survey to study a large-scale, social-service-contracting program in Shanghai. The results provide evidence of weak competition. Local governments in Shanghai were not able to effectively coordinate competitive bidding and thus strategically kept a decentralized implementation and management system based on existing relations.\n\n==\n\nJohnston 2012:\n\nTitle: Government Contracts and \u201cManaging the Market\u201d\n\nAbstract: Theory tells us that competition is the chief driver of improved efficiency and effectiveness in government contracting, yet contract provider markets are often noncompetitive. This study offers a detailed, contextualized examination of public administrative responses to thin contract markets. Following an inductive approach with data from semistructured interviews with contract administrators, the authors offer a preliminary typology of the conditions that give rise to thin markets, and the \u201cmarket management\u201d strategies used to create, enhance, and sustain competition in the markets from where governments purchase goods and services. The authors then review the efficacy and implications of these strategies for public services to citizens.\n\n==\n\nBrunjes 2020:\n\nTitle: Competition and Federal Contractor Performance\n\nAbstract: \n Contracts are increasingly used in government as a way to insert competition into public agencies and policy implementation. Competition among contractors is theorized to drive down costs, encourage innovation, and boost accountability. However, there have been few studies on whether competition actually leads to improved performance among contractors. This analysis examines nearly 25,000 federal contracts to determine how competition is related to performance. Findings indicate that competitively sourced contracts are more likely to suffer from performance problems. However, relationships between contractor and agency correlate with fewer performance problems. This suggests that competition may not be leading to the benefits that many might expect. Instead, relationships and shared experiences, along with other factors, may matter more when seeking to improve contractor performance.",
        "final_summary": "The papers suggest that the competitive dynamics of mid-sized companies in government contracting are influenced by various factors. Stumpf (2000) finds that medium-sized regional building contractors in the UK face increasing competitive pressures and exhibit poorer performance compared to larger or smaller firms. Payne (2009) explores competitive dynamics in service-intensive SMEs and highlights the complex interaction between firm specialization, environmental munificence, and rivalry intensity in relation to overall performance. Hefetz (2004) examines the dynamics of government contracting and identifies factors such as government management, monitoring, and principal-agent problems as significant in explaining both contracting out and contracting back-in. Flammer (2017) investigates the role of corporate social responsibility (CSR) in government procurement contracts and finds that firms with higher CSR receive more contracts, particularly in competitive industries. Str\u00f6mb\u00e4ck (2015) discusses the European Commission's encouragement of splitting procurement contracts to increase competitiveness for small and medium-sized enterprises (SMEs). Jing (2012) examines government-nonprofit collaboration in China and reveals that competitive contracting may not always be truly competitive due to decoupling and weak competition. Johnston (2012) explores public administrative responses to thin contract markets and offers strategies for creating, enhancing, and sustaining competition in government purchasing. Brunjes (2020) analyzes federal contracts and finds that competitively sourced contracts are more likely to experience performance problems, suggesting that relationships and shared experiences may be more influential in improving contractor performance."
    },
    {
        "query": "how long a patient with generalized anxiety disorder should take medication for",
        "paper_list_string": "Lam 2006:\n\nTitle: Generalized anxiety disorder: how to treat, and for how long?\n\nAbstract: Generalized anxiety disorder (GAD) is a common, chronic and disabling anxiety disorder with considerable comorbidity with depression as well as with other anxiety disorders. Although tricyclic antidepressants and benzodiazepines have been found to be efficacious in patients with GAD, tolerability problems and other risks limit their use in clinical practice. In placebo-controlled, acute (<8\u2009weeks) trials, several medications, including the selective serotonin reuptake inhibitors ([SSRIs] escitalopram, paroxetine, and sertraline) and others (venlafaxine, buspirone, pregabalin), have demonstrated efficacy in patients with GAD. Indeed, current guidelines for the treatment of GAD recommend SSRIs as first-line pharmacological therapy because of their efficacy and tolerability profiles. Although GAD is a chronic condition that is usually present for years, with symptoms typically fluctuating in intensity over time, there have been few randomized, controlled trials of pharmacotherapy beyond the acute phase of treatment. However, data from recent relapse-prevention studies and longer-term maintenance studies with paroxetine, venlafaxine and escitalopram strongly support the value of continued treatment for at least a further 6 months. This article focuses on pharmacological treatment, and reviews recently available data from acute, long-term and relapse-prevention trials in patients with GAD. In addition, issues relating to the natural course of GAD are highlighted as important considerations to guide selection of pharmacotherapy.\n\n==\n\nRouillon 2004:\n\nTitle: Long term therapy of generalized anxiety disorder\n\nAbstract: Abstract Generalized anxiety disorder (GAD) is a common (lifetime prevalence: 5.1%), recurrent condition, which often heralds other psychiatric disorders, notably depression. As by definition it is a disorder progressing over months, treatment should be designed on a long term basis. And yet, few studies have been conducted beyond the classical 6\u20138 weeks characterizing the acute treatment phase. This is especially true of anxiolytics, but also of antidepressants, with the exception of paroxetine and venlafaxine, which are the only drugs approved in this indication in Western countries. The efficacy of psychotherapy, notably relaxation and cognitive-behavioral therapy, is established in the treatment of GAD, but its preferred indications and possible combination with antidepressants are still to be specified. Long term, not to say very long term studies of GAD, as well as depression, will still be required in the future to improve its management and specify therapeutic modalities (combination treatment, optimal duration, continuous or intermittent therapy, choice of psychotherapeutic techniques or agents, \u2026). Early and adequately prolonged treatment should not only result in more numerous remission periods, but also in decreased frequency of co-morbidities whether depressive, addictive, or of another nature, and should also reduce the social impact of GAD.\n\n==\n\nMah\u00e9 2000:\n\nTitle: Long\u2010term pharmacological treatment of generalized anxiety disorder\n\nAbstract: &NA; Generalized anxiety disorder (GAD) is one of the most common anxiety disorders and has a poor prognosis, although it is often thought to be a minor complaint. This disorder has a chronic course of 5\u201015 years and longer. Long\u2010term treatment with the commonly used benzodiazepines is controversial because of concerns over tolerance and dependence. We performed a thorough search of the literature for clinical trials of a duration of over 2 months conducted in patients with generalized anxiety disorder in order to identify any successful long\u2010term treatment of this disorder. Only eight long\u2010term reports of studies conducted in well\u2010defined homogeneous groups of patients diagnosed with generalized anxiety disorder were found with the methodology of these studies presenting a number of limiting factors. The results are inconclusive and no reference drug could be identified. In addition, an adequate evaluation of the long\u2010term treatment of GAD has not yet been performed.\n\n==\n\nRickels 2010:\n\nTitle: Time to relapse after 6 and 12 months' treatment of generalized anxiety disorder with venlafaxine extended release.\n\nAbstract: CONTEXT\nGeneralized anxiety disorder (GAD) is a chronic disorder in need of reliable data to guide long-term treatment.\n\n\nOBJECTIVES\nTo assess the benefits of 6 and 12 months' treatment of GAD with venlafaxine hydrochloride extended release (XR) in patients who improved after 6 months' open-label venlafaxine XR treatment.\n\n\nDESIGN\nAfter 6 months' open-label venlafaxine XR treatment, improved patients were randomized to venlafaxine XR or placebo for 6 months. All venlafaxine XR patients still in the study at 12 months were randomized to receive venlafaxine XR or placebo, and all placebo patients continued taking placebo for another 6 months.\n\n\nSETTING\nOne urban site (5 locations).\n\n\nPATIENTS\nOf 268 patients with a diagnosis of GAD entering the open-label venlafaxine XR treatment phase, 158 (59.0%) completed 6 months, and 136 (50.7%) entered relapse phase 2 (6-12 months). Fifty-nine (43.4%) of 136 patients entered phase 3 (12-18 months).\n\n\nINTERVENTION\nSix months' open-label treatment with venlafaxine XR, followed by double-blind venlafaxine XR or placebo for 2 relapse phases, each lasting 6 months.\n\n\nMAIN OUTCOME MEASURES\nTime to relapse while receiving venlafaxine XR or placebo after 6 and after 12 months of treatment. Relapse was strictly defined to safeguard against assigning patients with venlafaxine XR discontinuation symptoms or temporary anxiety increase as relapse.\n\n\nRESULTS\nFor objective 1, relapse rates in phase 2 (months 6-12) were 9.8% on venlafaxine XR and 53.7% on placebo (P < .001). For objective 2, relapse rates after 12 months on placebo (32.4%) were lower than after 6 months on venlafaxine XR (53.7%) (P < .03).\n\n\nCONCLUSIONS\nTreatment of GAD with an antidepressant should be continued for at least 12 months. Preliminary data demonstrate that improved patients who relapse while off their antianxiety medication after at least 6 months of treatment will again most likely respond to a second course of treatment with the same medication. Trial Registration clinicaltrials.gov Identifier: NCT00183274.\n\n==\n\nGoodman 2005:\n\nTitle: Treatment of generalized anxiety disorder with escitalopram: pooled results from double-blind, placebo-controlled trials.\n\nAbstract: BACKGROUND\nEscitalopram 10 mg/day is an effective and well-tolerated antidepressant. Three randomized controlled trials recently evaluated the safety and efficacy of escitalopram in the treatment of generalized anxiety disorder (GAD).\n\n\nMETHODS\nThe trial designs were virtually identical, allowing data to be pooled across studies. Male and female outpatients, ages 18-80 years, with DSM-IV-defined GAD were randomized to double-blind treatment with escitalopram or placebo for 8 weeks. Escitalopram dose was fixed at 10 mg/day for the first 4 weeks, after which increases to 20 mg/day were permitted. The primary efficacy variable was the mean change from baseline in total Hamilton Anxiety Scale (HAMA) score.\n\n\nRESULTS\nApproximately 850 patients were randomized to double-blind treatment. In each individual study, escitalopram was significantly superior to placebo (p<0.05) as measured by change from baseline in HAMA score. By-visit analyses of data pooled across studies revealed significantly greater improvement (p<0.05) in the escitalopram group beginning at week 1 or 2 and continuing through week 8 for all primary and secondary efficacy variables. The mean change in HAMA total score from baseline to endpoint also was significantly greater for patients maintained at escitalopram 10 mg/day than for those receiving placebo. Escitalopram was generally well tolerated.\n\n\nLIMITATIONS\nThe studies included in this analysis were of short-term duration and excluded patients with significant medical and psychiatric comorbidities, such as major depressive disorder.\n\n\nCONCLUSION\nResults from the individual trials and the pooled analysis demonstrate that escitalopram is effective and well tolerated for the treatment of GAD.\n\n==\n\nBarlow 1992:\n\nTitle: Behavioral treatment of generalized anxiety disorder\n\nAbstract: Sixty-five carefully diagnosed patients with generalized anxiety disorder were treated with either relaxation, cognitive therapy, or their combination. These three active treatment conditions were compared to a wait-list control group. On several measures, in-cluding measures of worry, treated patients were significantly better than those in the wait-list control group at post treatment. These gains were maintained across the two-year follow-up period. Notably, these therapeutic gains were accompanied by substantial reductions in anxiolytic medication use over the period of follow-up. No differences emerged, however, among treatments at any point of comparison. In addition, drop-out rates among the active treatment groups were high (range = 5% to 38%). Moreover, most patients were left with residual anxiety suggesting the need for the development of more focused and efficient psychological treatments for generalized anxiety disorder.\n\n==\n\nKeller 2002:\n\nTitle: The long-term clinical course of generalized anxiety disorder.\n\nAbstract: Although generalized anxiety disorder (GAD) is a common disorder associated with significant levels of morbidity, little is known of its long-term course and outcomes. During the first 5 years, GAD follows a chronic course with low rates of remission and moderate rates of relapse/recurrence following remission. Retrospective studies suggest that this chronic pattern may last up to 20 years. It is hoped that, as with depression, long-term prospective studies in GAD will provide insight into the course, nature, and outcomes of the disorder over time. The studies will also identify any changes in the duration and severity of episodes of GAD over time, enabling treatments to effectively reflect the course of the disorder. Studies of other anxiety disorders and depression suggest that the course and outcome of the disorder may be influenced by certain factors such as stressful life events, anxiety sensitivity/negative affect, gender, subsyndromal symptoms, and comorbid disorders. Currently, studies are underway to determine the effects of these factors on the risk of relapse/recurrence, maintenance of full symptoms, and development of subsyndromal symptoms in GAD. GAD is currently underrecognized and undertreated, but it is hoped that this will change with the ever-increasing awareness of anxiety disorders. As treatment for GAD becomes more common, future prospective studies will identify the effect of therapy on the course and nature of the disorder, leading to increased understanding of GAD and the development of effective treatment strategies tailored for individual patients.\n\n==\n\nKessler 2005:\n\nTitle: Rethinking the duration requirement for generalized anxiety disorder: evidence from the National Comorbidity Survey Replication\n\nAbstract: Background. The proposed revisions of the ICD and DSM diagnostic systems have led to increased interest in evaluation of diagnostic criteria. This report focuses on the DSM-IV requirement that episodes of generalized anxiety disorder (GAD) must persist for at least 6 months. Community epidemiological data are used to study the implications of changing this requirement in the range 1\u201312 months for estimates of prevalence, onset, course, impairment, co-morbidity, associations with parental GAD, and sociodemographic correlates. Method. Data come from the US National Comorbidity Survey Replication (NCS-R), a US household survey carried out during 2001\u20132003. Version 3.0 of the WHO Composite International Diagnostic Interview (WMH-CIDI) was used to assess DSM-IV anxiety disorders, mood disorders, substance disorders, and impulse-control disorders. Results. Lifetime, 12-month, and 30-day prevalence estimates of DSM-IV GAD changed from 6\u00b71%, 2\u00b79%, and 1\u00b78% to 4\u00b72\u201312\u00b77%, 2\u00b72\u20135\u00b75%, and 1\u00b76\u20132\u00b76% when the duration requirement was changed from 6 months to 1\u201312 months. Cases with episodes of 1\u20135 months did not differ greatly from those with episodes of [ges ]6 months in onset, persistence, impairment, co-morbidity, parental GAD, or sociodemographic correlates. Conclusions. A large number of people suffer from a GAD-like syndrome with episodes of <6 months duration. Little basis for excluding these people from a diagnosis is found in the associations examined here.",
        "final_summary": "The papers provide mixed findings regarding the duration of medication treatment for patients with generalized anxiety disorder (GAD). Lam 2006 suggests that treatment should be continued for at least 6 months based on relapse-prevention studies. Rouillon 2004 emphasizes the need for long-term treatment due to the chronic nature of GAD. However, Mah\u00e9 2000 states that there is limited evidence for successful long-term treatment. Rickels 2010 supports the idea of continuing treatment for at least 12 months to prevent relapse. On the other hand, Barlow 1992 highlights the potential for psychological treatments, such as relaxation and cognitive therapy, to reduce anxiety symptoms and anxiolytic medication use. Keller 2002 emphasizes the chronic course of GAD and the need for long-term prospective studies to understand the disorder's nature and outcomes. Lastly, Kessler 2005 questions the 6-month duration requirement for GAD episodes, suggesting that a significant number of individuals experience GAD-like symptoms for shorter durations."
    },
    {
        "query": "Give me references about gay culture and sexual harassment",
        "paper_list_string": "Fileborn 2012:\n\nTitle: Sexual violence and gay, lesbian, bisexual, trans, intersex, and queer communities\n\nAbstract: Although the vast majority of literature and research on sexual violence has focused on the experiences of heterosexual women, a burgeoning body of work has highlighted the occurrence of sexual violence within and against gay, lesbian, bisexual, trans, intersex, and queer (GLBTIQ) communities. Research suggests that members of GLBTIQ communities may face significant levels of abuse, harassment and violence (Leonard, Mitchell, Pitts, Patel, & Fox, 2008; NSW Attorney General\u2019s Department [NSW AGD], 2003). Experiencing sexual or physical violence, or other forms of abuse and victimisation, is often associated with a range of negative health and social outcomes\u2014such as post-traumatic stress disorder, depression, anxiety, suicide, and drug and alcohol abuse (Ryan & Rivers, 2003). However, it should also be recognised that members of GLBTIQ communities show great resilience in the face of social exclusion, discrimination and abuse (Scourfield, Roen, & McDermott, 2008).\n\n==\n\nHerek 1993:\n\nTitle: Documenting prejudice against lesbians and gay men on campus: the Yale Sexual Orientation Survey.\n\nAbstract: College and university communities recently have begun to confront the problems of harassment, discrimination, and violence against lesbians, gay men, and bisexual people on campus. A first step in responding to attacks against gay and bisexual people is to document their frequency and the forms that they take. The present article reports the methodology and results of a survey conducted at Yale University in 1986, which subsequently has been replicated on several other campuses. The Yale survey revealed that many lesbians, gay men, and bisexual people on campus lived in a world of secretiveness and fear. Although experiences of physical assault on campus were relatively infrequent, many respondents reported other forms of discrimination and harassment. A majority reported that they feared antigay violence and harassment on campus, and that such fears affected their behavior. Replications on other campuses have yielded similar results. Suggestions are offered for researchers who wish to conduct such a survey on their own campus.\n\n==\n\nHuebner 2004:\n\nTitle: Experiences of harassment, discrimination, and physical violence among young gay and bisexual men.\n\nAbstract: OBJECTIVES\nWe examined the 6-month cumulative incidence of anti-gay harassment, discrimination, and violence among young gay/bisexual men and documented their associations with mental health.\n\n\nMETHODS\nGay/bisexual men from 3 cities in the southwestern United States completed self-administered questionnaires.\n\n\nRESULTS\nThirty-seven percent of men reported experiencing anti-gay verbal harassment in the previous 6 months; 11.2% reported discrimination, and 4.8% reported physical violence. Men were more likely to report these experiences if they were younger, were more open in disclosing their sexual orientation to others, and were HIV positive. Reports of mistreatment were associated with lower self-esteem and increased suicidal ideation.\n\n\nCONCLUSIONS\nAbsent policies preventing anti-gay mistreatment, empowerment and community-building programs are needed for young gay/bisexual men to both create safe social settings and help them cope with the psychological effects of these events.\n\n==\n\nRabelo 2014:\n\nTitle: Two sides of the same coin: gender harassment and heterosexist harassment in LGBQ work lives.\n\nAbstract: This project investigated the incidence, interplay, and impact of gender- and sexuality-based harassment, as experienced by lesbian, gay, bisexual, and queer (LGBQ) employees in higher education. Unlike much queer empirical research, participants in this study were residents of noncoastal regions of the U.S. that are predominantly White, rural, and conservative (i.e., \"red states\"). They completed surveys about their harassment experiences (gender harassment-sexist, gender harassment-policing, and heterosexist harassment), perceived support systems (from supervisors and organizations), and job attitudes (job burnout, job stress, and job satisfaction). Results showed that gender harassment-both sexist and policing subtypes-rarely occurred absent heterosexist harassment, and vice versa. Harassment severity (experiencing moderate to high levels of all three harassment types) was significantly associated with greater levels of job burnout (both disengagement and exhaustion) and job dissatisfaction. Even infrequent experiences of harassment related to large increases in the \"threat\" variety of job stress (i.e., sense of feeling hassled and overwhelmed on the job). Additionally, employees who perceived the lowest organizational support reported the most harassment. We interpret results in light of research on organizational behavior and LGBQ psychology. Moreover, we discuss our findings in the context of Title VII, currently interpreted to protect against harassment based on gender, sex, and sex stereotyping, but not sexual orientation. Our results can inform several possible avenues of expanding gay civil rights in employment: broadening judicial interpretations of Title VII, passing new legislation (e.g., the Employment Non-Discrimination Act, or ENDA), and strengthening organizational supports and policies that protect against sexuality-based abuses.\n\n==\n\nD\u2019augelli 1992:\n\nTitle: Lesbian and Gay Male Undergraduates' Experiences of Harassment and Fear on Campus\n\nAbstract: Harassment and discrimination based on sexual orientation was studied in a sample of 121 undergraduate students between 19 and 22 years of age. Over three fourths of the respondents reported verbal abuse and over one fourth had been threatened with violence. Other students were the most frequent victimizers. Few reported victimization to authorities. Fear for one's personal safety on campus was related to frequency of personal harassment. The implications of harassment and discrimination on the development of young lesbians and gay men are discussed.\n\n==\n\nTaulke\u2010Johnson 2008:\n\nTitle: Moving beyond homophobia, harassment and intolerance: gay male university students\u2019 alternative narratives\n\nAbstract: This paper draws on a small-scale qualitative study of the lived experiences of gay male students in their final year of undergraduate study at a UK university. In contrast to the narratives almost universally reported in academic literature, anti-gay victimisation and harassment were not experienced or framed as dominant discourses in the stories of the six participants. I discuss how despite heterosexuality being the assumed, expected and compulsory discourse at university, the participants made positive sense of their experiences, and how through careful negotiation they were able to address, explore and engage with their (homo)sexual identities and orientation. I challenge the common and (mostly) unquestioned practices of defining gay students solely on the basis of their negative accounts of their experiences, labelling them all as victims, and locating the entire population within a pathologised framework. Instead I advocate a nuanced and balanced perspective which acknowledges the alternate and non-victimised accounts of gay students to provide a more inclusive, comprehensive, fuller and richer understanding of their lived experiences at university.\n\n==\n\nD'Augelli 1989:\n\nTitle: Lesbians' and gay men's experiences of discrimination and harassment in a university community\n\nAbstract: A survey of 125 lesbians and gay men in a university community was conducted to determine the incidence of discrimination, harassment, and violence. Nearly three fourths had experienced verbal abuse; 26% were threatened with violence; and 17% had personal property damaged. Students and roommates were most often those responsible. Most incidents were not reported to authorities, and many made changes in their daily routines to avoid harm. Over half of the sample feared for their personal safety; their fear was related to the amount of harassment and previous property damage. Men were more often victimized than women.\n\n==\n\nBiaggio 1997:\n\nTitle: Sexual Harassment of Lesbians in the Workplace\n\nAbstract: Abstract This article explores the literature addressing sexual harassment of lesbians and discusses the ways in which one form of prejudice, that against lesbians, contributes to this problem. The first section of the article discusses the lesbian experience of violence, harassment, and discrimination. In exploring the above experiences of lesbians it becomes apparent that lesbians are the victims of a specific form of prejudice\u2013homophobia. Homophobia impacts lesbians in two ways: (1) in the form of negative attitudes and denigrating or destructive acts; and (2) by means of actual discrimination, whether overt or subtle. Unlike sexism and racism, homophobia has had little public discussion, and the article concludes with a discussion of the impact of homophobia on lesbians in the workplace.",
        "final_summary": "The papers collectively indicate a high prevalence of sexual harassment and discrimination within gay, lesbian, bisexual, and queer (GLBQ) communities. Fileborn (2012) underscores that these individuals often face abuse, harassment, and violence, leading to negative health and social outcomes. Herek (1993) reveals that many GLBQ individuals on college campuses experience verbal abuse, threats, and fear for personal safety. Huebner (2004) reports that young gay and bisexual men frequently encounter anti-gay harassment, discrimination, and physical violence, which are linked to lower self-esteem and increased suicidal ideation. Rabelo (2014) discusses the interplay of gender- and sexuality-based harassment, with severe harassment correlating with job burnout and dissatisfaction. Taulke-Johnson (2008) presents an alternative narrative, highlighting the positive experiences and resilience of gay male university students. Lastly, Biaggio (1997) explores the specific issue of workplace sexual harassment targeting lesbians, emphasizing the role of homophobia in their experiences."
    },
    {
        "query": "Cognitive Load Theory toward visualization",
        "paper_list_string": "Khalil 2005:\n\nTitle: Design of interactive and dynamic anatomical visualizations: the implication of cognitive load theory.\n\nAbstract: In improving the teaching and learning of anatomical sciences, empirical research is needed to develop a set of guiding principles that facilitate the design and development of effective dynamic visualizations. Based on cognitive load theory (CLT), effective learning from dynamic visualizations requires the alignment of instructional conditions with the cognitive architecture of learners and their levels of expertise. By improving the effectiveness and efficiency of dynamic visualizations, students will be able to be more successful in retaining visual information that mediates their understanding of complex and difficult aspects of anatomy. This theoretical paper presents instructional strategies generated by CLT and provides examples of some instructional implications of CLT on the design of dynamic visualizations for teaching and learning of anatomy.\n\n==\n\nHuang 2009:\n\nTitle: Measuring Effectiveness of Graph Visualizations: A Cognitive Load Perspective\n\nAbstract: Graph visualizations are typically evaluated by comparing their differences in effectiveness, measured by task performance such as response time and accuracy. Such performance-based measures have proved to be useful in their own right. There are some situations, however, where the performance measures alone may not be sensitive enough to detect differences. This limitation can be seen from the fact that the graph viewer may achieve the same level of performance by devoting different amounts of cognitive effort. In addition, it is not often that individual performance measures are consistently in favor of a particular visualization. This makes design and evaluation difficult in choosing one visualization over another. In an attempt to overcome the above-mentioned limitations, we measure the effectiveness of graph visualizations from a cognitive load perspective. Human memory as an information processing system and recent results from cognitive load research are reviewed first. The construct of cognitive load in the context of graph visualization is proposed and discussed. A model of user task performance, mental effort and cognitive load is proposed thereafter to further reveal the interacting relations between these three concepts. A cognitive load measure called mental effort is introduced and this measure is further combined with traditional performance measures into a single multi-dimensional measure called visualization efficiency. The proposed model and measurements are tested in a user study for validity. Implications of the cognitive load considerations in graph visualization are discussed.\n\n==\n\nKhalil 2005:\n\nTitle: Interactive and dynamic visualizations in teaching and learning of anatomy: a cognitive load perspective.\n\nAbstract: With the increasing use of computers in the classroom and the advancement of information technology, a requirement to investigate and evaluate different strategies for the presentation of verbal information in interactive and dynamic visualizations has risen to a high level of importance. There is a need for research efforts that apply cognitive load theory (CLT), cognitive learning strategies, and established principles of multimedia design to conduct empirical research that will add to our knowledge of designing and developing dynamic visualizations for teaching and learning anatomy. The impact of improved teaching and learning of anatomical sciences and the development of a set of guiding principles to facilitate the design and development of effective dynamic visualizations represent a significant achievement for medical education with wide application. This theoretical paper presents the foundations of CLT, cognitive learning strategies, and principles of multimedia design to guide the needed research on dynamic visualizations.\n\n==\n\nCastro-Alonso 2019:\n\nTitle: Instructional Visualizations, Cognitive Load Theory, and Visuospatial Processing\n\nAbstract: There are basically two formats used in instructional visualizations, namely, static pictures and dynamic visualizations (e.g., animations and videos). Both can be engaging and fun for university students in the fields of health and natural sciences. However, engagement by itself is not always conducive to learning. Consequently, teachers, lecturers, and instructional designers need to utilize the cognitive processing advantages of visualizations as well as engagement to achieve full instructional effectiveness. A cognitive processing focus has outlined many ways in which instructional visualization can be optimized. Specifically, cognitive load theory and the cognitive theory of multimedia learning are two research paradigms that provide several methods for directing the design of visualizations by considering how learners process visuospatial information. In this chapter, we describe five methods based on these cognitive theories: (a) the split attention effect and spatial contiguity principle, (b) the modality effect, (c) the redundancy effect and coherence principle, (d) the signaling principle, and (e) the transient information effect. For each of these effects, examples of applications for education in health and natural sciences are provided, where the influence of visuospatial processing is also considered. We end this chapter by discussing instructional implications for science education and providing future directions for research.\n\n==\n\nMurtianto 2022:\n\nTitle: Cognitive Load Theory on Virtual Mathematics Laboratory: Systematic Literature Review\n\nAbstract: The primary goal of cognitive load theory is to improve the learning of complex cognitive tasks by transforming current scientific knowledge on how cognitive structures and processes are organized into guidelines for instructional design. Cognitive load theory assumes that the bottleneck for acquiring new secondary biological knowledge is the limited working memory capacity. In the ideal situation, the working memory resources required for learning do not exceed the available resources. Despite this, in reality, there will often be a high cognitive load, or even \u201doverload,\u201d for two reasons. First, dealing with interactive information elements in complex cognition imposes a high intrinsic working memory load. Second, learners also have to use working memory resources for activities that are extraneous to performing and learning tasks, that is, activities that are not productive for learning. Virtual Laboratory is a form of animation that can visualize abstract phenomena or complex experiments in natural laboratories to increase learning activities and develop problem-solving skills. A virtual math laboratory was created to optimize dual coding memory, namely verbal and audio learning. The investigation tracked the approved reporting Items for Systematics Reviews and Meta-Analysis (PRISMA) guidelines, illustrating the outcomes of the literature searches and articles selection process. It is used to provide that the selection process is replicable and transparent. We accomplished a computerized bibliometric analysis from 2002-2022 for articles retrieved from the SCOPUS database. Data were collected in July 2022. \nKeywords: cognitive load theory, virtual laboratory, mathematics education\n\n==\n\nCook 2006:\n\nTitle: Visual representations in science education: The influence of prior knowledge and cognitive load theory on instructional design principles\n\nAbstract: Visual representations are essential for communicating ideas in the science classroom; however, the design of such representations is not always beneficial for learners. This paper presents instructional design considerations providing empirical evidence and integrating theoretical concepts related to cognitive load. Learners have a limited working memory, and instructional representations should be designed with the goal of reducing unnecessary cognitive load. However, cognitive architecture alone is not the only factor to be considered; individual differences, especially prior knowledge, are critical in determining what impact a visual representation will have on learners' cognitive structures and processes. Prior knowledge can determine the ease with which learners can perceive and interpret visual representations in working memory. Although a long tradition of research has compared experts and novices, more research is necessary to fully explore the expert\u2013novice continuum and maximize the potential of visual representations. \u00a9 2006 Wiley Periodicals, Inc. Sci Ed90:1073\u20131091, 2006\n\n==\n\nZagermann 2016:\n\nTitle: Measuring Cognitive Load using Eye Tracking Technology in Visual Computing\n\nAbstract: In this position paper we encourage the use of eye tracking measurements to investigate users' cognitive load while interacting with a system. We start with an overview of how eye movements can be interpreted to provide insight about cognitive processes and present a descriptive model representing the relations of eye movements and cognitive load. Then, we discuss how specific characteristics of human-computer interaction (HCI) interfere with the model and impede the application of eye tracking data to measure cognitive load in visual computing. As a result, we present a refined model, embedding the characteristics of HCI into the relation of eye tracking data and cognitive load. Based on this, we argue that eye tracking should be considered as a valuable instrument to analyze cognitive processes in visual computing and suggest future research directions to tackle outstanding issues.\n\n==\n\nAnderson 2011:\n\nTitle: A User Study of Visualization Effectiveness Using EEG and Cognitive Load\n\nAbstract: Effectively evaluating visualization techniques is a difficult task often assessed through feedback from user studies and expert evaluations. This work presents an alternative approach to visualization evaluation in which brain activity is passively recorded using electroencephalography (EEG). These measurements are used to compare different visualization techniques in terms of the burden they place on a viewer's cognitive resources. In this paper, EEG signals and response times are recorded while users interpret different representations of data distributions. This information is processed to provide insight into the cognitive load imposed on the viewer. This paper describes the design of the user study performed, the extraction of cognitive load measures from EEG data, and how those measures are used to quantitatively evaluate the effectiveness of visualizations.",
        "final_summary": "The papers suggest that cognitive load theory is relevant to the design and evaluation of visualizations in various educational contexts.\n\nKhalil 2005 emphasizes the need for instructional strategies aligned with learners' cognitive architecture and expertise levels to enhance the effectiveness of dynamic visualizations in anatomy education.\n\nHuang 2009 proposes measuring the effectiveness of graph visualizations from a cognitive load perspective, considering the cognitive effort required for achieving similar performance levels.\n\nCastro-Alonso 2019 highlights the importance of optimizing instructional visualizations based on cognitive load theory and the cognitive theory of multimedia learning, offering methods such as spatial contiguity, modality, redundancy, coherence, signaling, and transient information effects.\n\nMurtianto 2022 explores the application of cognitive load theory in virtual mathematics laboratories, aiming to reduce cognitive overload and enhance learning by considering the limited working memory capacity and minimizing extraneous activities.\n\nCook 2006 emphasizes the influence of prior knowledge on learners' cognitive structures and processes when interpreting visual representations, suggesting that instructional design should consider individual differences and reduce unnecessary cognitive load.\n\nZagermann 2016 advocates for the use of eye tracking technology to measure cognitive load during human-computer interaction, providing insights into cognitive processes and suggesting its potential in analyzing cognitive load in visual computing.\n\nAnderson 2011 presents an alternative approach to evaluating visualization effectiveness using electroencephalography (EEG) to measure cognitive load, allowing for quantitative comparisons between different visualization techniques.\n\nIn summary, these papers collectively demonstrate the relevance of cognitive load theory in understanding and optimizing visualizations for effective teaching and learning, considering factors such as learners' expertise, prior knowledge, individual differences, and the use of technologies like eye tracking and EEG."
    },
    {
        "query": "what is design thinking in teachers developing digital educational games?",
        "paper_list_string": "Jan 2018:\n\nTitle: Understanding Teachers' Design Thinking in Designing Game-Based Activities\n\nAbstract: There is a need to reframe teachers' roles from content area experts to that of learning experience designers because of 21st century teaching challenges. As learning experience designers, teachers help students develop 21st century competencies via guided cognitive and social participation in designed learning activities such as games and gamified activities. In this qualitative case study, we explore teachers' design thinking in designing a lesson plan that involves the use of a card game designed for complex system understanding. Six teachers' thoughts about learning activity design are unpacked via the following activities: playing a card game, crafting a game-based lesson plan, and reporting their design thinking via semi-structural interviews. We discuss similarities in the teachers' views on game-based learning and the structure of their designed lessons, as well as implications of the study. This baseline study helps us map out how teachers think about learning experience design. Such understanding is critical for developing teachers as designers.\n\n==\n\nAnnetta 2019:\n\nTitle: Teaching Technology Design: Practicing Teachers Designing Serious Educational Games\n\nAbstract: This chapter operationally defines the term design thinking and gives the historical and theoretical basis of design thinking. We further review current practices of design thinking in education. In a study of practicing K-12 science and instructional technology teachers designing Serious Educational Games (SEGs) (Annetta, 2008), this chapter illustrates how teaching and learning design changes how teachers think. It is the ultimate goal that this change in teachers\u2019 design thinking will enable teachers to transfer their approach to their students so their students learn to design using technologies beyond just Serious Educational Games.\n\n==\n\nMoffett 2023:\n\nTitle: Building a Digital Educational Escape Room Using an Online Design-Thinking Process\n\nAbstract: There is increasing interest in the application of game-based learning approaches to education. Educators across a wide range of contexts are using digital games such as educational escape rooms to promote learner motivation and support skills development. Whilst the literature describes multiple game-based learning theories that can underpin such strategies, there is little practical guidance on how to integrate such conceptual elements into the design of digital educational escape rooms. This study aims to address this gap, outlining the use of an online design-thinking process to plan, build, and test a digital educational escape room. Our findings suggest that this process provides an effective way of harnessing team collaboration and innovation in the development of digital educational resources. The process provides structure for game design teams, enabling them to address complex or \u201cmessy\u201d educational development problems. In utilising an online design-thinking process to design games for learning, we make a number of recommendations. These include taking time to establish psychological safety within the design team so as to facilitate creative team processes and supporting team members to adopt a design-thinking mindset throughout (e.g., regularly taking the perspective of the game user, and testing game prototypes early and frequently). Finally, our study offers a detailed description of how the online design-thinking process can be applied in an education context with the aim of offering guidance to educators and students who may want to design, build, and test their own digital educational escape rooms.\n\u00a0\n\n==\n\nHaddad 2022:\n\nTitle: Building a digital educational game supported by Socially Aware Design\n\nAbstract: This is a full paper research about building of a digital game. Traditional ways of teaching, based on theoretical explanations, make it difficult for students to learn related content, mainly in Software Engineering (SE), since connecting theory with practice is made more difficult in such approaches. The use of educational games emerges as a possibility to support a kind of teaching and learning process that changes the student\u2019s status from passive to active, and uses playful activities as a motivational element, since students learn by playing, in addition to allowing the simulation of problems and search for solutions. The objective of this article is to present the development of a digital educational game by students to support SE teaching and learning, using Socially Aware Design (SAwD), considering elements for the construction of learning objects, such as characteristics of learners and instructors, adoption of learning theories, activity sequencing, scope definition, granularity, instructional content, and evaluation criteria. SAwD seeks to discover characteristics of the social, cultural, environmental, economic, legal, and regional context, to build interactive and inclusive systems, one of its pillars being design for all. The methods for developing the work involve a life cycle model for learning object development, semi-participatory design workshops with brainstorming and braindrawing techniques, and prototyping. The prototype was finalized, applied as a didactic instrument and evaluated. It is expected to contribute to the expansion of the production and application of games in the teaching and learning of SE as well as the dissemination of SAwD.\n\n==\n\nArtym 2016:\n\nTitle: Pre-Service Teachers Designing and Constructing \"Good Digital Games\".\n\nAbstract: There is a growing interest in the application of digital games to enhance learning across many educational levels. This paper investigates pre-service teachers\u2019 ability to operationalize the learning principles that are considered part of a good digital game (Gee, 2007) by designing digital games in Scratch. Forty pre-service teachers, enrolled in an optional educational technology course, designed and constructed their own digital games in an authentic learning context. The course was structured to prepare pre-service teachers to use game design and construction in their future pedagogical practice. These pre-service teachers had various levels of game-playing experience, but little-to-no previous game-design/building experience. To evaluate the digital games, we created the Game Design Assessment Survey, which determined the degree to which a core set of learning principles, identified from the literature, were present in the digital games constructed by the pre-service teachers. Results suggested that pre-service teachers were generally unaware of the learning principles that should be included in the design of a good digital game, but were familiar with quality principles of interface usability. In addition, no relationship was found between the amount of time pre-service teachers played digital games and their ability to design and construct a good game.\n\n==\n\nFrossard 2015:\n\nTitle: Teachers Designing Learning Games\n\nAbstract: Abstract Creativity has become a key educational objective. How can game-based learning enhance creative pedagogies? This chapter proposes an approach in which teachers become game designers. It provides a model which analyzes creativity according to three dimensions: process, product, and teaching. We describe practical experiences in which teachers designed and applied their own learning games. Results highlight that game design promotes teaching practices that foster students\u2019 creativity.\n\n==\n\nAn 2017:\n\nTitle: Examining the Characteristics of Digital Learning Games Designed by In-service Teachers\n\nAbstract: In\ufefforder\ufeffto\ufeffbetter\ufeffunderstand\ufeffteachers\u2019\ufeffperspectives\ufeffon\ufeffthe\ufeffdesign\ufeffand\ufeffdevelopment\ufeffof\ufeffdigital\ufeffgamebased\ufeff learning\ufeff environments,\ufeff this\ufeff study\ufeff examined\ufeff the\ufeff characteristics\ufeff of\ufeff digital\ufeff learning\ufeff games\ufeff designed\ufeffby\ufeffteachers.\ufeffIn\ufeffaddition,\ufeffthis\ufeffstudy\ufeffexplored\ufeffhow\ufeffgame\ufeffdesign\ufeffand\ufeffpeer\ufeffcritique\ufeffactivities\ufeff influenced\ufefftheir\ufeffperceptions\ufeffof\ufeffdigital\ufeffgame-based\ufefflearning\ufeffenvironments\ufeffand\ufefflearning\ufeffthrough\ufeffgame\ufeff design.\ufeffQualitative\ufeffdata\ufeffwere\ufeffcollected\ufefffrom\ufefffifty\ufeffgame\ufeffdesign\ufeffdocuments\ufeffand\ufeffparticipant\ufeffresponses\ufeff to\ufeff reflection\ufeffquestions.\ufeffThe\ufeffanalysis\ufeffof\ufeffgame\ufeffdesign\ufeffdocuments\ufeff showed\ufeff that\ufeff the\ufeffmajority\ufeffof\ufeff the\ufeff participants\ufeffdesigned\ufeffimmersive\ufeffgame-based\ufefflearning\ufeffenvironments\ufeffwhere\ufeffplayers\ufeffare\ufeffrequired\ufeffto\ufeff use\ufeffhigher\ufefforder\ufeffthinking\ufeffand\ufeffreal-world\ufeffskills\ufeffas\ufeffwell\ufeffas\ufeffacademic\ufeffcontent\ufeffto\ufeffcomplete\ufeffmissions\ufeff or\ufeffsolve\ufeffproblems.\ufeffThe\ufeffresults\ufeffof\ufeffthis\ufeffstudy\ufeffprovide\ufeffimportant\ufeffimplications\ufefffor\ufeffteacher\ufeffprofessional\ufeff development\ufeffas\ufeffwell\ufeffas\ufefffor\ufeffeducational\ufeffgame\ufeffdevelopment. KEywoRDS Challenges, Digital Learning Games, Educational Game Design, Engagement, Game Design Strategies, GameBased Learning, Scaffolding, Teacher Perceptions, Teachers as Game Designers\n\n==\n\nSantos 2020:\n\nTitle: Adaptando o Design Thinking para a Defini\u00e7\u00e3o e Desenvolvimento de um Jogo Educacional N\u00e3o Digital no Ensino de Gerenciamento de Riscos\n\nAbstract: An alternative to traditional teaching is the use of educational games that can motivate students. However, there are still difficulties in the development of educational games when selecting their content and when designing their dynamics. This paper presents the experience report of the adaptation of the design thinking methodology in the development process of the non-digital educational game: Risking (Arriscando). Design Thinking combines the focus on the end user with multidisciplinary collaboration and iterative improvement to produce innovative products. A course content analysis was applied to define the subject of the game, as well as brainstorming and validation meetings with the students. Resumo. Uma alternativa para o ensino tradicional \u00e9 o uso de jogos educacionais que podem motivar os alunos. No entanto, ainda h\u00e1 dificuldade no desenvolvimento de jogos educacionais ao selecionar os conte\u00fados presentes e ao projetar a din\u00e2mica do jogo. Este trabalho apresenta o relato de experi\u00eancia da adapta\u00e7\u00e3o da metodologia design thinking no processo de desenvolvimento do jogo educacional n\u00e3o digital: Arriscando. O Design Thinking combina o foco no usu\u00e1rio final com colabora\u00e7\u00e3o multidisciplinar e aprimoramento iterativo para produzir produtos inovadores. Foi aplicada uma an\u00e1lise de conte\u00fados de disciplinas para definir o assunto do jogo, assim como reuni\u00f5es de brainstorming e valida\u00e7\u00e3o com os discentes.",
        "final_summary": "The papers suggest that teachers' design thinking plays a crucial role in developing digital educational games. Jan (2018) emphasizes the need to reframe teachers' roles as learning experience designers, highlighting the importance of teachers' design thinking in creating game-based activities. Annetta (2019) explores how teaching and learning design changes teachers' thinking, aiming to transfer this approach to students. Moffett (2023) addresses the practical guidance for integrating game-based learning theories into the design of digital educational escape rooms, emphasizing the effectiveness of an online design-thinking process. Haddad (2022) focuses on the development of a digital educational game using Socially Aware Design, aiming to enhance teaching and learning in Software Engineering. Artym (2016) investigates pre-service teachers' ability to operationalize learning principles in designing digital games, revealing a lack of awareness of these principles. Frossard (2015) proposes that teachers becoming game designers enhances creativity in pedagogies. An (2017) examines the characteristics of digital learning games designed by teachers, highlighting the creation of immersive and engaging learning environments. Santos (2020) presents the adaptation of design thinking in developing a non-digital educational game, emphasizing the selection of content and game dynamics. Overall, these papers emphasize the importance of teachers' design thinking in creating effective and engaging digital educational games."
    },
    {
        "query": "The Gross Domestic Product (GDP) in Vietnam was worth 408.80 billion US dollars in 2022, according to official data from the World Bank. The GDP value of Vietnam represents 0.18 percent of the world economy.",
        "paper_list_string": "Thai 2020:\n\nTitle: Vietnam Economic Issue: Looking from Enterprise Sector, Export and Import Situations\n\nAbstract: At first glance, Vietnam has relatively high growth rate in the region and in the world, the average growth in the period of 2011 - 2018 is about 6.2%. As soon as the Covid 19 pandemic became active, causing most countries to have a negative GDP growth rate, but Vietnam's GDP growth in the first quarter was still 3.82%. \nThis study tried to describe the situation in the context of the current economic situation in Viet Nam through the production results of the enterprise sector, import and export. \nThe study used official data sources from Vietnam General Statistics Office.\n\n==\n\nKalra 2015:\n\nTitle: Vietnam: The Global Economy and Macroeconomic Outlook\n\nAbstract: After almost a decade of high growth, Vietnam\u2019s growth rate fell during 2011\u201313. Since 2001, the country has also experienced two bouts of high inflation, booms and busts in equity and real estate markets, and episodes of large capital inflows and outflows. Against the backdrop of the global economy, this paper provides an account of macroeconomic developments in Vietnam during 2011 to 2013, examines the imbalances that came to a head in 2011, the macroeconomic stabilization achieved during 2012 to 2014, and the outlook and challenges going forward. The paper concludes that successfully designing and implementing a broad set of policies \u2014 staying the course on macroeconomic stabilization, while accelerating the pace of structural reform significantly, and integrating into the global economy \u2014 will allow Vietnam to further advance the remarkable gains that it has already made in poverty alleviation and achieving its Millenium Development Goals.\n\n==\n\nTarp 2002:\n\nTitle: Trade and Income Growth in Vietnam: Estimates from a New Social Accounting Matrix\n\nAbstract: Economic reforms and greater outward orientation are giving rise to extensive structural change in the Vietnamese economy. Because of the leverage that global markets can exert on an emerging economy, such adjustments will be particularly significant in the composition of domestic supply and demand. As domestic protection levels are reduced and external market access increases, trade growth and shifting trade patterns will have pervasive effects on income distribution in Vietnam. In this paper, we use a newly estimated Vietnam social accounting matrix to elucidate the links between trade and income in the country. With matrix decomposition methods, we show how the Vietnamese economy propagates the direct effects of external demand across the spectrum of domestic activities, factors, and households. This detailed analysis provides a blueprint for policies to improve economic participation of activities and households with relatively weak linkages to the rest of the economy.\n\n==\n\nGlewwe 2004:\n\nTitle: Economic Growth, Poverty, and Household Welfare in Vietnam\n\nAbstract: Viet Nam is an economic success story - it transformed itself from a country in the 1980s as one of the poorest in the world, to a country in the 1990s with one of the world's highest growth rates. With the adoption of a new market-oriented policies, Viet Nam averaged an economic growth rate of 8 percent per year from 1990 to 2000, a growth rate accompanied by a large reduction in poverty, stemming from significant increases in school enrollment, and a rapid decrease in child malnutrition. The book uses an unusually rich set of macroeconomic, and household survey data, to examine several topics: the causes of the economic turnaround, and prospects for future growth; the impact of economic growth on household welfare, as measured by consumption expenditures, health, education, and other socioeconomic indicators; and, the nature of poverty in Viet Nam, and the effectiveness of government policies, intended to reduce same. Although the country's past achievements are impressive, future progress is by no means ensured.\n\n==\n\nDinh 2022:\n\nTitle: Vietnam public debt forecast in the period of 2021 - 2025\n\nAbstract: Vietnam's public debt has an important achievement in the years 2017-2019. Specifically, the public debt/GDP ratio was 61.4% of GDP in 2017. However, this ratio fell to 58.3% in 2018 and 55% in 2019. This improvement in public debt/GDP is due to the large scale of the economy and high economic growth rate. This fact implies there is a relationship between public debt and macroeconomic factors. Theoretical research shows that the macroeconomic factors affecting public debt include the State budget deficit, economic growth, real interest rates and exchange rates. This study will build a model to analyze the impact of those macroeconomic factors on public debt/GDP, period 2000 - 2020. The study has an important finding. That is, there exist two mechanisms, which have opposite effects on the evolution of public debt/GDP of Vietnam. Firstly, it is the trend of expanding investment to promote growth. This trend increases public debt/GDP. And second, it is the tendency to increase the real size of the economy, as measured by real GDP growth. This trend reduces public debt/GDP. The author relies on the above two trends to explain the evolution of Vietnam's public debt/GDP, period 2000 - 2020. Next, based on the analytical model, the author will forecast the trend of public debt in the period 2021 -2025. The forecast takes into account the impact of Covid-19 on public debt.\n\n==\n\nBinh 2013:\n\nTitle: APPLYING GRAVITY MODEL TO ANALYZE TRADE ACTIVITIES OF VIETNAM\n\nAbstract: This paper applies gravity model in order to analyze bilateral trade activities between Vietnam and 60 countries from 2000 to 2010. We exploited the panel data on international trade of Vietnam taken from the data banks of International Trade Centre, International \u201eMonetary Fund and World Bank. The estimated results reveal that economic size of Vietnam, economic size and market size of foreign partners, distance and culture have huge effects on bilateral trade flows between Vietnam and these 60 countries. By applying method of speed of convergence, we also find out that Vietnam has trade potential especially with some new markets such as Africa and Western Asia.\n\n==\n\nNghiep 2000:\n\nTitle: Measuring the Impact of Doi Moi on Vietnam\u2019s Gross Domestic Product\n\nAbstract: In 1986 a wide range of policy measures, known as Doi Moi, was introduced to promote Vietnam's transition to a market economy. This paper represents the first attempt to measure the effect of Doi Moi on Vietnam's GDP. In the paper the level of GDP actually reached is compared with the level that would have been reached had the policy not been implemented, i.e. without the improvements in productivity and the increases in investment ratio that can be directly attributed to Doi Moi. Cross-time changes in GDP were depicted by a production function of capital stock, economically active labour force and technical progress. It was found that, after a time lag, Doi Moi appeared to have a significant positive effect on productivity, which by 1998 accounted for a 42% increase in GDP.\n\n==\n\nOh 2019:\n\nTitle: Vietnam Development Report 2019: Connecting Vietnam for Growth and Shared Prosperity\n\nAbstract: Globally, Vietnam is among the most open economies with a trade-to-GDP ratio of 190 percent in 2018. Through the removal of both tariff and non-tariff barriers and fulfilling its commitment in several regional trade agreements, the country has made remarkable achievements in trade liberalization. Vietnam\u2019s major trade partners located in East Asia, North America, and Europe are reached mostly by sea or air. Trade with bordering neighbors is limited and thus trade across border-crossing points is minimal except for northern borders with China, which has seen growth in recent years. The country\u2019s trade flows are concentrated at twelve of its 48 border gates\u2014two airports, five seaports and five border crossing points\u2014which collectively handled 86 percent of total trade value in 2016.1 As the trade grows, congestion at and near these international gateways and border-crossing points has also increased. In addition to the current major trade partners, various regional trade relations and connectivity initiatives are relevant to Vietnam, including with Southeast Asian neighbors, and South Asia\u2014particularly India\u2014over land, given the rapidly growing trade relationships. In the meantime, Vietnam\u2019s transport network has undergone a significant expansion over the past decades. The most remarkable development in network expansion has occurred in the road sector. As of 2016 the total length of the road network, excluding village roads, reached over 300,000 km, including about 1,000 km of expressways\u2014a fully access-controlled toll road system. Vietnam is endowed with an extensive network of natural waterways, including nearly 16,000 km of managed navigable routes carrying significant traffic around the Red River Delta and Mekong Delta areas. However, only about 2,600 km of the waterways can reliably handle barges greater than 300 deadweight tons, with rudimentary terminal infrastructure at most of its numerous river ports. Vietnam's century-old railway system is mostly single-tracked and non-electrified, which has remained unchanged over the past decades with very limited capital investments",
        "final_summary": "The papers collectively provide insights into Vietnam's GDP and its economic situation. Thai 2020 highlights Vietnam's relatively high growth rate, even during the Covid-19 pandemic. Kalra 2015 discusses Vietnam's macroeconomic developments, imbalances, and the need for structural reform. Tarp 2002 explores the links between trade and income growth in Vietnam, emphasizing the effects of trade patterns on income distribution. Glewwe 2004 examines Vietnam's economic growth, poverty reduction, and the effectiveness of government policies. Dinh 2022 focuses on Vietnam's public debt and its relationship with macroeconomic factors. Binh 2013 applies the gravity model to analyze Vietnam's trade activities with various countries. Nghiep 2000 measures the impact of Doi Moi, a set of policy measures, on Vietnam's GDP, showing a significant positive effect on productivity. Oh 2019 discusses Vietnam's trade liberalization, transport network expansion, and challenges related to congestion and infrastructure."
    },
    {
        "query": "what are Value sensitive design methods?",
        "paper_list_string": "Friedman 2018:\n\nTitle: A Survey of Value Sensitive Design Methods\n\nAbstract: Value sensitive design is a theoretically grounded approach to the designof technology that accounts for human values in a principled andsystematic manner throughout the design process. In this article weprovide a survey of 14 value sensitive design methods: 1 direct andindirect stakeholder analysis; 2 value source analysis; 3 co-evolutionof technology and social structure; 4 value scenario; 5 value sketch;6 value-oriented semi- structured interview; 7 scalable informationdimensions; 8 value-oriented coding manual; 9 value-oriented mockup,prototype, or field deployment; 10 ethnographically informed inquiryregarding values and technology; 11 model of informed consentonline; 12 value dams and flows; 13 value sensitive action-reflectionmodel; and 14 Envisioning Cards TM. Each of these methods is honedto the investigation of values in technology, serving such purposesas stakeholder identification and legitimation, value representation andelicitation, and values analysis. While presented individually, the methodsare intended to be integrated in a robust value sensitive designprocess. The survey article begins with a brief summary of value sensitivedesign methodology and theoretical constructs. We next providean overview of the 14 methods. Then, we turn to a broader discussion ofvalue sensitive design practice, focussing on some methodological strategiesand heuristics to support skillful value sensitive design practice.Following the broad discussion of practice, we illustrate one method inaction-value scenarios-providing details on its range of purposes andcontexts. We conclude with reflections on core characteristics of valuesensitive design methodology, and heuristics for innovation.\n\n==\n\nFriedman 2002:\n\nTitle: Value Sensitive Design: Theory and Methods\n\nAbstract: Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three case studies. The first study concerns information and control of web browser cookies, implicating the value of informed consent. The second study concerns using high-definition plasma displays in an office environment to provide a virtual window to the outside world, implicating the values of physical and psychological well-being and privacy in public spaces. The third study concerns an integrated land use, transportation, and environmental simulation system to support public deliberation and debate on major land use and transportation decisions, implicating the values of fairness (and specifically freedom from bias), accountability, and support for the democratic process, as well as a highly diverse range of values that might be held by different stakeholders, such as environmental sustainability, opportunities for business expansion, or walkable neighborhoods. We conclude with direct and practical suggestions for how to engage in Value Sensitive Design.\n\n==\n\nFriedmanBatya 2017:\n\nTitle: A Survey of Value Sensitive Design Methods\n\nAbstract: Value sensitive design is a theoretically grounded approach to the designof technology that accounts for human values in a principled andsystematic manner throughout the design process. In this art...\n\n==\n\nDantec 2009:\n\nTitle: Values as lived experience: evolving value sensitive design in support of value discovery\n\nAbstract: The Value Sensitive Design (VSD) methodology provides a comprehensive framework for advancing a value-centered research and design agenda. Although VSD provides helpful ways of thinking about and designing value-centered computational systems, we argue that the specific mechanics of VSD create thorny tensions with respect to value sensitivity. In particular, we examine limitations due to value classifications, inadequate guidance on empirical tools for design, and the ways in which the design process is ordered. In this paper, we propose ways of maturing the VSD methodology to overcome these limitations and present three empirical case studies that illustrate a family of methods to effectively engage local expressions of values. The findings from our case studies provide evidence of how we can mature the VSD methodology to mitigate the pitfalls of classification and engender a commitment to reflect on and respond to local contexts of design.\n\n==\n\nFriedman 2013:\n\nTitle: Value Sensitive Design and Information Systems\n\nAbstract: Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three case studies. The first study concerns information and control of web browser cookies, implicating the value of informed consent. The second study concerns using high-definition plasma displays in an office environment to provide a \u201cwindow\u201d to the outside world, implicating the values of physical and psychological well-being and privacy in public spaces. The third study concerns an integrated land use, transportation, and environmental simulation system to support public deliberation and debate on major land use and transportation decisions, implicating the values of fairness, accountability, and support for the democratic process, as well as a highly diverse range of values that might be held by different stakeholders, such as environmental sustainability, opportunities for business expansion, or walkable neighborhoods. We conclude with direct and practical suggestions for how to engage in Value Sensitive Design.\n\n==\n\nMithun 2018:\n\nTitle: The Realism of Value Sensitive Design on User Interface Development\n\nAbstract: Value Sensitive Design (VSD) is the theoretical approach that applied to technology design, in particular for Human-Computer Interaction (HCI). The User Interface (UI) design is one of the concerns in HCI, being multidisciplinary, it ensures the user's satisfaction and perception of technology which follows during the development process. The VSD method takes human values in a comprehensive manner throughout three investigations called tripartite methodology, and the investigations applied as the techniques to investigate the requirements of the development from various stakeholders values. Since variations of stakeholders, VSD method follows the designers, direct and indirect stakeholders to take out required values into the development. In spite of the widely used VSD method undetermined the values that to be considered on design as designers, direct or indirect stakeholders. The argument considering the values has been researching since the concept applied to HCI design which indicates the method limitation. In this research considered two values, designers and stakeholders and the key concern to observe the effects and realism of the applications of VSD method into the design of user interface also reviewed earlier researches on the application of VSD. The analysis through the development of hospital management system interface to investigate whose values to be considered in the HCI design. Performed user survey based on two criteria that evaluates the variations of stakeholders values which effects on the user interface design that is the contribution of the research, and it enlightens the concept of values of VSD should considered in the future user interface design.\n\n==\n\nYoo 2013:\n\nTitle: A value sensitive action-reflection model: evolving a co-design space with stakeholder and designer prompts\n\nAbstract: We introduce a design method for evolving a co-design space to support stakeholders untrained in design. Specifically, the purpose of the method is to expand and shape a co-design space so that stakeholders, acting as designers, focus not only on the form and function of a tool being envisioned but also on the social context of its use and values that lie with individuals, groups, and societies. The method introduces value sensitive stakeholder prompts and designer prompts into a co-design process, creating a particular kind of reflection-on-action cycle. The prompts provide a means for bringing empirical data on values and theoretical perspective into the co-design process. We present the method in terms of a general model, the Value Sensitive Action-Reflection Model; place the model within discourse on co-design spaces; and illustrate the model with a discussion of its application in a lo-fi prototyping activity around safety for homeless young people. We conclude with reflections on the model and method.\n\n==\n\nManders-Huits 2009:\n\nTitle: The Need for a Value-Sensitive Design of Communication Infrastructures\n\nAbstract: In this chapter, we will discuss Value-Sensitive Design as a general approach to thinking about design in the TUC \u2013 Towards Ultrafast Communication \u2013 project from an ethical perspective. We begin with the historical context by describing developments both in information technology and ethics that preceded Value-Sensitive Design. Both ethics and computer science demonstrate a converging development towards interest in the relation between values and technical design, respectively coined by us as \u201cThe Value Turn\u201d in (information) technology and \u201cThe Design Turn\u201d in ethics. After a sketch of the development and history of Value-Sensitive Design we will discuss the methodology of both Value-Sensitive Design and a related approach called \u201cValues at Play\u201d. Their methodology consists of three modes of inquiry, respectively the conceptual, empirical, and technical mode and the discovery, translation, and verification phase. These three parts of the methodology are used to deal with concerns arising from moral theory with respect to the way technology affects moral practices in direct and indirect ways. Towards the end of the chapter we will discuss in general how Value-Sensitive Design applies to the TUC project.",
        "final_summary": "The papers collectively provide an overview of Value Sensitive Design (VSD) methods. Friedman 2018 presents a survey of 14 VSD methods, including stakeholder analysis, value scenario, and value-oriented coding manual. Friedman 2002 explains VSD as an approach that integrates conceptual, empirical, and technical investigations to account for human values in technology design. Dantec 2009 identifies limitations in VSD, such as value classifications and the design process, and proposes ways to overcome them. Other papers, such as Friedman 2013 and Manders-Huits 2009, discuss the application of VSD in specific contexts, such as information systems and communication infrastructures. The papers collectively emphasize the importance of considering human values throughout the design process and provide practical suggestions for engaging in VSD."
    },
    {
        "query": "The theory of planned behavior and teacher decisions",
        "paper_list_string": "Lee 2010:\n\nTitle: Theory of Planned Behavior and Teachers' Decisions Regarding Use of Educational Technology\n\nAbstract: According to Ajzen's Theory of Planned Behavior (TPB), behavioral intention (BI) is predicted by attitude toward the behavior (AB), subjective norm (SN), and perceived behavioral control (PBC). Previous studies using the TPB to explain teachers' intentions to use technology have resulted in inconsistent findings. This inconsistency might be due to overly broad definitions of the target behavior. To investigate this potential weakness, we defined a specific target behavior, using computers only to create and deliver lessons, and then used the TPB to investigate teachers' decisions. An elicitation study was used to identify teachers' salient beliefs and develop a closed-ended questionnaire. Results of the closed-ended questionnaire revealed that AB, SN, and PBC all were significant predictors of teachers' intentions. However, AB had twice the influence of SN and three times that of PBC. This finding suggests that teachers must have positive attitudes about using computers to create and deliver lessons. They are less concerned about what others think of this practice, and far less bothered by internal or external constraints. Results provide specific information that can be used to design effective teacher development programs and remind TPB researchers of the importance of using specific definitions of the target behavior.\n\n==\n\nMartin 2004:\n\nTitle: Self-Efficacy Theory and the Theory of Planned Behavior: Teaching Physically Active Physical Education Classes\n\nAbstract: Abstract The purpose of our investigation was to examine determinants of teachers' intentions to teach physically active physical education classes (i.e., spend at least 50% of class time with the students engaged in moderate to vigorous physical activity). Based on the theory of planned behavior, a model was examined hypothesizing that teachers' intentions were determined by subjective norm, attitude, and perceived behavioral control. Grounded in self-efficacy theory, it was hypothesized that program goal importance and hierarchical and barrier self-efficacy would also predict intention. Using a series of hierarchical regression analyses, the theory of planned behavior was supported by accounting for 59% of the variance in intention due to attitude, perceived behavioral control, and subjective norm. Self-efficacy theory based variables received minimal support.\n\n==\n\nTeo 2016:\n\nTitle: Teachers and technology: development of an extended theory of planned behavior\n\nAbstract: Abstract\nThis study tests the validity of an extended theory of planned behaviour (TPB) to explain teachers\u2019 intention to use technology for teaching and learning. Five hundred and ninety two participants completed a survey questionnaire measuring their responses to eight constructs which form an extended TPB. Using structural equation modelling, the results showed that the constructs in the extended TPB were significant in explaining teachers\u2019 intention to use technology in their work. Among the constructs in the research model, attitude towards computer use had the largest positive influence on technology usage intention, followed by perceived behavioral control. However, subjective norm had a negative impact on intention. The inclusion of the antecedent variables had also strengthened the ability of the extended TPB model to explain intention. This study contributes to the growing discussions in applying psychological theories to explain behavioral intention in educational contexts.\n\n==\n\nDunn 2018:\n\nTitle: Using the Theory of Planned Behavior to explore teachers\u2019 intentions to engage in ongoing teacher professional learning\n\nAbstract: Abstract The purpose of this study was to examine the factors influencing teachers\u2019 intentions to engage in professional learning on the mathematics Common Core State Standards (CCSS). The aim was to identify specific factors teachers encounter that impinge upon or facilitates implementing new instructional practices. Ajzen\u2019s (1991) theory of planned behavior was used to examine the influence of three constructs (attitude toward the behavior, subjective norms, and perceived behavioral control) on teachers\u2019 intentions to implement the instructional expectations associated with the CCSS in mathematics. Data for this quantitative study was collected from 152 teachers in an urban school district in California. The result of the study provides support for the adequacy of the theory of planned behavior for predicting and understanding teachers\u2019 behavioral intention toward professional learning. Intention was predicted significantly by perceived behavioral control, subjective norm and attitude toward the behavior.\n\n==\n\nMacFarlane 2013:\n\nTitle: Teacher attitudes and behavior toward the inclusion of children with social, emotional and behavioral difficulties in mainstream schools: An application of the theory of planned behavior\n\nAbstract: Abstract The Theory of Planned Behavior (TPB) was used to examine relationships between teacher attitudes and behavior toward children with social, emotional and behavioral difficulties (SEBD). One hundred and eleven elementary school teachers completed questionnaires. Teacher perception of their school principals' expectations (subjective norm) predicted teacher behaviors. Teachers who had attended more in-service training (INSET) sessions held more positive feelings, but teachers with more experience were less willing to work with children with SEBD. Findings suggest that school principals have a central role in promoting an inclusive ethos within their schools. INSET could focus more on challenging beliefs.\n\n==\n\nStanec 2009:\n\nTitle: The Theory of Planned Behavior: Predicting Teachers\u2019 Intentions and Behavior during Fitness Testing\n\nAbstract: The twofold purpose of this study was to develop and validate an instrument that assessed teachers\u2019 intentions, attitudes, subjective norm, and perceived behavior control to administer fitness tests effectively, and to determine how well the instrument could predict teachers\u2019 intentions and actual behavior based on Ajzen\u2019s (1985, 1991) theory of planned behavior. In the development phase of the study, 104 physical educators completed the pilot version of the survey to refine the instrument. In the prediction of behavior phase of the study, a convenience sample of 195 physical educators completed (a) the Teachers\u2019 Intentions to Administer Physical Fitness Tests Effectively (TIAPFTE) before fitness testing and (b) a behavior self-report after they administered fitness testing. Standard multiple regression analyses showed perceived behavioral control and attitude significantly predicted intention. Furthermore, results showed that attitude significantly predicted teachers\u2019 behavior directly.\n\n==\n\nCrawley 1990:\n\nTitle: Intentions of Science Teachers To Use Investigative Teaching Methods: A Test of the Theory of Planned Behavior.\n\nAbstract: The purpose of this study was to explore the utility of the theory of planned behavior for predicting the behavioral intentions of teachers enrolled in the Institute in Physical Science, an EESA, Title II program funded by the Texas Higher Education Coordinating Board. In particular, the study investigated three determinants of teachers' behavioral intentions (BI) set forth in the theory of planned behavior, namely, attitude toward the behavior (AB), subjective norm (SN), and perceived behavioral control (PBC). The behavior of interest in this study was the intention of teachers in grades 5/6 or 9/10 who were enrolled in the Institute to use 50% of the activities and investigations completed in the program with students they would teach during the next school year. Data were collected from 50 elementary and secondary teachers. Simple and hierarchical regression analyses were used to determine the relative contributions of attitude, subjective norm, and perceived behavioral control (and their interactions) to the prediction of behavioral intention. Results of this study indicated that attitude, subjective norm, and perceived behavioral control made significant contributions to the prediction of behavioral intention. Examination of the data blocked on external variables revealed attitude toward the behavior to be the single most important predictor of behavioral intention. Intent to perform the behavior appears to be totally under the control of most teachers, with little need for social support and with ample resources and sufficient opportunities available to perform the behavior.\n\n==\n\nKnauder 2019:\n\nTitle: Individualized student support in primary school teaching: A review of influencing factors using the Theory of Planned Behavior (TPB)\n\nAbstract: Abstract According to the Theory of Planned Behavior (TPB; Ajzen, 1991), each behavior results from a behavioral intention, which in turn depends on attitudes, subjective norms and perceived behavioral control. In this study, the TPB was investigated with respect to individualized student support and teachers' intrinsic and extrinsic support intentions involving 488 primary school teachers. Employing individualized support interventions was predicted directly by self-efficacy beliefs and the intrinsic support intention as well as indirectly by teachers\u2019 attitudes. More practically experienced teachers implement support measures more often, while multi-groupcomparisons regarding advanced training showed no differences in the tested model for the TPB.",
        "final_summary": "The Theory of Planned Behavior (TPB) can be used to understand teachers' decisions and intentions regarding various aspects of their teaching practices. Lee (2010) found that teachers' intentions to use educational technology were influenced by their attitudes, subjective norms, and perceived behavioral control, with attitude having the strongest influence. Martin (2004) used the TPB to examine teachers' intentions to teach physically active physical education classes, with attitude, perceived behavioral control, and subjective norm accounting for a significant amount of variance in intention. However, self-efficacy variables received minimal support in this context. Teo (2016) found that attitude toward computer use and perceived behavioral control were significant predictors of teachers' intention to use technology, while subjective norm had a negative impact. Dunn (2018) found that perceived behavioral control, subjective norm, and attitude toward the behavior were significant predictors of teachers' intentions to engage in ongoing professional learning. MacFarlane (2013) found that teacher behavior toward the inclusion of children with social, emotional, and behavioral difficulties was predicted by subjective norm, influenced by school principals' expectations. Stanec (2009) found that perceived behavioral control and attitude significantly predicted teachers' intentions during fitness testing, and attitude directly predicted behavior. Crawley (1990) found that attitude, subjective norm, and perceived behavioral control all made significant contributions to the prediction of science teachers' intentions to use investigative teaching methods. Knauder (2019) used the TPB to investigate teachers' intentions to implement individualized support interventions, with self-efficacy beliefs, intrinsic support intention, and attitudes as predictors."
    },
    {
        "query": "informal economy and environmental economy",
        "paper_list_string": "Elgin 2014:\n\nTitle: Pollution and Informal Economy\n\nAbstract: In this study, we investigate the relationship between the size of the informal economy and the level of environmental pollution/energy use. To this end, we first use different indicators of environmental pollution along with a measure of energy use intensity in a panel dataset consisting of 152 countries over the period 1999\u20132009 and empirically examine the relationship between pollution and the shadow economy. The estimation results show that there is an inverse-U relationship between the size of the informal economy and environmental pollution, that is, small and large sizes of the informal economy are associated with lower environmental pollution and medium levels of informality are associated with higher levels of environmental pollution. Next, we build a two sector dynamic general equilibrium model to suggest an economic mechanism for this observation. Our model identifies two channels through which informality might affect environmental pollution: The scale effect, whereby a larger (smaller) informal economy size is associated with a lower (higher) level of environmental pollution, and the deregulation effect, whereby a larger (smaller) informal economy is associated with higher (lower) pollution levels. As these two effects work in opposite directions, the changing relative strength of one with respect to the informal sector size creates the inverted-U relationship between pollution indicators and informality.\n\n==\n\nEren 2022:\n\nTitle: The moderating role of informal economy on financial development induced ekc hypothesis in turkey\n\nAbstract: This study conducts an empirical investigation about the moderating role of the informal economy on Turkey's environmental performance by employing advanced econometric techniques that account numerous structural breaks in series. In this extent, we created three interaction variables by captivating the impact of informal economic activities on CO2 emissions through income, energy use, and financial sector development. Besides, we built a main effect model without the interaction variables to assess the direct effects of our variables on global environmental degradation. The outcomes of the carried analyses produced supporting evidence toward the confirmation of the Environmental Kuznets Curve (EKC) assumption. Obtained findings shown that energy use, financial development and the informal economy in Turkey transmit a deteriorating impact on environmental well-being. Furthermore, the moderating role of the informal economy was found to be statistically significant factor in terms of both economic and environmental efficiency.\n\n==\n\nImamoglu 2018:\n\nTitle: Is the informal economic activity a determinant of environmental quality?\n\nAbstract: This study aims to investigate whether the size of the informal economic activity is a determinant of the environmental quality in the case of Turkey. First, the MIMIC model approach will be used to estimate the size of the informal economic activity. Second, time series analysis will be conducted for the period from 1970 to 2014, in order to examine the effect of the size of the informal economic activity on the environmental quality by conducting Carrion-i-Silvestre et al. (Economet Theor 25:1754, 2009) unit root tests, Bound test and Maki (Econ Model 29:2011\u20132015, 2012) cointegration tests that allow structural breaks will be employed prior to the DOLS, FMOLS, and ARDL approaches for long-run and short-run models. The results suggest that informal economic activity is a positively significant determinant on the environmental quality. Moreover, this paper suggests that both formal and informal economies have significant impact on environmental quality. However, formal economy exerts positively higher effects on the environmental quality compared to informal economy. Turkish authorities need to pay more attention on both formal and informal economic activities to prevent environmental degradation in Turkey.\n\n==\n\nSmit 2015:\n\nTitle: Towards connecting green economy with informal economy in South Africa : a review and way forward\n\nAbstract: The informal economy is a vibrant and growing phenomenon, offering both opportunities and lessons on resilience and innovation. When considering global social, economic and environmental challenges, resilience and innovation are valuable response strategies. The notion of a green economy has similarly inspired a number of ideological, geopolitical and institutional responses, yet a review of the dominant approach indicates the propensity to undervalue or ignore the informal economy. Within the context of sustainable development and poverty eradication, connecting the informal economy with the green economy is imperative. This paper explores possible connections between the green economy and the informal economy in South Africa and argues that by engaging the informal economy in discussions on the green economy, a more informed policy and planning environment may ensue, resulting in more socially equitable and environmentally sustainable development.\n\n==\n\nMazhar 2013:\n\nTitle: Environmental Regulation, Pollution and the Informal Economy\n\nAbstract: The regulation of environmental pollution is challenging. Particularly, the presence of institutional weaknesses like informal economy may not allow effective regulation. In this context, this paper addresses three related questions: (a) How stringent environmental regulation affects pollution? (b) What is the link between stringency of environmental regulation and the size of the informal economy? (c) How informal economy affects formal sector pollution? We use a data set of more than 100 countries from 2007 to 2010, a multivariate framework that controls for the influence of important factors and an index of perceived stringency of environmental regulation. The main findings of the paper, in line with theoretical reasoning, are that (i) stringent environmental regulation reduces pollution and (ii) stringent environmental regulation increases the size of the informal economy. This evidence suggests that informal economy helps avoid environmental regulation by being outside the regulatory sphere. An additional support to this finding is provided by the robust negative correlation between the size of the informal economy and the formal sector pollution. Our findings are based on interactive and non-linear effects that are tested and verified. In this regard, the paper raises new issues about possible mechanisms to reduce pollution in the presence of the informal economy.\n\n==\n\nPerera 1996:\n\nTitle: Accommodating the Informal Sector: A Strategy for Urban Environmental Management\n\nAbstract: Abstract Livelihoods of the urban poor, particularly the informal economic activities operating on streets and other public places, are usually seen as undesirable for environmental management by urban authorities which are preoccupied with keeping their cities clean. Hence, informal sector activities are often seen as \u00ab\u00abeye-sores\u00bb\u00bb and are evicted from city centers in the name of \u00ab\u00abpublic cleanliness and orderliness\u00bb\u00bb. However, it is seen that environmental problems associated with the informal sector are mostly manifestations of unresponsive physical planning systems rather than attributes inherent to the sector's respective activities. An environmental impact analysis shows that provision of proper business premises to informal enterprises is an effective measure to curb the environmental problems associated with the sector. From this viewpoint, accommodating the informal sector in the urban built-environment is seen as an effective strategy for urban environmental management.\n\n==\n\nGuibrunet 2017:\n\nTitle: The contribution of the informal economy to urban sustainability \u2013 case study of waste management in Tepito, Mexico City\n\nAbstract: This thesis explores the role of the informal economy in urban environmental management. Cities\u2019 relation with the environment is mediated by the urban infrastructure, which provides services such as transport or waste management. Beyond the implementation of plans by local governments, the daily operation of such infrastructure is also the result of informal work. Yet, little is known about the nature and impacts of informal work in urban service provision. This thesis tackles this research gap by documenting the everyday operation of domestic waste collection and management in a neighbourhood of Mexico City. The aim of this research is twofold. Firstly, it aims to critically analyse the concept of \u201cinformality\u201d in the case of urban waste management, and to document how informality operates in that context. Secondly, it assesses the contribution of the informal economy to the waste management system, by contrasting it to the key components of urban sustainability. The research presents primary data collected through qualitative fieldwork. Using an urban metabolism framework, it documents waste flows through the urban infrastructure, identifying the role of formal and informal waste handlers along the way. In parallel, it explores the normative discourses of informality that are mobilised in the production of Mexico City\u2019s urban sustainability policies. The thesis argues that it is necessary to re-consider the role of informal workers in urban sustainability. In Mexico City, informal and formal waste workers\u2019 relationship is symbiotic. Formal waste collection services are sustained by informal work and cash flows. In parallel, informal waste handlers provide the main input (recyclable materials) to the formal recycling industry \u2013 this is achieved through the reliance on local solidarity networks and techniques of experimentation and innovation which are characteristic of the informal economy. The informal economy appears to contribute positively to the environmental and social components of urban sustainability. Yet, informal workers are not recognised as legitimate actors in policy making. Instead, the concept of informality is mobilised by civil servants to exclude informal workers from the policy process. This challenges the potential for inclusive governance, a key component of urban sustainability.\n\n==\n\nRuzek 2014:\n\nTitle: The Informal Economy as a Catalyst for Sustainability\n\nAbstract: Sustainability typically involves the balancing of three major factors: the economy, the environment, and some notion of equity. Though the economy is already a key aspect, the recognition of the informal economy, seems to be absent from the many possible permutations of these three. This paper will explore the various aspects of the informal economy and how it can make a considerable impact on achieving a more sustainable future. Specifically, this paper focuses on how the informal economy can encourage the sustainable use of goods, while offering an alternative to the regulated market economy. By supporting the informal sectors such as farmers markets, street vendors and non-market activities, a shift away from a car-dominated society and singular economic trajectory can begin. The informal sector can provide, social capital, promote local economies, create jobs and provide the need economic shift toward a sustainable future.",
        "final_summary": "The papers suggest that there is a relationship between the informal economy and the environmental economy, but the nature of this relationship is complex and varies depending on different factors.\n\nElgin 2014 finds an inverse-U relationship between the size of the informal economy and environmental pollution, indicating that small and large sizes of the informal economy are associated with lower pollution levels, while medium levels of informality are associated with higher pollution levels.\n\nEren 2022 supports the Environmental Kuznets Curve (EKC) assumption and finds that the informal economy in Turkey has a deteriorating impact on environmental well-being, indicating a negative relationship between the informal economy and environmental performance.\n\nImamoglu 2018, focusing on Turkey, suggests that the size of the informal economy is a positively significant determinant of environmental quality, but the formal economy has a higher positive impact on environmental quality compared to the informal economy.\n\nSmit 2015 argues for the importance of connecting the informal economy with the green economy in South Africa to achieve socially equitable and environmentally sustainable development.\n\nMazhar 2013 finds that stringent environmental regulation reduces pollution but also increases the size of the informal economy, suggesting that the informal economy helps avoid environmental regulation by operating outside the regulatory sphere.\n\nPerera 1996 highlights the need to accommodate the informal sector in urban environments as an effective strategy for urban environmental management, emphasizing that environmental problems associated with the informal sector are often due to unresponsive physical planning systems.\n\nGuibrunet 2017 explores the role of the informal economy in waste management in Mexico City and suggests that the informal economy contributes positively to the environmental and social components of urban sustainability, but informal workers are not recognized as legitimate actors in policy making.\n\nRuzek 2014 argues that the informal economy can play a significant role in achieving sustainability by promoting the sustainable use of goods, supporting local economies, creating jobs, and shifting towards a more sustainable future.\n\nIn summary, the papers indicate that the relationship between the informal economy and the environmental economy is complex, with varying impacts depending on factors such as the size of the informal economy, environmental regulations, and the specific context."
    },
    {
        "query": "digital parenting and artificial intelligence",
        "paper_list_string": "Siibak 2019:\n\nTitle: Digital parenting and the datafied child\n\nAbstract: Many parents of today are feeling increasingly concerned not only for the well-being and safety of their children, but also for their own abilities to take up the role of a \u201cgood\u201d and \u201cresponsible\u201d parent. Empirical research evidence is used in the chapter to illustrate how the data religion cultivated by tech industry, popular press, marketing discourses and general societal expectations of a \u201cresponsible parent\u201d have created a norm for plugged\u2011in parenting resulting in intimate dataveillance of children, both in online and offline contexts. Various digital parenting tools \u2013 from pregnancy apps and baby monitors to parental controls and tracking devices \u2013 and practices\u00a0\u2013 such as sharenting\u00a0\u2013 are used in the chapter to illustrate how the issues related to the digital rights and privacy of the child are almost entirely discarded against the overprotective and technologically moderated parenting stance leading to both commodification as well as datafication of childhood.\n\n==\n\nK 2021:\n\nTitle: Digital Parenting\n\nAbstract: Abstract Parenting helps socialization in children. The inevitable presence of digital media has a major impact on parent\u2013child relationships and parenting skills. The increased use of digital devices, such as smartphones and tablets, by children forces the parents to change their parenting styles. Digital parenting refers to a process adopted by parents to monitor and regulate the activities of their young ones in handling digital devices. The main aim of this style is to protect their children from various online dangers. Parents need to use several approaches for regulating digital media behaviors of children, such as setting limits, practicing constructive discipline, spending time with their kids, and so forth. However, there are challenges associated with digital parenting. Some of the issues include unlimited access to Internet, parents\u2019 job, reverse socialization, and so on. Pediatric nurses can help parents in successfully managing digital parenting styles through education and advocacy as part of health promotion activities of pediatric clients both in hospital and community settings.\n\n==\n\nBenedetto 2020:\n\nTitle: Digital Parenting: Raising and Protecting Children in Media World\n\nAbstract: Digital media have quickly changed ways in which parents and children communicate, enjoy themselves, acquire information, and solve problems daily (both in ordinary and exceptional circumstances such as COVID-19 home confinement). Very young children are regular users of smartphones and tablet, so their early digital engagement poses new challenges to parent-child relationships and parental role. First, the chapter introduces the \u201cdigital parenting\u201d construct, moving through the literature from \u201ctraditional\u201d parenting styles to more recent studies on \u201cparental mediation,\u201d that is, the different behaviors parents adopt to regulate children\u2019s engagement with the Internet and digital media. Second, the chapter reviews empirical researches on different parental mediation practices (active or restrictive behaviors) and how they are adjusted according to the child\u2019s characteristics (age, digital competences, etc.) or parent\u2019s media competence and beliefs. Finally, from a bidirectional perspective of parent-child relationships, the chapter discusses the role of youths\u2019 social involvement, communication, self-disclosure, and digital skills on parent\u2019s beliefs and practices. Implications for parent education and prevention of risks for early and excessive exposure to digital technologies are discussed.\n\n==\n\nAdhe 2020:\n\nTitle: Digital Parenting Services: University Integrated With Society\n\nAbstract: This research aims to make advancement in the area of parenting by building a digital service. This service integrates parenting information in the surrounding society. Services in the form of websites that can be opened by the public and contain information. The method used in this research are research and development with ADDIE type. The samples consist of 50 participants. Trials are conducted with small trials as well as large trials onwards. The results of this study are carried out starting from the assessment of products and materials and then testing the service that was done. The feasibility assessment by material experts obtained a score of 93.18% with both criteria and categories worthy. Eligibility assessment by media experts obtained a score of 93.75% with good criteria and feasible categories. Assessment of the feasibility of small group trials received a total score of 53.61% and large group trials received a total score of 68.56%. The implication of this research shows that digital parenting services help the public understand the parenting that will be applied to children.\n\n==\n\nK\u00fc\u00e7\u00fckoba 2023:\n\nTitle: Digital parenting and digital childhood: Raising gifted children born into the digital age\n\nAbstract: In an age when technology is effective in many areas, it's unreasonable to keep kids away from it. Children born in the age of technology, who shared their photos against their will from birth, do not hesitate to use technological devices and have the curiosity and ability to use multiple digital media at once. They use digital media and devices as creative, productive, and problem-solving tools. They need direction. Parents and teachers need skills and attitudes to help children use technology positively. Digital parenting and media guidance as new parent roles reflect this perspective. The digital parenting perspective says it's not true to keep kids away from media and devices by focusing on the negatives. Instead of focusing on the negative effects of technology, he suggests teaching children how to use digital media and tools to solve daily problems. The digital parenting approach seems more stimulating for children and adults. We need children who understand digital tools better than us in today's world, where problems can't be solved without technology. The research focused on the positive effects of technology on children in terms of \"being the parent of gifted children born in the digital age\" It allows them to use this content rationally to create new content and share it with new social connections. This research may contribute to social sciences by shedding light on digitalization, gifted children, and related applications.\n\n==\n\nAseptianova 2022:\n\nTitle: Digital Parenting of Children and Adolescents in Digital Era\n\nAbstract: The family is the first environment for children and adolescents so that it has a major influence on the formation of behavior, character, morals, character and education of children. Parenting in the family is the effort of parents to foster and guide children both physically and spiritually from birth to adulthood. This study aims to analyze digital parenting systems for children and adults. The method used in this study is a qualitative and participatory descriptive method, increasing participation and encouraging interest in and commitment to wider participation, encouraging community skills, awareness and attitudes towards parenting and bullying in the digital era. The application of parenting style must be accompanied by several family functions, including biological functions, educational functions, religious functions, protection functions, socialization functions, affection functions, economic functions, creative functions and family status functions. In addition, the way of parenting in the family must be adjusted to the child's developmental stage. Related to the problem of parenting in the family, this is a big task for all parents because at this time there are many conflicts and a lack of sympathy and empathy for children in the association is caused by the parenting style in the family. Therefore, parents should adapt by adjusting their parenting style to the technological developments.\n\n==\n\nLivingstone 2020:\n\nTitle: Parenting for a Digital Future\n\nAbstract: In the decades it takes to bring up a child, parents face challenges that are both helped and hindered by the fact that they are living through a period of unprecedented digital innovation. Drawing on extensive research with parents both rich and poor, parenting toddlers to teenagers, this book reveals how digital technologies give parenting struggles a distinctive character, as parents determine how to forge new territory with little precedent, or support. It argues that, in late modernity, parents are both more burdened with responsibilities and yet increasingly charged with respecting and developing the agency of their child\u2014leaving much to be negotiated. The book charts how parents enact authority and values through digital technologies\u2014as \u201cscreen time,\u201d videogames, and social media become ways of both being together and of setting boundaries, with digital technologies introducing valued opportunities and new sources of risk. To light their way, parents comb through the hazy memories of their own childhoods and look toward hard-to-imagine futures. This results in deeply diverse parenting in the present, as parents move between embracing, resisting, or balancing the role of technology in their own and their children\u2019s lives. This book moves beyond the panicky headlines to offer a deeply researched exploration of what it means to parent in a period of significant social and technological change. Drawing on qualitative and quantitative research in the United Kingdom, the book offers conclusions and insights relevant to parents, policymakers, educators, and researchers everywhere.\n\n==\n\nLupton 2016:\n\nTitle: Parenting and Digital Media: From the Early Web to Contemporary Digital Society\n\nAbstract: Parents have accessed websites, online discussion forums and blogs for advice, information and support since the early days of the World Wide Web (which first became widely available to users in the mid-1990s). In this century, the advent of mobile media such as smartphones and tablet computers and Wi-Fi has allowed parents to access the web from almost any location. They can use social-media platforms and apps (software applications for mobile devices) as part of their parenting practices. These technologies have brought with them opportunities for parents to seek information and support and exchange details of their experiences with each other in a variety of ways. These practices contribute to datafication, that is, rendering details of people\u2019s lives into digital data formats (van Dijck 2014) - not only of parents themselves but also of their children. The possibilities that now exist not only for voluntary sharing of one\u2019s personal data with others, but also for data leakage and commercial exploitation of this information, are key differences between the early digital media that were available to parents and those that they currently use. \n \nIn this article, we review the literature in sociology and related social research addressing the ways in which digital media have been used for parenting-related purposes. We begin with the longer-established media of parenting websites, online discussion forums, blogs, email, mobile phones and message and video services and then move on to the newer technologies of social media and apps. This is followed by a section on data privacy and security issues. The concluding section summarises some major issues arising from the review and points to directions for further research.",
        "final_summary": "The papers suggest that digital parenting in the age of artificial intelligence presents both challenges and opportunities. Siibak 2019 highlights how the data-driven culture and technological tools have led to intimate surveillance and commodification of childhood. K 2021 emphasizes the need for parents to adopt digital parenting strategies to protect children from online dangers. Benedetto 2020 discusses parental mediation practices and the bidirectional influence of children's digital skills on parental beliefs and practices. Adhe 2020 focuses on the development of digital parenting services to assist parents in understanding and applying effective parenting techniques. K\u00fc\u00e7\u00fckoba 2023 argues for a positive approach to digital parenting, teaching children how to use technology for problem-solving and creativity. Aseptianova 2022 emphasizes the importance of adapting parenting styles to technological advancements. Livingstone 2020 explores the complex negotiation of authority and values in parenting within a digital context. Lupton 2016 reviews the evolution of digital media in parenting practices and highlights the challenges of data privacy and security. Overall, these papers highlight the need for parents to navigate the digital landscape and develop strategies to ensure the well-being and safety of their children in the era of artificial intelligence."
    },
    {
        "query": "what is the differential diagnosis for traumatic adult new midfoot collapse?",
        "paper_list_string": "Steiner 2018:\n\nTitle: Combined Subtalar and Naviculocuneiform Fusion for Treating Adult Acquired Flatfoot Deformity With Medial Arch Collapse at the Level of the Naviculocuneiform Joint\n\nAbstract: Background: A challenge in treating acquired flatfoot deformities is the collapse of the medial arch at the level of the naviculocuneiform (NC) joint. Triple fusions, being a treatment option, may lead to problems such as increased foot stiffness. We thus established a method that combines subtalar (ST) fusion with NC fusion while preserving the Chopart joint. We analyzed the radiographic correction, fusion rate, and patient satisfaction with this procedure. Methods: 34 feet in 31 patients (female, 23; male, 8; age 67 [45-81] years) were treated with a ST and NC joint fusion. In 15 cases, a medial sliding-osteotomy was additionally necessary to fully correct hindfoot valgus. The following radiographic parameters were measured on weightbearing radiographs preoperatively and at 2 years: talo\u2013first metatarsal angle, talocalcaneal angle, calcaneal pitch, talonavicular coverage angle and calcaneal offset. Fusion was radiologically confirmed. Results: All parameters, except the calcaneal pitch, showed a significant improvement. Fusion was observed after 1 year in all but 2 cases (94.1%). One nonunion each occurred at the ST and NC joint without needing any subsequent treatment. One patient developed avascular necrosis of the lateral talus with need for total ankle replacement after 1 year. All patients were satisfied with the obtained results. Conclusion: Our data suggest that a combined fusion of the ST and NC joint was effective and safe when treating adult acquired flatfoot with collapse of the medial arch at the level of the NC joint. Although the talonavicular joint was not fused, its subluxation was significantly reduced. Level of Evidence: Level IV, case series.\n\n==\n\nCheng 2012:\n\nTitle: A Rare Midfoot Injury Pattern: Navicular\u2014Cuneiform and Calcaneal\u2014Cuboid Fracture\u2014Dislocation\n\nAbstract: A rare midfoot injury pattern of navicular\u2014cuneiform and calcaneal\u2014cuboid fracture\u2014dislocation is presented with two typical cases and a systematic review of the literature. This injury usually occurs as a result of high-energy crushing trauma and most often causes plantarly directed dislocation of the midfoot. Initial diagnosis includes a thorough physical examination and adequate radiological imaging, comprising anteroposterior, oblique and lateral X-radiography and computed tomography with three-dimensional reconstruction. Care should be taken to determine any injury to adjacent midfoot joints, in particular the Lisfranc joint. Intrasurgical protection of soft tissue is essential. Open reduction using two parallel incisions with minifragment plate fixation is recommended in more comminuted injuries. The navicular\u2014cuneiform and calcaneal\u2014cuboid joints play important roles in maintaining the arch of the foot and in weight-bearing during locomotion. Without proper therapy in the immediate post-traumatic phase, the long-term results are generally unsatisfactory.\n\n==\n\nSammarco 2009:\n\nTitle: Midtarsal arthrodesis in the treatment of Charcot midfoot arthropathy.\n\nAbstract: BACKGROUND\nFracture-dislocation of the midfoot with collapse of the longitudinal arch is common in patients with neuropathic arthropathy of the foot. In this study, we describe a technique of midfoot arthrodesis with use of intramedullary axial screw fixation and review the results and complications following use of this technique.\n\n\nMETHODS\nA retrospective study of twenty-two patients who had undergone surgical reconstruction and arthrodesis to treat Charcot midfoot deformity was performed. Bone resection and/or osteotomy were required to reduce deformity. Axially placed intramedullary screws, inserted either antegrade or retrograde across the arthrodesis sites, were used to restore the longitudinal arch. Radiographic measurements were recorded preoperatively, immediately postoperatively, and at the time of the last follow-up and were analyzed in order to assess the amount and maintenance of correction.\n\n\nRESULTS\nPatients were evaluated clinically and radiographically at an average of fifty-two months. Complete osseous union was achieved in sixteen of the twenty-two patients, at an average of 5.8 months. There were five partial unions in which a single joint did not unite in an otherwise stable foot. There was one nonunion, with recurrence of deformity. All patients returned to an independent functional ambulatory status within 9.5 months. Weight-bearing radiographs showed the talar-first metatarsal angle, the talar declination angle, and the calcaneal-fifth metatarsal angle to have improved significantly and to have been corrected to nearly normal values by the surgery. All measurements remained significantly improved, as compared with the preoperative values, at the time of final follow-up. There were no recurrent dislocations. Three patients had a recurrent plantar ulcer at the metatarsophalangeal joint that required additional surgery. There were eight cases of hardware failure.\n\n\nCONCLUSIONS\nOpen reduction and arthrodesis with use of multiple axially placed intramedullary screws for the surgical correction of neuropathic midfoot collapse provides a reliable stable construct to achieve and maintain correction of the deformity.\n\n==\n\nAkra 2010:\n\nTitle: An unusual etiology for adult-acquired flatfoot.\n\nAbstract: Rupture of the tibialis posterior tendon is widely believed to result from trauma in the presence of preexisting degenerative changes. We report a case of adult-acquired flatfoot in an otherwise healthy 19-year-old man who sustained a fracture of the medial malleolus.\n\n==\n\nFerris 1995:\n\nTitle: Late reconstruction of the midfoot and tarsometatarsal region after trauma.\n\nAbstract: The management of painful arthritis and deformity after trauma to the midfoot starts with careful assessment by physical examination and appropriate investigation to identify the affected joints. Conservative treatment may be very effective and includes the use of NSAIDs, custom insoles with arch support, and a rocker-bottom sole with extended steel shank with or without a SACH heel. If this treatment fails, usually a year after the injury, then arthrodesis of all the symptomatic joints with restoration of the arch and alignment of the weight-bearing surface is the recommended treatment. The long-term results of these fusions may be compromised by the subsequent development of arthritis in adjacent joints.\n\n==\n\nLi 2016:\n\nTitle: Categorization and surgical management of posttraumatic midfoot malunion\n\nAbstract: Objective To assess a classification system for midfoot injury that was based on the characteristics of the foot malunion and to evaluate the suggested treatment strategies. Methods This retrospective review of data from patients with posttraumatic midfoot malunion categorized each foot deformity into one of three types based on the foot arch and then separated these categories into one of three subtypes based on the forefoot deformity. According to the types of malunion, fascio-cutaneous flap, osteotomy, joint arthrodesis or realignment was used to correct the deformity. Patients were assessed before surgery and at 12 and 24 months postoperation. Results Of the 24 patients identified, six had Lisfranc joint injuries, nine had Lisfranc joint complex injuries combined with cuboid compression fractures and nine had Lisfranc joint complex injuries combined with navicular fractures. Overall, eight patients presented with poor soft tissue and massive scar at the dorsal foot. Visual analogue scale and American Orthopaedic Foot and Ankle Society midfoot scores significantly improved over the 24-month study period. At the end of the study, 21 of 24 patients (87.5%) rated their functional outcome as excellent or good. Conclusion The classification of the midfoot malunion evaluated in this study may be helpful in the decision making process for surgical intervention.\n\n==\n\nClaus 2008:\n\nTitle: [Differential diagnosis of acquired clubfoot (author's transl)].\n\nAbstract: The article introduces a patient of 14 years of age in whom clubfeet as well as paraspasticity had developed from his sixth year of life. Instead of the previously suspected heredodegenerative disease, a lipoma was found in the distal spinal canal which had prevented ascension of the spinal cord and had thus produced the described phenomenon.\n\n==\n\nSalvi 2012:\n\nTitle: [An unusual case of post-traumatic adult complex foot deformity].\n\nAbstract: A peculiar clinical presentation of post-traumatic complex left foot fracture deformity is presented in this report as the result of a motorbike accident. Notwithstanding the significant deformity following forefoot fractures, the patient complained only of the recent onset of metatarsalgia. Of particular interest, is that this severe foot injury as well following deformity was overlooked, probably because patient had sustained head injury that was the main problem to treat due to life risk.",
        "final_summary": "The papers collectively provide insights into the differential diagnosis for traumatic adult new midfoot collapse. Steiner 2018 suggests that a combined subtalar and naviculocuneiform fusion can be effective in treating adult acquired flatfoot deformity with medial arch collapse at the level of the naviculocuneiform joint. Cheng 2012 highlights a rare midfoot injury pattern involving navicular-cuneiform and calcaneal-cuboid fracture-dislocation, which requires thorough physical examination and imaging for diagnosis. Sammarco 2009 discusses midtarsal arthrodesis as a treatment option for Charcot midfoot arthropathy, showing positive outcomes in terms of correction and maintenance of deformity. Akra 2010 presents a case of adult-acquired flatfoot resulting from a fracture of the medial malleolus, suggesting an unusual etiology for this condition. Ferris 1995 emphasizes the importance of conservative treatment for midfoot trauma, including the use of NSAIDs and custom insoles, with arthrodesis recommended if conservative measures fail. Li 2016 proposes a classification system for posttraumatic midfoot malunion, aiding in the selection of appropriate surgical interventions. Claus 2008 presents an unusual case of acquired clubfoot, highlighting the need for careful diagnosis and consideration of atypical presentations. Salvi 2012 presents a case of post-traumatic complex foot deformity, further emphasizing the diversity of presentations in midfoot collapse."
    },
    {
        "query": "The Relationship between Systems Thinking, Self-Leadership, and Clinical Reasoning of the Nursing Students",
        "paper_list_string": "Joo 2018:\n\nTitle: The Effects of Critical Thinking Disposition, Clinical Performance Ability and Self-Concept of Nursing Profession in Nursing Students on Self Leadership\n\nAbstract: This study is a descriptive research to examine the relationship between critical thinking disposition, clinical performance ability, and self-concept of nursing profession according to degree of self-leadership among nursing college students. The data were collected from 165 nursing students in 3rd & 4th grade using self-report questionnaire and analyzed by t-test, ANOVA, Pearson`s correlation coefficient, and Multiple Regression. Self-leadership according to general characteristics showed significant differences according to grade, academic achievement, motivation, and major satisfaction. Self-leadership was found to have a positive correlation with critical thinking disposition, clinical performance ability, and self-concept of nursing professional. Analysis of the effects of general characteristics affecting sief-leadership, critical thinking disposition, nursing proFessional selF-concept and clinical performance on selF-leadership showed that they were explained by 49.9%, critical thinking disposition was the most important factor. This suggests that nursing students' self-leadership can be improved effectively by promoting critical thinking disposition. Also in reality, there are many difficulties such as the problems in the system of the majors, but it is necessary to operate a continuous education program to strengthen the self-leadership for the nursing college students by seeking various methods.\n\n==\n\nPark 2015:\n\nTitle: Self-leadership, critical thinking disposition, satisfaction of clinical practice and clinical practice competency of nursing students\n\nAbstract: The purpose of this study was to examine the relationship among self-leadership, critical thinking disposition, satisfaction of clinical practice and clinical practice competency of nursing students. Participants were 199 baccalaureate nursing students (3rd and 4th grades) in 2 cities. The data was collected by questionnaires and were analyzed with the SPSS/Win 21.0 program, using descriptive statistics, Pearson`s correlation coefficient and multiple regression. Significant positive correlations were among self-leadership, critical thinking disposition, satisfaction of clinical practice and clinical practice competency. The regression model explained 30.4% of satisfaction of clinical practice. The significant predictors of satisfaction of clinical practice were clinical experience, satisfaction of major, self-leadership and critical thinking disposition. The regression model explained 23.7% of clinical practice competency. Health status, self-leadership and critical thinking disposition were factors influencing clinical practice competency. It should strengthen self-leadership and encourage critical thinking disposition to improve nursing students` satisfaction of clinical practice and clinical practice competency.\n\n==\n\nDong 2016:\n\nTitle: Influence of Professional Self-concept and Self-leadership on Clinical Competence in Nursing Students\n\nAbstract: Purpose:This study was done in order to identify nursing students\u2019 professional self-concept, self-leadership and clinical competence and to analyze the correlation among the variables and the factors influencing clinical competence.Methods: The research participants were 294 senior nursing students in the nursing departments of 3 universities located in Jeollabuk-do. The students had completed 3 semesters of clinical practice. Data were analyzed using descriptive statistics, t-test, ANOVA, Pearson correlation coefficient, and Multiple Regression. Results:Participants' scores for professional self-concept, self-leadership, clinical competence were 2.78\u00b10.36, 3.63\u00b10.47, 3.80\u00b10.40 respectively. Professional self-concept, self-leadership and clinical competence had positive correlations. Factors influencing nursing students\u2019 clinical competence included professional self-concepts in professional practice, self-expectations in self-leadership, constructive thinking, self-compensation and self-criticism in that order, and these variables explained 48% of the variance in clinical competence.Conclusion: Based on these results, it is important to develop and apply educational programs to increase professional self-concept and self-leadership in order to improve nursing students\u2019 clinical competence.\n\n==\n\nShin 2023:\n\nTitle: Influence of Critical Thinking Disposition and Empathy Ability on Self-Leadership of Nursing Students\n\nAbstract: Objectives The purpose of this study is to understand the relationship between nursing students\u2019 critical thinking disposition, empathy ability and self-leadership, and to determine the effect of nursing students' critical thinking disposition and empathy ability to self-leadership. \nMethods The subjects of the study were nursing students enrolled in the 3rd and 4th grades of four universities in C province. Data analysis was performed using descriptive statistics, t-test, ANOVA, Pearson correlation coefficients, and multiple linear regression analysis using the IBM SPSS 20.0 program. \nResults The findings showed differences in the degree of gender, age, clinical experience, religion, motive of entering nursing department, satisfaction of majors, satisfaction of communication skills, satisfaction of personal relationships and hopeful employment fields in the difference between the degree of critical thinking disposition, empathy ability and self-leadership of nursing students according to the general characteristics of the subject, but there was no difference in grade level and club activities. It was found that there was a positive correlation between self-leadership of nursing students, critical thinking disposition and empathy ability. As the result, the most influential factor to self-leadership of nursing student was critical thinking disposition followed by empathy ability, satisfaction of majors. The explanatory power of the model was 48.7%. \nConclusions This study is significant in that it proved the importance of not only critical thinking disposition but also the humanities literacy factor such as empathy ability as an influencing factor of self-leadership. An educational approach that can improve self-leadership combined with creativity is required, and administrative and financial support for the continuous development and cultivation of self-leadership in the clinical field will be required. However, the process of recognizing the need for self-leadership by nursing students should be preceded.\n\n==\n\nLee 2016:\n\nTitle: The Effects of Critical Thinking Disposition and Self-esteem to Self-leadership of Nursing Student\n\nAbstract: Objectives : The purpose of this study was to evaluate the effect of critical thinking disposition and self-esteem to self-leadership in nursing students. Methods : In this study, 273 data was collected from nursing students of K-university in South Korea from Sep. 2015 to Nov. 2015, and the collected data was analyzed using SPSS 22.0 Statistics Program. Results : The results showed that both of critical thinking disposition and self-efficacy according to school year are significant to self-esteem. It was found that critical thinking disposition, self-esteem and self-leadership has a positive correlation. Critical thinking disposition and self-esteem were predictors of self-leadership with 46% of influence. Conclusions : The nursing training program needs to be developed to promote critical thinking and self-esteem in the nursing curriculum, so that the nursing students can demonstrate the ability as a nursing leader in the various fields of clinical practice and health care.\n\n==\n\nCho 2011:\n\nTitle: Self-Leadership and Self-Concept in the Freshmen of Nursing College\n\nAbstract: Purpose: The purpose of this study was to identify the relationship between selfleadership and self-concept in nursing students. Methods: We used a descriptive correlation survey in a convenient sample of 818 subjects recently admitted into college in Seoul and the Kyungki province, Korea. The self-leadership and self-concept levels were measured using the self-leadership and self-concept scales, respectively. The subjects were given a self-report questionnaire. Collected data were analyzed by frequency, percentage, mean, t-test, ANOVA, scheffe-test and pearson correlation, using the SPSS software version 17.0. Result: Positive correlations were identified between the self-leadership and self-concept in the investigated nursing students. Conclusion: The results of this study indicate that it is important to provide opportunities for the nursing students to participate in the programs designed for self-leadership and self-concept development.\n\n==\n\nSeung 2017:\n\nTitle: Influence of Self\u2013Efficacy and Critical Thinking Disposition on Self-Leadership of Nursing Students\n\nAbstract: This study examined the influence of self-efficacy and critical thinking disposition on the self-leadership of nursing students. The research subjects were114 nursing students in D city and K province. The data werecollected from November 9 to November 27, 2015, and analyzed by an independent t-test, one-way ANOVA, Pearson's correlation, and Stepwise multiple regression using the IBM SPSS/Win 20.0 program. The results showed that self-efficacy was 3.73, critical thinking disposition was 3.73, and self-leadership was 3.73. The self-leadership has positive correlations with self-efficacy and critical thinking disposition. Self-efficacy(\u03b2=.277, p<.008), critical thinking disposition(\u03b2=.274, p=.006), grade(\u03b2=.273, p<.001), religion(\u03b2=.163, p=.023),motivation of nursing(\u03b2=-.155, p=.033) explained 45.6% of total variance of self-leadership of nursing students. The most significant predictor of self-leadership were self-efficacy, followed by critical thinking disposition. Therefore, it is necessary to develop aneducation program to improve the self-efficacy and critical thinking disposition of nursing students.\n\n==\n\nKuiper 2009:\n\nTitle: Promoting the Self-Regulation of Clinical Reasoning Skills in Nursing Students\n\nAbstract: Aim: The purpose of this paper is to describe the research surrounding the theories and models the authors united to describe the essential components of clinical reasoning in nursing practice education. The research was conducted with nursing students in health care settings through the application of teaching and learning strategies with the Self-Regulated Learning Model (SRL) and the Outcome-Present-State-Test (OPT) Model of Reflective Clinical Reasoning. Standardized nursing languages provided the content and clinical vocabulary for the clinical reasoning task. Materials and Methods: This descriptive study described the application of the OPT model of clinical reasoning, use of nursing language content, and reflective journals based on the SRL model with 66 undergraduate nursing students over an 8 month period of time. The study tested the idea that self-regulation of clinical reasoning skills can be developed using self-regulation theory and the OPT model. Results: This research supports a framework for effective teaching and learning methods to promote and document learner progress in mastering clinical reasoning skills. Self-regulated Learning strategies coupled with the OPT model suggest benefits of self-observation and self-monitoring during clinical reasoning activities, and pinpoints where guidance is needed for the development of cognitive and metacognitive awareness. Recommendations and Conclusions: Thinking and reasoning about the complexities of patient care needs requires attention to the content, processes and outcomes that make a nursing care difference. These principles and concepts are valuable to clinical decision making for nurses globally as they deal with local, regional, national and international health care issues.",
        "final_summary": "The collected papers suggest a positive correlation between self-leadership and critical thinking disposition, clinical performance ability, self-concept of the nursing profession, and clinical practice competency in nursing students (Joo 2018, Park 2015, Dong 2016, Shin 2023). Self-leadership is influenced by self-efficacy and critical thinking disposition (Shin 2023, Seung 2017). Shin (2023) further adds that empathy ability, satisfaction with major, and communication skills also impact self-leadership. The development of educational programs that enhance critical thinking, self-esteem, and self-leadership is recommended to improve nursing students' clinical competence and satisfaction with clinical practice (Park 2015, Lee 2016, Cho 2011). The application of self-regulated learning strategies and reflective clinical reasoning models can further promote the self-regulation of clinical reasoning skills in nursing students (Kuiper 2009)."
    },
    {
        "query": "0.54 g/L urea as nitrogen source for PHA production",
        "paper_list_string": "Arumugam 2019:\n\nTitle: Low-cost production of PHA using cashew apple (Anacardium occidentale L.) juice as potential substrate: optimization and characterization\n\nAbstract: Polyhydroxyalkanoates are polyesters of R-hydroxyalkonic acids, prominently used as bioplastics on grounds of their complete biodegradable and environment-friendly characteristics. There is an upsurge in need of an alternative low-cost, renewable carbon source for the production of PHA for enhanced economic and to exert a positive impact on the industries. In the present work, cashew apple juice (CAJ) was supplemented as a carbon source for Cupriavidus necator to produce PHA. (NH 4 ) 2 SO 4 , NH 4 Cl, NH 4 NO 3 and CO(NH 2 ) 2 , and NaNO 3 were tested and urea was found to be the best nitrogen source that supports optimal growth of the microorganism. The production process was then optimized using response surface methodology by incorporating the effects of total reducing sugar concentration, urea concentration, and inoculum size. Under optimized condition, the resulting PHA yield was found to be 15.78\u00a0g/L with total reducing sugar concentration of 50\u00a0g/L, inoculum size of 50\u00a0mL/L, and urea concentration of 3\u00a0g/L. FT-IR, NMR, TGA, and DSC analysis revealed the product to be a copolymer of hydroxybutyrate and hydroxyvalerate. Graphical abstract\n\n==\n\nStanley 2017:\n\nTitle: Fed-Batch Strategies for Production of PHA Using a Native Isolate of Halomonas venusta KT832796 Strain\n\nAbstract: In this study, polyhydroxyalkanoates (PHA) accumulation by Halomonas venusta KT832796, a moderate halophilic bacteria isolated from marine source was studied. Both nutritional requirements and process parameters for submerged cultivation of the organism in bioreactor have been standardized. From the shake flask studies, glucose and ammonium citrate as carbon and nitrogen source produced maximum PHA at a ratio 20 with 3.52\u00a0g/L of dry cell weight and 70.56% of PHA content. However, ammonium sulfate as the nitrogen source was found to be more suitable for fed-batch cultivation. Several feeding strategies including pH-based fed-batch and variants of pulse feeding were studied to improve the PHA levels. pH-based feeding, although improved PHA level to 26\u00a0g/L, most of the carbon flux was diverted towards biomass formation; hence, the percent PHA was only 39.15% of the dry cell weight. Maximum PHA of 33.4\u00a0g/L, which corresponded to 88.12% of the dry cell, was obtained from high concentration single pulse method. There was a net 8.65-fold increase in PHA using this feeding strategy when compared to batch studies. According to our knowledge, this is the highest amount of PHA reported for a Halomonas venusta strain.\n\n==\n\nLakshman 2004:\n\nTitle: Simultaneous and comparative assessment of parent and mutant strain of Rhizobium meliloti for nutrient limitation and enhanced polyhydroxyalkanoate (PHA) production using optimization studies\n\nAbstract: Abstract Nutrient limitation conditions, optimization and comparison of polyhydroxyalkanoate (PHA) yields and biomass production by parent and mutant strains of Rhizobium meliloti were investigated. Complex interactions among concentrations of sucrose (5\u201355\u00a0g/l), urea (0.05\u20130.65\u00a0g/l) inoculum (10\u2013250\u00a0ml/l) and K2HPO4 (0.5\u20132\u00a0g/l), were studied using central composite rotatable design (CCRD) experiments. Phosphate-limiting medium (0.33\u00a0g K2HPO4/l) in the presence of excess carbon (sucrose 42.5\u00a0g/l) results in more production of PHA (2.2\u00a0g/l) in the parent strain. In comparison, the mutant strain required moderate levels of sucrose (30\u00a0g/l), along with excess of phosphate (1\u00a0g/l) for high PHA content of cell biomass (80%) and PHA yield (3.3\u00a0g/l). Optimised PHA production (biomass 4.8\u00a0g/l and PHA 3.09\u00a0g/l) by the parent strain occurred at: sucrose 51.58\u00a0g/l, urea 0.65\u00a0g/l, K2HPO4 0.48\u00a0g/l and inoculum 10\u00a0ml/l. In the mutant strain, higher yields of biomass (9.05\u00a0g/l) and PHA (5.66\u00a0g/l) were obtained in Optimised medium containing: sucrose 55\u00a0g/l, urea 0.65\u00a0g/l, K2HPO4 1.0\u00a0g/l and inoculum 150.58\u00a0ml/l.\n\n==\n\nKoller 2005:\n\nTitle: Production of polyhydroxyalkanoates from agricultural waste and surplus materials.\n\nAbstract: To be competitive with common plastics, the production costs of polyhydroxyalkanoates (PHAs) have to be minimized. Biotechnological polymer production occurs in aerobic processes; therefore, only about 50% of the main carbon sources and even a lower percentage of the precursors used for production of co-polyesters end up in the products wanted. A second cost factor in normally phosphate-limited production processes for PHAs is the costs for complex nitrogen sources. Both cheap carbon sources and cheap nitrogen sources are available from agricultural waste and surplus materials and make a substantial contribution for minimizing PHA production costs. In this study, fermentations for PHA production were carried out in laboratory-scale bioreactors on hydrolyzed whey permeate and glycerol liquid phase from the biodiesel production using a highly osmophilic organism. Without any precursor, the organism produced a poly[3(hydroxybutyrate-co-hydroxyvalerate)] copolyester on both carbon sources. During the accumulation phases, a constant 3-hydroxyvalerate content of 8-10% was obtained at a total PHA concentration of 5.5 g/L (on hydrolyzed whey permeate) and 16.2 g/L (glycerol liquid phase). In an additional fermentation, an expensive nitrogen source was substituted by meat and bone meal beside the glycerol liquid phase as a carbon source, resulting in a final PHA concentration of 5.9 g/L.\n\n==\n\nRay 2016:\n\nTitle: Optimization and characterization of PHA from isolate Pannonibacter phragmitetus ERC8 using glycerol waste.\n\nAbstract: Polyhydroxyalkanoates (PHAs) have been considered as a good alternative for petrochemical based polymers due to its biodegradability. However, a high production cost limits their acceptance in industries. In present work, efforts have been made to optimize the production of PHA by Pannonibacter phragmitetus ERC8 using glycerol waste as a sole carbon source, with enhanced polymer production in a cost effective way. To check the possibility of growth and polymer accumulation potential of P. phragmitetus ERC8, various low cost substrates such as food waste, mutton tallow, whey, sugarcane bagasse, corn steep liquor and glycerol waste were used. Optimum concentration of selected factors obtained as response of statistical experimental design were 0.8% (v/v) glycerol waste, 0.26% (w/v) BHM and 1.25%OD as an inoculum for the maximum PHA production. The suggested model was validated and maximum 1.36 g/L of PHA production was obtained after 96 h. PHA production of 1.87 g/L was achieved in 5L (working volume 3 L) lab scale bioreactor with the suggested media components by RSM (Response Surface Methodology). Characterization of the PHA by NMR spectroscopy revealed that the polymer was a hetromonomer of (R)-3-hydroxybutyrate and medium chain length 3HA[(R)-3-hydroxyalkanoate] monomers.\n\n==\n\nWen 2010:\n\nTitle: Effects of phosphorus and nitrogen limitation on PHA production in activated sludge.\n\nAbstract: The effects of phosphorus and nitrogen limitation on polyhydroxyalkanoate (PHA) production and accumulation by activated sludge biomass with acetate as a carbon source were investigated. Pre-selected influent carbon-phosphorus (C:P, W/W) of 100, 160, 250, 500 and 750, and carbon-nitrogen (C:N, W/W) of 20, 60, 100, 125 and 180 were applied in the phosphorus limitation experiments and the nitrogen limitation experiments, respectively. The maximum PHA accumulation up to 59% of the cell dry weight with a PHA productivity of 1.61 mg PHA/mg COD consumed was observed at the C:N 125 in the nitrogen limitation experiment. This value was much higher than that obtained in previous studies with a normal substrate feeding. The study showed that activated sludge biomass would produce more polyhydroxybutyrate than polyhydroxyvalerate under the stress of nutrient limitation, especially under phosphorus limitation conditions. The experimental result also indicated that both phosphorus and nitrogen limitation may cause sludge bulking.\n\n==\n\nGowda 2014:\n\nTitle: Agrowaste-based Polyhydroxyalkanoate (PHA) production using hydrolytic potential of Bacillus thuringiensis IAM 12077\n\nAbstract: The study identified the innate enzymatic potential (amylase) of the PHB producing strain B.thuringiensis IAM 12077 and explored the same for cost-effective production of PHB using agrowastes, eliminating the need for pretreatment (acid hydrolysis and/or commercial enzyme). Comparative polyhydroxyalkanoate (PHA) production by B. thuringiensis IAM 12077 in biphasic growth conditions using glucose and starch showed\u00a0 appreciable levels of growth (5.7 and 6.8 g/L) and PHA production (58.5 and 41.5%) with a PHA yield of 3.3 and 2.8 g/L, respectively. Nitrogen deficiency supported maximum PHA yield (2.46 g/L) and accumulation (53.3%). Maximum growth (3.6 g/L), PHB yield (2.6 g/L) and PHA accumulation (72.8%) was obtained with C:N ratio of 8:1 using starch as the carbon source (10 g/L). Nine substrates (agro and food wastes) viz. rice husk, wheat bran, ragi husk, jowar husk, jackfruit seed powder, mango peel, potato peel, bagasse and straw were subjected to two treatments- acid hydrolysis and hydrolysis by innate enzymes, and\u00a0 the reducing sugars released thereby were utilized for polymer production. All the substrates tested supported comparable PHB production with acid hydrolysis (0.96 g/L-8.03 g/L) and enzyme hydrolysis (0.96 g/L -5.16 g/L). Mango peel yielded the highest PHB (4.03 g/L; 51.3%), followed by\u00a0 jackfruit seed powder (3.93 g/L; 29.32%). Varied levels of amylase activity (0.25U-10U) in all the substrates suggested the enzymatic hydrolysis of\u00a0 agrowastes.\n\n==\n\nZhang 2018:\n\nTitle: Polyhydroxyalkanoates (PHA) production from phenol in an acclimated consortium: Batch study and impacts of operational conditions.\n\nAbstract: Microbial intracellular biopolymer PHA was synthesized from toxic pollutant phenol by an acclimated consortium. Various operational conditions were experimented for their effects on biomass growth and PHA accumulation. Carbon to nitrogen ratios from 5 to 40 (w/w) showed little impact, as did the levels of Fe, Ca and Mg in a short term. Acidic pH inhibited both growth and PHA synthesis, and an optimal dissolved oxygen level of 1-4\u202fmg\u202fL-1 was identified. Low temperature (7\u202f\u00b0C) significantly slowed but did not totally repress microbial activities. A 2% NaCl shock retarded reactions and 4% NaCl caused irreversible damage. Various initial phenol (S0) and biomass concentrations (X0) were combined to study the effect of food to microbe (F/M) ratio. High S0 and F/M exerted toxicity, reducing reaction rates but generating higher ultimate PHA wt% in biomass. Increasing X0 alleviated phenol inhibition and improved productivity and carbon conversion from phenol. A pseudo-optimized F/M ratio of 0.2-0.4 and a maximum PHA% rate of 1.15%\u202fmin-1 were identified under medium S0/high X0. This study is the first to systematically investigate the feasibility of toxic industrial waste as the carbon source for PHA production, and likely the only one indicating potential for scaling-up and industrialization.",
        "final_summary": "Arumugam (2019) found that urea was the best nitrogen source for optimal growth of Cupriavidus necator. Lakshman (2004) reported that urea, along with other optimized conditions, led to higher PHA yields in both parent and mutant strains of Rhizobium meliloti. However, Stanley (2017) found that ammonium sulfate, not urea, was a more suitable nitrogen source for PHA production in Halomonas venusta KT832796."
    },
    {
        "query": "why study purchase intention study",
        "paper_list_string": "Bebber 2017:\n\nTitle: Antecedents of Purchase Intention in the Online Context\n\nAbstract: ABSTRACT The understanding of the determinant factors of customer purchase intention is necessary, and it is equally important to study the online purchase context, since this context is disseminated among customers. A theoretical model has been elaborated on and tested, considering the constructs of information quality, distrust, and perceived risk as antecedents of purchase intention, and aiming to analyze the relationship among these constructs in the online purchase context. A quantitative research study has been performed by means of the application of a survey. Multivariate statistics techniques have been applied for data analyses, including structural equation modeling. This study contributes to the evolution of the empirically tested concepts by providing a greater individual understanding of each construct presented in the theoretical model, as well as the relationship among them as determinants of purchase intention; the indication is that meaningful relationships were found which may impact greater profitability and, consequently, greater competition for online retailers.\n\n==\n\nToldos-Romero 2015:\n\nTitle: Brand personality and purchase intention\n\nAbstract: Purpose \u2013 The purpose of this paper is to analyze the effects of brand personality dimensions on purchase intention. Furthermore, the brand personality dimensions are compared to study the differences between users and non-users of 12 brands. Design/methodology/approach \u2013 An estimated 400 undergraduate students participated. They were given a questionnaire divided into two sessions (six brands of think products in one session and six brands of feel products in another session). In the end, 313 participants completed the questionnaire on the six brands of think products, and 320 completed the questionnaire on the six brands of feel products. Findings \u2013 Multiple regression analysis revealed that Hipness/Vivacity, Success, Sincerity and Sophistication brand personality dimensions are significant predictors of purchase intention. In addition, Domesticity/Emotionality and Professionalism also explain purchase intention but with a negative weight. The results are also broken down into product categories. Compar...\n\n==\n\nYounus 2015:\n\nTitle: Identifying the Factors Affecting Customer Purchase Intention\n\nAbstract: In the worst competitive market the consumer products manufacturing industries pay attention on customer purchase intention for maintain their repute in market and enhanced their goodwill. Because loyal customer are good source for create revenue. This study learns and contributes the factors that affect customer purchase intention. The purpose of this study is to observe the effect of independent variable (customer knowledge ,purchase intention, celebrity endorsement and perceived value ) on dependent variable (purchase intention).The study describe that the relation between dependent variable have significant relationship with purchase intention. This is quantitative study and sample size of this study is 100. And 100 questionnaires were used for collection of data. The results of this study shows that perceived value, customer knowledge, celebrity endorsement have significant relationship with purchase intention.\n\n==\n\nApdillah 2022:\n\nTitle: WORD OF MOUTH RELATIONSHIP ANALYSIS ON PURCHASE INTENTION AND ITS IMPACT ON PURCHASE DECISIONS\n\nAbstract: This study aims to analyze the relationship of Word of Mouth in Purchase Intention toward Purchase Decision. This study is a quantitative study using SEM PLS. The sample in this study was 100 respondents to consumers of Alfamart Kramat Pulo 2 with a non-probability sampling technique, namely a sampling technique that provides equal opportunities for each element (member) of the population to be selected as a member of the sample. The data the analyzed using validity test and hypothesis test to obtain the result. The finding reveals that by empirical analysis, word of mouth has a statistically significant positive impact on purchasing interest, such that as word of mouth improves, so does purchasing intention. Meanwhile, in empirical evidence, purchasing intention has a positive and significant impact on purchasing decisions, such that if the purchasing interest in a product becomes increasingly attached to the minds of customers, the likelihood of making a purchase increases. Furthermore, according to empirical research, word of mouth has a positive and statistically significant impact on purchasing decisions.\n\n==\n\nWang 2013:\n\nTitle: Understanding the purchase intention towards remanufactured product in closed-loop supply chains: An empirical study in China\n\nAbstract: Purpose \u2013 The paper aims to explore the reasons underlying the key assumption in the closed-loop supply chain (CLSC) literature that consumers' purchase intention is lower for remanufactured products than for new products. It aims to complement the predominantly operation-focused CLSC research by examining consumers' perception of and behavior relating to remanufactured products. Design/methodology/approach \u2013 A theoretical model is developed by integrating the concepts of perceived benefits and product knowledge with the theory of planned behavior and the theory of perceived risk. Then the model is examined through an empirical study in the Chinese automobile spare parts industry involving 288 respondents and using structural equation modeling. Findings \u2013 The results indicate that purchase intention is directly influenced by purchase attitude followed by perceived behavioral control and indirectly influenced by perceived risk, perceived benefit and product knowledge via attitude. Therefore, effective meas...\n\n==\n\nTakaya 2019:\n\nTitle: Antecedents Analysis of Purchase Intention\n\nAbstract: The advancement of technology has resulted in the creation of a new form of shopping transactions. This technology is used by residents to shop online. Thus, customers\u2019 involvements in online purchasing have become an important trend.\u00a0 The objective of this research was to identify the determinants of customer purchases online. This study used a surveymethod using questionnaires and the target is an online customer in Central Jakarta.This research used simple regression to determine the effect of purchace intention to factors that influence it. Data questionnaire distributed directly to the respondents who never buy online shopping.\u00a0 Findings revealed that impulse purchase intention, quality orientation, brand orientation, online trust and prior online purchase experience were positively related to the customer online purchase intention.\n\n==\n\nDelafrooz 2011:\n\nTitle: Understanding consumers internet purchase intention in Malaysia\n\nAbstract: This study aims to explore the antecedents relating to the extent of both the attitude and the purchasing intention of online shopping. It examined the factors influencing consumers\u2019 attitude toward online shopping and shopping intention from the Malaysian perspectives. From an e-commerce perspective, the understanding of the Theory of Reasoned Action (TRA), Theory of Planned Behavior (TPB), and Technology Acceptance Model (TAM) could provide a valid basis in explaining and predicting consumers\u2019 intention towards adopting an online shopping behavior. A purposive sampling method was used and the sample comprised of 370 students studying at a public University in Malaysia. Data were collected via self-administered questionnaire. The data were examined using frequency and path analysis. Result of path analysis showed that trust and attitude had stronger direct effect on online shopping intention, whereas utilitarian orientation, convenience, prices wider selection, and income had stronger indirect effect on online shopping intention through the attitude towards online shopping as mediation. This paper outlined the key online shopping intention and events in Malaysia and pioneered the building of an integrated research framework to understand how consumers form their attitude and make purchase intentions toward online shopping. \n \n \u00a0 \n \n Key words:\u00a0Intention, attitude, online shopping, consumer, Malaysia.\n\n==\n\nMorwitz 2006:\n\nTitle: When Do Purchase Intentions Predict Sales?\n\nAbstract: Marketing managers routinely use purchase intentions to predict sales. The purpose of this paper is to identify factors associated with an increased or decreased correlation between purchase intentions and actual purchasing. In two studies, we examine data collected from a wide range of different settings that reflect the real world diversity in how intentions studies are conducted. The results indicate that intentions are more correlated with purchase: 1) for existing products than for new ones, 2) for durable goods than for non-durable goods, 3) for short than for long time horizons, 4) when respondents are asked to provide intentions to purchase specific brands or models than when they are asked to provide intentions to buy at the product category level, 5) when purchase is measured in terms of trial rates than when it is measured in terms of total market sales, and 6) when purchase intentions are collected in a comparative mode than when they are collected monadically.",
        "final_summary": "The papers suggest several factors that influence purchase intention. Bebber 2017 found that information quality, distrust, and perceived risk are antecedents of purchase intention in the online context. Toldos-Romero 2015 identified brand personality dimensions such as Hipness/Vivacity, Success, Sincerity, and Sophistication as predictors of purchase intention. Younus 2015 highlighted the significant relationship between perceived value, customer knowledge, celebrity endorsement, and purchase intention. Apdillah 2022 demonstrated the positive impact of word of mouth on purchase intention, which in turn affects purchase decisions. Wang 2013 explored the influence of perceived benefits, product knowledge, and perceived risk on purchase intention for remanufactured products in closed-loop supply chains. Takaya 2019 identified impulse purchase intention, quality orientation, brand orientation, online trust, and prior online purchase experience as determinants of customer purchase intention. Delafrooz 2011 found that trust, attitude, utilitarian orientation, convenience, prices, wider selection, and income influence online shopping intention. Morwitz 2006 highlighted factors that increase the correlation between purchase intentions and actual purchasing, including existing products, durable goods, short time horizons, specific brand intentions, trial rates, and comparative data collection."
    },
    {
        "query": "Aim. The objective of our study has been to evaluate the WHO-5 as a new early screening instrument for apathy in a group of elderly persons. Methods. The WHO-5 was compared to the Geriatric Depression Scale (GDS-15). The GDS contains five items measuring well-being and ten items measuring depression. The internal validity of the WHO-5 (total score being a sufficient statistic) was evaluated with both parametric and nonparametric item response theory models. The external validity of the WHO-5 and the GDS was evaluated by ROC using depression as index of validity. Results. The item response theory analyses confirmed that the total score of the WHO-5 is a sufficient statistic. The ROC analysis shows an adequate sensitivity (61%) and specificity (84%). The GDS15 and its two subscales obtained low sensitivity (25\u201342%), but high specificity (90\u201398%). Conclusion. The WHO-5 was found both internally and externally valid when considering decreased positive well-being to be an early indication of apathy reflecting that the wind has begun to be taken out of the \u201cmotivation sail.\u201d",
        "paper_list_string": "Lucas-Carrasco 2012:\n\nTitle: The Validity of the WHO-5 as an Early Screening for Apathy in an Elderly Population\n\nAbstract: Aim. The objective of our study has been to evaluate the WHO-5 as a new early screening instrument for apathy in a group of elderly persons. Methods. The WHO-5 was compared to the Geriatric Depression Scale (GDS-15). The GDS contains five items measuring well-being and ten items measuring depression. The internal validity of the WHO-5 (total score being a sufficient statistic) was evaluated with both parametric and nonparametric item response theory models. The external validity of the WHO-5 and the GDS was evaluated by ROC using depression as index of validity. Results. The item response theory analyses confirmed that the total score of the WHO-5 is a sufficient statistic. The ROC analysis shows an adequate sensitivity (61%) and specificity (84%). The GDS15 and its two subscales obtained low sensitivity (25\u201342%), but high specificity (90\u201398%). Conclusion. The WHO-5 was found both internally and externally valid when considering decreased positive well-being to be an early indication of apathy reflecting that the wind has begun to be taken out of the \u201cmotivation sail.\u201d\n\n==\n\nHeun 1999:\n\nTitle: Internal and external validity of the WHO Well\u2010Being Scale in the elderly general population\n\nAbstract: The objectives of this study were (i) to evaluate the validity of the WHO Well\u2010Being Scale in elderly subjects and (ii) to assess the influence of demographic variables on subjective quality of life. A sample of 254 elderly subjects completed the 22\u2010item WHO Well\u2010Being Scale. The scale had an adequate internal and external validity. However, the short 10\u2010item and 5\u2010item versions were equally valid. Low scores indicating decreased well\u2010being were related to the presence of a psychiatric disorder or, independently, to poor living conditions. The Well\u2010Being Scale and their short versions would appear to be useful instruments for identifying subjects with reduced subjective quality of life.\n\n==\n\nBertens 2017:\n\nTitle: Validity of the three apathy items of the Geriatric Depression Scale (GDS\u20103A) in measuring apathy in older persons\n\nAbstract: The Geriatric Depression Scale (GDS)\u20103A, a three\u2010item subset of the GDS\u201015, is increasingly used as a measure for apathy in research settings to assess factors associating with this neuropsychiatric syndrome. We aimed to assess how accurately the GDS\u20103A discriminates between presence and absence of apathy in two populations of community\u2010dwelling older persons, using the Apathy Scale as reference standard.\n\n==\n\nAllgaier 2013:\n\nTitle: Beside the Geriatric Depression Scale: the WHO\u2010Five Well\u2010being Index as a valid screening tool for depression in nursing homes\n\nAbstract: The aim of the study was to compare criterion validities of the WHO\u2010Five Well\u2010being Index (WHO\u20105) and the Geriatric Depression Scale 15\u2010item version (GDS\u201015) and 4\u2010item version (GDS\u20104) as screening instruments for depression in nursing home residents.\n\n==\n\nHeun 2009:\n\nTitle: Validity of the five-item WHO Well-Being Index (WHO-5) in an elderly population\n\nAbstract: Background Depression has a high prevalence in the elderly population; however it often remains undetected. The WHO 5-item Well-Being Index (WHO-5) is a short screening instrument for the detection of depression in the general population, which has not yet been evaluated. The goals of the present study were: 1) to assess the internal and external validity of WHO-5 and 2) to compare the two recent versions of WHO-5.Study population and methods 367 subjects above 50 years of age were examined with the WHO-5. ICD-10 diagnoses were made using a structured interview (CIDI). The internal validity of the well-being index was evaluated by calculating Loevinger\u2019s and Mokken\u2019s homogeneity coefficients. External validity for detection of depression was evaluated by ROC analysis.Results The scale was sufficiently homogeneous (Loevinger\u2019s coefficient: version 1=0.38, version 2=0.47; Mokken coefficient \u03c4; 0.3 in nearly all items). ROC analysis showed that both versions adequately detected depression. Version 1 additionally detected anxiety disorders, version 2 being more specific for detection of depression.Conclusion The WHO-5 showed a good internal and external validity. The second version is a stronger scale and was more specific for the detection of depression. The WHO-5 is an useful instrument for identifying elderly subjects with depression.\n\n==\n\nYesavage 1982:\n\nTitle: Development and validation of a geriatric depression screening scale: a preliminary report.\n\nAbstract: A new Geriatric Depression Scale (GDS) designed specifically for rating depression in the elderly was tested for reliability and validity and compared with the Hamilton Rating Scale for Depression (HRS-D) and the Zung Self-Rating Depression Scale (SDS). In constructing the GDS a 100-item questionnaire was administered to normal and severely depressed subjects. The 30 questions most highly correlated with the total scores were then selected and readministered to new groups of elderly subjects. These subjects were classified as normal, mildly depressed or severely depressed on the basis of Research Diagnostic Criteria (RDC) for depression. The GDS, HRS-D and SDS were all found to be internally consistent measures, and each of the scales was correlated with the subject's number of RDC symptoms. However, the GDS and the HRS-D were significantly better correlated with RDC symptoms than was the SDS. The authors suggest that the GDS represents a reliable and valid self-rating depression screening scale for elderly populations.\n\n==\n\nHoyl 2000:\n\nTitle: [Depression in the aged: preliminary evaluation of the effectiveness, as an screening instrument, of the 5-item version of the Geriatric Depression Scale].\n\nAbstract: INTRODUCTION\nThe best approach to improve under-recognition of depression is routine screening, ideally using an instrument that is highly effective and easy to administer.\n\n\nOBJECTIVE\nTo test the effectiveness of the 5-item version of the Geriatric Depression Scale (5-GDS) for depression screening in a community-dwelling Chilean elderly population.\n\n\nPARTICIPANTS AND METHODS\n110 subjects were evaluated at the geriatric outpatient clinic of a university teaching hospital. Patients answered a questionnaire that included the Geriatric Depression Scale (GDS), and demographic information. Using the 15-item GDS score as reference standard to classify subjects as depressed, test characteristics of the 5-GDS were evaluated.\n\n\nRESULTS\nSubjects had a mean 15-item GDS score of 5.4; 47% classified as depressed. Depressed and not depressed subjects were similar with regard to demographics, educational level and comorbid conditions. The mean score was 1.9 for the 5-item GDS. Pearson correlation for 15-item and 5-item GDS scores was 0.92, p < 0.001. Using 15-item GDS score as reference standard, the 5-item GDS had a sensitivity of 0.88, specificity 0.90, positive predictive value 0.88 and negative predictive value 0.90.\n\n\nCONCLUSIONS\nThe 5-item GDS seems to be a promising screening tool for depression. If revalidated against clinical evaluation, it might be the preferred screening tool for depression in the Chilean community-dwelling elderly.\n\n==\n\nHalliday 2017:\n\nTitle: Validation of the WHO-5 as a first-step screening instrument for depression in adults with diabetes: Results from Diabetes MILES - Australia.\n\nAbstract: AIMS\nScreening for depression is recommended internationally. The World Health Organization's 5-item Well-being Index (WHO-5) is used clinically to screen for depression but its empirical suitability for this purpose is not well documented. We investigated the psychometric properties of the WHO-5 and its suitability for identifying likely depression in Australian adults with diabetes.\n\n\nMETHODS\nThe Diabetes MILES - Australia study dataset provided a sample of N=3249 who completed the WHO-5 (positively-worded 5-item measure of emotional well-being) and the PHQ-9 (9-item measure of depressive symptoms). Analyses were conducted for the full sample, and separately by diabetes type and treatment (type 1, non-insulin-treated type 2, and insulin-treated type 2 diabetes). Construct (convergent and factorial) validity and reliability of the WHO-5 were examined. ROC analyses were used to examine the sensitivity and specificity of the WHO-5 as a depression screening instrument, comparing two commonly used WHO-5 cut-off values (\u22647 and <13) with the PHQ-9.\n\n\nRESULTS\nFor the whole sample, the WHO-5 demonstrated satisfactory internal consistency reliability (\u03b1=0.90) and convergent validity with the PHQ-9 (r=-0.73, p<0.001). Confirmatory factor analysis partially supported factorial validity: \u03a72(5)=834.94, p<0.001; RMSEA=0.23, 90% CI 0.21-0.24; CFI=0.98, TLI=0.96; factor loadings=0.78-0.92. The AUC was 0.87 (95% CI: 0.86-0.89, p<0.001). The sensitivity/specificity of the WHO-5 for detecting likely depression was 0.44/0.96 for the \u22647 cut-off, and 0.79/0.79 for the <13 cut-off, with similar findings by diabetes type and treatment.\n\n\nCONCLUSIONS\nThese findings support use of a WHO-5 cut-point of <13 to identify likely depression in Australian adults with diabetes, regardless of type/treatment.",
        "final_summary": "The WHO-5 is a valid screening instrument for apathy in elderly individuals, as supported by Lucas-Carrasco 2012, who found both internal and external validity, with adequate sensitivity (61%) and specificity (84%) in detecting decreased positive well-being as an early indication of apathy. This was further corroborated by Heun 1999, who also affirmed the internal and external validity of the WHO-5. Bertens 2017 evaluated the GDS-3A, a subset of the GDS-15, for its accuracy in measuring apathy. Allgaier 2013 compared the WHO-5 and GDS-15 as screening tools, while Heun 2009 found the second version of WHO-5 to be more specific for detecting depression. Yesavage 1982 developed and validated the GDS, which correlated well with symptoms of depression. Hoyl 2000 evaluated the effectiveness of the 5-item version of the GDS as a depression screening tool. Halliday 2017, while examining the psychometric properties of the WHO-5 in adults with diabetes, did not provide specific findings regarding its suitability for identifying likely depression."
    },
    {
        "query": "recent experiments using the highly selective 5-HT2A receptor antagonist.",
        "paper_list_string": "Knight 2004:\n\nTitle: Pharmacological characterisation of the agonist radioligand binding site of 5-HT2A, 5-HT2B and 5-HT2C receptors\n\nAbstract: In the present study we compared the affinity of various drugs for the high affinity \u201cagonist-preferring\u201d binding site of human recombinant 5-HT2A, 5-HT2B and 5-HT2C receptors stably expressed in monoclonal mammalian cell lines. To ensure that the \u201cagonist-preferring\u201d conformation of the receptor was preferentially labelled in competition binding experiments, saturation analysis was conducted using antagonist and agonist radiolabels at each receptor. Antagonist radiolabels ([3H]-ketanserin for 5-HT2A receptor and [3H]-mesulergine for 5-HT2B and 5-HT2C receptor) bound to a larger population of receptors in each preparation than the corresponding agonist radiolabel ([125I]-DOI for 5-HT2A receptor binding and [3H]-5-HT for 5-HT2B and 5-HT2C receptor binding). Competition experiments were subsequently conducted against appropriate concentrations of the agonist radiolabels bound to the \u201cagonist-preferring\u201d subset of receptors in each preparation. These studies confirmed that there are a number of highly selective antagonists available to investigate 5-HT2 receptor subtype function (for example, MDL 100907, RS-127445 and RS-102221 for 5-HT2A, 5-HT2B and 5-HT2C receptors respectively). There remains, however, a lack of highly selective agonists. (\u2212)DOI is potent and moderately selective for 5-HT2A receptors, BW723C86 has poor selectivity for human 5-HT2B receptors, while Org 37684 and VER-3323 display some selectivity for the 5-HT2C receptor. We report for the first time in a single study, the selectivity of numerous serotonergic drugs for 5-HT2 receptors from the same species, in mammalian cell lines and using, exclusively, agonist radiolabels. The results indicate the importance of defining the selectivity of pharmacological tools, which may have been over-estimated in the past, and highlights the need to find more selective agonists to investigate 5-HT2 receptor pharmacology.\n\n==\n\nSoto 2017:\n\nTitle: Novel Bivalent 5-HT2A Receptor Antagonists Exhibit High Affinity and Potency in Vitro and Efficacy in Vivo.\n\nAbstract: The 5-HT2A receptor (5-HT2AR) plays an important role in various neuropsychiatric disorders, including substance use disorder and schizophrenia. Homodimerization of this receptor has been suggested, but tools that allow direct assessment of the relevance of the 5-HT2AR:5-HT2AR homodimer in these disorders are necessary. We chemically modified the selective 5-HT2AR antagonist M100907 to synthesize a series of homobivalent ligands connected by ethylene glycol linkers of varying lengths that may be useful tools for probing 5-HT2AR:5-HT2AR homodimer function. We tested these molecules for 5-HT2AR antagonist activity in a cell line stably expressing the functional 5-HT2AR and quantified a downstream signaling target, activation (phosphorylation) of extracellular regulated kinases 1/2 (ERK1/2), in comparison to in vivo efficacy of altering spontaneous or cocaine-evoked locomotor activity in rats. All of the synthetic compounds inhibited 5-HT-mediated phosphorylation of ERK1/2 in the cellular signaling assay; the potency of the bivalent ligands varied as a function of linker length, with the intermediate linker lengths being the most potent. The Ki values for the binding of bivalent ligands to 5-HT2AR were only slightly lower than the values for the parent (+)-M100907 compound, but significant selectivity for 5-HT2AR over 5-HT2BR or 5-HT2CR binding was retained. In addition, the 11-atom-linked bivalent 5-HT2AR antagonist (2 mg/kg, intraperitoneally) demonstrated efficacy on par with that of (+)-M100907 in inhibiting cocaine-evoked hyperactivity. As we develop further strategies for ligand-evoked receptor assembly and analyses of diverse signaling and functional roles, these novel homobivalent 5-HT2AR antagonist ligands will serve as useful in vitro and in vivo probes of 5-HT2AR structure and function.\n\n==\n\nWillmann 2021:\n\nTitle: Radiosynthesis and Biological Evaluation of [18F]R91150, a Selective 5-HT2A Receptor Antagonist for PET-Imaging.\n\nAbstract: Serotonergic 5-HT2A receptors in cortical and forebrain regions are an important substrate for the neuromodulatory actions of serotonin in the brain. They have been implicated in the etiology of many neuropsychiatric disorders and serve as a target for antipsychotic, antidepressant, and anxiolytic drugs. Positron emission tomography imaging using suitable radioligands can be applied for in vivo quantification of receptor densities and receptor occupancy for therapy evaluation. Recently, the radiosynthesis of the selective 5-HT2AR antagonist [18F]R91150 was reported. However, the six-step radiosynthesis is cumbersome and time-consuming with low radiochemical yields (RCYs) of <5%. In this work, [18F]R91150 was prepared using late-stage Cu-mediated radiofluorination to simplify its synthesis. The detailed protocol enabled us to obtain RCYs of 14 \u00b1 1%, and the total synthesis time was reduced to 60 min. In addition, autoradiographic studies with [18F]R91150 in rat brain slices revealed the typical uptake pattern of 5-HT2A receptor ligands.\n\n==\n\nSchmidt 1992:\n\nTitle: The 5-HT2 receptor antagonist, MDL 28,133A, disrupts the serotonergic-dopaminergic interaction mediating the neurochemical effects of 3,4-methylenedioxymethamphetamine.\n\nAbstract: The selective 5-HT2 receptor antagonist MDL 28,133A dose dependently-blocked the long-term deficits in rat brain 5-HT concentrations produced by the substituted amphetamine analogue 3,4-methylenedioxymethamphetamine (MDMA). This protective effect of MDL 28,133A could be abolished by coadministration of the dopamine precursor, L-dihydroxyphenylalanine (L-DOPA). Electrophysiological experiments demonstrated that the ability of MDL 28,133A to block the MDMA-induced slowing of A9 dopaminergic neurons was also sensitive to L-DOPA administration. Both sets of experiments suggest an interaction of MDL 28,133A at the level of dopamine synthesis. Consistent with this explanation, MDL 28,133A antagonized the MDMA-induced stimulation of dopamine synthesis in vivo. MDMA-induced 5-HT release did not reduce the firing rate of dopaminergic neurons as assessed by dopamine depletion following synthesis inhibition with alpha-methyl-p-tyrosine (alpha-MPT). This indicates that the effect of 5-HT2 receptor antagonists on MDMA-induced dopamine synthesis is not due simply to the removal of an inhibitory serotonergic input followed by an increase in dopamine cell firing and autoreceptor activation. MDL 28,133A was also shown to be without effect on the sensitivity of terminal dopamine autoreceptors. The results are consistent with the hypothesis that 5-HT2 receptors are permissive for the stimulation of dopamine synthesis necessary to support MDMA-induced transmitter efflux.\n\n==\n\nBartoszyk 2003:\n\nTitle: EMD 281014, a new selective serotonin 5-HT2A receptor antagonist.\n\nAbstract: The 5-HT2A receptor ligand 7-[4-[2-(4-fluoro-phenyl)-ethyl]-piperazine-1-carbonyl]-1H-indole-3-carbonitrile HCl (EMD 281014) selectively binds to human (h) and rat 5-HT2A receptors (IC50 values 0.35 and 1 nM, respectively; vs. 1334 nM for h5-HT2C) and inhibited 5-HT-stimulated [35S]guanosine 5'-O-3-thiotriphosphate (GTPgammaS)-accumulation in h5-HT2A transfected Chinese hamster ovary cells (IC50 9.3 nM). EMD 28014 counteracted the N-ethoxycarbonyl-2-ethoxy-1,2-dihydroquinoline (EEDQ)-induced decrease of [3H]ketanserin binding in rat frontal cortex (ID50 0.4 mg/kg p.o.) and R-(-)-1-(2,5-dimethoxy-4-iodophenyl)-aminopropane (DOI)-induced head-twitch behaviour in mice (ID50 0.01 mg/kg s.c., 0.06 mg/kg p.o.), demonstrating unique selectivity and efficacy.\n\n==\n\nBONHAUS 1997:\n\nTitle: RS-102221: A Novel High Affinity and Selective, 5-HT2C Receptor Antagonist\n\nAbstract: The 5-HT2C receptor is one of three closely related receptor subtypes in the 5-HT2 receptor family. 5-HT2A and 5-HT2B selective antagonists have been described. However, no 5-HT2C selective antagonists have yet been disclosed. As part of an effort to further explore the function of 5-HT2C receptors, we have developed a selective 5-HT2C receptor antagonist, RS-102221 (a benzenesulfonamide of 8-[5-(5-amino-2,4-dimethoxyphenyl) 5-oxopentyl]-1,3,8-triazaspiro[4.5]decane-2,4-dione). This compound exhibited nanomolar affinity for human (pKi = 8.4) and rat (pKi = 8.5) 5-HT2C receptors. The compound also demonstrated nearly 100-fold selectivity for the 5-HT2C receptor as compared to the 5-HT2A and 5-HT2B receptors. RS-102221 acted as an antagonist in a cell-based microphysiometry functional assay (pA2 = 8.1) and had no detectable intrinsic efficacy. Consistent with its action as a 5-HT2C receptor antagonist, daily dosing with RS-102221 (2 mg/kg intraperitoneal) increased food-intake and weight-gain in rats. Surprisingly, RS-102221 failed to reverse the hypolocomotion induced by the 5-HT2 receptor agonist 1-(3-chlorophenyl)piperazine (m-CPP). It is concluded that RS-102221 is the first selective, high affinity 5-HT2C receptor antagonist to be described.\n\n==\n\nBryant 1996:\n\nTitle: A novel class of 5-HT2A receptor antagonists: aryl aminoguanidines.\n\nAbstract: Local delivery of serotonin (5-HT) produces a rapid edematous response in soft tissues via increased fluid extravasation which is prevented by 5-HT2 antagonists such as ketanserin or mianserin. Here we report the effects of a new class of aminoguanidine 5-HT2 antagonists, with relative selectivity for 5-HT2A receptors which are potent inhibitors of 5-HT-induced paw edema in the rat. Radioligand binding studies with 125I DOI on human 5-HT2A and 5-HT2C receptors and with 3H-5-HT on human 5-HT2B receptors demonstrated that, LY314228, and LY320954 displayed some selectivity for the 5-HT2A receptor. When compared to binding at other 5-HT2 receptor subtypes, LY314228 had an 18.6-fold greater affinity for the 5-HT2A site over the 5-HT2B site, and 2.6 fold greater at the 5-HT2C site. LY320954 displayed similar preference for 5-HT2A sites. Both compounds also inhibited 5-HT-induced paw swelling in rats, with ED50's of 6.4 and 4.8 mg/kg (for LY314228 and LY320954, respectively). These studies offer evidence for a novel class of pharmacophores for the 5-HT2 receptor family which show greater relative affinities for the 5-HT2A receptor subclass.\n\n==\n\nStevenson 2000:\n\nTitle: 2-Aryl tryptamines: selective high-affinity antagonists for the h5-HT2A receptor.\n\nAbstract: A series of 2-aryl tryptamines have been identified as high-affinity h5-HT2A antagonists. Structure-activity relationship studies have shown that h5-HT2A affinity can be attained via modifications to the tryptamine side chain and that selectivity over h5-HT2C and hD2 receptors can be controlled by suitable C-2 aryl groups.",
        "final_summary": "The papers collectively provide insights into recent experiments using highly selective 5-HT2A receptor antagonists. Knight 2004 highlights the need for more selective agonists to investigate 5-HT2 receptor pharmacology. Soto 2017 introduces novel bivalent ligands that can probe 5-HT2A receptor homodimer function. Willmann 2021 presents a simplified synthesis method for the selective 5-HT2A receptor antagonist [18F]R91150, enabling in vivo quantification of receptor densities. Schmidt 1992 demonstrates the protective effect of the 5-HT2 receptor antagonist MDL 28,133A on MDMA-induced deficits in rat brain serotonin concentrations. Bartoszyk 2003 introduces EMD 281014, a selective 5-HT2A receptor antagonist with high affinity and efficacy. BONHAUS 1997 describes RS-102221 as the first selective, high-affinity 5-HT2C receptor antagonist. Bryant 1996 identifies a new class of aminoguanidine 5-HT2 antagonists with selectivity for the 5-HT2A receptor. Stevenson 2000 presents 2-aryl tryptamines as selective high-affinity h5-HT2A antagonists. These papers collectively contribute to the understanding and development of highly selective 5-HT2A receptor antagonists."
    },
    {
        "query": "In the context of autonomous vehicles, the importance of the control zone, where vehicles exchange information about the autonomous intersection, and the case of overlaps of different intersections",
        "paper_list_string": "Li 2020:\n\nTitle: Intersection management for autonomous vehicles with vehicle-to-infrastructure communication\n\nAbstract: This paper proposes an intersection management strategy for autonomous vehicles under the vehicle-to-infrastructure circumstance. All vehicles are supposed to be fully autonomous and can communicate with the intersection management unit to check the traffic situation. Priority of passing the intersection is decided by a static conflict matrix which represents the potential conflict between lanes of different directions and a dynamic information list which could capture the real-time occupation of each lane in the intersection. Compared with the existing approaches in the literature, the intersection management unit in our strategy is more like a database rather than a computational center, and therefore, requires less computational resource and more likely satisfies the real-time requirement in heavy traffic situations. Simulations are conducted using SUMO (Simulation of Urban MObility), in which the proposed strategy is compared with both fixed and adaptive traffic light methods. The results indicate that the proposed strategy could significantly reduce the average time delay caused by the intersection and the corresponding variance, which shows the efficiency and fairness of the proposed strategy in intersection management.\n\n==\n\nWuthishuwong 2015:\n\nTitle: Safe trajectory planning for autonomous intersection management by using vehicle to infrastructure communication\n\nAbstract: The development of autonomous vehicle or self-driving car integrates with the wireless communication technology which would be a forward step for road transportation in the near future. The autonomous crossing of an intersection with an autonomous vehicle will play a crucial role in the future of intelligent transportation system (ITS). The fundamental objectives of this work are to manage autonomous vehicles crossing an intersection with no collisions, maintaining that a vehicle drives continuously, and to decrease the waiting time at an intersection. In this paper, a discrete model of the one-way single intersection is designed. The vehicle-to-infrastructure (V2I) communication is implemented to exchange information between a vehicle and an intersection manager which is the roadside infrastructure. The safe trajectory of autonomous vehicles for the autonomous intersection management is determined and presented by using discrete mathematics.\n\n==\n\nHausknecht 2011:\n\nTitle: Autonomous Intersection Management: Multi-intersection optimization\n\nAbstract: Advances in autonomous vehicles and intelligent transportation systems indicate a rapidly approaching future in which intelligent vehicles will automatically handle the process of driving. However, increasing the efficiency of today's transportation infrastructure will require intelligent traffic control mechanisms that work hand in hand with intelligent vehicles. To this end, Dresner and Stone proposed a new intersection control mechanism called Autonomous Intersection Management (AIM) and showed in simulation that by studying the problem from a multiagent perspective, intersection control can be made more efficient than existing control mechanisms such as traffic signals and stop signs. We extend their study beyond the case of an individual intersection and examine the unique implications and abilities afforded by using AIM-based agents to control a network of interconnected intersections. We examine different navigation policies by which autonomous vehicles can dynamically alter their planned paths, observe an instance of Braess' paradox, and explore the new possibility of dynamically reversing the flow of traffic along lanes in response to minute-by-minute traffic conditions. Studying this multiagent system in simulation, we quantify the substantial improvements in efficiency imparted by these agent-based traffic control methods.\n\n==\n\nHausknecht 2011:\n\nTitle: Autonomous Intersection Management: Multi-intersection optimization\n\nAbstract: Advances in autonomous vehicles and intelligent transportation systems indicate a rapidly approaching future in which intelligent vehicles will automatically handle the process of driving. However, increasing the efficiency of today's transportation infrastructure will require intelligent traffic control mechanisms that work hand in hand with intelligent vehicles. To this end, Dresner and Stone proposed a new intersection control mechanism called Autonomous Intersection Management (AIM) and showed in simulation that by studying the problem from a multiagent perspective, intersection control can be made more efficient than existing control mechanisms such as traffic signals and stop signs. We extend their study beyond the case of an individual intersection and examine the unique implications and abilities afforded by using AIM-based agents to control a network of interconnected intersections. We examine different navigation policies by which autonomous vehicles can dynamically alter their planned paths, observe an instance of Braess' paradox, and explore the new possibility of dynamically reversing the flow of traffic along lanes in response to minute-by-minute traffic conditions. Studying this multiagent system in simulation, we quantify the substantial improvements in efficiency imparted by these agent-based traffic control methods.\n\n==\n\nWuthishuwong 2017:\n\nTitle: Consensus-based local information coordination for the networked control of the autonomous intersection management\n\nAbstract: Autonomous intersection management (AIM) will be a future method for improving traffic efficiency in the urban area. Instead of using the traffic signal control like nowadays, it uses wireless communication with autonomous vehicles to support the management of road traffic more safely and efficiently. A single AIM shows an exceptional performance in managing traffics at an intersection. However, it could not be represented a traffic in the real world, which is composed of multiple intersections. We show that coordination of traffic information among vehicles and infrastructures is an essential part of macroscopic traffic management. Coordination of traffic information among the network of AIMs is the key to improve the overall traffic flow throughout the network not only has an optimal flow in some intersections and very heavy traffic in others. In this paper, we introduce the distributed control to a graph-based intersection network to control traffic in a macroscopic level. Vehicle to infrastructure and infrastructure to infrastructure communication are used to exchange the traffic information between a single autonomous vehicle to the network of autonomous intersections. We implement a discrete time consensus algorithm to coordinate the traffic density of an intersection with its neighborhoods and determine the control policy to maximize a traffic throughput of each intersection as well as stabilizing the overall traffic in the network. We use the Greenshields traffic model to define the boundary condition of various traffic flows to the corresponded traffic density and velocity. Our proposed method represents the ability to maintain traffic flow rate of each intersection without having a back up traffic. As well, every intersection operates under the uncongested flow condition. The simulation results of the graph-based networked control of a multiple autonomous intersection showed that the overall traffic flow in the network achieves up to $$20\\%$$20% higher than using traffic signal system.\n\n==\n\nAzimi 2014:\n\nTitle: STIP: Spatio-temporal intersection protocols for autonomous vehicles\n\nAbstract: Autonomous driving is likely to be the heart of urban transportation in the future. Autonomous vehicles have the potential to increase the safety of passengers and also to make road trips shorter and more enjoyable. As the first steps toward these goals, many car manufacturers are investing in designing and equipping their vehicles with advanced driver-assist systems. Road intersections are considered to be serious bottlenecks of urban transportation, as more than 44% of all reported crashes in U.S. occur within intersection areas which in turn lead to 8,500 fatalities and approximately 1 million injuries every year. Furthermore, the impact of road intersections on traffic delays leads to enormous waste of human and natural resources. In this paper, we therefore focus on intersection management in Intelligent Transportation Systems (ITS) research. In the future, when dealing with autonomous vehicles, it is critical to address safety and throughput concerns that arise from autonomous driving through intersections and roundabouts. Our goal is to provide vehicles with a safe and efficient passage method through intersections and roundabouts. We have been investigating vehicle-to-vehicle (V2V) communications as a part of co-operative driving in the context of autonomous driving. We have designed and developed efficient and reliable intersection protocols to avoid vehicle collisions at intersections and increase traffic throughput. In this paper, we introduce new V2V intersection protocols to achieve the above goals. We show that, in addition to intersections, these protocols are also applicable to vehicle crossings at roundabouts. Additionally, we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols and suggest required modifications to guarantee their safety and efficiency despite these impairments. Our simulation results show that we are able to avoid collisions and also increase the throughput of the intersections up to 87.82% compared to common traffic-light signalized intersections.\n\n==\n\nDai 2016:\n\nTitle: Quality-of-Experience-Oriented Autonomous Intersection Control in Vehicular Networks\n\nAbstract: Recent advances in autonomous vehicles and vehicular communications are envisioned to enable novel approaches to managing and controlling traffic intersections. In particular, with intersection controller units (ICUs), passing vehicles can be instructed to cross the intersection safely without traffic signals. Previous efforts on autonomous intersection control mainly focused on guaranteeing the safe passage of vehicles and improving intersection throughput, without considering the quality of the travel experience from the passengers' perspective. In this paper, we aim to design an enhanced autonomous intersection control mechanism, which not only ensures vehicle safety and enhances traffic efficiency but also cares about the travel experience of passengers. In particular, we design the metric of smoothness to quantitatively capture the quality of experience. In addition, we consider the travel time of individual vehicles when passing the intersection in scheduling to avoid a long delay of some vehicles, which not only helps with improving intersection throughput but also enhances the system's fairness. With the above considerations, we formulate the intersection control model and transform it into a convex optimization problem. On this basis, we propose a new algorithm to achieve an optimal solution with low overhead. Finally, we build the simulation model and implement the algorithm for performance evaluation. Comprehensive simulation results demonstrate the superiority of the proposed algorithm.\n\n==\n\nWuthishuwong 2013:\n\nTitle: Coordination of multiple autonomous intersections by using local neighborhood information\n\nAbstract: Traffic congestion is considered one of the most important problems of road transportation. The concept of Intelligent Transportation System (ITS) has been introduced to improve the efficiency of the traffic flow. The autonomous vehicle technology is slowly becoming a reality, alongside wireless communication technology, which is becoming a major role in vehicle communication. Wireless communication devices will be generally used in vehicles for the future road transportation. The purpose of this research work is to use the local traffic information to reduce delay time of a vehicle at an intersection. In this paper, the fully autonomous intersection system is represented as a single intersection model. In addition, the multiple intersections are created based on the connection of a single autonomous intersection agent. The communication topology of the intersection network is modeled by using the undirected graph. The traffic information in the network is able to be exchanged by Infrastructure-to-Infrastructure (I2I) communication. This discrete consensus algorithm is implemented for coordinating the local traffic information and the neighborhood intersection. The proposed method works, based on the concept Green Wave, which maximizes the throughput, as well as minimizing the delay time. It maintains the traffic density of every intersection in the network to manage every vehicle for a continuous drive with a minimal delay time. The simulation of variable traffic flow rate of multiple autonomous intersections is implemented based on the proposed method.",
        "final_summary": "The papers suggest that the control zone, where vehicles exchange information about the autonomous intersection, is important in the context of autonomous vehicles. Li 2020 proposes an intersection management strategy that utilizes vehicle-to-infrastructure communication to check the traffic situation and determine priority of passing the intersection. Wuthishuwong 2015 focuses on safe trajectory planning for autonomous intersection management using vehicle-to-infrastructure communication to exchange information and ensure collision-free crossing. Hausknecht 2011 extends the study of autonomous intersection management to a network of interconnected intersections, exploring navigation policies and the possibility of dynamically reversing traffic flow. Wuthishuwong 2017 emphasizes the coordination of traffic information among vehicles and infrastructures in a network of autonomous intersections to improve overall traffic flow. Azimi 2014 introduces spatio-temporal intersection protocols for autonomous vehicles, aiming to provide safe and efficient passage through intersections and roundabouts. Dai 2016 proposes a quality-of-experience-oriented autonomous intersection control mechanism that considers passenger travel experience, vehicle safety, and traffic efficiency. Lastly, Wuthishuwong 2013 focuses on coordinating multiple autonomous intersections using local neighborhood information and implementing a discrete consensus algorithm to minimize delay time and maximize throughput."
    },
    {
        "query": "Recurrence of gonadotroph pituitary adenomas after primary surgery",
        "paper_list_string": "Dubois 2007:\n\nTitle: Relevance of Ki-67 and prognostic factors for recurrence/progression of gonadotropic adenomas after first surgery.\n\nAbstract: OBJECTIVE\nGonadotropin-secreting pituitary adenomas carry a high risk of local recurrence or progression (R/P) of remnant tumor after first surgery. The clinical characteristics and the long-term outcome of these silent adenomas, which show no signs of endocrine hyperfunction, differ from those of other types of pituitary adenomas. However, to date, no study has focused specifically on gonadotropic adenomas.\n\n\nMATERIALS AND METHODS\nTo identify prognostic factors of R/P of remnants, we studied the postoperative outcome of 32 gonadotropic pituitary adenomas, defined on immunohistochemical staining, according to their clinical and radiological characteristics as well as the Ki-67 labeling index (LI).\n\n\nRESULTS\nThe Ki-67 LI failed to provide independent information for the identification of patients at risk of progression of remnants or recurrence. Multivariate survival analysis (Cox regression) showed that neither invasiveness nor remnant tumors nor hyposomatotropism influenced tumor recurrence. The strongest predicting factors of R/P were the antero-posterior (AP) diameter in the sagittal plane (P = 0.014), and the age of the patient at surgery (P = 0.047), with younger patients being at greater risk. Hazard ratios were 2.11 for each 5 mm increase in AP diameter and 0.57 for every 10 years of age.\n\n\nCONCLUSION\nThe two simple clinical criteria revealed by our study, the AP diameter of the tumor and the age of the patient, should be helpful in planning clinical management and radiological monitoring after first surgery of gonadotropic adenomas, while awaiting the identification of other pathological parameters.\n\n==\n\nManieri 2000:\n\nTitle: Gonadotroph cell pituitary adenomas in males.\n\nAbstract: BACKGROUND\nConsidered exceptional in the past, gonadotroph cell pituitary adenomas account for 3.5-6.4% of total surgically excised pituitary adenomas when examined with immunospecific staining. The aim of this study was to describe the clinical, hormonal, radiological and immunohistochemical features, the management and the follow-up of our patients with gonadotroph adenoma.\n\n\nMETHODS\nIn this retrospective study we describe 14 male subjects aged 19-70 yrs affected by gonadotroph cell pituitary adenomas; the patients were studied by hormonal, radiological and immunohistochemical investigations and followed up for 3-13 yrs by ambulatory and/or hospitalized care.\n\n\nRESULTS\nVisual impairment and/or decreased libido and erectile dysfunction were the symptoms at presentation. Increased serum gonadotropin concentrations were shown in 3 patients. Reduced levels of testosterone were present in 9 patients, and normal in the remainder. At diagnosis all patients had pituitary macroadenomas, with wide extrasellar extension in 12. All patients underwent trans-sphenoidal surgery and immunohistochemical staining of surgically excised specimens showed the presence of gonadotroph and alpha-subunit cells in all pituitary adenomas. After surgery 3 patients had clear radiological evidence of normal pituitary; in the others a doubtful MRI picture or a residual adenomatous tissue were present. In the patients who did not undergo radiotherapy immediately after surgery, a regrowth of tumoral tissue was shown in 1-10 yrs.\n\n\nCONCLUSIONS\nWe stress the importance of a close follow-up of patients with gonadotroph adenomas after surgery, and we raise the question of whether radiotherapy may be useful for avoiding any further adenomatous regrowth.\n\n==\n\nSalmi 1982:\n\nTitle: Recurrence of chromophobe pituitary adenomas after operation and postoperative radiotherapy\n\nAbstract: The rate of recurrence is reported in a prospective study of 56 patients (28 men, 28 women) with large chromophobe pituitary adenoma (with or without hyperprolactinemia). The surgical approach was transfrontal in 44 and transseptosphenoidal in 12 patients. Cryoapplication was combined with the transsphenoidal operation. All but one patient received post\u2010operative pituitary irradiation. Altogether, 11 (20 %) clinical relapses (10 men) occurred between 0.5 and 6 years after the transfrontal operation. Patients that relapsed had had larger tumors than those remaining in remission. Occurrence of the tumors appeared with a deterioration of the visual field defect in 9 patients. There were no differences in the degrees of hypopituitarism in patients who relapsed as compared to patients remaining in remission.\n\n==\n\nLosa 2004:\n\nTitle: Endocrine inactive and gonadotroph adenomas: diagnosis and management\n\nAbstract: Endocrine inactive pituitary adenomas represent about one quarter of all pituitary tumors. By immunocytochemistry, most of these tumors are positive for intact gonadotropins and/or their subunits. Clinical presentation is usually secondary to mass effect symptoms, such as visual disturbances, headache, and hypopituitarism. Differential diagnosis is usually accomplished by neuroradiologic studies, even though in selected cases positron emission tomography and/or single photon emission tomography may aid to distinguish pituitary adenomas from other endocrine inactive lesions, such as meningiomas and craniopharyngiomas. Surgical management is usually considered the first choice treatment for patients with endocrine inactive pituitary adenomas because it is very effective in ameliorating symptoms of chiasmal compression and headache. Radical removal of the tumor, however, is difficult to obtain because of the frequent invasiveness into the cavernous sinus. Radiation therapy diminishes the likelihood of tumor recurrence, especially in patients with demonstrable tumor remnants after surgery. Medical therapy with dopaminergic drugs, somatostatin analogs, or gonadotropin-releasing hormone agonists or antagonists causes mild reduction of tumor size in few patients and, therefore, seems to be of limited value in the therapeutic management of patients with endocrine inactive pituitary adenomas.\n\n==\n\nSnyder 1987:\n\nTitle: Gonadotroph cell pituitary adenomas.\n\nAbstract: The frequency of gonadotroph cell adenomas among all unselected pituitary adenomas is likely much higher than previously suspected. The prevalence in one series of 139 men with pituitary macroadenomas was 17 per cent (24 per cent if adenomas secreting only alpha subunit are included). The clinical characteristics of patients with gonadotroph cell adenomas are similar. Most are middle-aged men who have a history of normal pubertal development and a normal fertility history and by examination are normally virilized and have testes of normal size. They are brought to medical attention because of visual impairment, which is the result of the enormous size of the adenoma. The most common hormonal characteristics of gonadotroph cell adenomas in vivo is hypersecretion of FSH, which is often accompanied by hypersecretion of FSH-beta and alpha subunits and less often by hypersecretion of LH-beta or intact LH. Another common characteristic is secretion of FSH and/or LH-beta in response to TRH. A few patients with gonadotroph cell adenomas hypersecrete intact LH and therefore have supranormal serum testosterone concentrations. A larger number have secondary hypogonadism, because the adenomas are not secreting intact LH but are compressing the normal gonadotroph cells and impairing LH secretion. These patients have concentrations of intact LH that are not elevated, despite subnormal testosterone concentrations. The testosterone increases markedly in response to human chorionic gonadotropin. Both the clinical and hormonal characteristics of gonadotroph cell adenomas usually make them readily distinguishable from pituitary enlargement due to long-standing primary hypogonadism. Most gonadotroph cell adenomas are now managed first by transsphenoidal surgery to attempt to restore vision as quickly as possible, and then by supervoltage radiation to prevent regrowth of the remaining adenomatous tissue. Surgery usually does improve vision, as well as the pretreatment hormonal abnormalities, and radiation reduces FSH hypersecretion further. Dopamine agonist therapy is experimental but warrants further trial. The hormonal abnormalities detected prior to treatment, such as supranormal basal concentrations of FSH, alpha, and FSH-beta and the FSH and LH-beta responses to TRH, can be used to monitor the response to therapy.\n\n==\n\nMor 2005:\n\nTitle: Diagnosis of pituitary gonadotroph adenomas in reproductive-aged women.\n\nAbstract: OBJECTIVE\nTo describe the clinical symptoms associated with the diagnosis of pituitary gonadotroph adenoma in premenopausal women.\n\n\nDESIGN\nReport of three separate cases.\n\n\nSETTING\nUniversity medical center.\n\n\nPATIENT(S)\nThree patients: a 31-year-old woman with primary infertility, recurrent adnexal masses, and highly elevated estradiol level; a 30-year-old woman with recurrent multicystic ovaries following multiple cystectomies and transvaginal cyst aspirations, and elevated estradiol level; a 43-year-old woman with bilateral complex cystic adnexal masses and an elevated estradiol level, who underwent a total abdominal hysterectomy and bilateral salpingo-oophorectomy for a suspected granulosa cell tumor.\n\n\nINTERVENTION(S)\nTranssphenoidal resection of a pituitary mass.\n\n\nMAIN OUTCOME MEASURE(S)\nSerum estradiol, FSH, and LH levels; transvaginal ultrasonography of the ovaries; histologic examination of pituitary tumors.\n\n\nRESULT(S)\nTranssphenoidal resection of pituitary adenomas resulted in normalization of serum estradiol and FSH levels and resolution of adnexal masses in two of the women.\n\n\nCONCLUSION(S)\nPituitary gonadotroph adenoma must be considered in the differential diagnosis in reproductive-aged women presenting with the clinical symptom triad of new onset oligomenorrhea, bilateral cystic adnexal masses, and elevated estradiol and FSH levels with suppressed levels of LH; timely diagnosis may prevent unnecessary and potentially damaging surgical procedures.\n\n==\n\nLanglois 2017:\n\nTitle: Clinical profile of silent growth hormone pituitary adenomas; higher recurrence rate compared to silent gonadotroph pituitary tumors, a large single center experience\n\nAbstract: PurposeStudy and comparison of characteristics of silent growth hormone adenomas (SGHA), silent corticotroph adenomas (SCA), and silent gonadotroph adenomas (SGA) in a single institution cohort of surgically treated pituitary adenomas.MethodsRetrospective analysis of SGHA surgically resected over 10 years: SGHA was defined as no clinical or biochemical evidence of acromegaly and positive GH immunostaining.ResultsOf 814 pituitary surgeries; 2.1% (n\u2009=\u200917) were SGHA, 4.5% (n\u2009=\u200937) SCA, and 18.9% (n\u2009=\u200970/371; 2011\u20132016) SGA. Mean age at SGHA diagnosis was 43 years, with a large female predominance (82%). Mean tumor size and cavernous/sphenoid sinus invasiveness for SGHA, SCA, and SGA were 1.5\u2009\u00b1\u20091.0\u2009cm and 25%, 2.5\u2009\u00b1\u20091.2\u2009cm and 43%, 2.9\u2009\u00b1\u20092.0\u2009cm and 41%, respectively (tumor size p\u2009=\u20090.009, SGHA vs. SGA, and invasion p; not-significant). During mean follow-up of 3.9 years, two patients (11%) developed elevated insulin-like growth factor-1 and five patients (29%) required a second surgery for tumor recurrence. Rate of surgical reintervention was similar to SCA (31%), but higher than SGA (10%) (p\u2009=\u20090.035, SGHA vs. SGA), and 18% underwent radiation therapy, similar to SCA (19%, p; not-significant) but higher than SGA (2.9%, p\u2009=\u20090.018).ConclusionThis is the largest single center study characterizing SGHA behavior with SGA and SCA control groups in a cohort of surgically resected pituitary adenomas. SGHA present mostly in young females, and should be closely followed due to their higher likelihood of recurrence and potential of progression to clinical acromegaly. We propose that a complete hormonal staining panel be routinely performed for all pituitary adenomas.\n\n==\n\nYoung 1996:\n\nTitle: Gonadotroph adenoma of the pituitary gland: a clinicopathologic analysis of 100 cases.\n\nAbstract: OBJECTIVE\nTo determine the clinical and pathologic features in a large cohort of randomly selected patients with gonadotroph pituitary adenomas.\n\n\nDESIGN\nWe retrospectively reviewed clinical, surgical, and pathologic findings in 100 patients (79 men and 21 women, 30 to 82 years old) with this tumor.\n\n\nRESULTS\nDiagnosis of a pituitary tumor was prompted by visual loss (43%), symptoms of hypopituitarism (22%), headache (8%), or a combination of these findings (10%); 17% of the patients were asymptomatic. Visual field defects were present in 68% of the study group, and complete or partial anterior pituitary failure was present in 77%. Serum prolactin concentrations were increased (maximum, 110 ng/mL) in 33% of patients. Hypersecretion of follicle-stimulating hormone (FSH) and luteinizing hormone (LH) was documented in 11 and 5 patients, respectively. The serum level of alpha-subunit was increased in 1 of 29 patients in whom it was measured. All patients had pituitary macroadenomas, only 21% of which were grossly invasive. The surgical approach was transsphenoidal in all but two patients, who underwent transfrontal craniotomy; gross total tumor resection was achieved in 90%. By definition, all tumors had at least 10% immunoreactivity for LH, FSH, or both. All tumors were chromophobic. Ultrastructurally, the tumors were characterized as gonadotroph adenomas of \"male\" (45%) or \"female\" (9%) type as well as null-cell adenomas of the oncocytic (35%) or nononcocytic (11%) type. After a median follow-up of 4.3 years, 69% of the patients who had had visual field defects noted normalization or improvement. Persistent or clinically recurrent pituitary tumor tissue was present in 42%. A second pituitary surgical procedure was required in eight patients.\n\n\nCONCLUSION\nMost patients with clinically evident gonadotroph pituitary tumors have loss of vision and hypopituitarism. Hypersecretion of FSH or LH is unusual, and no distinct hormone-dependent clinical phenotype is present. Transsphenoidal surgical treatment generally yields normalization or improvement of visual field defects.",
        "final_summary": "The recurrence of gonadotroph pituitary adenomas after primary surgery is influenced by various factors. Dubois 2007 identified the antero-posterior diameter of the tumor and the age of the patient at surgery as significant predictors of recurrence. Salmi 1982 reported a 20% recurrence rate in patients with chromophobe pituitary adenomas. Losa 2004 suggested that radiation therapy could diminish the likelihood of tumor recurrence. Langlois 2017 found that silent growth hormone adenomas had a higher recurrence rate compared to silent gonadotroph adenomas. Young 1996 observed that transsphenoidal surgical treatment generally led to normalization or improvement of visual field defects in patients with gonadotroph pituitary tumors."
    },
    {
        "query": "What do scenarios mean for the future of geopolitics? How are scenarios being used in the Arctic?",
        "paper_list_string": "Nilsson 2019:\n\nTitle: Towards improved participatory scenario methodologies in the Arctic\n\nAbstract: ABSTRACT Participatory scenario methodologies are increasingly used for studying possible future developments in the Arctic. They have the potential to contribute to several high-priority tasks for Arctic research, such as integration of indigenous and local knowledge in futures studies, providing a platform for activating Arctic youth in shaping their futures, identifying Arctic-relevant indicators for sustainable development, and supporting decision-making towards sustainable futures. Yet, to achieve this potential, several methodological challenges need to be addressed. These include attention to whose voices are amplified or silenced in participatory research practices, with special attention to diversification and the engagement of youth. Given the historic and potential future role of disruptive events for Arctic development trajectories, methods are needed in participatory scenario exercises to include attention to the dynamics and consequences of such events and regime shifts. Participatory scenarios can also be further improved through approaches that effectively combine qualitative and quantitative information. Finally, there is a need for systematic studies of how the results of scenario exercises influence decision-making processes. This article elaborates on ways in which attention to these aspects can help make scenarios more robust for assessing a diversity of potential Arctic futures in times of rapid environmental and social change.\n\n==\n\nErokhin 2020:\n\nTitle: Regional scenarios of the Arctic futures: A review\n\nAbstract: The future of the Arctic region is a subject of heated debates in both scientific and policy circles. The region has an enormous economic potential as a storehouse of mineral resources and as a provider of shorter and more cost-effective transportation between Europe and Asia. The Arctic is therefore an essential strategic element of the domestic and foreign policies of all Arctic states. In addition, there is an increasing economic interest in the region on the part of non-Arctic states. However, at present, the future of the Arctic region development remains highly uncertain. Scenario building is a suitable methodology to imagine alternative plausible futures of such a complex and multi-dimensional process and to elaborate successful and robust development strategies. This paper provides an overview of the scenario frameworks of Arctic futures presented in the literature and analyses key factors that determine these scenarios. Overall, we find a growing interest of the international foresight research community in the Arctic region that is evident from a number of thorough scenario-building exercises published recently. At the same time, we observe two drawbacks. First, the existing studies lack a numerical element, that is, the overwhelming majority of the scenario frameworks that can be found in the literature are fully qualitative. Quantitative estimates would strengthen the scenario narratives and enrich communication, which make them a useful addition to support a qualitative scenario framework. Second, the existing studies use a mixture of both internal and external factors to describe the underlying uncertainty. This limits the number of factors that can be taken into consideration and may be confusing for a potential user of these scenario frameworks due to the lack of a systemic view. Such a confusion can happen, for example, if some of the external factors underpinning a particular scenario suddenly develop in a direction that was not anticipated within the scenario framework. The effect of such a change on the set of scenarios and the validity of the scenarios despite this change will be of interest to the user, and a clear systems perspective would be conducive to address these questions. Separating internal and external factors in a scenario building exercise is particularly useful given that the volatility of the global geopolitical, geoeconomic and environmental dynamics is only increasing. It is our intention to address these two drawbacks in a scenario building exercise within the \u201cEmerging trade routes between Europe and Asia\u201d scenario-building project led by IIASA within the Northern Dimension Institute (NDI) Think Tank Action co-funded by the European Union and coordinated by Aalto University, Finland.\n\n==\n\nHaavisto 2016:\n\nTitle: Socio-economic scenarios for the Eurasian arctic by 2040\n\nAbstract: Improved weather and marine services (WMS) can have a role to play in the safe and secure development of the Arctic region through either a demand-pull (enhanced by growth in activity) or a supply-push (enhances growth in activity) process. To analyse the nature of the process and the future use and benefits of WMS, a better understanding of possible future developments in the Eurasian Arctic is needed. This report presents six socio-economic scenarios for the Eurasian Arctic by 2040, and a brief synopsis of the implications of each scenario for WMS. The scenarios focus on the development of shipping, resource extraction and tourism industries. The scenario futures, called Wild West, Silicon Valley, Exploited Colony, Shangri La, Conflict Zone and Antarctic, describe the scale and scope of activities in the Eurasian Arctic by 2040. The scenarios have three dimensions: open \u2013 closed, public private and dirty \u2013 clean, which describe the political, economic, social, technological and environmental aspects of different futures. The scenarios are based on a literature review, pre-survey, expert workshop and restructuring and analysis of this material. The methodology used for scenario construction is described in detail and may be used widely by other scenario developers. Our analysis shows that plenty of potential pressures for major changes in the Eurasian Arctic exist. Environmental changes, political shifts and technological development can all push forward drastic new developments in the region. Then again, it is possible that despite all the hype and interest, the Eurasian Arctic remains backwater areas in the global economy. This emphasizes the need for any decision-maker to be able to respond to very different futures. Therefore, robust decision making, a good eye for weak signals and tipping points, and the ability to prepare for risks and seize opportunities as they emerge is required in the Eurasian Arctic. The development of WMS is important in ensuring the safe and secure development of the Eurasian Arctic, unless the development follows the path of \u201cAntarctica\u201d with tourism and research as main activities in the marine regions.\n\n==\n\nKeys 2021:\n\nTitle: Visions of the Arctic Future: Blending Computational Text Analysis and Structured Futuring to Create Story\u2010Based Scenarios\n\nAbstract: The future of Arctic social systems and natural environments is highly uncertain. Climate change will lead to unprecedented phenomena in the pan\u2010Arctic region, such as regular shipping traffic through the Arctic Ocean, urban growth, military activity, expanding agricultural frontiers, and transformed Indigenous societies. While intergovernmental to local organizations have produced numerous synthesis\u2010based visions of the future, a challenge in any scenario exercise is capturing the \u201cpossibility\u201d space of change. In this work, we employ a computational text analysis to generate unique thematic input for novel, story\u2010based visions of the Arctic. Specifically, we develop a corpus of more than 2,000 articles in publicly accessible, English\u2010language Arctic newspapers that discuss the future in the Arctic. We then perform a latent Dirichlet allocation, resulting in 10 distinct topics and sets of associated keywords. From these topics and keywords, we design ten story\u2010based scenarios employing the M\u0101noa mashup, science fiction prototyping, and other methods. Our results demonstrate that computational text analysis can feed directly into a creative futuring process, whereby the output stories can be traced clearly back to the original topics and keywords. We discuss our findings in the context of the broader field of Arctic scenarios and show that the results of this computational text analysis produce complementary stories to the existing scenario literature. We conclude that story\u2010based scenarios can provide vital texture toward understanding the myriad possible Arctic futures.\n\n==\n\nZaikov 2019:\n\nTitle: Scenarios for the development of the Arctic region (2020\u20132035)\n\nAbstract: . The importance of selecting the development of the Arctic seems to be relevant since rapid and irreversible changes are taking place there. Climate change and globalization are their prominent examples. A complex of factors has both positive and negative impacts on the use of natural resources and the positioning of states located not only within the Arctic but also outside it. The questions arise: what is the significance of these changes for geography, politics, and the management system? How should the compre-hension of these processes be built? The relevance of the topic is enhanced by the fact that Russia has the most significant Arctic sector among the states with access to the Arctic Ocean. Therefore, our country has a leading role in working out strategies for the development of the Arctic. The comprehensive approach (considering the economic and political-geographical positions) is central in the article to analyze the directions of development of the Arctic territories. The method reveals the possibilities of sustainable development, which will provide Russia with strategic benefits within the Arctic and globally. The article discusses scenarios for the development of the Arctic, including the Arctic zone of the Russian Federation, in the long-term perspective (until 2035). Substantiation of the long-term prospects for the development of the Arctic, despite Russian and foreign research, seems to be unrealistic due to lack of knowledge about the nature and consequences of climatic changes currently observed in this region and affecting global environmental management. The authors concluded that the priority directions of the Arctic development should be the ones based on positive and innovative trends.\n\n==\n\nBrigham 2008:\n\nTitle: The Future of Arctic Marine Navigation in Mid-Century - Scenario Narratives\n\nAbstract: This document serves as the final Scenario Narratives Report for the Future of Arctic Marine Navigation in Mid-Century, a project of the Arctic Council\u2019s Protection of the Arctic Marine Environment (PAME) working group and Global Business Network (GBN), a member of the Monitor Group. The purpose of this project, and these scenarios, is to systematically consider the long-term social, technological, economic, environmental, and political impacts on Arctic Marine Navigation of Key Finding #6 of the Arctic Climate Impact Assessment (ACIA) published by the Arctic Council and the International Arctic Science Committee in November 2004. These scenarios are meant to summarize and communicate a set of plausible and different stories of the future in which critical uncertainties play out in ways that challenge planning decisions being made in the present. For this scenarios project on the Future of Arctic Marine Navigation, we convened two workshops to gather the perspectives and ideas of a highly diverse group of stakeholders. The first workshop was held at the GBN office in San Francisco in April 2007, and the second at the offices of Aker Arctic Technology in Helsinki in July 2007. Because this project rests on Key Finding #6, all of the scenarios assume continued global climate change that results in significantly less Arctic ice cover, at least in the summer, throughout the 2030s and 2040s. It is our intention that these scenarios will provide material for deeper discussions about the future and earlier decisions by the countries, peoples, and industries active in the Arctic region.\n\n==\n\nWormbs 2017:\n\nTitle: Arctic Futures: Agency and Assessing Assessments\n\nAbstract: The future is a common theme in discussions of the Arctic, whether in media, policy, or scientific communications. The future is not a given, and there are several possible futures that different actors strive to enable at any given time. At present considerable attention is given to monolithic \u201cdrivers\u201d of change in this region, including melting sea ice, technological development, and global resource geopolitics; and although this discourse is far from new (Doel et al. 2014b), the end of the Cold War and the amplification of climate and global change have reframed the discussion (Christensen et al. 2013). The media has played a large role in propagating the \u201cdrivers\u201d discourse, usually understating the role of human agency. While recognizing that these \u201cdriving\u201d factors are important, this chapter will analyze some of this \u201cfuture-talk\u201d, in relation to the future of the Arctic. We would argue that there is considerable discursive power (Foucault 2002) in these images of the future, which explains why they are so visible and articulated with such fervor. It is not our intention to suggest that this talking about the future in and of itself constructs the future that actually unfolds. Rather, we hold that the genres of future-talk are closely connected to real interests connected to particular versions of Arctic futures.\n\n==\n\nKauppila 2022:\n\nTitle: China\u2019s rise and the Arctic region up to 2049 \u2013 three scenarios for regional futures in an era of climate change and power transition\n\nAbstract: ABSTRACT Although China has emerged as an increasingly influential global actor over recent decades, it is unclear whether a more fundamental transformation is shaking processes of regionalisation in this context. Our scenario-based case study considers the spatial reconfiguration of the rapidly globalising Arctic with varying degrees of Chinese engagement. More specifically, we examine alternative and transformational configurations of the Arctic in 2049, and ponder upon the plausibility of the hypothesised changes in light of three schools of thought on International Relations \u2013 realism, liberal institutionalism and relationalism. Hence, we explore how the rise of China could potentially alter the regional dynamics and whether, consequently, regions should be rethought both empirically and theoretically. We conclude that pluralistic discussion on the multiple regional outcomes is a necessary precondition for achieving a balanced and democratic future in the Arctic and beyond.",
        "final_summary": "Scenarios are being used in the Arctic to explore and understand the future of geopolitics in the region. Nilsson 2019 discusses the potential of participatory scenario methodologies in integrating indigenous knowledge, engaging youth, and supporting decision-making for sustainable futures. Erokhin 2020 emphasizes the importance of scenario building in imagining alternative futures and developing robust strategies for Arctic development. Haavisto 2016 presents socio-economic scenarios for the Eurasian Arctic. Keys 2021 employs computational text analysis to generate story-based scenarios for the Arctic, providing a textured understanding of possible futures. Zaikov 2019 discusses scenarios for the development of the Arctic region, emphasizing the need for a comprehensive approach and sustainable development strategies. Brigham 2008 presents scenarios for the future of Arctic marine navigation. Wormbs 2017 examines the discursive power of future-talk in shaping perceptions of the Arctic's future and the role of human agency. Kauppila 2022 explores the potential influence of China's rise on the Arctic region, considering alternative configurations and the need for pluralistic discussions on regional outcomes."
    },
    {
        "query": "what is the relationship between conflict communication styles and marital satisfaction? give 20 studies summary",
        "paper_list_string": "Kurdek 1995:\n\nTitle: Predicting change in marital satisfaction from husbands' and wives' conflict resolution styles.\n\nAbstract: Identifying what specific conflict resolution styles are linked to change in marital satisfaction is important because managing conflict is one of the central tasks of maintaining a marriage (Gottman, 1994) and because declines in marital satisfaction herald a series of processes indicative of a deteriorating marriage (Gottman & Levenson, 1992). Findings regarding the concurrent link between conflict resolution styles and marital satisfaction have been consistent in indicating that each spouse's marital satisfaction is positively related to the frequency with which each spouse uses constructive strategies to resolve conflict (such as agreement, compromise, and humor) and negatively related both to the frequency with which each spouse uses destructive strategies to resolve conflict (such as conflict engagement, withdrawal, and defensiveness) and to the joint frequency with which the wife uses conflict engagement and the husband uses withdrawal (the \"demand-withdraw\" pattern). This pattern of findings has been obtained regardless of whether conflict resolution styles were assessed by means of brief behavioral observations (Gottman & Krokoff, 1989; Heavey, Layne, & Christensen, 1993; Noller, Feeney, Bonnell, & Callan, 1994) or by means of self-report and partner-report questionnaire data (Christensen, 1988; Heavey et al., 1993; Huston & Vangelisti, 1991; Noller et al., 1994). Given that the concurrent link between conflict resolution styles and marital satisfaction is well-established, researchers have used longitudinal data to assess the plausibility of two causal relations involving conflict resolution styles and marital satisfaction. The first causal relation--the assumption that the use of certain conflict resolution styles causes marital satisfaction--is based on interdependence theory (Rusbult, 1983) which posits that perceived rewards to a relationship (such as the frequent use of constructive conflict resolution strategies) and perceived costs to the relationship (such as the frequent experience of negative conflict resolution styles) determine satisfaction with the relationship. The longitudinal data consistent with this causal relation would indicate that the frequency with which certain conflict resolution styles are used at Time 1 predicts change in marital satisfaction. The second causal relation--the assumption that the level of marital satisfaction is causally related to the frequency with which certain conflict resolution styles are used--is based on self-fulfilling prophecy theory (e.g., Snyder, Tanke, & Berscheid, 1977) which posits that one's attitude (e.g., level of satisfaction with the marriage) provides a psychological environment that elicits behavior (e.g., conflict resolution styles) that reinforces and is consistent with the initial attitude. The longitudinal data consistent with this causal relation would indicate that the level of marital satisfaction at Time 1 predicts the degree of change in the frequency with which certain conflict resolution styles are used. Unfortunately, longitudinal findings relevant to the plausibility of either causal relation have been inconsistent (Gottman & Krokoff, 1989; Heavey et al., 1993; Huston & Vangelisti, 1991; Noller et al., 1994). In part, this could be due to four methodological and two conceptual limitations of these studies. Regarding the methodological limitations, first, because Gottman and Krokoff (1989) and Heavey et al. (1993) used measures of marital satisfaction that also tapped frequency of conflict and disagreement, the correlations from these measures may have been inflated (see review by Fincham & Bradbury, 1987). Second, only Huston and Vangelisti (1991) reported that marital satisfaction scores changed appreciably over the time interval studied, raising the possibility that, in the other studies, a restriction of range accounted for nonsignificant findings. Third, because the first assessments made by Huston and Vangelisti (1991) and Noller et al. \u2026\n\n==\n\nGreeff 2000:\n\nTitle: Conflict management style and marital satisfaction.\n\nAbstract: The aim of this study was to investigate whether there is one conflict management style that correlated more significantly with marital satisfaction than any other. In addition, spousal satisfaction with how marital conflict is managed was also examined, as were gender differences. Fifty-seven couples who had been married for at least 10 years took part in the study. Results showed that the collaborative conflict management style has the highest correlation with both marital satisfaction and spousal satisfaction with conflict management in the marriage. In contrast, where one or both of the spouses used the competitive conflict management style, the lowest marital satisfaction was reported. The results were also interpreted in terms of cultural and gender differences.\n\n==\n\nBruyne 2000:\n\nTitle: Conflict Management Style and Marital Satisfaction\n\nAbstract: The aim of this study was to investigate whether there is one conflict management style that correlated more significantly with marital satisfaction than any other. In addition, spousal satisfaction with how marital conflict is managed was also examined, as were gender differences. Fifty-seven couples who had been married for at least 10 years took part in the study. Results showed that the collaborative conflict management style has the highest correlation with both marital satisfaction and spousal satisfaction with conflict management in the marriage. In contrast, where one or both of the spouses used the competitive conflict management style, the lowest marital satisfaction was reported. The results were also interpreted in terms of cultural and gender differences.\n\n==\n\nRussell-Chapin 2001:\n\nTitle: The Relationship of Conflict Resolution Styles and Certain Marital Satisfaction Factors to Marital Distress\n\nAbstract: The data presented in this article report significant predictors and relationships among high levels of marital distress, lack of couple time together, and problems associated with lack of communication. The results are explored in terms of counseling implications.\n\n==\n\nUnal 2020:\n\nTitle: Conflict resolution styles as predictors of marital adjustment and marital satisfaction: an actor\u2013partner interdependence model\n\nAbstract: The aim of the study was to examine the mediating role of marital adjustment on the relationship between conflict resolution styles and marital satisfaction in terms of actor and partner effects. I...\n\n==\n\nRehman 2011:\n\nTitle: Marital Satisfaction and Communication Behaviors During Sexual and Nonsexual Conflict Discussions in Newlywed Couples: A Pilot Study\n\nAbstract: The way couples communicate during conflict discussions has been found to be a reliable predictor of marital satisfaction. However, in previous research, there has been little experimental control over the selection of topics. The present study examined, in a sample of 15 newlywed couples, whether affective displays during the discussion of a sexual and a nonsexual conflict topic differentially predict current marital satisfaction. Communication behaviors were coded using an adaptation of the Specific Affect Coding System, resulting in composite \u201cnegative behavior\u201d and \u201cpositive behavior\u201d categories. Data were analyzed using multilevel modeling. Negative behaviors displayed during the nonsexual conflict discussions were not significantly related to concurrent self-reported relationship satisfaction. In contrast, for wives, negative behaviors displayed during the discussion of a sexual problem were significantly related to lower levels of relationship satisfaction. For the sexual and nonsexual conflict discussions, positive behaviors were positively associated with relationship satisfaction, although this effect did not reach statistical significance. Overall, the authors\u2019 findings emphasize the importance of incorporating sexual variables in the study of marriage. Furthermore, their study represents an important step in recognizing that marital research benefits from an examination of specific topics of conflict as a factor to consider in studies of marital functioning.\n\n==\n\nSchudlich 2011:\n\nTitle: Relations between spouses' depressive symptoms and marital conflict: a longitudinal investigation of the role of conflict resolution styles.\n\nAbstract: This study investigated longitudinal relations between spouses' depressive symptoms and styles of conflict resolution displayed by husbands and wives in marital conflict, including angry, depressive, and constructive patterns of expression. Behavioral observations were made from a community sample of 276 couples during marital conflict resolution tasks once a year for 3 years. Couples were observed engaging in a major and minor conflict resolution task. Constructive, angry, and depressive conflict resolution styles were derived from the behavioral observation coding. Couples self-reported on depressive symptoms and marital dissatisfaction. Path analyses provided support for an extension of the marital discord model of depression (Beach, Sandeen, & O'Leary, 1990). Specifically, angry, depressive, and constructive styles of conflict each mediated the link between marital dissatisfaction and depressive symptoms. Significant cross-spouse effects were found. Implications for the treatment of depressed and/or relationally discordant couples are discussed.\n\n==\n\nRands 1981:\n\nTitle: Patterns of Conflict Resolution and Marital Satisfaction*\n\nAbstract: an intensive survey of fertility decision-making, both Spouses in 244Northern California marriages responded to questions a bout the kinds of they encounter, their style of conflict resolution, the expected oftheir conflict, and their marital satisfaction. Four main \"tYPes\"0f resolution were found based on the 488 individual partnen' reports: (1) a nonintimaie-aggressivc pattern, (11) a nonintimatc-nonaggressivc one, ('11) an intimate-aggressive pattern, and (1V) an intimate-nonaggressive One. who perceived their marriage in Type 1 were generally least aatisficd. tlnd those in Type I V most satisfied, with Types I1 and 111 intermediate. The meaning of this typology and its correlates, as well as its wider implications. are discussed.",
        "final_summary": "The relationship between conflict communication styles and marital satisfaction is explored in the following studies:\n\n1. Kurdek (1995) found that constructive conflict resolution strategies, such as agreement, compromise, and humor, were positively related to marital satisfaction, while destructive strategies, like conflict engagement, withdrawal, and defensiveness, were negatively related to marital satisfaction.\n\n2. Greeff (2000) and Bruyne (2000) both found that the collaborative conflict management style had the highest correlation with marital satisfaction, while the competitive style was associated with lower marital satisfaction.\n\n3. Russell-Chapin (2001) reported significant predictors and relationships between high levels of marital distress, lack of couple time together, and communication problems.\n\n4. Unal (2020) examined the mediating role of marital adjustment on the relationship between conflict resolution styles and marital satisfaction, finding actor and partner effects.\n\n5. Rehman (2011) highlighted the importance of communication behaviors during conflict discussions, with negative behaviors during sexual conflict linked to lower marital satisfaction for wives.\n\n6. Schudlich (2011) found that different conflict resolution styles, including angry, depressive, and constructive patterns, mediated the link between marital dissatisfaction and depressive symptoms.\n\n7. Rands (1981) identified four patterns of conflict resolution: nonintimate-aggressive, nonintimate-nonaggressive, intimate-aggressive, and intimate-nonaggressive. Those in the intimate-nonaggressive pattern reported the highest marital satisfaction.\n\nIn summary, these studies collectively suggest that constructive conflict resolution styles, such as collaboration and agreement, are associated with higher marital satisfaction, while destructive styles and communication problems contribute to lower satisfaction and potential distress in marriages."
    },
    {
        "query": "flexible markov switching",
        "paper_list_string": "Deschamps 2006:\n\nTitle: A Flexible Prior Distribution for Markov Switching Autoregressions With Student-T Errors\n\nAbstract: This paper proposes an empirical Bayes approach for Markov switching autoregressions that can constrain some of the state-dependent parameters (regression coefficients and error variances) to be approximately equal across regimes. By flexibly reducing the dimension of the parameter space, this can help to ensure regime separation and to detect the Markov switching nature of the data. The permutation sampler with a hierarchical prior is used for choosing the prior moments, the identification constraint, and the parameters governing prior state dependence. The empirical relevance of the methodology is illustrated with an application to quarterly and monthly real interest rate data.\n\n==\n\nDewachter 2001:\n\nTitle: Can Markov switching models replicate chartist profits in the foreign exchange market\n\nAbstract: Abstract In this paper we show that the Markov switching model is a relevant statistical alternative to the classical martingale model for exchange rates. By extending the standard Markov switching model we decisively reject the martingale model. Moreover, the model generates autocorrelations and linear structures in line with what is observed in reality. Subsequently, we test whether this model can explain chartist profits. We find that the extended Markov switching model is able to explain the profitability of a simple MA-30 rule. Finally, we decompose the profitability of the MA-30 rule into a linear and nonlinear part. We find that, although the implied linear structure of the Markov model explains a substantial part of the profitability, part of the profits of the MA-30 rule can be attributed to the specific nonlinearities implicit in the Markov model.\n\n==\n\nGuidolin 2011:\n\nTitle: Markov Switching Models in Empirical Finance\n\nAbstract: I review the burgeoning literature on applications of Markov regime switching models in empirical finance. In particular, distinct attention is devoted to the ability of Markov Switching models to fit the data, filter unknown regimes and states on the basis of the data, to allow a powerful tool to test hypotheses formulated in light of financial theories, and to their forecasting performance with reference to both point and density predictions. The review covers papers concerning a multiplicity of sub-fields in financial economics, ranging from empirical analyses of stock returns, the term structure of default-free interest rates, the dynamics of exchange rates, as well as the joint process of stock and bond returns.\n\n==\n\nDark 2015:\n\nTitle: Futures hedging with Markov switching vector error correction FIEGARCH and FIAPARCH\n\nAbstract: Markov switching vector error correction asymmetric long memory volatility models with fat tailed innovations are proposed. Bivariate two state versions of the models are applied to a futures hedge of the S&P500. Regime switches occur between high and low cost of carry states via changes in the error correction term or basis. Regime identification is therefore dominated by switches in the mean, not volatility. Relative to a number of alternatives, the proposed models provide superior out of sample forecasts of the covariance matrix particularly for horizons greater than 10days ahead. When hedging, Markov switching with long memory improves the tail risk of hedged returns beyond 10day horizons, however there is mixed support for models with volatility asymmetries. These findings have important implications for the development of multivariate models and other applications including portfolio management, spread option pricing and arbitrage.\n\n==\n\nBauwens 2007:\n\nTitle: Theory and Inference for a Markov Switching GARCH Model\n\nAbstract: We develop a Markov-switching GARCH model (MS-GARCH) wherein the conditional mean and variance switch in time from one GARCH process to another. The switching is governed by a hidden Markov chain. We provide sufficient conditions for geometric ergodicity and existence of moments of the process. Because of path dependence, maximum likelihood estimation is not feasible. By enlarging the parameter space to include the state variables, Bayesian estimation using a Gibbs sampling algorithm is feasible. We illustrate the model on SP500 daily returns.\n\n==\n\nSheu 2014:\n\nTitle: Optimal Futures Hedging Under Multichain Markov Regime Switching\n\nAbstract: Most of the existing Markov regime switching GARCH-hedging models assume a common switching dynamic for spot and futures returns. In this study, we release this assumption and suggest a multichain Markov regime switching GARCH (MCSG) model for estimating state-dependent time-varying minimum variance hedge ratios. Empirical results from commodity futures hedging show that MCSG creates hedging gains, compared with single-state-variable regime-switching GARCH models. Moreover, we find an average of 24% cross-regime probability, indicating the importance of modeling cross-regime dynamic in developing optimal futures hedging strategies. \u00a9 2012 Wiley Periodicals, Inc. Jrl Fut Mark 34:173\u2013202, 2014\n\n==\n\nEngel 1992:\n\nTitle: Can the Markov Switching Model Forecast Exchange Rates?\n\nAbstract: A Markov-switching model is fit for eighteen exchange rates at quarterly and monthly frequencies. This model fits well in-sample at the quarterly frequency for many exchange rates. By the mean-squared-error or mean-absolute-error criterion. the Markov model does not generate superior forecasts at a random walk or at the forward rate. There appears to be some evidence that the forecast of the Markov model are superior at predicting the direction of change of the exchange rate.\n\n==\n\nDueker 1997:\n\nTitle: Markov Switching in GARCH Processes and Mean-Reverting Stock-Market Volatility\n\nAbstract: This article introduces four models of conditional heteroscedasticity that contain Markov-switching parameters to examine their multiperiod stock-market volatility forecasts as predictions of options-implied volatilities. The volatility model that best predicts the behavior of the options-implied volatilities allows the Student-t degrees-of-freedom parameter to switch such that the conditional variance and kurtosis are subject to discrete shifts. The half-life of the most leptokurtic state is estimated to be a week, so expected market volatility reverts to near-normal levels fairly quickly following a spike.",
        "final_summary": "Flexible Markov switching models have been applied across various fields, demonstrating their ability to capture regime changes and explain complex dynamics. Deschamps (2006) proposes an empirical Bayes approach for Markov switching autoregressions, aiding in the detection of the Markov switching nature of data. Dewachter (2001) extends the Markov switching model to explain chartist profits in the foreign exchange market. Guidolin (2011) reviews the application of these models in empirical finance, emphasizing their capacity to fit data, filter unknown regimes, test hypotheses, and make accurate forecasts. Dark (2015) suggests that Markov switching models with long memory volatility can enhance futures hedging and forecast covariance matrices. Bauwens (2007) develops a Markov-switching GARCH model, without specific application to SP500 daily returns. Sheu (2014) introduces a multichain Markov regime switching GARCH model for optimal futures hedging, showing improved hedging gains compared to single-state-variable models. Engel (1992) applies the Markov switching model to exchange rates, finding it fits well but does not generate superior forecasts compared to random walk or forward rate models. Lastly, Dueker (1997) examines Markov switching models with discrete shifts in conditional variance and kurtosis, noting that volatility reverts to near-normal levels quickly following a spike."
    },
    {
        "query": "The increased importance of cassava in agricultural and economic development as well as in food security particularly in Nigeria should give its processing and waste handling more attention. Presently, the methods involved in cassava processing are referred to as crude and unstandardized, lacking scientific principles and waiting for possible upgrade (Okunade and Adekalu, 2013). ",
        "paper_list_string": "Kolawole 2010:\n\nTitle: Sustaining world food security with improved cassava processing technology: the Nigeria experience.\n\nAbstract: Cassava is a very important food crop that is capable of providing food security. However, a lot of problems prevent the development and use of modern equipment for its production. Most of the cassava produced still comes from peasant farmers who depend on manual tools for their field operations and these farmers have made Nigeria the world\u2019s largest producer of the crop. An increase in production of cassava to sustain the world food security needs improved machinery to allow its continuous cultivation and processing. Reasons for the low success recorded in the mechanization of cassava harvesting and processing were traced, and the attempts that have been made in the recent past by various engineers in Nigeria researching towards achieving mechanized harvesting and processing of cassava are well explained. The machinery required for cassava production in Africa, the development of new machines, and the need for more research and development in harvesting and processing machineries, which can reduce poverty worldwide and make food available and accessible for all, are also discussed. Research efforts made and the challenges facing the engineers, farmers, scientists and food processors towards achieving mechanical harvesting and processing of cassava are presented. Breeding a cassava variety with a regular shape for easy mechanization is one solution that could help the engineers worldwide.\n\n==\n\nAchi 2018:\n\nTitle: Cassava Processing Wastes: Options and Potentials for Resource Recovery in Nigeria\n\nAbstract: Agro-food processing industries are major contributors of wastes in most developing countries. With Nigeria leading in cassava food production, little attention has been paid to provide a sustainable and profit-oriented solution to the problem of solid waste resulting from cassava processing industries.\n\n==\n\nAgbaeze 2020:\n\nTitle: Management of Food Crop for National Development: Problems and Challenges of Cassava Processing in Nigeria\n\nAbstract: This article examined the problems and challenges of managing cassava processing in Nigeria. Based on the findings of our study, agricultural policy framework, level of mechanization, infrastructure, agricultural technical experts, and access to capital are the major factors that affect cassava processing in Nigeria, while the challenges are policy inconsistency, high cost of mechanization and inadequate capital investment in infrastructure, inadequate extension agents, as well as stringent credit facility conditions. This study, therefore, concludes that for cassava to play a major role in Nigeria\u2019s national development government must ensure that there is a full exploration of all opportunities that have the potential to yield strong advantage. This could be achieved through a holistic assessment of the cassava process that is geared toward improving the quality of life and well-being of citizens based on Nigeria\u2019s comparative advantage in cassava production and processing.\n\n==\n\nOnyenwoke 2014:\n\nTitle: Cassava post-harvest processing and storage in Nigeria: A review\n\nAbstract: Cassava is an important root crop consumed as a staple food, boiled, baked or often fermented into other foods and beverages all over the world. It is a very good vehicle for addressing some health related problems and also serve as security food. Cassava undergoes postharvest physiological deterioration (PPD) once the tubers are separated from the main plant. PPD is one of the main obstacles currently preventing farmers from exporting fresh cassava abroad thereby generating income from foreign exchange. Cassava can be preserved in various ways such as coating with wax and freezing. Recent development in plant breeding has resulted in cassava that is tolerant to PPD. Genetic manipulation was considered most appropriate to solving the PPD challenge by adding new traits to elite genotypes without altering other desired characteristics. Processing cassava affects the nutritional value of cassava roots through modification and losses in nutrients of high value. The processing methods include peeling, boiling, steaming, slicing, grating, soaking or seeping, fermenting, pounding, roasting, pressing, drying, and milling. The products from cassava are: High Quality Cassava Flour (HQCF), cassava chips, garri, starch, ethanol etc.\n\n==\n\nJimoh 2012:\n\nTitle: An Automated Cassava Peeling System for the Enhancement of Food Security in Nigeria\n\nAbstract: ABSTRACT Cassava is second only to sweet potato as the most important starchy root crop of the tropical world. In most parts of the tropics, cassava is grown on small plots. However, in some countries, e.g. Mexico, Brazil and Nigeria, large plantations have been started and interest in mechanization is growing. The degree of mechanization depends on the size of the land and availability of machines for each unit operation involved in cassava processing. However, of all the unit operations involved in cassava processing, cassava peeling remains a serious global challenge to process engineers. An automated cassava peeling machine was thus developed in the Department of Agricultural Engineering, Federal University of Technology, Akure. The design of the machine was based on the development and modification of the peeling tool of previous cassava peeling machines and the peeling principle is by impact. This machine was evaluated using newly harvested improved variety of cassava tuber (TMS 30572) grouped into different sizes: 100\u00a0\u2264\u00a0L\n\n==\n\nOdebode 2008:\n\nTitle: Appropriate Technology for Cassava Processing in Nigeria: User\u2019s Point of View\n\nAbstract: This study examined appropriate agricultural extension technological needs of users in cassava processing activities in Nigeria. Purposive sampling technique was used in selecting 160 participating and non-participating users making a total of 320 users in Oyo state. Data was collected with Interview Schedule and analysed using chi-square and t-test. (58%) of women (users) use traditional processing equipments in cassava processing. Improved processing technologies used include vibrating sieve, abrasive peeler, motorised grater, drum drier, and screw \u2013jack . Processed cassava products include \u201cgari\u201d, \u201clafun\u201d, starch, and \u201cfufu\u201d. Significant relationships exist between the use of improved technologies for processing and age (X 2 = 6.15, p= 0.05), educational Status (X 2 = 5.80, p= 0.05), religion (X 2 = 12.20, p= 0.05) and type of technology utilized. Significant difference exists between mean adoption scores of participating and nonparticipating users (t= 6.53, p= 0.05) . Problems encountered by the users include high cost of processing equipment, transportation difficulties, poor infrastructural facilities, shortage of labour, poor access to market, lack of fund and poor shortage facilities. Time-saving and simpler prototype processing equipment should be introduced to the users of cassava processing technologies during extension training.\n\n==\n\nEwebiyi 2021:\n\nTitle: Constraints to utilization of improved processing technologies among cassava processors in Oyo State, Nigeria\n\nAbstract: This study investigated the constraints associated with utilization of improved processing technologies among cassava processors in Oyo State, Nigeria. A structured questionnaire was used to collect data from a random sample of 176 respondents. Data was analyzed using descriptive statistics and Pearson Product Moment Correlation. The findings were that most (56.8%) of the respondents had a positive attitude towards utilization of mechanical grater, screw press hydraulic and Hammer mill technologies. However, inadequate funds, lack of technical know-how, lack of information, high cost of purchase and inadequate engineers for the improved processing technologies were identified as severe constraints militating against utilization of improved processing technologies. Significant relationship was established between these constraints and utilization of the improved processing technologies. It is recommended that government 1) builds factories that will lower the cost of the technologies; and 2) ensures that research institutes, departments of Agricultural engineering in universities and polytechnics and adult education programs in the community are adequately funded to promote knowledge of the technologies.\n\n==\n\nEhinmowoO 2014:\n\nTitle: Analysis of Technical Efficiency of Cassava Processing Methods among Small Scale Processors in South - West, Nigeria\n\nAbstract: This study employed a stochastic frontier model to analyze the productivity and technical efficiency (TE) of cassava processing methods among small scale processors in South \u2013 West, Nigeria. The study was carried out in three states of Ogun, Oyo and Ondo, Nigeria. Data were obtained from primary sources using a set of structured questionnaire assisted with interview schedule. The multistage sampling technique was used. Data were analyzed using: descriptive statistics and the stochastic frontier production function using a farm level survey data collected from 373 small scale cassava processors. Results showed that cassava processing under local and modern methods was in stage one (stage of inefficiency) of the production region and that processors using the local method of processing cassava were more technically efficient than those using the modern method.",
        "final_summary": "The papers suggest that cassava processing in Nigeria requires more attention and improvement. Kolawole (2010) emphasizes the need for improved machinery to sustain cassava production and processing for food security. Achi (2018) highlights the lack of attention given to sustainable waste management in cassava processing industries. Agbaeze (2020) identifies factors such as agricultural policy, mechanization, infrastructure, and access to capital as major challenges in cassava processing. Onyenwoke (2014) discusses the importance of addressing post-harvest physiological deterioration and the potential for genetic manipulation to improve cassava preservation. Jimoh (2012) focuses on the challenge of cassava peeling and presents an automated peeling machine as a solution. Odebode (2008) examines the need for appropriate technology in cassava processing and the factors influencing its adoption. Ewebiyi (2021) identifies constraints such as inadequate funds, lack of technical know-how, and high costs hindering the utilization of improved processing technologies. EhinmowoO (2014) analyzes the technical efficiency of cassava processing methods and finds that processors using the local method are more efficient than those using the modern method. Overall, the papers highlight the importance of upgrading cassava processing methods in Nigeria to enhance agricultural and economic development, food security, and waste handling."
    },
    {
        "query": "Knee cartilage repair",
        "paper_list_string": "Richter 2015:\n\nTitle: Knee Articular Cartilage Repair and Restoration Techniques\n\nAbstract: Context: Isolated chondral and osteochondral defects of the knee are a difficult clinical challenge, particularly in younger patients for whom alternatives such as partial or total knee arthroplasty are rarely advised. Numerous surgical techniques have been developed to address focal cartilage defects. Cartilage treatment strategies are characterized as palliation (eg, chondroplasty and debridement), repair (eg, drilling and microfracture [MF]), or restoration (eg, autologous chondrocyte implantation [ACI], osteochondral autograft [OAT], and osteochondral allograft [OCA]). Evidence Acquisition: PubMed was searched for treatment articles using the keywords knee, articular cartilage, and osteochondral defect, with a focus on articles published in the past 5 years. Study Design: Clinical review. Level of Evidence: Level 4. Results: In general, smaller lesions (<2 cm2) are best treated with MF or OAT. Furthermore, OAT shows trends toward greater longevity and durability as well as improved outcomes in high-demand patients. Intermediate-size lesions (2-4 cm2) have shown fairly equivalent treatment results using either OAT or ACI options. For larger lesions (>4 cm2), ACI or OCA have shown the best results, with OCA being an option for large osteochondritis dissecans lesions and posttraumatic defects. Conclusion: These techniques may improve patient outcomes, though no single technique can reproduce normal hyaline cartilage.\n\n==\n\nWelton 2018:\n\nTitle: Knee Cartilage Repair and Restoration: Common Problems and Solutions.\n\nAbstract: Focal cartilage defects in the knee are commonly found on MRI and arthroscopically. When these lesions are symptomatic and fail nonoperative management several surgical strategies are available. Common surgical techniques include reparative (ie, microfracture) and restorative procedures (ie, autologous chondrocyte implantation, particulated juvenile allograft cartilage, osteochondral autograft transfer, and osteochondral allograft). Each of these surgical procedures have shared and novel complications associated with their use. This article provides a detailed, case-based discussion of common complications encountered in surgical procedures for focal cartilage defects of the knee, highlighting causes, clinical recognition, and how to address and avoid these complications.\n\n==\n\nKalson 2010:\n\nTitle: Current strategies for knee cartilage repair\n\nAbstract: Defects in knee articular cartilage (AC) can cause pain and disability and present the clinician with an extremely challenging clinical situation. This article describes the most up\u2010to\u2010date surgical techniques that aim to repair and/or regenerate symptomatic focal defects in AC, which include arthroscopic debridement, microfracture bone marrow stimulation and autologous osteochondral allografting, with an emphasis on autologous chondrocyte implantation. In the future, refinement of tissue\u2010engineering approaches promises to further improve outcome for these patients.\n\n==\n\nBrittberg 2016:\n\nTitle: Cartilage repair in the degenerative ageing knee\n\nAbstract: Background and purpose \u2014 Cartilage damage can develop due to trauma, resulting in focal chondral or osteochondral defects, or as more diffuse loss of cartilage in a generalized organ disease such as osteoarthritis. A loss of cartilage function and quality is also seen with increasing age. There is a spectrum of diseases ranging from focal cartilage defects with healthy surrounding cartilage to focal lesions in degenerative cartilage, to multiple and diffuse lesions in osteoarthritic cartilage. At the recent Aarhus Regenerative Orthopaedics Symposium (AROS) 2015, regenerative challenges in an ageing population were discussed by clinicians and basic scientists. A group of clinicians was given the task of discussing the role of tissue engineering in the treatment of degenerative cartilage lesions in ageing patients. We present the outcomes of our discussions on current treatment options for such lesions, with particular emphasis on different biological repair techniques and their supporting level of evidence. Results and interpretation \u2014 Based on the studies on treatment of degenerative lesions and early OA, there is low-level evidence to suggest that cartilage repair is a possible treatment for such lesions, but there are conflicting results regarding the effect of advanced age on the outcome. We concluded that further improvements are needed for direct repair of focal, purely traumatic defects before we can routinely use such repair techniques for the more challenging degenerative lesions. Furthermore, we need to identify trigger mechanisms that start generalized loss of cartilage matrix, and induce subchondral bone changes and concomitant synovial pathology, to maximize our treatment methods for biological repair in degenerative ageing joints.\n\n==\n\nSalzmann 2018:\n\nTitle: Articular Cartilage Repair of the Knee in Children and Adolescents\n\nAbstract: Articular cartilage predominantly serves a biomechanical function, which begins in utero and further develops during growth and locomotion. With regard to its 2-tissue structure (chondrocytes and matrix), the regenerative potential of hyaline cartilage defects is limited. Children and adolescents are increasingly suffering from articular cartilage and osteochondral deficiencies. Traumatic incidents often result in damage to the joint surfaces, while repetitive microtrauma may cause osteochondritis dissecans. When compared with their adult counterparts, children and adolescents have a greater capacity to regenerate articular cartilage defects. Even so, articular cartilage injuries in this age group may predispose them to premature osteoarthritis. Consequently, surgery is indicated in young patients when conservative measures fail. The operative techniques for articular cartilage injuries traditionally performed in adults may be performed in children, although an individualized approach must be tailored according to patient and defect characteristics. Clear guidelines for defect dimension\u2013associated techniques have not been reported. Knee joint dimensions must be considered and correlated with respect to the cartilage defect size. Particular attention must be given to the subchondral bone, which is frequently affected in children and adolescents. Articular cartilage repair techniques appear to be safe in this cohort of patients, and no differences in complication rates have been reported when compared with adult patients. Particularly, autologous chondrocyte implantation has good biological potential, especially for large-diameter joint surface defects.\n\n==\n\nGobbi 2011:\n\nTitle: One-Step Cartilage Repair with Bone Marrow Aspirate Concentrated Cells and Collagen Matrix in Full-Thickness Knee Cartilage Lesions\n\nAbstract: Objective: The purpose of our study was to determine the effectiveness of cartilage repair utilizing 1-step surgery with bone marrow aspirate concentrate (BMAC) and a collagen I/III matrix (Chondro-Gide, Geistlich, Wolhusen, Switzerland). Materials and Methods: We prospectively followed up for 2 years 15 patients (mean age, 48 years) who were operated for grade IV cartilage lesions of the knee. Six of the patients had multiple chondral lesions; the average size of the lesions was 9.2 cm2. All patients underwent a mini-arthrotomy and concomitant transplantation with BMAC covered with the collagen matrix. Coexisting pathologies were treated before or during the same surgery. X-rays and MRI were collected preoperatively and at 1 and 2 years\u2019 follow-up. Visual analog scale (VAS), International Knee Documentation Committee (IKDC), Knee injury and Osteoarthritis Outcome Score (KOOS), Lysholm, Marx, SF-36 (physical/mental), and Tegner scores were collected preoperatively and at 6, 12, and 24 months\u2019 follow-up. Four patients gave their consent for second-look arthroscopy and 3 of them for a concomitant biopsy. Results: Patients showed significant improvement in all scores at final follow-up (P < 0.005). Patients presenting single lesions and patients with small lesions showed higher improvement. MRI showed coverage of the lesion with hyaline-like tissue in all patients in accordance with clinical results. Hyaline-like histological findings were also reported for all the specimens analyzed. No adverse reactions or postoperative complications were noted. Conclusion: This study showed that 1-step surgery with BMAC and collagen I/III matrix could be a viable technique in the treatment of grade IV knee chondral lesions.\n\n==\n\nSiclari 2013:\n\nTitle: Cartilage repair in the knee with subchondral drilling augmented with a platelet-rich plasma-immersed polymer-based implant\n\nAbstract: PurposeThe aim of our study was to analyse the clinical and histological outcome after the treatment of focal cartilage defects in non-degenerative and degenerative knees with bone marrow stimulation and subsequent covering with a cell-free resorbable polyglycolic acid\u2013hyaluronan (PGA-HA) implant immersed with autologous platelet-rich plasma (PRP).MethodsFifty-two patients (mean age 44\u00a0years) with focal chondral defects in radiologically confirmed non-degenerative or degenerative knees were subjected to subchondral drilling arthroscopically. Subsequently, defects were covered with the PGA-HA implant immersed with autologous PRP. At 2-year follow-up, the patients\u2019 situation was assessed using the Knee Injury and Osteoarthritis Outcome Score (KOOS) and compared to the pre-operative situation and 3\u201312-month follow-up. Biopsies (n\u00a0=\u00a04) were harvested at 18\u201324\u00a0months after implantation and were analysed by histology and collagen type II immune staining.ResultsAt 1- and 2-year follow-up, the KOOS showed clinically meaningful and significant (p\u00a0<\u00a00.05) improvement in all subcategories compared to baseline and to 3-month follow-up. There were no differences in KOOS data obtained after 2\u00a0years compared to 1\u00a0year after the treatment. Histological analysis of the biopsy tissue showed hyaline-like to hyaline cartilage repair tissue that was rich in cells with a chondrocyte morphology, proteoglycans and type II collagen.ConclusionsCovering of focal cartilage defects with the PGA-HA implant and PRP after bone marrow stimulation improves the patients\u2019 situation and has the potential to regenerate hyaline-like cartilage.Level of evidenceCase series, Level IV.\n\n==\n\nNiemeyer 2016:\n\nTitle: Cartilage repair surgery for full-thickness defects of the knee in Germany: indications and epidemiological data from the German Cartilage Registry (KnorpelRegister DGOU)\n\nAbstract: PurposeTreatment of cartilage defects of the knee remains an important issue with high relevance. In October 2013 the German Cartilage Registry (KnorpelRegister DGOU) was initiated in order to study indications, epidemiology and (clinical) outcome of different cartilage repair techniques. The present evaluation of the registry baseline data was initiated to report common practices of cartilage repair surgery in Germany.Materials and methods1065 consecutive patients who underwent surgical cartilage treatment of the knee have been included (complete data sets available in 1027 cases; FU rate 96.4\u00a0%) between October 1, 2013 and June 30, 2015. Data collection was performed using a web-based RDE System. All data were provided by the attending physician at the time of arthroscopic or open surgery of the affected knee.ResultsIn 1027 cartilage repair procedures, single defects were treated in 80\u00a0% of the cases with the majority of the defects located on the medial femoral condyle, followed by the patella. Degenerative defects grade III or IV according to ICRS were treated in 60\u00a0% of the cases and therefore were found more frequently compared to traumatic or post-traumatic lesions. Autologous chondrocyte implantation (ACI) was the most common technique followed by bone marrow stimulation (BMS) and osteochondral transplantation (OCT). While ACI was performed in defects with a mean size of 4.11\u00a0cm2 SD SD 2.16), BMS and OCT (1.51\u00a0cm2, SD 1.19; p\u00a0<\u00a00.01) were applied in significantly smaller defects (both p\u00a0<\u00a00.01). Independent of defect size, the ratio of ACI versus BMS applications differed between different defect locations. ACI was used preferably in defects located on the patella.ConclusionThe present analysis of data from the German Cartilage Registry shows that the vast majority of cartilage repair procedures were applied in degenerative, non-traumatic cartilage defects. Experts in Germany seem to follow the national and international guidelines in terms that bone marrow stimulation is applied in smaller cartilage defects while cell-based therapies are used for the treatment of larger cartilage defects. In patellar cartilage defects a trend towards the use of cell-based therapies has been observed.",
        "final_summary": "The papers collectively provide insights into knee cartilage repair techniques. Richter 2015 categorizes treatment strategies into palliation, repair, and restoration. Smaller lesions are best treated with microfracture or osteochondral autograft transfer (OAT), while larger lesions benefit from autologous chondrocyte implantation (ACI) or osteochondral allograft (OCA). Welton 2018 discusses common complications associated with reparative and restorative procedures. Kalson 2010 highlights surgical techniques like debridement, microfracture, and ACI for repairing focal cartilage defects. Brittberg 2016 emphasizes the need for improvements in focal defect repair techniques before using them for degenerative lesions. Salzmann 2018 suggests that articular cartilage repair techniques are safe and effective in children and adolescents. Gobbi 2011 demonstrates the effectiveness of 1-step surgery with bone marrow aspirate concentrate (BMAC) and a collagen matrix for grade IV knee cartilage lesions. Siclari 2013 shows positive clinical and histological outcomes with subchondral drilling and a platelet-rich plasma-immersed polymer-based implant. Niemeyer 2016 reports that bone marrow stimulation is commonly used for smaller defects, while cell-based therapies like ACI are preferred for larger defects."
    },
    {
        "query": "attentional engagement",
        "paper_list_string": "Zivony 2018:\n\nTitle: Contingent Attentional Engagement: Stimulus- and Goal-Driven Capture Have Qualitatively Different Consequences\n\nAbstract: We examined whether shifting attention to a location necessarily entails extracting the features at that location, a process referred to as attentional engagement. In three spatial-cuing experiments (N = 60), we found that an onset cue captured attention both when it shared the target\u2019s color and when it did not. Yet the effects of the match between the response associated with the cued object\u2019s identity and the response associated with the target (compatibility effects), which are diagnostic of attentional engagement, were observed only with relevant-color onset cues. These findings demonstrate that stimulus- and goal-driven capture have qualitatively different consequences: Before attention is reoriented to the target, it is engaged to the location of the critical distractor following goal-driven capture but not stimulus-driven capture. The reported dissociation between attentional shifts and attentional engagement suggests that attention is best described as a camera: One can align its zoom lens without pressing the shutter button.\n\n==\n\nBecker 2020:\n\nTitle: The attentional blink: A relational accountof attentional engagement\n\nAbstract: Visual attention allows selecting relevant information from cluttered visual scenes and is largely determined by our ability to tune or bias visual attention to goal-relevant objects. Originally, it was believed that this top-down bias operates on the specific feature values of objects (e.g., tuning attention to orange). However, subsequent studies showed that attention is tuned to in a context-dependent manner to the relative feature of a sought-after object (e.g., the reddest or yellowest item), which drives covert attention and eye movements in visual search. However, the evidence for the corresponding relational account is still limited to the orienting of spatial attention. The present study tested whether the relational account can be extended to explain attentional engagement and specifically, the attentional blink (AB) in a rapid serial visual presentation (RSVP) task. In two blocked conditions, observers had to identify an orange target letter that could be either redder or yellower than the other letters in the stream. In line with previous work, a target-matching (orange) distractor presented prior to the target produced a robust AB. Extending on prior work, we found an equally large AB in response to relatively matching distractors that matched only the relative color of the target (i.e., red or yellow; depending on whether the target was redder or yellower). Unrelated distractors mostly failed to produce a significant AB. These results closely match previous findings assessing spatial attention and show that the relational account can be extended to attentional engagement and selection of continuously attended objects in time.\n\n==\n\nOakes 2004:\n\nTitle: Attentional Engagement in Infancy: The Interactive Influence of Attentional Inertia and Attentional State.\n\nAbstract: We evaluated the interactive influences of attentional state and attentional inertia on infants' level of attentional engagement. We assessed infants' distraction latencies longitudinally at 6.5 and 9 months as they explored toys, and we coded both their attentional state (focused vs. casual) and how long they had been looking at the toy at each distractor onset. Consistent with previous results, both attentional state and attentional inertia contributed to differences in distraction latency. Importantly, the level of attentional engagement was interactively determined by attentional state and attentional inertia. Infants were most resistant to distraction when they were judged to be in a state of focused attention following relatively long looks to the toy, and they were equivalently less resistant to distraction under all other conditions. These results are consistent with a general conceptualization of attentional engagement resulting from the interaction of multiple processes.\n\n==\n\nAbuhamdeh 2012:\n\nTitle: Attentional involvement and intrinsic motivation\n\nAbstract: The current study used the Experience Sampling Method to examine attentional involvement\u2014the degree to which one\u2019s attention is devoted to moment-to-moment activity\u2014as a potential mediator of two previously identified relationships within the field of intrinsic motivation: (1) the positive relationship between a balance of challenges and skills and enjoyment, and (2) the positive relationship between competence valuation and enjoyment. Multilevel, within-person analyses indicated attentional involvement fully mediated both relationships. Implications of the findings for intrinsic motivation processes are discussed.\n\n==\n\nZivony 2020:\n\nTitle: Distractor intrusions are the result of delayed attentional engagement: A new temporal variability account of attentional selectivity in dynamic visual tasks.\n\nAbstract: When observers must identify targets among distractors in a rapid serial visual presentation (RSVP) stream, distractor intrusion errors are frequent, demonstrating the difficulty of allocating attention to the right object at the right moment in time. However, the mechanisms responsible for such intrusion errors remain disputed. We propose a new attentional engagement account of selective visual processing in RSVP tasks. Engagement is triggered by the preattentive detection of target-defining features. Critically, the success versus failure of target identification is determined by the speed of such engagement processes on individual trials. To test this account, we measured electrophysiological markers of attentional engagement (N2pc components) in 3 experiments where observers had to report the identity of a target digit in 1 of 2 lateral RSVP streams. On most trials, the target was immediately followed by a digit distractor, resulting in many posttarget distractor intrusions. Critically, N2pcs components measured on distractor intrusion trials were significantly delayed relative to trials with correct target reports. This was the case regardless of whether the target was defined by a shape cue or by its color and even when the location of shape-defined targets was known in advance. These findings show that distractor intrusions are the result of delayed attentional engagement. They demonstrate that temporal variability in attentional selectivity across trials can strongly affect visual awareness and perceptual reports. Our temporal variability account of attentional engagement offers a new framework for assessing the temporal dynamics of attention in visual object recognition. (PsycInfo Database Record (c) 2020 APA, all rights reserved).\n\n==\n\nLi 2020:\n\nTitle: Closed-Loop Attention Restoration Theory for Virtual Reality-Based Attentional Engagement Enhancement\n\nAbstract: Today, as media and technology multitasking becomes pervasive, the majority of young people face a challenge regarding their attentional engagement (that is, how well their attention can be maintained). While various approaches to improve attentional engagement exist, it is difficult to produce an effect in younger people, due to the inadequate attraction of these approaches themselves. Here, we show that a single 30-min engagement with an attention restoration theory (ART)-inspired closed-loop software program (Virtual ART) delivered on a consumer-friendly virtual reality head-mounted display (VR-HMD) could lead to improvements in both general attention level and the depth of engagement in young university students. These improvements were associated with positive changes in both behavioral (response time and response time variability) and key electroencephalography (EEG)-based neural metrics (frontal midline theta inter-trial coherence and parietal event-related potential P3b). All the results were based on the comparison of the standard Virtual ART tasks (control group, n = 15) and closed-loop Virtual ART tasks (treatment group, n = 15). This study provides the first case of EEG evidence of a VR-HMD-based closed-loop ART intervention generating enhanced attentional engagement.\n\n==\n\nPerra 2012:\n\nTitle: Attention engagement in early infancy.\n\nAbstract: We report a longitudinal study investigating developmental changes in the structure of attention engagement during early infancy. Forty-three infants were observed monthly from 2 to 4 months. Attention engagement was assessed from play interactions with parents, using a coding system developed by Bakeman and Adamson (1984). The results indicated a developmental transition in attention engagement at 3 months: after this age infants engaged for longer periods and in a wider variety of states. Most infants displayed person engagement at 2 months, passive joint engagement at 3 months, and object engagement at 4 months. To address whether emerging abilities of attention engagement allow infants to follow the attention of social partners, we compared attention engagement to performance on an experimental measure of attention control (reported by Perra & Gattis, 2010). Analyses revealed a positive relation between passive joint engagement and checking back, suggesting that changes in passive joint engagement reflect the development in attention control.\n\n==\n\nBrockmole 2009:\n\nTitle: Should I stay or should I go? Attentional disengagement from visually unique and unexpected items at fixation.\n\nAbstract: Distinctive aspects of a scene can capture attention even when they are irrelevant to one's goals. The authors address whether visually unique, unexpected, but task-irrelevant features also tend to hold attention. Observers searched through displays in which the color of each item was irrelevant. At the start of search, all objects changed color. Critically, the foveated item changed to an unexpected color (it was novel), became a color singleton (it was unique), or both. Saccade latency revealed the time required to disengage overt attention from this object. Singletons resulted in longer latencies, but only if they were unexpected. Conversely, unexpected items only delayed disengagement if they were singletons. Thus, the time spent overtly attending to an object is determined, at least in part, by task-irrelevant stimulus properties, but this depends on the confluence of expectation and visual salience.",
        "final_summary": "The papers suggest that attentional engagement is a complex process influenced by various factors. Zivony 2018 found that attentional engagement differs between stimulus-driven and goal-driven capture, with goal-driven capture leading to greater engagement. Becker 2020 extended this idea by showing that attentional engagement can be context-dependent and relational, not solely based on specific feature values. Oakes 2004 highlighted the interactive influences of attentional state and attentional inertia on infants' level of attentional engagement. Abuhamdeh 2012 demonstrated that attentional involvement, or the degree of attention devoted to moment-to-moment activity, mediates the relationship between challenge-skill balance and enjoyment, as well as competence valuation and enjoyment. Zivony 2020 proposed a temporal variability account of attentional selectivity, suggesting that distractor intrusions in rapid serial visual presentation tasks result from delayed attentional engagement. Li 2020 showed that a closed-loop attention restoration theory intervention using virtual reality improved attentional engagement in young university students. Perra 2012 reported developmental changes in attention engagement during early infancy, with infants transitioning from person engagement to passive joint engagement to object engagement. Finally, Brockmole 2009 found that visually unique and unexpected items at fixation can hold attention longer, suggesting that attentional disengagement is influenced by task-irrelevant stimulus properties."
    },
    {
        "query": "Psychology and novel character development",
        "paper_list_string": "Retnomurti 2022:\n\nTitle: Psychological Approach in Personality Development of Theodore Decker\u2019s Character in \u201cThe Goldfinch\u201d\n\nAbstract: The research aims to describe the personality and the personality development of Theodore Decker through psychosocial development perspectives in the main character of The Goldfinch novel. His personality becomes an example of how family and social environment contribute to the development of one\u2019s personality. The research method used in understanding and in analysing the novel was descriptive qualitative. The research results are: According to Erik Erikson\u2019s eight stages in psychosocial development theory, Theo has passed four major stages they are the stages of initiative vs guilt (14%), industry vs inferiority (14%), identity vs role confusion (46%), and intimacy vs isolation (29%). Theo struggles to pass the stage of identity vs role confusion. It has led him to the situation where he faces difficulties on understanding his identity and role within society. Theo is grown up as a person with less empathy for others, struggles with his drug addiction, and has trouble making commitments; Erik Erikson\u2019s psychosocial development of the personality emphasizes the social influences in the development of the ego. Erikson\u2019s theory of psychosocial development has eight distinct stages. Completing each step results in a healthy personality and the acquisition of the ego to resolve subsequent crises. Conversely, failure to complete a set will lead to ability reduction in completing further stages and an unhealthier personality and sense of self. Instead of focusing on sexual development, this theory is interesting in how children socialize and how social environment affects their senses.\n\n==\n\nWibowo 2022:\n\nTitle: Defining the Study of Humanistic Personality Psychology of the Main Character of the Novel Si Anak Pelangi by Tere Liye: The Value of Character Education and Its Relevance to Learning\n\nAbstract: The purpose of this study is to examine the psychological aspects of the main character in the novel \u201cSi Anak Pelangi\u201d by Tere Liye. Moreover, this study also investigates the value of character education and its relevance to learning Indonesian in elementary school. The research method uses a descriptive qualitative approach. The data was the document of the novel which was analyzed using content analysis method. The results of this study are related to the Haberman's humanistic psychological theory, namely (1) technical learning, (2) practical learning, and (3) emancipatory learning. In addition, there are several values of character education such as (1) hard work in the form of doing activities diligently and accompanied by sincere intentions, (2) politeness in speech and behavior, (3) giving awards by sincerely accepting someone's gift, (4 ) positive, in acting both in sports and interacting with others, and (5) being patient in dealing with insults. This study concludes that this novel has suitable content for elementary school students since it contains positive values and educating characters.\n\n==\n\nYuan 2022:\n\nTitle: Research on the psychology of fictional characters based on artificial intelligence - an example study on The Family\n\nAbstract: \n Using ecological recognition based on artificial intelligence technology and Chinese psychology analysis system, by comparing the characters\u2019 dialogs in The Family, we got the main characters\u2019 big-five personality scores. In order to confirm the validity of this method, we compared the predicted scores with documents related to characters\u2019 psychological analysis and novel descriptions. The prediction results are supported by the literature and plot. This indicates that the analysis of characters\u2019 personalities by literary intelligence is effective. This research method provides data support for literary critics and quantitatively reflects the personality differences of characters in novels.\n\n==\n\nAlmond 1989:\n\nTitle: Psychological change in Jane Austen's Pride and Prejudice.\n\nAbstract: In the spirit of Jerome Frank's (1973) pioneering studies of universal characteristics in psychotherapeutic change, I have tried to demonstrate that character development in Pride and Prejudice can be understood in terms of psychoanalytic process. In the course of this thesis certain observations have emerged. Change occurs in the context of a relationship intense enough to disturb the tendency of personality to homeostasis (engagement). The change-inducing relationship is composed of a sequence of effects and countereffects (mutual influence). For these influences to be salutory (therapeutic) there must be a directional pull provided by the attitude of the primary agent of change, a pull that resonates with important motivations of the object of change (directionality). In Pride and Prejudice we recurrently found self-concept, and particularly the question of worth, to be an important interface of these phenomena. As therapists we are familiar with this as a clinical issue. Study of the novel suggests that self-esteem may play a central role in motivating therapeutic change. This observation raises intriguing unanswered questions regarding the conceptualization of self-esteem in psychoanalytic thinking.\n\n==\n\nHalperin 1996:\n\nTitle: Writing Great Characters : The Psychology of Character Development in Screenplays\n\nAbstract: This valuable book identifies and solves a major problem for writers - creating characters who are so real they literally jump of the page. Halperin has developed an easy-to-understand, logical system, which gives all screenwriters a foolproof and fail-proof method of developing great characters. \"Writing Great Characters\" is a book for all writers, from the expert who is looking to polish his techniques to the novice who wants to learn the craft from an expert.\n\n==\n\nLapsley 2006:\n\nTitle: Character Psychology and Character Education\n\nAbstract: This collection of essays provides new perspectives on the nature of character and moral education by utilizing insights from the disciplines of moral psychology, moral philosophy, and education. The volume draws from personality and developmental research as well as educational and ethical theory. Character Psychology and Character Education distinguishes itself by bringing moral philosophers, who believe that ethical reflection about virtue and character must be tied to defensible notions of personality and selfhood, into dialogue with academic psychologists, who believe that the developmental study of the moral self requires adequate grounding in various psychological literatures. The first group embraces a \"naturalized\" ethics, while the second group favors a \"psychologized\" morality. Among the topics explored in this volume are the constructs of moral selfhood, personality, and identity, as well as defensible models of character education. One of the primary arguments of the volume is that problems of character education cannot be addressed until an adequate model of character psychology is developed. In addition to the excellent theoretical essays, this collection includes applied chapters that consider the challenge of character education in the context of schools, families, and organized sports. This book will be an invaluable resource both for scholars and practitioners in the fields of psychology and education.\n\n==\n\nSamuels 2005:\n\nTitle: A social psychological view of morality: why knowledge of situational influences on behaviour can improve character development practices\n\nAbstract: Results from research in social psychology, such as findings about the fundamental attribution error and other situational influences on behaviour, are often used to justify attacking the existence of character traits. From this perspective, character development is an illusion, an impossibility, or both. We offer a different interpretation of how these issues interact with character development concerns. Rather than undermining the very idea of character traits, social psychology actually sheds light on the manner in which character development can occur. It reaffirms Spinozistic and Aristotelian points about character, namely that: (1) knowledge of the fundamental attribution error can help us minimize the influence environment and situation have on our behaviour, and (2) proper habituation only takes place in appropriately structured environments. Acknowledging these important results from social psychology helps us improve some of our character education practices rather than destroying their basis.\n\n==\n\nUtami 2013:\n\nTitle: The Psychological Changes of a Main Character in a Novel A Walk To Remember by Nicholas Sparks\n\nAbstract: Literature is a field in which most of the human\u2019s life is included on it. It is able to symbolize the presence of any literature related to human beings; behavior, thoughts, attitude, and others. Part of the literature which is generally cites many human live is a novel. In analyzing characters\u2019 characteristics in a novel, the writer examines the psychological aspects associated with the literary novel in titled A Walk to Remember by Nicholas Sparks. The objectives of the study were: to find out Landon Carter\u2019s characteristics before and after he fell in love with Jamie Sullivan and to find out the cause that makes his characteristics changed. This research uses qualitative method, which describes the data. It helps the readers to understand that love influences someone\u2019s characteristics and it also can make someone\u2019s characteristics changed. After analyzing the data, the writer found that Landon\u2019s characteristics before fall in love with Jamie are impolite, naughty, heartless, helpful, and annoying. His characteristics after fall in love with Jamie are polite, helpful, responsible, and religious. In addition, all those changes happened because of love, \nwhich is the main factor that makes Landon\u2019s characteristics changed.",
        "final_summary": "Psychology significantly influences novel character development. Retnomurti 2022 uses Erik Erikson's psychosocial development theory to analyze Theodore Decker's personality in \"The Goldfinch\". Wibowo 2022 explores the psychological aspects of the main character in \"Si Anak Pelangi\" and the value of character education. Yuan 2022 applies artificial intelligence to analyze character personalities in \"The Family\", validating this method. Almond 1989 uses psychoanalytic process to understand character development in \"Pride and Prejudice\". Lapsley 2006 discusses the relationship between character psychology and character education, emphasizing the need for a robust character psychology model. Samuels 2005 suggests that understanding social psychology and situational influences on behavior can enhance character development practices. Lastly, Utami 2013 examines the psychological changes of the main character in \"A Walk to Remember\", emphasizing love's role in character development."
    },
    {
        "query": "false step kinematics",
        "paper_list_string": "Sato 1997:\n\nTitle: Kinematical Bound States of Steps Caused by Asymmetry in Step Kinetics.\n\nAbstract: We study time evolution of parallel straight steps with repulsive interaction between steps. If step kinetics is asymmetric in the upper and the lower terraces (Schwoebel effect), a vicinal face becomes unstable when undersaturation exceeds a critical value, and an array of large bunches described by the Benney equation appears. In the one-sided model (the extreme limit of the asymmetry) a pairing instability occurs. In this case the instability always ends up with formation of step pairs, and with large undersaturation hierarchical bound states of step pairs are formed. On the contrary many-body bound states appear in the general asymmetric model.\n\n==\n\nLi 2011:\n\nTitle: Implementation of kinematic mechanism data exchangebased on step\n\nAbstract: In this paper, the first known valid implementation of kinematic mechanism based on STEP(ISO 10303, STandard for the Exchange of Product data) is presented. The result includes a generalconceptual ...\n\n==\n\nZhang 1992:\n\nTitle: Forward kinematics of a class of parallel (Stewart) platforms with closed-form solutions\n\nAbstract: This article studies the geometrical condition for closed-form solutions of forward kinematics of parallel platforms. It is shown that closed-form solutions are available if 1 rotational degree of freedom (dof) of the moving platform is decoupled from the other 5 dof. Geometrically, this condition is satisfied when five end-points at the moving platform (or at the base) are colinear. A general case that these five points do not coincide with each other is studied first and is shown to have 16 possible closed-form solutions. The variations of parallel platforms that satisfy the above-mentioned geometrical condition are then discussed. Some of them have the additional feature that the three rotational dof are fully decoupled from the 3 translational dof and their closed-form solutions are further simplified. One particular case has extremely simple forward kinematics and could be used as an alternative to the Stewart platform.\n\n==\n\nZhang 1991:\n\nTitle: Forward kinematics of a class of parallel (Stewart) platforms with closed-form solutions\n\nAbstract: The condition under which closed-form solutions of forward kinematics of parallel platforms are obtainable is explored. It is found that forward position analysis has closed-form solutions if one rotational degree of freedom (DOF) of a parallel platform is decoupled from the other five DOFs. Geometrically, this condition is satisfied when five end points at the platform or at the base are on the same line. A general case in which these five points do not coincide with each other is studied first and is shown to have 16 possible closed-form solutions. The computation of these solutions is very efficient compared to that of Stewart platforms. The different geometries of parallel platforms with closed-form solutions are discussed.<<ETX>>\n\n==\n\nZarrugh 1979:\n\nTitle: Computer generation of human gait kinematics.\n\nAbstract: Abstract The paper describes a computer program that generates absolute motion variables of human gait from predetermined relative motions. Relative displacements are measured over a range of step rates during both free (self-determined step rate at different speeds) and forced (forced step rate at a constant speed) walking, converted into harmonic coefficients and stored in an array as a function of step rate. Only six variable identifiers need to be specified to compute any absolute variable or its derivatives at any desirable step rate. The paper displays some examples of measured relative motions and reconstituted absolute variables.\n\n==\n\nWu 2010:\n\nTitle: Goal-directed stepping with momentum control\n\nAbstract: This paper proposes a technique for animating simulated characters to perform controlled steps. The desired step is controlled by high-level goals, namely step position and step duration. These stepping goals guide the desired time-varying values for the center of mass and the stepping foot which in turn lead to objectives dictating the desired changes in momentum and joint angles over the duration of the step. Our approach employs a multiobjective optimization to solve for joint accelerations from the objectives and uses inverse dynamics to compute joint torques. Our approach can guide a character with purposeful, directable steps for controlling careful navigation of the character's position and orientation. In addition, the same system can be used to create protective steps to prevent falling as a reaction to a disturbance. A novel supervisory routine automatically chooses when and where to step based on an analysis of the momentum conditions for the character. We contrast this approach to previous methods for step recovery using the inverted pendulum.\n\n==\n\nShoham 1985:\n\nTitle: Naive Kinematics: One Aspect of Shape\n\nAbstract: Ways in which physical objects interact are explored, and in particular the concept of freedom is analysed. Intuitively, the fit between two shapes in a given spatial configuration is a statement about how much one shape needs to mutilated in order to be made identical to the other. The freedom of one object with respect to another specifies what motions the First object can go through without the second one moving. The formulations, termed naive kinematics, are compared to work that was done in the kinematics of machinery in the 10th century and that has since been somewhat neglected.\n\n==\n\nSol\u00e0 2015:\n\nTitle: Quaternion kinematics for the error-state Kalman filter\n\nAbstract: A tutorial of quaternion algebra, especially suited for the error-state Kalman filter, with the aim of building Visual-Inertial SLAM and odometry systems.",
        "final_summary": "The papers do not provide direct information on the research question of \"false step kinematics.\" Sato (1997) discusses the instability of parallel straight steps. Zhang (1992) and Zhang (1991) focus on forward kinematics of parallel platforms. Zarrugh (1979) presents a computer program for generating human gait kinematics. Wu (2010) discusses goal-directed stepping with momentum control. Shoham (1985) explores physical object interactions and freedom. Sol\u00e0 (2015) provides a tutorial on quaternion kinematics for the error-state Kalman filter. None of these papers mention false step kinematics."
    },
    {
        "query": "epidermal growth factor receptors overexpression mesothelium",
        "paper_list_string": "Rena 2011:\n\nTitle: Epidermal growth factor receptor overexpression in malignant pleural mesothelioma: Prognostic correlations\n\nAbstract: To evaluate epidermal growth factor receptor (EGFR) phenotypic expression and related gene status in malignant pleural mesothelioma (MPM) and to correlate the results with patients' prognosis.\n\n==\n\nAndl 2003:\n\nTitle: Epidermal growth factor receptor mediates increased cell proliferation, migration, and aggregation in esophageal keratinocytes in vitro and in vivo.\n\nAbstract: Epidermal growth factor receptor (EGFR) overexpression is observed in a number of malignancies, especially those of esophageal squamous cell origin. However, little is known about the biological functions of EGFR in primary esophageal squamous epithelial cells. Using newly established primary human esophageal squamous epithelial cells as a platform, we overexpressed EGFR through retroviral transduction and established novel three-dimensional organotypic cultures. Additionally, EGFR was targeted in a cell type- and tissue-specific fashion to the esophageal epithelium in transgenic mice. EGFR overexpression in primary esophageal keratinocytes resulted in the biochemical activation of Akt and STAT pathways and induced enhanced cell migration and cell aggregation. When established in organotypic culture, EGFR-overexpressing cells had evidence of epithelial cell hyperproliferation and hyperplasia. These effects were also observed in EGFR-overexpressing transgenic mice and the esophageal cell lines established thereof. In particular, EGFR-induced effects upon aggregation appear to be mediated through the relocalization of p120 from the cytoplasm to the membrane and increased interaction with E-cadherin. EGFR modulates cell migration through the up-regulation of matrix metalloproteinase 1. Taken together, the functional effects of EGFR overexpression help to explain its role in the initiating steps of esophageal squamous carcinogenesis.\n\n==\n\nVelcheti 2009:\n\nTitle: Absence of mutations in the epidermal growth factor receptor (EGFR) kinase domain in patients with mesothelioma.\n\nAbstract: To the Editor: Malignant mesothelioma, a debilitating and often fatal malignancy occurs most commonly in patients with a history of exposure to asbestos. Majority of patients with pleural malignant mesothelioma present with unresectable disease. Epidermal Growth Factor Receptor (EGFR) is one of erbB family of receptor tyrosine kinases (TK) known to play a critical role in the cell proliferation and survival in malignant neoplasms. Several studies have reported an increased expression of EGFR in mesothelioma. EGFR-expressing mesothelioma cell lines seemed to be sensitive to treatment with EGFR tyrosine kinase inhibitors.1 After these observations, the Cancer and Leukemia Study Group B conducted a phase II trial of gefitinib, an EGFR TK inhibitor in 43 previously untreated patients with mesothelioma.2 Of the 28 samples tested for EGFR expression, 27 (97%) had an overexpression of EGFR. Despite EGFR overexpression, gefitinib was not active in malignant mesothelioma. EGFR expression is not an independent prognostic factor in patients with mesothelioma.2,3 There is a striking discordance between EGFR expression and lack of response to EGFR TK inhibitors in malignant mesothelioma. Mutations in exons 18\u201321 of the EGFR are associated with response to EGFR TK inhibitors in non-small cell lung cancer. There are only limited data on the prevalence of EGFR TK mutations in mesothelioma. We sought to investigate the presence of any EGFR mutations in our patients with mesothelioma. A total of 32 patients with a histologically confirmed diagnosis of mesothelioma seen at the Washington University School of Medicine were included in this study. DNA samples were obtained from paraffin-embedded tissue blocks containing representative malignant cells. High-throughput (96well plate) bidirectional dideoxynucleotide sequencing of polymerase chain reactionamplified gene products was performed at the Genome Sequencing Center (Washington University in St. Louis) as per standard protocol http://genome.wustl. edu/activity/med_seq/protocols.cgi. Exonic regions 18 \u201321, including the exon/intron boundaries of EGFR (Entrez GeneID 2065) were amplified by polymerase chain reaction. The sequence traces were assembled and scanned for variations from the reference sequence through the use of PolyPhred and PolyScan. All detailed protocols and primer sequences are available through the Washington University School of Medicine GSC website (http://genome.wustl. edu/platforms.cgi?id 7). After sequencing the DNA from the 32 mesothelioma samples, we found no evidence of mutations in our set of patients with mesothelioma. Our findings are consistent with the three earlier published studies.3\u20135 We further confirm the findings that although increased expression of EGFR is common in patients with mesothelioma, mutations of the EGFR kinase region commonly occurring in non-small cell lung cancer seem to be uncommon in mesothelioma. Lack of activating mutations in the EGFR TK domain may explain the inactivity of EGFR tyrosine kinase inhibitors in mesothelioma.\n\n==\n\nItakura 1994:\n\nTitle: Epidermal growth factor receptor overexpression in esophageal carcinoma. An immunohistochemical study correlated with clinicopathologic findings and DNA amplification\n\nAbstract: Background. Many studies have reported the increased expression of epidermal growth factor receptor (EGFR) in various human malignancies and its association with the biologic behavior of the tumors.\n\n==\n\nRamael 2005:\n\nTitle: Immunohistochemical distribution patterns of epidermal growth factor receptor in malignant mesothelioma and non-neoplastic mesothelium\n\nAbstract: An immunohistochemical study of the epidermal growth factor (EGF) receptor in non-neoplastic pleural mesothelium (35 cases) and in human malignant mesothelioma (36 cases) was made, using a murine monoclonal antibody OM-11-951. All malignant mesotheliomas and non-neoplastic pleural biopsies exhibited a strong cytoplasmic immunoreactivity in mesothelial cells. Nuclear immunoreactivity was detected in mesothelial cells of all specimens of both malignant and non-neoplastic pleura. No statistically significant differences were found between malignant mesothelioma and non-neoplastic pleural mesothelium. There were differences, between the three subtypes of mesothelioma, in the number of cells that exhibited nuclear staining. Statistically significant differences were noted between the epithelial subtype and the mesenchymal subtype (P< 0.005), epithelial subtype versus the mixed cell type (P< 0.005) and between the mesenchymal component of the mixed cell type and the mesenchymal type (P<0.0005). We conclude that there is strong expression of EGF receptor in both malignant mesothelioma and in non-neoplastic pleural mesothelium. Different staining patterns are seen when comparing the different subtypes of mesotheliomas with each other. EGF receptor expression cannot be used to distinguish between malignant and benign mesothelium.\n\n==\n\nPalm\u00e9r 1999:\n\nTitle: Epidermal growth factor receptor ligands are chemoattractants for normal human mesothelial cells.\n\nAbstract: Signalling through epidermal growth factor (EGF) receptor leads to several cellular responses including cell division and cell migration. Since EGF receptors are expressed on normal mesothelial cells, this study investigated whether EGF receptor ligands act as chemoattractants on these cells. The study used Boyden chambers fitted with filters coated with the adhesive matrix proteins fibronectin, laminin, collagen type IV and the nonmatrix adhesive molecule poly-L-lysine, for the migration studies. Normal mesothelial cells migrated to EGF receptor ligands such as EGF, transforming growth factor (TGF)-alpha and heparin-binding epidermal growth factor (HB-EGF) at concentrations ranging 0.024-100 ng x mL(-1) (with a peak stimulation at 6.25 ng x mL(-1)), if matrix proteins were present as adhesive substrates. This migration was integrin-dependent, since the same cells failed to migrate in the absence of extracellular matrix molecules or when the Boyden chamber assay was performed in the presence of anti-beta1 integrin monoclonal antibodies. These findings describe for the first time epidermal growth factor receptor ligands acting as chemoattractants on normal mesothelial cells, and that signalling through epidermal growth factor receptors leading to mesothelial cell migration also requires the activation of integrins.\n\n==\n\nClark 1985:\n\nTitle: Epidermal growth factor regulates the expression of its own receptor.\n\nAbstract: The epidermal growth factor (EGF) receptor gene is the cellular homolog of the avian erythroblastosis virus erbB oncogene. Control of EGF receptor expression determines cellular responsiveness to EGF and might play an important role in neoplastic development. Using RNA blot hybridization, we have found that exposure of human KB carcinoma cells to EGF results in elevated levels of EGF receptor mRNA. The phorbol ester 4 beta-phorbol 12-myristate 13-acetate also stimulates EGF receptor RNA accumulation. Immunoprecipitation of metabolically labeled (30 min) EGF receptor protein revealed that synthesis of new EGF receptor follows the increase in receptor RNA. Addition of cycloheximide together with EGF further enhances EGF receptor RNA accumulation. Results of nuclear runoff-transcription experiments suggest that the stimulatory effects of EGF and cycloheximide are most likely due to a posttranscriptional control mechanism.\n\n==\n\nGreen 1983:\n\nTitle: Distribution and number of epidermal growth factor receptors in skin is related to epithelial cell growth.\n\nAbstract: Epidermal growth factor (EGF), a low-molecular-weight polypeptide (G. Carpenter and S. Cohen, 1979, Annu. Rev. Biochem. 48, 193-216), stimulates the proliferation and keratinisation of cultured embryonic epidermis (S. Cohen, 1965, Dev. Biol. 12, 394-407) and promotes epidermal growth, thickening, and keratinisation when injected into neonatal mice (S. Cohen and G.A. Elliott, 1963, J. Invest. Dermatol, 40, 1-5). We have determined the distribution of the available receptors for epidermal growth factor in rat skin using autoradiography following incubation of explants with 125I-labelled mouse EGF. EGF receptors are detected on the epithelial cells overlying the basement membranes of the epidermis, sebaceous gland, and regions of the hair follicle all of which have proliferative capacity. In marked contrast, tissues which have started to differentiate and lost their growth potential, carry either an undetectable or sharply reduced number of EGF receptors. The EGF receptor number and receptor affinity of epidermal basal cells freshly isolated from rats of increasing age has also been determined. We find that receptor affinity remains unchanged (3.3 nM) but that basal cell surface receptor number decreases markedly with age. This decrease in receptor number is similar in trend to the known drop in basal cell [3H]thymidine labelling index which occurs over the same time period. The data suggest that the distribution of EGF receptors and EGF cell surface receptor number in skin are important in the spatial and temporal control of epithelial proliferation.",
        "final_summary": "EGFR overexpression in esophageal keratinocytes leads to increased cell proliferation, migration, and aggregation (Andl 2003). Despite the increased expression of EGFR in mesothelioma, mutations in the EGFR kinase domain are uncommon in this disease (Velcheti 2009). Both malignant mesothelioma and non-neoplastic pleural mesothelium exhibit strong EGFR expression, with different staining patterns observed among mesothelioma subtypes (Ramael 2005). EGF receptor ligands act as chemoattractants for normal mesothelial cells, and this migration is integrin-dependent (Palm\u00e9r 1999). EGF can regulate the expression of its own receptor, leading to increased receptor mRNA and protein synthesis (Clark 1985). The distribution and number of EGFR in the skin are related to epithelial cell growth, with higher receptor levels in proliferative regions (Green 1983)."
    },
    {
        "query": "information summarization large language models",
        "paper_list_string": "Verma 2023:\n\nTitle: Large Scale Multi-Lingual Multi-Modal Summarization Dataset\n\nAbstract: Significant developments in techniques such as encoder-decoder models have enabled us to represent information comprising multiple modalities. This information can further enhance many downstream tasks in the field of information retrieval and natural language processing; however, improvements in multi-modal techniques and their performance evaluation require large-scale multi-modal data which offers sufficient diversity. Multi-lingual modeling for a variety of tasks like multi-modal summarization, text generation, and translation leverages information derived from high-quality multi-lingual annotated data. In this work, we present the current largest multi-lingual multi-modal summarization dataset (M3LS), and it consists of over a million instances of document-image pairs along with a professionally annotated multi-modal summary for each pair. It is derived from news articles published by British Broadcasting Corporation(BBC) over a decade and spans 20 languages, targeting diversity across five language roots, it is also the largest summarization dataset for 13 languages and consists of cross-lingual summarization data for 2 languages. We formally define the multi-lingual multi-modal summarization task utilizing our dataset and report baseline scores from various state-of-the-art summarization techniques in a multi-lingual setting. We also compare it with many similar datasets to analyze the uniqueness and difficulty of M3LS. The dataset and code used in this work are made available at \u201chttps://github.com/anubhav-jangra/M3LS\u201d.\n\n==\n\nChouikhi 2022:\n\nTitle: Deep Transformer Language Models for Arabic Text Summarization: A Comparison Study\n\nAbstract: Large text documents are sometimes challenging to understand and time-consuming to extract vital information from. These issues are addressed by automatic text summarizing techniques, which condense lengthy texts while preserving their key information. Thus, the development of automatic summarization systems capable of fulfilling the ever-increasing demands of textual data becomes of utmost importance. It is even more vital with complex natural languages. This study explores five State-Of-The-Art (SOTA) Arabic deep Transformer-based Language Models (TLMs) in the task of text summarization by adapting various text summarization datasets dedicated to Arabic. A comparison against deep learning and machine learning-based baseline models has also been conducted. Experimental results reveal the superiority of TLMs, specifically the PEAGASUS family, against the baseline approaches, with an average F1-score of 90% on several benchmark datasets.\n\n==\n\nVarab 2021:\n\nTitle: MassiveSumm: a very large-scale, very multilingual, news summarisation dataset\n\nAbstract: Current research in automatic summarisation is unapologetically anglo-centered\u2013a persistent state-of-affairs, which also predates neural net approaches. High-quality automatic summarisation datasets are notoriously expensive to create, posing a challenge for any language. However, with digitalisation, archiving, and social media advertising of newswire articles, recent work has shown how, with careful methodology application, large-scale datasets can now be simply gathered instead of written. In this paper, we present a large-scale multilingual summarisation dataset containing articles in 92 languages, spread across 28.8 million articles, in more than 35 writing scripts. This is both the largest, most inclusive, existing automatic summarisation dataset, as well as one of the largest, most inclusive, ever published datasets for any NLP task. We present the first investigation on the efficacy of resource building from news platforms in the low-resource language setting. Finally, we provide some first insight on how low-resource language settings impact state-of-the-art automatic summarisation system performance.\n\n==\n\nLiu 2014:\n\nTitle: Enhanced language modeling for extractive speech summarization with sentence relatedness information\n\nAbstract: Extractive summarization is intended to automatically select a set of representative sentences from a text or spoken document that can concisely express the most important topics of the document. Language modeling (LM) has been proven to be a promising framework for performing extractive summarization in an unsupervised manner. However, there remain two fundamental challenges facing existing LM-based methods. One is how to construct sentence models involved in the LM framework more accurately without resorting to external information sources. The other is how to additionally take into account the sentence-level structural relationships embedded in a document for important sentence selection. To address these two challenges, in this paper we explore a novel approach that generates overlapped clusters to extract sentence relatedness information from the document to be summarized, which can be used not only to enhance the estimation of various sentence models but also to allow for the sentencelevel structural relationships for better summarization performance. Further, the utilities of our proposed methods and several state-of-the-art unsupervised methods are analyzed and compared extensively. A series of experiments conducted on a Mandarin broadcast news summarization task demonstrate the effectiveness and viability of our method.\n\n==\n\nPerez-Beltrachini 2022:\n\nTitle: Models and Datasets for Cross-Lingual Summarisation\n\nAbstract: We present a cross-lingual summarisation corpus with long documents in a source language associated with multi-sentence summaries in a target language. The corpus covers twelve language pairs and directions for four European languages, namely Czech, English, French and German, and the methodology for its creation can be applied to several other languages. We derive cross-lingual document-summary instances from Wikipedia by combining lead paragraphs and articles\u2019 bodies from language aligned Wikipedia titles. We analyse the proposed cross-lingual summarisation task with automatic metrics and validate it with a human study. To illustrate the utility of our dataset we report experiments with multi-lingual pre-trained models in supervised, zero- and few-shot, and out-of-domain scenarios.\n\n==\n\nLawrie 2003:\n\nTitle: Language models for hierarchical summarization\n\nAbstract: Hierarchies have long been used for organization, summarization, and access to information. In this dissertation we define summarization in terms of a probabilistic language model and use this definition to explore a new technique for automatically generating topic hierarchies. We use the language model to characterize the documents that will be summarized and then apply a graph-theoretic algorithm to determine the best topic words for the hierarchical summary. This work is very different from previous attempts to generate topic hierarchies because it relies on statistical analysis and language modeling to identify descriptive words for a document and organize the words in a hierarchical structure. \nWe compare our new technique to previous methods proposed for constructing topic hierarchies, including subsumption and lexical hierarchies. We also compare the words chosen to be part of the hierarchy to the top ranked words using TF.IDF in terms of how well each summarizes the document set. Our results show that the language modeling approach performs as well as or better than these other techniques in non user-based evaluations. We also show that the hierarchies provide better access to the documents described in the summary than does a ranked list using one of the non-user based evaluations we have developed. In a user study that compares the ability of users to find relevant instances using both the hierarchy and a ranked list to using the ranked list alone, we find that users like the information provided by the hierarchy and after some practice can use it as effectively as they can a ranked list.\n\n==\n\nSingh 2018:\n\nTitle: Long-Span Language Models for Query-Focused Unsupervised Extractive Text Summarization\n\nAbstract: Effective unsupervised query-focused extractive summarization systems use query-specific features along with short-range language models (LMs) in sentence ranking and selection summarization subtasks. We hypothesize that applying long-span n-gram-based and neural LMs that better capture larger context can help improve these subtasks. Hence, we outline the first attempt to apply long-span models to a query-focused summarization task in an unsupervised setting. We also propose the A cross S entence B oundary LSTM-based LMs, ASB LSTM and bi ASB LSTM, that is geared towards the query-focused summarization subtasks. Intrinsic and extrinsic experiments on a real word corpus with 100 Wikipedia event descriptions as queries show that using the long-span models applied in an integer linear programming (ILP) formulation of MMR criterion are the most effective against several state-of-the-art baseline methods from the literature.\n\n==\n\nAhmad 2004:\n\nTitle: Summarization of Multimodal Information\n\nAbstract: Information Summarization is one of the key challenges for current and future information systems. In this paper, we will outline a system that comprises modules for summarizing texts and time series to study the link between the two. Summaries of texts are generated using a lexical analysis of cohesion in texts focusing on key sentences that provide cohesion: by implication, these are the sentences that comprise chief points of a given text. Time series summarization is accomplished using the so-called wavelet analysis to separate out the trend, cyclical fluctuations and autocorrelational effects and generating verbal signals to describe each phenomenon. Finally, we present a case study performed on the UK financial market with regards to multimodal information processing, namely textual and numerical summarization.",
        "final_summary": "Verma 2023 introduces a large-scale multi-lingual multi-modal summarization dataset. Chouikhi 2022 explores the use of deep Transformer-based Language Models (TLMs), specifically the PEAGASUS family, for Arabic text summarization and finds them to outperform baseline approaches. Varab 2021 presents a large-scale multilingual summarization dataset and discusses the challenges of low-resource languages. Liu 2014 enhances language modeling for extractive speech summarization by incorporating sentence relatedness information. Perez-Beltrachini 2022 introduces a cross-lingual summarization corpus for multiple European languages. Lawrie 2003 uses language models for hierarchical summarization, demonstrating their effectiveness in generating topic hierarchies. Singh 2018 investigates the effectiveness of long-span language models for query-focused unsupervised extractive text summarization. Ahmad 2004 discusses the summarization of multimodal information, combining text and time series analysis."
    },
    {
        "query": "why is forecasting weekly oil prices useful? who would benefit from that? how are weekly oil prices more useful than monthly oil prices?",
        "paper_list_string": "Degiannakis 2018:\n\nTitle: Forecasting oil prices: High-frequency financial data are indeed useful\n\nAbstract: The paper examines the importance of combining high frequency financial information, along with the oil market fundamentals, in order to gain incremental forecasting accuracy for oil prices. Inspired by French et al. (1987) and Bollerslev et al. (1988), who maintain that future asset returns are also influenced by past volatility, we use daily volatilities and returns from financial and commodity markets to generate real out-of-sample forecasts for the monthly oil futures prices. Our results convincingly show that although the oil market fundamentals are useful for long-run forecasting horizons, the combination of the latter with high-frequency financial data significantly improve oil price forecasts, by reducing the RMSE of the no-change forecast by approximately 68%. Results are even more impressive during the oil price collapse period of 2014\u201315. These findings suggest that we cannot ignore the information extracted from the financial markets when forecasting oil prices. Our results are both statistically and economically significant, as suggested by several robustness tests.\n\n==\n\nYanagisawa 2009:\n\nTitle: Usefulness of the Forward Curve in Forecasting Oil Prices\n\nAbstract: When people analyse oil prices, the forward curve is often referred to as it reflects the average view among market participants. In this paper, to what extent the forward curve provides useful information in forecasting oil prices was analysed quantitatively. Although the usefulness of the forward curve is confirmed in forecasting oil prices, the effect in reducing forecast error is small. Additionally, the forward curve is actually useful for one week ahead and for one month ahead in daily and weekly forecasts, respectively. However, the forward curve is scarcely useful in long-term forecast.\n\n==\n\nBaumeister 2012:\n\nTitle: What Central Bankers Need to Know About Forecasting Oil Prices\n\nAbstract: Recent research has shown that recursive real-time VAR forecasts of the real price of oil tend to be more accurate than forecasts based on oil futures prices of the type commonly employed by central banks worldwide. Such monthly forecasts, however, differ in several important dimensions from the forecasts central banks require when making policy decisions. First, central banks are interested in forecasts of the quarterly real price of oil rather than forecasts of the monthly real price of oil. Second, many central banks are interested in forecasting the real price of Brent crude oil rather than any of the U.S. benchmarks. Third, central banks outside the United States are interested in forecasting the real price of oil measured in domestic consumption units rather than U.S. consumption units. Addressing each of these three concerns involves modeling choices that affect the relative accuracy of alternative forecasting methods. In addition, we investigate the costs and benefits of allowing for time variation in VAR model parameters and of constructing forecast combinations. We conclude that quarterly forecasts of the real price of oil from suitably designed VAR models estimated on monthly data generate the most accurate forecasts among a wide range of methods including forecasts based on oil futures prices, nochange forecasts and forecasts based on models estimated on quarterly data.\n\n==\n\nAmin-Naseri 2007:\n\nTitle: A Hybrid Artificial Intelligence Approach to Monthly Forecasting of Crude Oil Price Time Series\n\nAbstract: Due to the important role of crude oil in the global economy, oil price is a key factor affecting economic plans and decisions of governments and commercial firms. Therefore, proactive knowledge of its future movements can lead to better decisions in various governmental and managerial levels. However, oil price forecasting with a satisfying accuracy has proved to be a difficult task because of the complex underlying mechanism governing oil price evolution. This paper proposes a hybrid artificial intelligence model for monthly crude oil price forecasting by means of feed-forward neural networks, genetic algorithm and k-means clustering. In order to evaluate the performance of the model, its forecasts are compared with those of the econometric model of Energy Information Administration as the best representative of econometric models, as well as three artificial intelligence models from the literature. Results show that our proposed model outperforms the above mentioned models.\n\n==\n\nZhang 2019:\n\nTitle: Do high-frequency stock market data help forecast crude oil prices? Evidence from the MIDAS models\n\nAbstract: Extensive studies have used stock market information to forecast crude oil prices, and stock market can more easily derive high-frequency data than crude oil market due to no revisions, which raises a question that whether high-frequency stock market data can improve the forecast performance of crude oil prices. Therefore, this paper employs the MIDAS model and the high-frequency data of four stock market indices to forecast WTI and Brent crude oil prices at lower frequency. The results indicate that the high-frequency stock market indices have certain advantage over the lower-frequency data in forecasting monthly crude oil prices, and the MIDAS model using high-frequency data proves superior to the ordinary model.\n\n==\n\nBaumeister 2013:\n\nTitle: Forecasting the Real Price of Oil in a Changing World: A Forecast Combination Approach\n\nAbstract: The U.S. Energy Information Administration (EIA) regularly publishes monthly and quarterly forecasts of the price of crude oil for horizons up to 2 years, which are widely used by practitioners. Traditionally, such out-of-sample forecasts have been largely judgmental, making them difficult to replicate and justify. An alternative is the use of real-time econometric oil price forecasting models. We investigate the merits of constructing combinations of six such models. Forecast combinations have received little attention in the oil price forecasting literature to date. We demonstrate that over the last 20 years suitably constructed real-time forecast combinations would have been systematically more accurate than the no-change forecast at horizons up to 6 quarters or 18 months. The MSPE reductions may be as high as 12% and directional accuracy as high as 72%. The gains in accuracy are robust over time. In contrast, the EIA oil price forecasts not only tend to be less accurate than no-change forecasts, but are much less accurate than our preferred forecast combination. Moreover, including EIA forecasts in the forecast combination systematically lowers the accuracy of the combination forecast. We conclude that suitably constructed forecast combinations should replace traditional judgmental forecasts of the price of oil.\n\n==\n\nChatziantoniou 2019:\n\nTitle: Futures-based forecasts: How useful are they for oil price volatility forecasting?\n\nAbstract: Oil price volatility forecasts have recently attracted the attention of many studies in the energy finance field. The literature mainly concentrates its attention on the use of daily data, using GARCH-type models. It is only recently that efforts to use more informative intraday data to forecast oil price realized volatility have been made. Despite all these previous efforts, no study has examined the usefulness of futures-based models for oil price realized volatility forecasting, although the use of such models is extensive for oil price predictions. This study fills this void and shows that futures-based forecasts based on intra-day data provide informative forecasts for horizons that span between 1-day and 66-days ahead. More importantly, these results hold true even during turbulent times for the oil market, such as the Global Financial Crisis of 2007-09 and the oil collapse period of 2014-15.\n\n==\n\nDelis 2021:\n\nTitle: What matters when developing oil price volatility forecasting frameworks\n\nAbstract: Forecasting oil price volatility is considered of major importance for numerous stakeholders, including, policy makers, industries and investors. This paper examines and evaluates the main factors that oil price volatility forecasters should take before constructing their forecasting models. Such factors are related to: i) direct vs iterated forecasts, ii) the incorporation of continuous and jump components, iii) the importance of semi variance volatility measures, and iv) OLS vs time-varying parameter (TVP) estimation procedures. Even more, we evaluate the performance of these factors for both realized and implied volatility measures, based on statistical loss functions, as well \nas, their economic use. The results show that depending on whether end-users are interested in forecasting the realized or the implied volatility, the factors influencing the accuracy of forecasts are different. In particular, for the realized volatility, direct forecasting based on TVP estimation procedure, as well as, using the information obtained in the semi variance measures are capable of producing significantly superior forecasts. By contrast, separating the continuous and the jump components of the realized volatility does not provide any added value to these forecasts. Turning to the OVX, based on the economic evaluation of our forecasts, the TVP estimation procedure \nseems to performbetter. In addition, we find evidence that the continuous component and the semi variance measures of the realized volatility also yield better OVX forecasts in the longer run horizons.",
        "final_summary": "The papers suggest that forecasting weekly oil prices is useful for various stakeholders. Degiannakis 2018 demonstrates that combining high-frequency financial data with oil market fundamentals improves oil price forecasts, particularly during periods of oil price collapse. Yanagisawa 2009 supports the usefulness of the forward curve in short-term oil price forecasting, specifically for one week and one month ahead. Baumeister 2012 highlights the superiority of recursive real-time VAR forecasts over oil futures prices for central banks' quarterly forecasting needs. Amin-Naseri 2007 proposes a hybrid artificial intelligence model that outperforms econometric and other artificial intelligence models in monthly crude oil price forecasting. Zhang 2019 shows that high-frequency stock market data, when used with the MIDAS model, enhance the forecast performance of monthly crude oil prices. Baumeister 2013 suggests constructing forecast combinations of real-time econometric models for accurate oil price forecasts, surpassing judgmental forecasts and official forecasts from the U.S. Energy Information Administration. Chatziantoniou 2019 finds that futures-based models using intra-day data provide informative forecasts for oil price realized volatility. Delis 2021 emphasizes the importance of factors such as direct vs. iterated forecasts, continuous and jump components, semi variance measures, and estimation procedures in constructing oil price volatility forecasting frameworks. These findings collectively demonstrate the usefulness of weekly oil price forecasting for various stakeholders, including central banks, policymakers, industries, and investors."
    },
    {
        "query": "topical benefifical",
        "paper_list_string": "He 2016:\n\nTitle: Extracting Topical Phrases from Clinical Documents\n\nAbstract: \n \n In clinical documents, medical terms are often expressed in multi-word phrases. Traditional topic modelling approaches relying on the \"bag-of-words\" assumption are not effective in extracting topic themes from clinical documents. This paper proposes to first extract medical phrases using an off-the-shelf tool for medical concept mention extraction, and then train a topic model which takes a hierarchy of Pitman-Yor processes as prior for modelling the generation of phrases of arbitrary length. Experimental results on patients' discharge summaries show that the proposed approach outperforms the state-of-the-art topical phrase extraction model on both perplexity and topic coherence measure and finds more interpretable topics.\n \n\n\n==\n\nPorras-Luque 2007:\n\nTitle: [Topical antimicrobial agents in dermatology].\n\nAbstract: Topical antimicrobial agents are chemical substances that, directly applied to the skin, inhibit the growth or destroy any microorganism, either fungi, viruses or bacteria. Within this term, we generally refer to those that are active against the latter. They are divided into antiseptics and antibiotics. Antiseptics are directly applied to a living organism to eradicate the existing microorganism on the mucocutaneous surfaces, preventing their proliferation. Topical antibiotics are either produced by living organisms or manufactured through synthesis and are mainly used to fight infections. Topical antimicrobials represent an important option in the prophylaxis and treatment of primary and secondary superficial bacterial infections. Antibiotics for topical use have a lower incidence of systemic toxicity, secondary effects and development of resistance than parenteral antibiotics. The ideal topical antimicrobial must have a broad spectrum of activity, fast and prolonged antibacterial effect, bactericidal activity, easy diffusion through tissues and detritus, with minimal toxicity, very low or null incidence of irritation or allergy and sparse activity against normal skin flora. The approach of this article is the description of the classical and new antimicrobials that are, more important due to their wide use and proven efficacy. We review their pharmacological characteristics, spectrum of activity, possible secondary effects and interactions, as well as the combinations that improve their antimicrobial activity, main indications and possible development of resistance, all this properly documented.\n\n==\n\n\u30d1\u30f3\u30c9\u30e4\u3001\u30a2\u30ca\u30f3\u30c8\u30fb\u30b1\u30fc 2006:\n\nTitle: Topical compositions and the use thereof\n\nAbstract: Suitable for topical application to the skin, in a composition comprising a fugitive solvent base comprising at least one alcohol, to provide an emollient component. One of the advantages of the present invention is to reduce the irritancy potential of the alcoholic fugitive solvent base with the composition.\n\n==\n\n\u30c0\u30b9\u30b0\u30d7\u30bf\uff0c\u30d3\u30d0\u30c4\u30b7\u30e6\u30fb\u30e9\u30f3\u30b8\u30e4\u30f3 2007:\n\nTitle: Topical compositions and the use thereof\n\nAbstract: Topical compositions and describes a method for improving skin characteristics. Topical composition comprising a mixture of physical scatterers having a fully equipped particle size of up to 2 microns beyond the 100nm below and 300nm 5. The topical compositions of the present invention, when applied, brings the brightness of the desired skin, no redness and blemishes are often characterized, red intensity of less than -1 75 micron thickness of the film of the topical composition exhibit (a).\n\n==\n\nAhmed 2016:\n\nTitle: Topical Gel Formulation : A review\n\nAbstract: The motivation behind composing this survey on pharmaceutical gel was to aggregate the late writing with unique spotlight on discerning way to deal with topical detailing and fundamental parts of topical medication conveyance frameworks. Topical use of medications offers potential favorable circumstances of conveying the medication specifically to the site of activity and acting for an augmented timeframe. Skin is a standout amongst the most broad and promptly open organs on human body for topical organization and is primary course of topical medication conveyance framework. Gels have better potential as a vehicle to controlled medication topically in contrast with balm, since they are non-sticky requires low vitality amid the plan. Topical gels are proposed for skin application on the other hand to certain mucosal surfaces for neighborhood activity or percutaneous infiltration of medicament or for their emollient or defensive activity. Gels are assessed by taking after parameters, for example, pH, drug content, thickness (Brookfield viscometer), spreadability, and extrudability, skin disturbance on templates, in-vitro discharge, in steadiness. By and large, the clinical confirmation shows that topical gel is a sheltered and powerful treatment choice for use in the administration of skin related illnesses.\n\n==\n\nHuang 2000:\n\nTitle: Topical anesthetics in dermatology.\n\nAbstract: Topical anesthetics are valuable tools for the dermatologists. The purpose of this study is to educate dermatologists about the pharmacology and clinical applications of topical anesthetics. A review of the current topical anesthetics was conducted. Several topical anesthetics are available to provide safe and effective cutaneous analgesia. Patient care can be improved by reducing the discomfort of local procedures and by minimizing the side effects of the anesthetics.\n\n==\n\nBoyce 1982:\n\nTitle: Beyond topicality : A two stage view of relevance and the retrieval process\n\nAbstract: Abstract Topicality is an operationally necessary but insufficient condition for requestor judged relevance. Documents are independent of one another as to any judgement of their topicality but not independent as to any judgement of their relevance which is a function of their informativeness to a requestor. Recall depends solely upon topicality but precision depends upon informativeness as well. A retrieval system which aspires to the retrieval of relevant documents should have a second stage which will order the topical set in a manner so as to provide maximum informativeness to the requestor. Should a system be concerned only with topicality then a two stage system which generates a high recall set and discards imprecise documents by measuring their distance from a seed document can be iterated to provide topicality feedback without user input.\n\n==\n\nHadgraft 2016:\n\nTitle: Advanced topical formulations (ATF).\n\nAbstract: Topical formulations aim to target the skin for a variety of cosmetic, protective or therapeutic needs. Despite the use of creams and ointments over the millennia, the bioavailability of actives from topical preparations remains quite low, often not exceeding 1-2% of the applied dose. In this review we examine the reasons underlying the poor performance of topical preparations. We also outline a rational approach, based on Fick's laws of diffusion, to develop advanced topical formulations. Methodologies which are currently used in research and development are critically examined and the importance of understanding the fate of the vehicle as well as the active is emphasised. Advanced topical formulation development will also be facilitated by emerging and sophisticated analytical techniques that are able to probe real time delivery of actives to the skin. A good understanding of the underlying physical chemistry of both the formulation and the skin is crucial in the development of optimised topical products.",
        "final_summary": "Topical formulations have diverse beneficial applications. Porras-Luque 2007 highlights the effectiveness of topical antimicrobial agents in treating superficial bacterial infections. He 2016 introduces a method for extracting topical phrases from clinical documents, enhancing topic interpretability. \u30d1\u30f3\u30c9\u30e4\u3001\u30a2\u30ca\u30f3\u30c8\u30fb\u30b1\u30fc 2006 discusses the development of topical compositions with a fugitive solvent base to reduce irritancy potential. \u30c0\u30b9\u30b0\u30d7\u30bf\uff0c\u30d3\u30d0\u30c4\u30b7\u30e6\u30fb\u30e9\u30f3\u30b8\u30e4\u30f3 2007 focuses on topical compositions that improve skin characteristics. Ahmed 2016 underscores the advantages of topical gel formulations for localized drug delivery. Huang 2000 emphasizes the value of topical anesthetics in dermatology. Boyce 1982 proposes a two-stage retrieval process to increase the relevance of topical documents. Lastly, Hadgraft 2016 addresses the challenges in achieving optimal bioavailability of actives in topical formulations and suggests a rational approach for their development."
    },
    {
        "query": "job performance definition",
        "paper_list_string": "Ramawickrama 2017:\n\nTitle: A Synthesis towards the Construct of Job Performance\n\nAbstract: Job performance is highly relevant for organizations and individuals alike. Individual Job performance is the behavioural outcome of an employee which points out that the employee is showing positive attitudes towards his or her organization. Job performance is differently defined and measured in different disciplines in different ways. The main purpose of this paper is to define and to review theoretically and empirically the concept of job performance, measurement dimensions of job performance and empirical findings for measurement dimensions of job performance with reference to the various professions in service oriented organizations. As a desk research, this study reviewed literature regarding job performance and its dynamic nature, compared and analyzed dimensions (taxonomies) related to job performance, created a new definition and explained the importance of job performance adding novelty to the existing literature and provided suggestions for further studies.\n\n==\n\nMing-sum 1998:\n\nTitle: A Job Performance Model for Professional Social Workers\n\nAbstract: This paper defines job performance as a social construct from different perspectives, such as standards, behaviour, and process. A comprehensive model was constructed to explain the multiple realities of the job performance of professional social workers. The different perspectives of job performance: organizational performance, staff performance and quality of service were discussed. The dynamic relationships between various factors behind these components were also identified. Supervision and professional development for social workers, goals setting and strategy formulation for human service organizations, and programme evaluation for service delivery were proposed as guidelines for improving job performance of professional social workers.\n\n==\n\nSonnentag 2010:\n\nTitle: Job Performance\n\nAbstract: Individual performance is of high relevance for organizations and individuals alike. Showing high performance when accomplishing tasks results in satisfaction, feelings of selfefficacy and mastery (Bandura, 1997; Kanfer et aL, 2005). Moreover, high performing individuals get promoted, awarded and honored. Career opportunities for individuals who perform well are much better than those of moderate or low performing individuals (Van Scotter et aI., 2000). This chapter summarizes research on individual performance and addresses performance as a multi-dimensional and dynamic concept. First, we define the concept of performance, next we discuss antecedents of between-individual variation of performance, and describe intraindividual change and variability in performance, and finally, we present a research agenda for future research.\n\n==\n\nViswesvaran 2008:\n\nTitle: Job Performance: Assessment Issues in Personnel Selection\n\nAbstract: An important construct in Industrial, Work and Organizational (TWO) psychology, organizational behavior, and human resources management (personnel selection, training, and performance evaluation) in general, and personnel selection in particular, is the construct of job performance. Job performance is the most important dependent variable in TWO psychology (Schmidt & Hunter, 1992). A general definition of the construct of job performance reflects behaviors (both visually observable and non-observable) that can be evaluated (Viswesvaran, Ones, & Schmidt, 1996). In other words, job performance refers to scalable actions, behaviors, and outcomes that employees engage in or bring about that are linked with and contribute to organizational goals (Viswesvaran & Ones, 2000). To date, most researchers focusing on the construct of job performance have confined themselves to particular situations and settings with no attempt to generalize their findings. Also, there has been an emphasis on prediction and practical application rather than explanation and theory building. The consequence of these two trends has been a proliferation of the various measures of job performance in the extant literature. Virtually every measurable individual differences dimension thought to be relevant to the productivity, efficiency, or profitability of the unit or organization has been used as a measure of job performance. Absenteeism, productivity ratings, violence on the job, and teamwork ratings are some examples of the variety of measures used to measure job performance. There are multiple uses for job performance data. In selection contexts, measures of job performance are used to validate predictors. \u2018Thus, the choice of the job performance measure has important substantive implications for our practice and science of personnel selection. Measures of individual job performance play a central role at each step of the personnel selection function. Consider the first step in selection: recruitment of qualified applicants. One question in recruitment is whether the different sources of recruitment result in attraction of individuals who differ in job performance levels (Barber, 1998). Following successful recruitment efforts, attempts are made to identify individual differences variables that are related to individual differences in job performance, and select individuals based on those characteristics (Guion, 1998). Individual differences in job performance are assessed and those assessments are used in placement and promotion decisions.\n\n==\n\nJohnson 2009:\n\nTitle: A multi-level investigation of overall job performance ratings\n\nAbstract: Multi-level modeling was used to understand how supervisor assign overall job performance ratings. Results indicated a uniform relationship between task and overall performance ratings across supervisors but significant variability in the relationship between contextual and overall performance ratings. Employee and supervisor attributes were examined to explain this variability. Job performance is typically conceptualized as \" actions and behaviors that are under the control of the individual that contribute to the goals of the organization, \" (Rotundo & Sackett, 2002, p. 66). Campbell (1990) asserts that job performance is inherently multidimensional, a view that is shared by the vast majority of performance researchers (e. However, it is also generally accepted that, while multidimensional conceptions of performance are appropriate when conducting research, for decision making in organizations, a unidimensional, or composite criteria is preferred (e.g., Schmidt & Kaplan, 1971). And while the notion that raters will differ in the way they combine information to arrive at an overall rating is far from new (e.g., Naylor & Wherry, 1965), in the job performance domain, research dedicated to explaining this variability is limited. Performance Task performance refers to behaviors and activities that support the organization's technical core, which can involve the execution of technical processes (transforming raw materials into the goods or services provided by the organization) or the maintenance of those processes, for instance by providing raw materials, distributing products, or through planning and coordination functions (Borman & Motowidlo, 1993; Motowidlo, et al., 1997). The term contextual performance was coined by Borman and Motowidlo (1993) who argued that performance measures used in selection research and practice ignored activities such as persisting, helping, and endorsing organizational objectives. They argue that the criterion domain consists of task performance as well as contextual performance, or behaviors that support the broader psychological and social environment in which that technical core must function. An individual's overall performance rating can be thought of as a measure of his or her organizational worth (Motowidlo & Van Scotter, 1994). Implicit in this definition is that job performance must include only those behaviors that contribute to the organization's goals (Campbell, 1990). Accordingly, the rating assigned to an incumbent's overall job performance will depend not only on the level at which he or she performs certain behaviors, but also on the rater's beliefs about the goals of the organization and his or her mental model relating job behaviors to those goals. According to \u2026\n\n==\n\nMotowidlo 2012:\n\nTitle: 5 Job Performance\n\nAbstract: This chapter presents an overview of job performance as it is conceptualized in the industrial\u2013organizational psychology literature. It includes a definition of job performance that emphasizes the behavioral, episodic, and aggregate nature of the construct. The chapter reviews and discusses several well-established multidimensional models of job performance, along with emerging theories of performance such as adaptive performance and trait activation. Causal antecedents of job performance are discussed, including personality traits, cognitive ability, motivation, knowledge, and skill, along with their interrelations. Various dilemmas in theories of performance are explored, including whether organizational citizenship behavior is likely a latent or aggregate construct, whether counterproductive work behavior and organizational citizenship should be considered opposite ends of a single continuum or separate constructs, and how counterproductive work behavior could be a product of knowledge and skill. \n \n \nKeywords: \n \njob performance; \ncontextual performance; \norganizational citizenship behavior; \ncounterproductive work behavior; \nimplicit trait policies\n\n==\n\nCarlos 2016:\n\nTitle: Development and Validation of a Self-Reported Measure of Job Performance\n\nAbstract: The current turbulent context in which we live in requires, more and more, that organisations focus on improving the workers\u2019 performance. Individual performance is more than just the execution of specific tasks and it involves an ample variety of organisational activities that have important implications for the understanding and measurement of job performance. Considering the fact that most individual performance measures are developed in order to be applied in particular job-related contexts or cultures, our goal is to develop a job performance measure that might be applicable across jobs and cultures. After an extensive literature review, and based on studies that were developed in different cultural and job-related contexts, two dimensions\u2014task and contextual\u2014and eight sub dimensions of job performance were found: job knowledge, organisational skills, efficiency, persistent effort, cooperation, organisational consciousness, personal characteristics and interpersonal and relational skills. Confirmatory factorial analysis was used in order to test their relevance. The dimensions \u2018personal characteristics\u2019 and \u2018persistent effort\u2019 were merged. The resulting 29 item scale presents appropriate psychometric properties.\n\n==\n\nAketch 2012:\n\nTitle: Effects of quality of work life on job performance: theoretical perspectives and literature review\n\nAbstract: The concept of quality of work life exhibits positive emotional reactions and attitudes an individual has towards their job. It has been conceptualized as a general attitude toward the job. Job performance is often viewed as the degree to which employees execute their job tasks, responsibilities and assignments adequately. Motivation is the willingness to work at a certain level of effort and drives employees to action. Motivation emerges out of needs, values, goals, intentions and expectations. The study utilizes desk research to review literature on quality of work life that informs organization performance and motivation. The importance of considering these factors is demonstrated in the strong relationship between employee\u2019s well-being at work and performance of such organizations.",
        "final_summary": "Job performance is a multidimensional construct that varies across different disciplines and professions (Ramawickrama, 2017). Ming-sum (1998) presents a model that considers organizational performance, staff performance, and quality of service as components of job performance. Sonnentag (2010) discusses job performance as a dynamic concept, addressing individual variation and intraindividual change. Viswesvaran (2008) defines job performance as scalable actions, behaviors, and outcomes that contribute to organizational goals. Johnson (2009) explores the relationship between task and contextual performance ratings, highlighting the influence of employee and supervisor attributes. Motowidlo (2012) discusses job performance, including emerging theories such as adaptive performance and trait activation. Carlos (2016) develops a self-reported measure of job performance, incorporating various dimensions. Aketch (2012) emphasizes the relationship between quality of work life and job performance."
    },
    {
        "query": "Main findings and conflicts about the urbanization of agricultural land in Chile",
        "paper_list_string": "Madaleno 2004:\n\nTitle: \u201cUrban versus rural\u201d no longer matches reality: an early public agro-residential development in periurban Santiago, Chile\n\nAbstract: Abstract The paper discusses the validity of the \u201curban versus rural\u201d dichotomy in scientific literature, statistics and urban planning practice, especially in empirical studies developed around the issue of vegetable and animal farming inside and around cities. Santiago\u2019s rural\u2013urban interface is presented as a case study. Sample results extracted in early 2003 have shown that farming mixed with other land uses is even now a reality in Southern Santiago, supported in solidarity alliances and a characteristic life philosophy, giving researchers the possibility of exploring a living ecocity laboratory.\n\n==\n\nBarrado 2020:\n\nTitle: Promoted Urbanization of the Countryside: The Case of Santiago\u2019s Periphery, Chile (1980\u20132017)\n\nAbstract: Urbanization of the countryside affects rural areas, especially in the immediate surroundings of large cities. Normally, this occurs as an unpromoted process, but in Chile, it is driven by the legal framework. This research focuses on rural residential plots (RRPs) around the capital city, Santiago. The analysis seeks to understand the significance and consequences of RRPs during the last four decades and the role of a favorable legal framework in affecting their development. By examining data and official cartography on rural residential plots, the analysis shows a large phenomenon of rapid RRP development in the Metropolitan Region of Santiago de Chile (MR). The study confirms the existence of an ongoing process that is still partially latent and potentially both uncontrolled and evolving. This work demonstrates the negative effect that land liberalization policies can have by promoting territorial transformations that policymakers cannot subsequently control. The conclusions provide a critical perspective on the counter-urbanization process in the context of fragility and scarce resources.\n\n==\n\nContesse 2018:\n\nTitle: Is urban agriculture urban green space? A comparison of policy arrangements for urban green space and urban agriculture in Santiago de Chile\n\nAbstract: Urban green spaces are crucial for citizens\u2019 wellbeing. Nonetheless, many Latin American cities struggle to provide sufficient and equitable green space distribution for their citizens. By looking at the Chilean capital Santiago as an example, this paper examines whether the growing urban agriculture movement provides a feasible opportunity to increase public urban green space access. It does so by using the policy arrangement approach to analyse change and stability in two policy domains: urban green space planning and urban agriculture. The paper investigates urban green spaces and urban agriculture and the role of practitioners, urban planners and policymakers. The analysis found opportunities for urban agriculture to facilitate the expansion of urban green spaces in Santiago if policy mechanisms enable private or public spaces to be maintained by citizen organizations. Such mechanisms may, however, encounter resistance from public agencies, as it is unresolved who is involved and who benefits from urban agriculture. The paper concludes that urban agriculture is an opportunity for urban greening in Santiago, although changes are needed in how green areas are planned and conceived. Additionally, urban agriculture should not be understood as a substitute for parks but as a complementary form of green space provision with a distinctive value.\n\n==\n\nPauchard 2006:\n\nTitle: Multiple effects of urbanization on the biodiversity of developing countries: the case of a fast-growing metropolitan area (Concepci\u00f3n, Chile).\n\nAbstract: Urbanization is increasingly homogenizing the biota of less developed countries. Even though urban sprawl is a worldwide problem, most studies on the effects of urbanization, and the conceptual models have focused on developed countries. South America has not escaped urbanization, and here we discuss the potential impacts of urban sprawl with respect to three ecosystems in the metropolitan area of Concepcion, Chile. We consider this area a good model and fairly representative of other cities in developing countries which are also experiencing rapid and uncontrolled growth. We found that the impacts of urban sprawl on biodiversity in the metropolitan area of Concepcion differ little from cities in other parts of the world: native ecosystems are replaced by pavements and buildings and what is left of the natural soil is covered with green areas dominated by non-native ornamental species. Wetlands and other peri-urban ecosystems are rapidly being destroyed, fragmented or invaded by non-native species. We found that from a study area of 32,000 ha, there was a net loss to urbanization of 1734 ha of wetlands (23% of the original) and 1417 ha (9%) of agricultural, forest and shrub land cover types between 1975 and 2000. From the total area urbanized (3151 ha), 55% corresponded to wetlands and 45% to agricultural, forest and shrub lands cover types. We see the lack of environmental awareness as a major cause of the increasing deterioration of biodiversity in urban areas of developing countries. More research is needed to fully understand the effects of urban sprawl on the biodiversity of developing countries to include these ecosystems in global conservation strategies.\n\n==\n\nGeisse 1978:\n\nTitle: Urbanizaci\u00f3n e industrializaci\u00f3n en Chile.\n\nAbstract: INTRODUCCION En este trabajo se sintetizaran los principales aspectos desarrollados con mayor detalle en un proyecto de investigacion mas extenso sobre las interrelaciones entre desarrollo economico y la urbanizacion en el caso chileno durante el presente siglo hasta fines de la decada de los anos sesenta . El campo de referencia del estudio es el sistema urbano nacional, entendido como el componente principal de la integracion territorial de las diferentes actividades economicas y de las estructuras de clases y de poder. El supuesto basico, entonces, ha sido que los sistemas urbanos no son sino manifestaciones espaciales de este conjunto de estructuras y relaciones sociales. Por ello, desde un punto de vista metodologico, el estudio de su funcionamiento y transformaciones solo puede ser abordado a la luz de las leyes que rigen el\n\n==\n\nFoster 2016:\n\nTitle: Geographic disparities in rural land appreciation in a transforming economy: Chile, 1980 to 2007\n\nAbstract: This paper reports on a research effort to gather and analyze rural land value data during a period of unprecedented growth in Chilean agriculture. This information is important to understand the geographical distribution of gains associated with the transformation of the rural sector during a period of rapid development, trade liberalization and transition toward a predominant emphasis on export earnings in agriculture. A large set of data of rural land transactions for 1980, 1990, 1997 and 2007 were collected from a sample of land registry offices. Results show notable declines in the physical size of transactions, significant average annual rates of increase in real per-hectare values, and a small-parcel premium for rural land associated with non-farm land use. Overall real land values have increased faster than the average annual growth rates in the agricultural sector\u2019s value added, suggesting that land owners have gained proportionately more than other claimants to sectoral income. Tests show significant geographic disparities in annual rates of land appreciation across regions and municipalities. Consistent with differential net gains due to integration into world markets and the geographic heterogeneity of suitability for different land uses, northern areas, with greater emphasis on export-oriented crops, have experienced the highest average rates of annual real per-hectare value growth, in the order of 7 percent, while southern areas, emphasizing traditional crops and pastures/livestock, have experienced growth rates of half that. Geographic disparities are also explained by proximity to urban population and income centers.\n\n==\n\nMarques 2012:\n\nTitle: Urban Agriculture in the Metropolitan Area of Santiago de Chile An Environmental Instrument to Create a Sustainable Urban Model\n\nAbstract: Currently, the city of Santiago de Chile represents an example of the rapid and expansive development experienced by capitals of developing countries. If Santiagos urban model should be classified, the balance tips towards the dispersed city model, characterized by consuming large amounts of resources faster than it can produce them. This paper explores the possible implementation strategies of urban agriculture in Santiago de Chile, related to urban policies and regulations. Urban agriculture is part of the toolkit that enable sustainable social and\n\n==\n\nAz\u00f3car 2007:\n\nTitle: Urbanization patterns and their impacts on social restructuring of urban space in Chilean mid-cities: The case of Los Angeles, Central Chile\n\nAbstract: Abstract Urban growth patterns of Chilean mid-cities are explored, analyzing land use/land cover changes between 1955\u20131978 and 1978\u20131998 in Los Angeles, a mid-city located in central Chile. Urban sprawl is analyzed with respect to population growth and possible effects on the city's socio-spatial structure. Markov's matrix was used to analyze sequences of land use/land cover changes, whereas regression analysis was used to study the relationship between city and population growth. The study is based on remotely sensed data and processed in a GIS environment using Arcview 3.2, IDRISI, and Arc-Info N.T.",
        "final_summary": "The urbanization of agricultural land in Chile is a complex issue with multiple facets. Madaleno (2004) discusses the coexistence of farming and other land uses in periurban Santiago, challenging the traditional urban-rural dichotomy. Barrado (2020) highlights the rapid development of rural residential plots around Santiago, driven by a legal framework that has led to uncontrolled and evolving territorial transformations. Contesse (2018) sees urban agriculture as a potential solution to increase public urban green space access in Santiago, but notes that policy changes are needed for this to be effective. Pauchard (2006) underscores the negative impacts of urbanization on biodiversity in Concepci\u00f3n, with a significant loss of native ecosystems. Foster (2016) provides an analysis of the geographical disparities in rural land appreciation in Chile, linking faster land value increases to areas with greater emphasis on export-oriented crops. Marques (2012) proposes urban agriculture as a tool for creating a sustainable urban model in Santiago. Lastly, Az\u00f3car (2007) studies the urban growth patterns and social restructuring in Los Angeles, Chile, focusing on the effects of land use changes on the city's socio-spatial structure."
    },
    {
        "query": "What is the definition of primary data?",
        "paper_list_string": "Khuc 2021:\n\nTitle: Primary data survey: a step-by-step procedure for researchers in social sciences and humanities\n\nAbstract: Primary data, pilot survey, final survey\n\n==\n\nKhuc 2020:\n\nTitle: Primary data\n\nAbstract: The primary investigation, primary data, questionnaire method, model,\n\n==\n\nKhuc 2021:\n\nTitle: Primary data\n\nAbstract: The primary investigation, primary data, questionnaire method, model,\n\n==\n\nBoslaugh 2007:\n\nTitle: Secondary Data Sources for Public Health: An Introduction to Secondary Data Analysis\n\nAbstract: What Are Secondary Data? In the fields of epidemiology and public health, the distinction between primary and secondary data depends on the relationship between the person or research team who collected a data set and the person who is analyzing it. This is an important concept because the same data set could be primary data in one analysis and secondary data in another. If the data set in question was collected by the researcher (or a team of which the researcher is a part) for the specific purpose or analysis under consideration, it is primary data . If it was collected by someone else for some other purpose, it is secondary data . Of course, there will always be cases in which this distinction is less clear, but it may be useful to conceptualize primary and secondary data by considering two extreme cases. In the first, which is an example of primary data , a research team conceives of and develops a research project, collects data designed to address specific questions posed by the project, and performs and publishes their own analyses of the data they have collected. In this case, the people involved in analyzing the data have some involvement in, or at least familiarity with, the research design and data collection process, and the data were collected to answer the questions examined in the analysis.\n\n==\n\nSilman 2018:\n\nTitle: Use of secondary data\n\nAbstract: Primary data collection is challenging and with increasing electronic data capture in routine healthcare and other aspects of life, it is possible to address several epidemiological questions by robust analysis of such \u2018secondary data\u2019. There are considerable advantages in terms of scope, size, and speed of study to be balanced against the quality and depth of using primary data. Even when such direct contact is not required, there is often the need to extract necessary information from individual subject records such as medical files. There is often no alternative source of information, although the greater digitization of information is changing that scenario with the potential that the availability of such information might preclude the need for primary data.\n\n==\n\nGlass 1976:\n\nTitle: Primary, Secondary, and Meta-Analysis of Research1\n\nAbstract: M y subject is data analysis at three levels. Primary analysis is the original analysis of data in a research study. It is what one typically imagines as the application of statistical methods. Secondary analysis is the re-analysis of data for the purpose of answering the original research question with better statistical techniques, or answering new questions with old data. Secondary analysis is an important feature of the research and evaluation enterprise. Tom Cook (1974) at Northwestern University has written about its purposes and methods. Some of our best methodologists have pursued secondary analyses in such grand style that its importance has eclipsed that of the primary analysis. We can cite with pride some state of the art documents: the MostellerMoynihan secondary analysis of the Coleman study; the Campbell-Erlebacher analysis of the Ohio-Westinghouse Headstart evaluation; and the Elashoff-Snow secondary analysis of Pygmalion in the Classroom, to name three. About all that can effectively be done to insure that secondary analyses of important studies are carried out is to see that the data from the original studies are preserved and that secondary analyses are funded. The preservation of original data could improve. Last month, one of our graduate students, Karl White, spent 15 hours and made 30 phone calls attempting to obtain from the government a copy of the data tapes for the Coleman study only to learn in the end that they had been irretrievably filed in unmarked tape cannisters with some 2,000 other unmarked data tapes. Tom Cook remarked in an Annual Meeting symposium on secondary analysis that you can get the data if you have chutzpah or if you're socio metrically well-connected. The whole business is too important to be treated so casually. On the other extreme, one can point with satisfaction to the ready availability to any researcher of the data tapes from Project TALENT or the National Assessment of Educational Progress. Others are advancing the practice of secondary analysis. My major interest currently is in what we have come to call\u2014not for want of a less pretentious name\u2014the meta-analysis of research. The term is a bit grand, but it is precise, and apt, and in the spirit of \"metamathematics,\" \"meta-psychology,\" and \"meta-evaluation.\" Meta-analysis refers to the analysis of analyses. I use it to refer to the statistical analysis of a large collection of analysis results from individual studies for the purpose of integrating the findings. It connotes a rigorous alternative to the casual, narrative discussions of research studies which typify our attempts to make sense of the rapidly expanding research literature. The need for the meta-analysis of research is clear. The literature on dozens of topics in education is growing at an astounding rate. In five years time, researchers can produce literally hundreds of studies on IQ and creativity, or impulsive vs. reflective cognitive styles, or any other topic.\n\n==\n\nSchindler 2005:\n\nTitle: Webservices Infrastructure for the Registration of Scientific Primary Data\n\nAbstract: Registration of scientific primary data, to make these data citable as a unique piece of work and not only a part of a publication, has always been an important issue. In the context of the project \u201dPublication and Citation of Scientific Primary Data\u201d funded by the German Research Foundation (DFG) the German National Library of Science and Technology (TIB) has become the first registration agency worldwide for scientific primary data. Registration has started for the field of earth science, but will be widened for other subjects in the future. This paper shall give an overview about the technical realization of this important usage field for a digital library.\n\n==\n\nQuinn 1997:\n\nTitle: Primary methods of measurement and primary standards\n\nAbstract: The definition and concept of primary method are explained, taking examples from the fields of thermometry and measurements of amount of substance. It is shown that although few methods can be considered strictly primary, small departures from the strict definition can be quantified and useful practical primary methods are thus available. The links between primary methods of measurement, primary standards of measurement and primary reference materials are discussed.",
        "final_summary": "The definition of primary data is addressed in the papers. Boslaugh 2007 explains that primary data refers to data collected by the researcher or research team specifically for the purpose of their analysis. Secondary data, on the other hand, is data collected by someone else for a different purpose. Silman 2018 highlights the advantages of using secondary data, such as its scope and speed, but acknowledges that primary data collection can provide more in-depth information. Glass 1976 discusses the concept of meta-analysis, which involves analyzing multiple primary analyses to integrate findings. Overall, primary data is data collected by researchers for their own analysis, while secondary data is collected by others for different purposes. (Boslaugh 2007; Silman 2018; Glass 1976)"
    },
    {
        "query": "adrenal and renal pecoma",
        "paper_list_string": "Lau 2012:\n\nTitle: Malignant PEComa of the adrenal gland.\n\nAbstract: Perivascular epithelioid cell neoplasms, also known as PEComas, are unique mesenchymal tumors exhibiting perivascular epithelioid cell differentiation, characterized by a mixed myogenic and melanocytic phenotype. PEComas arising in visceral organs outside of the kidney, liver, and lung are rare, and often pose problems in diagnosis. Examples of this neoplasm originating in the adrenal gland are limited. The present report details the clinical and pathologic features of an unusual case of a pure epithelioid PEComa (epithelioid angiomyolipoma) of the adrenal gland exhibiting clinically malignant behavior in the form of pulmonary metastases, a feature not previously described in tumors of this site. The diagnosis was supported by immunohistochemical studies demonstrating expression of myoid and melanocytic antigens. The present case serves to emphasize the potential of PEComa for clinically aggressive behavior and the importance of distinguishing this tumor from other epithelioid neoplasms that are more commonly encountered in the adrenal gland.\n\n==\n\nBattistella 2023:\n\nTitle: Metastatic Adrenal PEComa: Case Report and Short Review of the Literature\n\nAbstract: PEComa has become a widely accepted entity, and increased recognition has led to descriptions of this tumor in a wide variety of anatomic sites, including the adrenal gland. PEComa (perivascular epithelioid cell tumor) is a mesenchymal tumor composed of perivascular cells, and the most frequent sites of PEComas are the uterus and retroperitoneum. The incidence is <1 per 1,000,000 people. We report a case of adrenal metastatic PEComa in a 63-year-old man discovered by a spontaneous hematoma of the rectus abdominis. In our case, PEComa of the adrenal gland was a significant diagnostic dilemma as the morphologic and immunophenotypic features of this neoplasm may easily be confused with those of other more commonly encountered lesions.\n\n==\n\nPant 2015:\n\nTitle: Malignant Perivascular Epithelioid Cell Tumor (PEComa) of the Adrenal Gland: Report of a Rare Case Posing Diagnostic Challenge with the Role of Immunohistochemistry in the Diagnosis\n\nAbstract: Histological diagnosis of adrenal tumors is often challenging as diverse groups of tumors, both primaries and metastatic, may be seen in the adrenal gland with overlapping morphological features. Immunohistochemistry (IHC) plays the most important role in their diagnosis. Perivascular epithelioid cell tumor (PEComa), a rarely reported tumor in the adrenal gland, shares many features with another rare tumor sarcomatoid adrenocortical carcinoma (ACC). Extensive immunohistochemical study is required to distinguish this tumor from adrenocortical carcinoma and from other morphologically similar tumors. The unique combination of immunoreactivity for melanocytic markers, such as HMB-45 and Melan A, and myogenic markers, such as smooth muscle actin, is the hallmark of PEComas biological behavior, and prognosis of malignant PEComas is yet to be fully understood. Few cases of malignant PEComa have been reported in the adrenal gland. We report a case of malignant PEComa of the adrenal gland posing diagnostic challenge and compare its morphological and immunohistochemical features with those of sarcomatoid ACC.\n\n==\n\nIi 2016:\n\nTitle: Angiomyolipoma and Malignant PEComa: Discussion of Two Rare Adrenal Tumors\n\nAbstract: Angiomyolipoma and PEComa are rare tumors descending from perivascular epithelial cells (PECs), with distinctive IHC, morphological, and ultrastructural features. The kidney is the most frequent site of origin, but not the only one; however, adrenal gland angiomyolipomas are extremely rare. We describe two cases being found in the adrenal glands. Given the paucity of literature on the subject, more information on this disease is necessary for diagnosis and treatment. Here, we describe two complete case reports, from presentation to treatment and follow-up, along with imaging and microscopic pathology samples, and provide a comprehensive review as to the history and current literature available regarding these extremely rare tumors.\n\n==\n\nKumar 2010:\n\nTitle: Perivascular epithelioid cell tumour (PEComa) of the inferior vena cava presenting as an adrenal mass\n\nAbstract: Abstract A 54-year-old woman had a mass located in the right suprarenal area. On imaging, this mass appeared to be infiltrating the inferior vena cava (IVC). Exploratory laparotomy was undertaken and excision of the tumour was done with the sleeve of the involved IVC. The mass turned out to be a perivascular epithelioid cell tumour (PEComa) on histopathological examination. This report describes previously reported cases of PEComa in brief and highlights the problems associated with the management of this tumour.\n\n==\n\nMartignoni 2015:\n\nTitle: PEComas of the kidney and of the genitourinary tract.\n\nAbstract: PEComas are mesenchymal tumors composed of histologically and immunohistochemically distinctive perivascular epithelioid cells that are characterized by the coexpression of muscle and melanogenetic markers. This group of lesions includes angiomyolipoma, clear cell \"sugar\" tumor of the lung and extrapulmonary sites, lymphangioleiomyomatosis, clear cell myomelanocytic tumor of the falciform ligament/ligamentum teres, and rare clear cell tumors of other anatomical sites. In the genitourinary tract, PEComas have been described in the kidney, bladder, prostate, testis, and urethra. Although most PEComas behave as benign tumors, some are potentially malignant, and criteria for malignancy have been suggested for both and renal and extrarenal lesions. Recently, the expression of cathepsin K has been demonstrated in a large number of PEComas and has been proposed as a relatively specific marker to distinguish these proliferations from the majority of human cancers. In addition, a distinctive subset of PEComas harboring TFE3 gene fusions has been reported, giving rise to a possible relationship between them and MiTF/TFE family translocation renal cell carcinomas. The genetic alterations of tuberous sclerosis complex that promote activation of the mTOR pathway have been identified in PEComas. Therapy with mTORC1 inhibitors has been shown to be effective in some cases.\n\n==\n\nMcligeyo 1996:\n\nTitle: Renal vein and intracaval invasion by an adrenal phaeochromocytoma with extension Into the right atrium: a case study.\n\nAbstract: A 30 year old female with an unexpected right adrenal phaechromacytoma invading the renal vein, the inferior vena cava and extending into the right atrium is presented. She also had BuddChiari syndrome due to invasion of the hepatic veins by the tumour. Additionally, the tumour had metastasised to the liver and the lungs. Despite elevated 24 hour urinary vanillylmandelic acid (VMA) the patient was normotensive pre-operatively. The patient underwent right adrenalectomy and extended nephrectomy with milking of the tumaur from the inferior vena cava. Unfortunately, the patient developed multiple hypotensive episodes and adult respiratory distress syndrome post-operatively and died three weeks after surgery.\n\n==\n\nZhao 2014:\n\nTitle: Sclerosing PEComa of the kidney: clinicopathologic analysis of 2 cases and review of the literature.\n\nAbstract: Introduction Perivascular epithelioid cell tumors (PEComas) are a group of uncommon neoplasms that include angiomyolipoma, clear cell \u201csugar\u201d tumor of the lung, lymphangiomyomatosis, and a variety of unusual visceral, intra-abdominal, and soft tissue/bone tumors (such as clear cell myomelanocytic tumor of the falciform ligament/ ligamentum teres, abdominopelvic sarcoma of perivascular epithelioid cells, and primary extrapulmonary sugar tumor). The prototypic cell called perivascular epithelioid cell (PEC) has unique morphologic, immunohistochemical, ultrastructural, and genetic features that are characterized by perivascular location, often with a radial arrangement of cells around the vascular lumen. PECs typically show immunoreactivity for both melanocytic markers (such as HMB-45 [human melanoma black], melan-A [MLANA], and microphthalmia transcription factor [MiTF]) and muscle markers (such as smooth muscle actin [SMA] and calponin). Hornick and Fletcher have described a distinctive variant of PEComa having a hyalinized stroma and a striking predilection for the retroperitoneum of middle-aged women, for which they proposed the term sclerosing PEComa. Subsequently, there have been several case reports that are summarized in Table 1.",
        "final_summary": "The papers collectively provide insights into adrenal and renal PEComa. PEComas are rare mesenchymal tumors composed of perivascular epithelioid cells. While PEComas are commonly found in the uterus and retroperitoneum, they can also occur in the adrenal gland (Lau 2012, Battistella 2023). These tumors can exhibit clinically aggressive behavior, including metastasis and invasion of adjacent structures (Lau 2012, Pant 2015, Mcligeyo 1996). Diagnosis of adrenal PEComa can be challenging due to overlapping features with other neoplasms (Lau 2012, Battistella 2023, Pant 2015). Immunohistochemistry plays a crucial role in distinguishing PEComas from other adrenal tumors (Pant 2015). While most PEComas are considered benign, some have malignant potential (Martignoni 2015). Sclerosing PEComa is a distinctive variant with a hyalinized stroma and a preference for the retroperitoneum (Zhao 2014). Further research is needed to understand the biological behavior and prognosis of malignant PEComas (Pant 2015, Martignoni 2015)."
    },
    {
        "query": "Critical evaluation of governance structure of Unilever PLC UK. Based the discussion on the UK Corporate Governance Code (April 2016 and July 2018), Financial Reporting Council (FRC). The discussion should focus on board leadership, board committees, risk management strategy, internal control mechanisms and key performance indicators (financial and financial indicators)",
        "paper_list_string": "Vafeas 1998:\n\nTitle: THE RELATIONSHIP BETWEEN BOARD STRUCTURE AND FIRM PERFORMANCE IN THE UK\n\nAbstract: Abstract This study examines the relationship between board structure with firm performance in the UK by employing data from 250 publicly traded firms. Consistent with general findings from the USA, the tests as a whole do not discern a significant link between board structure (director affiliation and ownership, chairman affiliation, and committee composition) with firm performance. These results are robust to alternative measures of performance, outlier definitions, various model specifications, and statistical estimation techniques. The most significant determinants of corporate performance are the level of R&D spending and current operating performance. These results are consistent with governance needs varying across firms, and contrast the notion that uniform board structures should be mandated.\n\n==\n\nPesqueux 2004:\n\nTitle: A critical view on corporate governance and its performance measurement and evaluation systems\n\nAbstract: Performance measurement and evaluation systems cannot be taken as \u201cpure\u201d tools because they are the product of a given society. That is why they take their meanings and their sense from this society. They are particularly linked with corporate governance which is also linked with the contemporary developments of capitalism seen as a political order. That is why performance measurement and evaluations sytems in today\u2019s companies are the production of a social game which has to be understood. Key indicators like shareholder\u2019s value and actors like auditing firms play a specific role which has to be evaluated. This paper will introduce the discussion about these systems (and moreover management tools) as the production of their society from a posture taken from Political Philosophy. They will be evaluated in relation which what is capitalism today (which has to be understood in relation with what it was yesterday). That is why notions like shareholder\u2019s value will be linked with the increasing weight of investments funds and with the systematic search for financial surplus. It will be induced that all these \u201cconcrete\u201d notions build a system and are reciprocally re\u2010enforced. A special mention will be made to auditing firms as a cartel. Corporate governance will, as well as auditing firms, be presented as the concretisation of a new social game.\n\n==\n\nKyere 2020:\n\nTitle: Corporate governance and firms financial performance in the United Kingdom\n\nAbstract: The objective of this study is to examine empirically the impact of good corporate governance on financial performance of United Kingdom non-financial listed firms. Agency theory and stewardship theory serve as the bases of a conceptual model. Five corporate governance mechanisms are examined on two financial performance indicators, return on assets (ROA) and Tobin's Q, employing cross-sectional regression methodology. The conclusion drawn from empirical test so performed on 252 firms listed on London Stock Exchange for the year 2014 indicates a positive or a negative relationship, but also sometimes no effect, of corporate governance mechanisms impact on financial performance. The implications are discussed. Thereby, so distinguishing effects due to causes, we present a proof that, when the right corporate governance mechanisms are chosen, the finances of a firm can be improved. The results of this research should have some implication on academia and policy makers thoughts.\n\n==\n\nWeir 2001:\n\nTitle: Governance structures, director independence and corporate performance in the UK\n\nAbstract: A number of Committees have been set up in recent years to investigate the governance of UK quoted companies. The key one was the Cadbury Committee, which recommended a number of governance structures as examples of best practice. These included the separation of the posts of CEO and chairman, a significant representation of non\u2010executive directors, the importance of non\u2010executive director independence and the setting up of board subcommittees. This study finds that there has been widespread adoption of the recommended governance structures. However, there is no clear relationship between governance structures and corporate performance. This raises questions about the most effective type of governance mechanism and whether or not the prescriptive recommendations of Cadbury should be replaced with a more flexible approach.\n\n==\n\nPage 2009:\n\nTitle: Corporate governance and corporate performance: UK FTSE 350 companies\n\nAbstract: In the wake of the recent financial crisis, attention has once again turned to corporate governance, with policy reviews of UK corporate governance being undertaken by the FRC and the Walker Review. One key question may relate to the purpose of corporate governance \u2013 is it about the control of risks, the improvement of performance, or both? If this could be clarified, criteria could be developed to measure the success of corporate governance procedures or codes. This research investigates whether companies with particular corporate governance characteristics outperform other companies and have lower levels of risk. The governance characteristics investigated in the report are: board independence; board size; directors\u2019 ownership of equity; and extent of ownership by large block holders. The effects of these characteristics were measured over two three year periods between 1999 and 2004. Three measures of performance were used: one stock market measure (market to book ratio); and two accounting based measures (return on assets and ratio of sales to total assets). Risk was measured in three ways: total risk; systematic risk; and a measure of sudden share price falls. The findings reveal no clear systematic relationship between governance factors and improved performance, and no strong evidence that governance reduces either total or systematic risk. This project was funded by the Scottish Accountancy Trust for Education and Research (SATER). The Research Committee of The Institute of Chartered Accountants of Scotland (ICAS) as also been happy to support this project. The Committee recognises that the views expressed do not necessarily represent those of ICAS itself, but hopes that the project will add to the knowledge about the interaction between corporate governance factors, company performance and risk.\n\n==\n\nShaukat 2017:\n\nTitle: Board Governance and Corporate Performance\n\nAbstract: We examine the link between the monitoring capacity of the board and corporate performance of UK listed firms. We also investigate how firms use the flexibility offered by the voluntary governance regime to make governance choices. We find a strong positive association between the board governance index we construct and firm operating performance. Our results imply that adherence to the board-related recommendations of the UK Corporate Governance Code strengthens the board's monitoring capacity, potentially helping mitigate agency problems, but that investors do not value it correspondingly. Moreover, in contrast to prior UK findings suggesting efficient adoption of Code recommendations, we find that firms at times use the Code flexibility opportunistically, aiming to decrease the monitoring capacity of the board, which is followed by subsequent underperformance. This finding questions the effectiveness of the voluntary approach to governance regulation followed in the UK as in many countries around the world. \n \nThis article is protected by copyright. All rights reserved\n\n==\n\nPaniagua 2018:\n\nTitle: Corporate governance and financial performance: The role of ownership and board structure\n\nAbstract: This study examines how corporate governance and ownership structure relate to the financial performance of firms. We estimated this relationship using fsQCA. We enhanced our analysis using complementary linear and non-linear multiple regression analysis. The panel data used in this study covered 1207 companies from 59 countries across 19 sectors for the period 2013 to 2015. The study makes two main contributions. First, the multiple empirical techniques employed in this study offer a broader approach to the empirical analysis of financial performance. Second, the study aids our understanding of the role of corporate governance and ownership in the financial performance of firms.\n\n==\n\nDalwai 2015:\n\nTitle: A critical review of relationship between corporate governance and firm performance: GCC banking sector perspective\n\nAbstract: Purpose \u2013 The purpose of this paper is to evaluate existing studies on the relationship of corporate governance with firm performance in different regions and address the need for similar analysis for the Gulf Coperation Council (GCC) sector. The banking sector comprises the conventional and Islamic banks in the GCC sector and is important due to their ability to bring stability to this region. Existing studies that measure the relationship of GCC sector conventional banks and firm performance are limited. This study proposes a need for future research on corporate governance in the GCC region. Design/methodology/approach \u2013 This paper will review and analyze the different empirical and theoretical contributions in establishing the relationship between corporate governance and firm performance. Findings \u2013 This paper will create a focus for future research of measuring the impact of corporate governance mechanism on firm performance. The regulators will be encouraged to focus on more research studies for th...",
        "final_summary": "The papers collectively provide mixed findings regarding the governance structure of Unilever PLC UK. Vafeas 1998 and Weir 2001 suggest that there is no clear relationship between board structure and firm performance. Pesqueux 2004 highlights the societal context in which performance measurement and evaluation systems, including corporate governance, operate. Kyere 2020 finds that the impact of corporate governance mechanisms on financial performance varies, with some positive, negative, or no effects observed. Page 2009 concludes that there is no systematic relationship between governance factors and improved performance or risk reduction. Shaukat 2017 indicates a positive association between adherence to the UK Corporate Governance Code and firm operating performance, but also highlights opportunistic use of the Code flexibility leading to underperformance. Paniagua 2018 employs fsQCA to examine the relationship between corporate governance, ownership structure, and financial performance, contributing to our understanding of their role. Dalwai 2015 emphasizes the need for future research on the relationship between corporate governance and firm performance in the GCC banking sector."
    },
    {
        "query": "fiscal policy, bank lending ",
        "paper_list_string": "Melina 2012:\n\nTitle: Fiscal Policy and Lending Relationships\n\nAbstract: This paper studies how fiscal policy affects loan market conditions. First, it conducts a Structural Vector-Autoregression analysis showing that the bank spread responds negatively to an expansionary government spending shock, while lending increases. Second, it illustrates that these results are mimicked by a Real Business Cycle model where the bank spread is endogenized via the inclusion of a banking sector exploiting lending relationships. Third, it shows that lending relationships represent a friction that generates a financial accelerator effect in the transmission of the fiscal shock.\n\n==\n\nMelina 2014:\n\nTitle: Fiscal Policy and Lending Relationships\n\nAbstract: type=\"main\" xml:lang=\"en\"> This paper studies how fiscal policy affects loan market conditions in the United States. First, it conducts a structural vector-autoregression analysis showing that the bank spread responds negatively to an expansionary government spending shock, while lending increases. Second, it illustrates that these results are mimicked by a dynamic stochastic general equilibrium model where the bank spread is endogenized via the inclusion of a banking sector exploiting lending relationships. Third, it shows that lending relationships represent a friction that generates a financial accelerator effect in the transmission of the fiscal shock. (JEL E44, E62)\n\n==\n\nAydin 2011:\n\nTitle: Bank Lending in Turkey: Effects of Monetary and Fiscal Policies\n\nAbstract: We study the impact of monetary and fiscal policies on credit growth in Turkey using bank-level data from the last quarter of 2002 to the first quarter of 2008. We find evidence that the liquidity-constrained banks have a sharper decline in lending during contractionary monetary policies and that the crowding-out effect diminishes more for banks with a retail banking focus when the government adopts fiscal discipline. However, the results are statistically weak. Hence, the evidence is not strong enough to irrefutably document the bank lending channel and the impact of government finances on loan supply in Turkey even though these effects may be operational.\n\n==\n\nHauner 2006:\n\nTitle: Fiscal Policy and Financial Development\n\nAbstract: We examine the effects of public sector borrowing from the domestic banking system on financial development in middle-income countries. While these countries' external debt has been falling, the share of bank credit absorbed by the public sector has been rising rapidly. We argue that this runs the risk of slowing financial development by affecting structural characteristics of the banking systems. We find empirical evidence that too much public sector borrowing harms financial deepening, and that banks mainly lending to the public sector tend to be more profitable but less efficient. We note that these effects add to the costs of fiscal prolificacy.\n\n==\n\nHauner 2006:\n\nTitle: Fiscal Policy and Financial Development\n\nAbstract: We examine the effects of public sector borrowing from the domestic banking system on financial development in middle-income countries. While these countries' external debt has been falling, the share of bank credit absorbed by the public sector has been rising rapidly. We argue that this runs the risk of slowing financial development by affecting structural characteristics of the banking systems. We find empirical evidence that too much public sector borrowing harms financial deepening, and that banks mainly lending to the public sector tend to be more profitable but less efficient. We note that these effects add to the costs of fiscal prolificacy.\n\n==\n\nYan 2021:\n\nTitle: How Does Fiscal Policy Affect Bank Credit? Evidence from China\n\nAbstract: Using a set of Chinese economic data and a structural vector autoregression (SVAR) model, this paper investigates the transmission channels of fiscal policy to bank credit in China. We find that increases in tax revenue can increase bank credit through external financing premium channel, collateral channel, and bank liquidity channel. We also find that increases in government spending can reduce bank credit through bank liquidity channel and increase bank credit through external financing premium channel and collateral channel.\n\n==\n\nKollmann 2012:\n\nTitle: Fiscal Policy, Banks and the Financial Crisis\n\nAbstract: This paper studies the effectiveness of Euro Area (EA) fiscal policy, during the recent financial crisis, using an estimated New Keynesian model with a bank. A key dimension of policy in the crisis was massive government support for banks \u2014 that dimension has so far received little attention in the macro literature. We use the estimated model to analyze the effects of bank asset losses, of government support for banks, and other fiscal stimulus measures, in the EA. Our results suggest that support for banks had a stabilizing effect on EA output, consumption and investment. Increased government purchases helped to stabilize output, but crowded out consumption. Higher transfers to households had a positive impact on private consumption, but a negligible effect on output and investment. Banking shocks and increased government spending explain half of the rise in the public debt/GDP ratio since the onset of the crisis.\n\n==\n\nKollmann 2012:\n\nTitle: Fiscal Policy, Banks and the Financial Crisis\n\nAbstract: This paper studies the effectiveness of Euro Area (EA) fiscal policy, during the recent financial crisis, using an estimated New Keynesian model with a bank. A key dimension of policy in the crisis was massive government support for banks\u2014that dimension has so far received little attention in the macro literature. We use the estimated model to analyze the effects of bank asset losses, of government support for banks, and other fiscal stimulus measures, in the EA. Our results suggest that support for banks had a stabilizing effect on EA output, consumption and investment. Increased government purchases helped to stabilize output, but crowded out consumption. Higher transfers to households had a positive impact on private consumption, but a negligible effect on output and investment. Banking shocks and increased government spending explain half of the rise in the public debt/GDP ratio since the onset of the crisis.",
        "final_summary": "The papers suggest that fiscal policy can have an impact on bank lending. Melina 2012 and Melina 2014 find that expansionary government spending shocks lead to a decrease in bank spreads and an increase in lending. Aydin 2011, however, finds weak evidence for the bank lending channel and the impact of government finances on loan supply in Turkey. Hauner 2006 and Hauner 2006 argue that excessive public sector borrowing can harm financial development and lead to less efficient banks. Yan 2021 shows that tax revenue increases can increase bank credit through various channels, while government spending increases can affect bank credit through different channels as well. Kollmann 2012 and Kollmann 2012 find that government support for banks during the financial crisis had a stabilizing effect on output, consumption, and investment in the Euro Area."
    },
    {
        "query": "What are the effects of estrogen on spatial memory?",
        "paper_list_string": "Luine 1998:\n\nTitle: Estradiol Enhances Learning and Memory in a Spatial Memory Task and Effects Levels of Monoaminergic Neurotransmitters\n\nAbstract: The effects of chronic estrogen treatment on radial arm maze performance and on levels of central monoaminergic and amino acid neurotransmitters were examined in ovariectomized (Ovx) rats. In an eight arms baited paradigm, choice accuracy was enhanced following 12 days but not 3 days of treatment. In addition, performance during acquisition of the eight arms baited maze task was better in estrogen-treated Ovx rats than in Ovx rats. Performance of treated rats was also enhanced in win-shift trials conducted 12 days postestrogen treatment. Working, reference, and working-reference memory was examined when four of the eight arms were baited, and only working memory was improved by estrogen and only after long-term treatment. Activity of Ovx rats on an open field, crossings and rearings, was increased at 5 but not at 35 days following estrogen treatment. In medial prefrontal cortex, levels of NE, DA, and 5-HT were decreased but glutamate and GABA levels were not affected following chronic estrogen treatment. Basal forebrain nuclei also showed changes in monoamines following estrogen. Hippocampal subfields showed no effects of estrogen treatment on monoaminergic or amino acid transmitters. Levels of GABA were increased in the vertical diagonal bands following chronic estrogen. Results show that estrogen enhances learning/memory on a task utilizing spatial memory. Effects in Ovx rats appear to require the chronic (several days) presence of estrogen. Changes in activity of both monoaminergic and amino acid transmitters in the frontal cortex and basal forebrain may contribute to enhancing effects of estrogen on learning/memory.\n\n==\n\nHarburger 2007:\n\nTitle: Effects of estrogen and progesterone on spatial memory consolidation in aged females\n\nAbstract: Interpretation of data illustrating that estrogen, with or without progestin, is detrimental to memory in post-menopausal women is complicated by the fact that little is known about the effects of progestins on memory. The present study examined if estrogen, alone or with progesterone, affects spatial memory consolidation in ovariectomized aged female mice. Mice received eight training trials in a spatial Morris water maze followed immediately by injection of water-soluble 17beta-estradiol (E(2); 0.2 mg/kg) or vehicle. Mice were re-tested 24 h later. All mice learned to find the platform on Day 1. On Day 2, the performance of control, but not E(2) mice, deteriorated, suggesting that E(2) enhanced memory for the platform location. In a second experiment, mice were injected with E(2) and 10 or 20 mg/kg water-soluble progesterone. The 10 mg/kg dose of progesterone did not affect estrogen's ability to enhance spatial memory consolidation, but 20 mg/kg blocked this effect. These data indicate that estrogen can improve spatial memory consolidation in aged females and that this effect can be attenuated by progesterone.\n\n==\n\nRissman 2002:\n\nTitle: Disruption of estrogen receptor \u03b2 gene impairs spatial learning in female mice\n\nAbstract: Here we provide the first evidence, to our knowledge, that estradiol (E2) affects learning and memory via the newly discovered estrogen receptor \u03b2 (ER\u03b2). In this study, ER\u03b2 knockout (ER\u03b2KO) and wild-type littermates were tested for spatial learning in the Morris water maze after ovariectomy, appropriate control treatment, or one of two physiological doses of E2. Regardless of treatment, all wild-type females displayed significant learning. However, ER\u03b2KOs given the low dose of E2 were delayed in learning acquisition, and ER\u03b2KOs administered the higher dose of E2 failed to learn the task. These data show that ER\u03b2 is required for optimal spatial learning and may have implications for hormone replacement therapy in women.\n\n==\n\nVarga 2002:\n\nTitle: Weak if any effect of estrogen on spatial memory in rats\n\nAbstract: In a number of species, males appear to have spatial abilities that are superior to those of females. The favored explanation for this cognitive difference is hormonal: higher testosterone levels in males than in females. An alternative explanation focuses on the role of varying levels of estrogens in females during the estrus cycle; females perform as well as males on days of low estrogen, but more poorly on days of high estrogen. Other investigators have reported that estrogens improve both types of memory processes, which depend on the striatal (nonspatial navigation) and hippocampal (spatial) memory systems. Additionally, estrogens have been found to protect the working memory. These contradictory results initiated the present study, in which ovariectomized female rats were trained to escape in a Morris water maze. The daily trials were preceded by estradiol application in low doses (Experiment I) or in higher doses (Experiment II). In Experiment I, no differences at all were found between the latencies of the treated and control groups to reach a submerged platform in a Morris water maze. In Experiment II, however, the animals treated with the higher dose of estradiol showed a small deficit in the acquisition of the Morris water maze task. This study indicates that estradiol at around the physiological level has no effect on spatial learning and memory functions. + Dedicated to Professor Otto Feher on the occasion of his 75 th birthday.\n\n==\n\nSilverman 1993:\n\nTitle: Effects of estrogen changes during the menstrual cycle on spatial performance\n\nAbstract: Abstract Four sequential, interrelated studies of the relationship of menstrual cycle phase to three-dimensional mental rotations performance were conducted, using both between-and within-subjects designs. All studies showed significant increases in mean mental rotations scores during the menstrual period phase, when estrogen levels were at their lowest. Effects occurred only for mental rotations; relationships with hormonal status did not occur for control tests, which were not of a spatial nature, and a different spatial test (Space Relations). Findings are discussed as they relate to ontogenetic development and evolutionary origins of sex-specific differences in spatial behaviors.\n\n==\n\nGibbs 1999:\n\nTitle: Estrogen Replacement Enhances Acquisition of a Spatial Memory Task and Reduces Deficits Associated with Hippocampal Muscarinic Receptor Inhibition\n\nAbstract: A delayed matching-to-position (DMP) T-maze task was used to examine the effects of estrogen replacement on spatial learning and memory, as well as the ability of estrogen replacement to reduce performance deficits produced by acute systemic and intrahippocampal muscarinic cholinergic inhibition. Two experiments were performed. In Experiment 1, ovariectomized animals were trained to criterion on the DMP task and then tested with increased intertrial delays and following systemic scopolamine administration. The animals then received either continuous estrogen replacement or sham surgery and were retested beginning 10 days later. In Experiment 2, ovariectomized animals received guide cannulae implanted bilaterally into the hippocampus. Half of these animals also began receiving continuous estrogen replacement. Two months later, the animals were trained on the DMP task and then tested with increased intertrial delays and following systemic as well as intrahippocampal scopolamine administration. Animals received the same test battery 8 months later and were then immediately trained on a reversal task. The results indicate that estrogen-treated animals acquired the DMP task at a significantly faster rate than the ovariectomized, non-estrogen-treated controls. In addition, estrogen replacement significantly reduced deficits in DMP performance produced by intrahippocampal, but not systemic, scopolamine administration. This occurred when animals were tested after 3.5 months, as well as after 12 months, of continuous estrogen replacement. No evidence for an effect of estrogen replacement on spatial working memory or reversal learning was detected. These findings demonstrate that estrogen replacement can enhance acquisition of a spatial memory task and reduce performance deficits associated with hippocampal cholinergic impairment.\n\n==\n\nFrick 2002:\n\nTitle: Estrogen replacement improves spatial reference memory and increases hippocampal synaptophysin in aged female mice\n\nAbstract: Estrogen deficiency during menopause is often associated with memory dysfunction. However, inconsistencies regarding the ability of estrogen to improve memory in menopausal women highlight the need to evaluate, in a controlled animal model, the potential for estrogen to alleviate age-related mnemonic decline. The current study tested whether estrogen could ameliorate spatial reference memory decline in aged female mice. At the conclusion of testing, levels of the presynaptic protein synaptophysin, and activities of the synthetic enzymes for acetylcholine and GABA, were measured in the hippocampus and neocortex. Aged (27-28-month-old) female C57BL/6 mice were given daily subcutaneous injections of 1 microg or 5 microg of beta-estradiol-3-benzoate dissolved in sesame oil. Control mice received daily injections of sesame oil or no injections. Estradiol treatment began 5 days prior to behavioral testing and continued throughout testing. Spatial and non-spatial memory were assessed in the Morris water maze. The 5 microg dose of estradiol significantly improved spatial learning and memory in aged females. The performance of 5 microg females improved significantly more rapidly than that of control females; estradiol-treated females performed at asymptotic levels by session 2. Furthermore, 5 microg females exhibited a more robust spatial bias than controls during probe trials. In contrast, 1 microg of estradiol did not improve spatial task performance. Neither dose affected performance of the non-spatial task. In the hippocampus, synaptophysin was increased in 5 microg females relative to controls. Estrogen did not affect enzyme activities in either brain region. This study is the first to examine the effects of estrogen replacement on spatial reference memory and synaptophysin expression in aged post-estropausal female rodents. The results suggest that: (1) estrogen can profoundly improve spatial reference memory in aged females, and (2) this improvement may be related to increased hippocampal synaptic plasticity, but not modulation of the synthetic enzymes for acetylcholine and GABA.\n\n==\n\nSandstrom 2004:\n\nTitle: Spatial memory retention is enhanced by acute and continuous estradiol replacement\n\nAbstract: Estradiol replacement to ovariectomized female rats causes dramatic changes in hippocampal structure and function as well as in performance on hippocampally dependent tasks. Using a delayed matching-to-place version of the water maze, the present study examines the time course of estradiol-induced enhancements in memory retention as well as the effectiveness of acute and continuous patterns of replacement. One 10-microg injection of estradiol administered on each of two successive days resulted in significant improvements in memory retention that persisted for approximately 4 days following the second injection. When estradiol administration continued for 10 consecutive days, these improvements in memory retention persisted. These findings indicate that estradiol replacement can improve memory retention and that these improvements can be maintained by continuous replacement for at least 10 days.",
        "final_summary": "The papers suggest that estrogen has a positive effect on spatial memory. Luine 1998 found that chronic estrogen treatment improved spatial memory performance in rats. Harburger 2007 demonstrated that estrogen enhanced spatial memory consolidation in aged female mice, and this effect could be attenuated by progesterone. Rissman 2002 showed that estrogen receptor \u03b2 is required for optimal spatial learning in female mice. However, Varga 2002 found weak or no effect of estrogen on spatial memory in rats. Silverman 1993 reported that mental rotations performance improved during the menstrual period phase when estrogen levels were lowest. Gibbs 1999 found that estrogen replacement enhanced acquisition of a spatial memory task and reduced deficits associated with hippocampal muscarinic receptor inhibition. Frick 2002 showed that estrogen replacement improved spatial reference memory in aged female mice and increased synaptophysin levels in the hippocampus. Sandstrom 2004 demonstrated that estradiol replacement improved spatial memory retention in ovariectomized rats. Overall, these findings suggest that estrogen has a positive impact on spatial memory. (Luine 1998; Harburger 2007; Rissman 2002; Varga 2002; Silverman 1993; Gibbs 1999; Frick 2002; Sandstrom 2004)"
    },
    {
        "query": "The rise of AI-based decision-making tools in the criminal justice system: implications for judicial integrity",
        "paper_list_string": "Barabas 2019:\n\nTitle: Beyond Bias: Re-Imagining the Terms of \u2018Ethical AI\u2019 in Criminal Law\n\nAbstract: Data-driven decision-making regimes, often branded as \u201cartificial intelligence,\u201d are rapidly proliferating across the US criminal justice system as a means of predicting and managing the risk of crime and addressing accusations of discriminatory practices. These data regimes have come under increased scrutiny, as critics point out the myriad ways that they can reproduce or even amplify pre-existing biases in the criminal justice system. This essay examines contemporary debates regarding the use of \u201cartificial intelligence\u201d as a vehicle for criminal justice reform, by closely examining two general approaches to, what has been widely branded as, \u201calgorithmic fairness\u201d in criminal law: 1) the development of formal fairness criteria and accuracy measures that illustrate the trade-offs of different algorithmic interventions and 2) the development of \u201cbest practices\u201d and managerialist standards for maintaining a baseline of accuracy, transparency and validity in these systems. The essay argues that attempts to render AI-branded tools more accurate by addressing narrow notions of \u201cbias,\u201d miss the deeper methodological and epistemological issues regarding the fairness of these tools. The key question is whether predictive tools reflect and reinforce punitive practices that drive disparate outcomes, and how data regimes interact with the penal ideology to naturalize these practices. The article concludes by calling for an abolitionist understanding of the role and function of the carceral state, in order to fundamentally reformulate the questions we ask, the way we characterize existing data, and how we identify and fill gaps in existing data regimes of the carceral state.\n\n==\n\nHartmann 2021:\n\nTitle: Uncertainty, risk and the use of algorithms in policy decisions: a case study on criminal justice in the USA\n\nAbstract: Algorithms are increasingly used in different domains of public policy. They help humans to profile unemployed, support administrations to detect tax fraud and give recidivism risk scores that judges or criminal justice managers take into account when they make bail decisions. In recent years, critics have increasingly pointed to ethical challenges of these tools and emphasized problems of discrimination, opaqueness or accountability, and computer scientists have proposed technical solutions to these issues. In contrast to these important debates, the literature on how these tools are implemented in the actual everyday decision-making process has remained cursory. This is problematic because the consequences of ADM systems are at least as dependent on the implementation in an actual decision-making context as on their technical features. In this study, we show how the introduction of risk assessment tools in the criminal justice sector on the local level in the USA has deeply transformed the decision-making process. We argue that this is mainly due to the fact that the evidence generated by the algorithm introduces a notion of statistical prediction to a situation which was dominated by fundamental uncertainty about the outcome before. While this expectation is supported by the case study evidence, the possibility to shift blame to the algorithm does seem much less important to the criminal justice actors.\n\n==\n\nPutera 2022:\n\nTitle: Artificial Intelligence-Powered Criminal Sentencing in Malaysia: A conflict with the rule of law\n\nAbstract: Artificial Intelligence (AI) promises to heighten human decision-making, including in court. AI sentencing would be better at detecting, organizing, and calibrating all of the variables correlated to sentencing, such as prior criminal records, educational background, substance abuse history, and employment history, resulting in consistencies that traditional sentencing may not be able to provide. AI pervades the Malaysian judiciary system when AI criminal sentencing was launched for the first time to augment the process of meting out sentences in Sabah courts. Despite its promising benefits, AI sentencing may infringe the fundamental principle of due process, presents unacceptable risks of error and implicit bias, and reliance on AI to predict recidivism which forms significant components of the rule of law. The rule of law guarantees that all entities are subject to and accountable to a clear and known law. It enables the judicial branch of the government to be independent and to resolve dispute in a fair manner while upholding the presumption of innocence and preventing the exercise of arbitrary powers. The present research, therefore, examines the use of AI in supporting court processes and human judges, discovering its technical characteristics, practical constraints, and legal theoretical consequences for decision-making processes. Employing jurisprudential analysis as the method of research, this research explores an adjudicatory paradigm that prefers standardisation over discretion, leading in the waning of the notion of rule of law pertinent to the justice system. The metamorphosis to AI adjudication will undoubtedly promote the growth of digitalized dispute resolution by providing efficiency and at least a semblance of impartiality, but it is also poised to birth concerns by making the legal system data-driven, alienating, and disillusioning.\nKeywords: Artificial Intelligence, Artificial Intelligence in Courts, Artificial Intelligence in Criminal Justice\u00a0\neISSN: 2398-4287\u00a9 2022. The Authors. Published for AMER ABRA cE-Bs by e-International Publishing House, Ltd., UK. This is an open access article under the CC BYNC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Peer\u2013review under responsibility of AMER (Association of Malaysian Environment-Behaviour Researchers), ABRA (Association of Behavioural Researchers on Asians/Africans/Arabians) and cE-Bs (Centre for Environment-Behaviour Studies), Faculty of Architecture, Planning & Surveying, Universiti Teknologi MARA, Malaysia.\nDOI: https://doi.org/10.21834/ebpj.v7iSI7%20(Special%20Issue).3813\n\n==\n\nYeung 2023:\n\nTitle: How do \"technical\" design-choices made when building algorithmic decision-making tools for criminal justice authorities create constitutional dangers?\n\nAbstract: Automated, internet-enabled digital decision tools, particularly those that utilise some form of machine learning (\u2018ML\u2019), are widely touted as offering transformative \u2018solutions in government\u2019.1 Also called \u2018narrow\u2019 or \u2018taskspecific\u2019 artificial intelligence (\u2018AI\u2019), ML is a computational technique that uses algorithms to find patterns and correlations in large datasets, \u2018learning\u2019 from past experience to create a mathematical model that can generate predictions when applied to unseen data. These models may, in turn, be embedded into a digital tool to inform, or even to automate, organisational decisions. The variety of ML-based algorithmic systems now being rapidly taken-up the public sector within EU Member States is showcased in the \u2018AI Watch\u2019 EU Joint Research Centre\u2019s 2020 Report,2 with public sector adoption continuing apace.3 Examples range from the use of automated image recognition technologies to detect whether agricultural grasslands have been mowed in Estonia, to the automation of welfare benefit decisions in Sweden, and the delivery of personalised social services to the unemployed in Poland.4 British criminal justice authorities are also rapidly embracing these tools, incentivised by the government\u2019s Police Transformation Fund (and its predecessor) from which \u00a3720 million was allocated to projects aimed at transforming how police use technology between 2015-2020 alone.5 The Home Office claims these technologies will \u2018deliver cash savings, as well as improving efficiency by, for example, freeing\n\n==\n\nMcKay 2019:\n\nTitle: Predicting risk in criminal procedure: actuarial tools, algorithms, AI and judicial decision-making\n\nAbstract: ABSTRACT Risk assessments are conducted at a number of decision points in criminal procedure including in bail, sentencing and parole as well as in determining extended supervision and continuing detention orders of high-risk offenders. Such risk assessments have traditionally been the function of the human discretion and intuition of judicial officers, based on clinical assessments, framed by legislation and common-law principles, and encapsulating the concept of individualised justice. Yet, the progressive technologisation of criminal procedure is witnessing the incursion of statistical, data-driven evaluations of risk. Human judicial evaluative functions are increasingly complemented by a range of actuarial, algorithmic, machine learning and Artificial Intelligence (AI) tools that purport to provide accurate predictive capabilities and objective, consistent risk assessments. But ethical concerns have been raised globally regarding algorithms as proprietary products with in-built statistical bias as well as the diminution of judicial human evaluation in favour of the machine. This article focuses on risk assessment and what happens when decision-making is delegated to a predictive tool. Specifically, this article scrutinises the inscrutable proprietary nature of such risk tools and how that may render the calculation of the risk score opaque and unknowable to both the offender and the court.\n\n==\n\nChiao 2019:\n\nTitle: Fairness, accountability and transparency: notes on algorithmic decision-making in criminal justice\n\nAbstract: Abstract Over the last few years, legal scholars, policy-makers, activists and others have generated a vast and rapidly expanding literature concerning the ethical ramifications of using artificial intelligence, machine learning, big data and predictive software in criminal justice contexts. These concerns can be clustered under the headings of fairness, accountability and transparency. First, can we trust technology to be fair, especially given that the data on which the technology is based are biased in various ways? Second, whom can we blame if the technology goes wrong, as it inevitably will on occasion? Finally, does it matter if we do not know how an algorithm works or, relatedly, cannot understand how it reached its decision? I argue that, while these are serious concerns, they are not irresolvable. More importantly, the very same concerns of fairness, accountability and transparency apply, with even greater urgency, to existing modes of decision-making in criminal justice. The question, hence, is comparative: can algorithmic modes of decision-making improve upon the status quo in criminal justice? There is unlikely to be a categorical answer to this question, although there are some reasons for cautious optimism.\n\n==\n\nElyounes 2020:\n\nTitle: Bail or Jail? Judicial Versus Algorithmic Decision-Making in the Pretrial System\n\nAbstract: To date, there are approximately sixty risk assessment tools deployed in the criminal justice system. These tools aim to differentiate between low-, medium-, and high-risk defendants and to increase the likelihood that only those who pose a risk to public safety or who are likely to flee are detained. Proponents of actuarial tools claim that these tools are meant to eliminate human biases and to rationalize the decision-making process by summarizing all relevant information in a more efficient way than can the human brain. Opponents of such tools fear that in the name of science, actuarial tools reinforce human biases, harm defendants\u2019 rights, and increase racial disparities in the system. The gap between the two camps has widened in the last few years. Policymakers are torn between the promise of technology to contribute to a more just system and a growing movement that calls for the abolishment of the use of actuarial risk assessment tools in general and the use of machine learning-based tools in particular. \nThis paper examines the role that technology plays in this debate and examines whether deploying artificial intelligence (\u201cAI\u201d) in existing risk assessment tools realizes the fears emphasized by opponents of automation or improves our criminal justice system. It focuses on the pretrial stage and examines in depth the seven most commonly used tools. Five of these tools are based on traditional regression analysis, and two have a machine-learning component. This paper concludes that classifying pretrial risk assessment tools as AI-based tools creates the impression that sophisticated robots are taking over the courts and pushing judges from their jobs, but that impression is far from reality. Despite the hype, there are more similarities than differences between tools based on traditional regression analysis and tools based on machine learning. Robots have a long way to go before they can replace judges, and this paper does not argue for replacement. The long list of policy recommendations discussed in the last chapter highlights the extensive work that needs to be done to ensure that risk assessment tools are both accurate and fair toward all members of society. These recommendations apply regardless of whether machine learning or regression analysis is used. Special attention is paid to assessing how machine learning would impact those recommendations. For example, this paper argues that carefully detailing each of the factors used in the tools and including multiple options to choose from (i.e., not just binary \u201cyes-or-no\u201d questions) will be useful for both regression analysis and machine learning. However, machine learning would likely lead to more personalized and meaningful scoring of criminal defendants because of the ability of machine learning techniques to \u201czoom in\u201d on the unique details of each individual case.\n\n==\n\nW\u00f3jcik 2020:\n\nTitle: Machine-Learnt Bias? Algorithmic Decision Making and Access to Criminal Justice\n\nAbstract: The pressure on the criminal justice system in England and Wales is mounting. Recent figures reveal that despite a rise in recorded crime, the number of defendants in court proceedings has been the lowest in 50 years. This indicates a crisis of access to criminal justice. Predictive policing and risk assessment programmes based on algorithmic decision making (ADM) offer a prospect of increasing efficiency of law enforcement, eliminating delays and cutting the costs. These technologies are already used in the UK for crime-mapping and facilitating decisions regarding prosecution of arrested individuals. In the US their deployment is much wider, covering also sentencing and parole applications.",
        "final_summary": "The papers collectively highlight the implications of AI-based decision-making tools in the criminal justice system for judicial integrity. Barabas 2019 argues that attempts to address bias in AI tools miss deeper issues regarding fairness and the interaction between data regimes and punitive practices. Hartmann 2021 shows how the introduction of risk assessment tools in the criminal justice sector has transformed decision-making processes, introducing statistical prediction to previously uncertain outcomes. Putera 2022 raises concerns about AI sentencing in Malaysia, highlighting conflicts with the rule of law and the risks of error and implicit bias. Yeung 2023 explores the constitutional dangers posed by technical design choices in algorithmic decision-making tools for criminal justice authorities. McKay 2019 examines the ethical concerns surrounding the use of actuarial tools and algorithms in risk assessment, emphasizing the proprietary nature of these tools and their impact on judicial evaluation. Chiao 2019 argues that while fairness, accountability, and transparency are important concerns, they also apply to existing modes of decision-making in criminal justice, and the question is whether algorithmic decision-making can improve upon the status quo. Elyounes 2020 discusses the debate between proponents and opponents of actuarial risk assessment tools, highlighting the need for accuracy and fairness in their implementation. W\u00f3jcik 2020 explores the use of algorithmic decision-making in the criminal justice system in England and Wales, emphasizing the potential for increased efficiency but also the need to address access to justice concerns."
    },
    {
        "query": "AI as a teacher assistant",
        "paper_list_string": "Boulay 2016:\n\nTitle: Artificial Intelligence as an Effective Classroom Assistant\n\nAbstract: The field of artificial intelligence in education (AIED) uses techniques from AI and cognitive science to better understand the nature of learning and teaching and to build systems to help learners gain new skills or understand new concepts. This article studies metareviews and meta-analyses to make the case for blended learning, wherein the teacher can offload some work to AIED systems.\n\n==\n\nBoulay 2016:\n\nTitle: Artificial Intelligence as an Effective Classroom Assistant\n\nAbstract: The field of artificial intelligence in education (AIED) uses techniques from AI and cognitive science to better understand the nature of learning and teaching and to build systems to help learners gain new skills or understand new concepts. This article studies metareviews and meta-analyses to make the case for blended learning, wherein the teacher can offload some work to AIED systems.\n\n==\n\nAlzaabi 2023:\n\nTitle: 'AI-Teacher' Assistant System: A Smart Attendance and Participation tracking system for students\n\nAbstract: Students and teachers are connected in an orderly way to attain learning outcomes, which is the foundation of attendance systems. Academic institutions, however, encounter poor strategies for these parties' participation. In order to address the low attendance and participation among students and faculty in a university community, a university-wide \u2018AI-Teacher\u2019 system will be built in one of the educational institutions in the Gulf region. The system was prepared for by planning, gathering, modelling, and analyzing the requirements, designing the interfaces and databases, and then coding and testing. The AI-Teacher assistant system is approachable and simple to use. A warning is sent to parents after a student misses a predetermined number of classes, along with an automatic or manual attendance record, alerts, and notifications. Also, the application system, \u2018AI-Teacher\u2019 which works with iOS and Android devices, can improve connections between \"students and instructor\". The study concluded with some recommendations for further research and development.\n\n==\n\nKim 2020:\n\nTitle: My Teacher Is a Machine: Understanding Students\u2019 Perceptions of AI Teaching Assistants in Online Education\n\nAbstract: ABSTRACT An increase in demand for online education has led to the creation of a new technology, machine teachers, or artificial intelligence (AI) teaching assistants. In fact, AI teaching assistants have already been implemented in a small number of courses in the United States. However, little is known about how students will perceive AI teaching assistants. Thus, the present study investigated students\u2019 perceptions about AI teaching assistants in higher education by use of an online survey. Primary findings indicate that perceived usefulness of an AI teaching assistant and perceived ease of communication with an AI teaching assistant are key to understanding an eventual adoption of AI teaching assistant-based education. These findings provide support for AI teaching assistant adoption. Based on the present study\u2019s findings, more research is needed to better understand the nuances associated with the learning experience one may have from an AI teaching assistant.\n\n==\n\nZhang 2019:\n\nTitle: Development of an AI based teaching assisting system\n\nAbstract: Nowadays, people are facing the issues of declining birthrates and an aging society. Teaching resources are far from enough and the teachers usually have great amount of works, especially in countryside or small towns. Aim to reduce the work of teachers, we developed a teaching assisting system to automatically track and recognize the motions and behaviors of students based on artificial intelligence (AI) technology. The humans are detected accurately by combing the detection results of OpenPose and human area projection method. Each person is identified by fusing his/her personal information, including the features of color, face and moving history, and tracked by using an extended particle filter method based on Markov Chain Monte Carlo (MCMC). By observing the students' behaviors without getting tired with the sensor system instead of the teachers, the performances and growing processes of all the students can be analyzed, referring the excellent knowledge and experiences of professional teachers. This information can be provided to the teacher which can help the teachers to adjust guidance to the students. Moreover, the information can also be provided to education robot so that the robot can also held a class or have interactions with the students.\n\n==\n\nKim 2019:\n\nTitle: Assisting Teachers with Artificial Intelligence: Investigating the Role of Teachers Using a Randomized Field Experiment\n\nAbstract: This study investigates whether artificial intelligence (AI) can transform the teacher\u2019s role by delivering personalized learning to each individual student. Conducting a randomized controlled trial in collaboration with an education company, we evaluate how providing teachers with AI assistance impacts students\u2019 academic outcomes. We find that providing AI-generated reports to teachers significantly improves students\u2019 study effort and performance, although those effects vary by teacher and class characteristics. Consistent with the organizational literature on technology and worker productivity, our findings indicate that technology overload could undermine teachers\u2019 effective use of AI coaching despite its highly accurate diagnostic ability and ready availability. Although some teachers did not utilize the AI coaching program, we found positive effects among the students of both compliers and non-compliers. We, therefore, decompose our results to identify direct and spillover effects on students\u2019 academic performance and show that failing to account for spillover effects across teachers (externality among peers) and within teachers (learning-by-doing across students) may understate the effects of AI coaching. Finally, we provide practical guidelines for implementing technology in educational settings.\n\n==\n\nEdwards 2018:\n\nTitle: I, teacher: using artificial intelligence (AI) and social robots in communication and instruction*\n\nAbstract: ABSTRACT Human\u2013machine communication has emerged as a new relational context of education and should become a priority for instructional scholarship in the coming years. With artificial intelligence and robots offering personalized instruction, teachers\u2019 roles may shift toward overseers who design and select machine-led instruction, monitor student progress, and provide support. In this essay, we argue that bringing the sensibilities of instructional researchers to bear on these issues involving machine agents, within and outside the traditional classroom walls, is vitally important.\n\n==\n\nal-Zyoud 2020:\n\nTitle: The Role of Artificial Intelligence in Teacher Professional Development\n\nAbstract: This paper aims to present the idea, development, and field of artificial intelligence (AI), considering its impact on teachers' professional development. In addition to the technical applications that support AI in the educational sector, and the problems facing its uses, the importance of artificial intelligence in the education field has been addressed and previous study cases that competed AI with commentary have been identified. This study uses an analytical approach based on a theoretical survey of previous literature to answer the study questions. The results refer to a set of intelligent applications as well as areas of artificial intelligence that can serve the professional development of teachers. Furthermore, we recommend to: Design educational software based on artificial intelligence to raise the qualifications of teachers, Build training pathways based on artificial intelligence for all those working in the field of education, Provide accurate databases in all areas' education including human resources, Provide educational training environments that add to improving the part of artificial intelligence applications in the professional development of teachers, and Link databases within a global network that helps challenge teachers' career paths.",
        "final_summary": "AI can effectively assist in the classroom by offloading some work from teachers, facilitating blended learning (Boulay 2016). An AI-Teacher system can improve student-instructor communication and track attendance (Alzaabi 2023). Students' adoption of AI teaching assistants is influenced by their perceived usefulness and ease of communication (Kim 2020). AI can also provide insights to teachers by tracking and analyzing student behavior (Zhang 2019). AI-generated reports can enhance students' study effort and performance (Kim 2019). The role of AI in education needs to be addressed by instructional researchers (Edwards 2018). AI has potential in aiding teacher professional development (al-Zyoud 2020)."
    },
    {
        "query": "write an essay on how general knowledge and awareness correlate with perceived behavioural control in relation to consuming food products especially unfamiliar low trophic level aquaculture products.",
        "paper_list_string": "Pieniak 2013:\n\nTitle: Consumer knowledge and use of information about fish and aquaculture\n\nAbstract: This paper explores consumers\u2019 knowledge about fish and aquaculture and assesses the use and importance of different information cues about fish. Cross-sectional data were collected in 2008 through a consumer survey (n=3213) in the Czech Republic, Germany, Greece, Italy, Portugal, Romania, Sweden and the UK. Consumers\u2019 knowledge about fish generally, and about aquaculture in particular, was relatively low and differed significantly between countries. Consumers from all countries reported an indication of quality and/or food safety as an information cue when buying fish. The information sources most frequently used by Europeans were labelling and sellers in retail or supermarkets. The Internet was identified by consumers in all of the countries as one of the most important sources of information about sea and freshwater fish products. Policy makers and food marketers are encouraged to develop a simple and easily recognisable mark (relating to quality, food safety and nutrition) to assist consumer decision-making. Information campaigns focusing on issues such as the nutritional benefits of eating fish are also recommended.\n\n==\n\nBanovi\u0107 2019:\n\nTitle: \u201cOne Fish, Two Fish, Red Fish, Blue Fish\u201d: How ethical beliefs influence consumer perceptions of \u201cblue\u201d aquaculture products?\n\nAbstract: Respecting ethical beliefs of consumers is an important precondition for food manufacturers in their attempt to improve their positioning in the European food market. Based on a cross-cultural survey of 2511 European participants, this research demonstrates how ethical beliefs affect consumer perceptions of \u201cblue\u201d (i.e. environmentally friendly) aquaculture products. The study further emphasises that the positive effect of ethical beliefs on purchase intention operates via an indirect route mediated by consumers\u2019 trust in a product category. Consumer involvement has limited moderation effect on the above relationships. To expand its \u201cblue\u201d business, a key policy recommendation to aquaculture product manufacturers and policy makers is to urge stable and reliable standards of control in environmentally responsible aquaculture production so that consumers can rely on the information source and increase their trust in aquaculture products.\n\n==\n\nVerbeke 2005:\n\nTitle: Individual determinants of fish consumption: application of the theory of planned behaviour\n\nAbstract: This study investigates individual determinants of fish consumption behaviour based on cross-sectional data collected in Belgium. Analyses show that determinants as hypothesised by the theory of planned behaviour (TPB) and personal characteristics influence fish consumption intention and frequency. Favourable attitude, high subjective norm and high perceived behavioural control have a positive impact on fish consumption decisions. Significant habit effects are detected when including habit as a separate regressor of behavioural intention and behaviour. Appreciation of the attribute taste emerges as the most important driver for eating fish, followed closely by health. Bones and price constitute the negative attitude factor, which, however, does not directly reduce behavioural intention. Individual determinants pertain to gender, age, children, income, education level and region. Fish consumption frequency in compliance with health recommendations is higher among women and increases with increasing age, while the presence of children in the household leads to lower fish consumption. The lowest income class has the lowest fish consumption frequency. Higher education results in a higher intention to eat fish but has no effect on the consumption frequency itself. The coastal region of West Flanders is the region with the highest consumption. Food involvement correlates positively with fish consumption intention and frequency, whereas no significant impact of food-health awareness is found.\n\n==\n\nAitken 2020:\n\nTitle: The positive role of labelling on consumers\u2019 perceived behavioural control and intention to purchase organic food\n\nAbstract: Abstract The consumption of organic food has increased dramatically in recent years in response to consumers\u2019 concerns with issues related to health, well-being and the environment. However, further increases are restricted by a number of barriers, one of the most important of which is information. Using the Reasoned Action Approach (RAA), this study is the first to examine the role of product specific information (labelling) to understand the gap between consumer attitude and behavioural intention to purchase organic food products. Based on responses from 1,052 New Zealand consumers, analysis using structural equation modelling demonstrates that labelling plays an important role within the perceived behavioural control dimension of the RAA, as well as directly on attitudes, in influencing behavioural intention. The more that respondents agree that labelling is actionable, the more positive their attitude and sense of control, and hence intention and (self-reported) behaviour. These findings suggest that by improving labelling systems to include more actionable information, such as the health, environmental and societal benefits of products, consumers perceived behavioural control can be increased to strengthen intentions to purchase organic products.\n\n==\n\nGempesaw 1995:\n\nTitle: Consumer Perceptions of Aquaculture Products\n\nAbstract: The consumption of seafood products, including aquacultural products, significantly increased during the 1970s and 1980s. However, during the late 1980s and early 1990s, per capita seafood consumption declined. The U.S. Department of Agriculture reports that current per capita consumption of seafood products is estimated at around 14.9 pounds, down from a peak of 16.1 pounds in 1987 (USDA). Uncertainty in the seafood supply has contributed to retail seafood prices rising faster than prices for other meat products (Harvey). To reduce the fluctuation in retail seafood prices, there is a need to stabilize production and offer consistency in supplies. However, consistency in the seafood supply can be expected to come primarily from aquaculture because of the problems of overutilization of natural or wild stocks (United Nations). Proponents of aquaculture are advocating its commercial expansion. However, several problems must be resolved before aquaculture's potential can be achieved. The first problem lies in the biological potential of a species to survive in an aquaculture environment. The second factor deals with the consumer's willingness to buy aquaculture products. The stiff competition offered by the beef, pork, poultry, and wild fisheries industries requires the aquaculture industry to use effective marketing strategies. This study focuses on the second issue raised regar ing the willingness of consumers to purchase products obtained from either the wild fishery or aquaculture. In particular, it deals with consumer preferences for fresh finfish and shellfish products purchased for home preparation and consumption in northeastern and MidAtlantic households.\n\n==\n\nSparks 1992:\n\nTitle: An investigation into the relationship between perceived control, attitude variability and the consumption of two common foods\n\nAbstract: The study reported here takes its lead from the literatures which emphasize the importance of attitude variability and the role of perceived control over action. Within-person variability and perceptions of control are investigated in the context of people's attitudes towards the consumption of two common foods. The role of attitude ambivalence is also examined. The findings indicate that higher attitude variability is associated with weaker relationships between the components of the theory of reasoned action and that attitude variability is negatively related to perceived control. Moreover, perceived control is shown to be related to different sorts of control problem for different behaviours. It is advocated that a more in-depth assessment of attitude variability and the perceived control construct is merited and that recent calls for more serious examination of attitude ambivalence are well-founded.\n\n==\n\nHasan 2020:\n\nTitle: The Influence of Attitude, Subjective Norm and Perceived Behavioral Control towards Organic Food Purchase Intention\n\nAbstract: This study aims to determine the determinants of intention in buying organic food. This thesis focuses on the consumer of organic food products in Indonesia who use online media in Instagram to make their organic food purchases with Theory of Planned Behavior. The data collection in this study was conducted in two stages, namely pre-survey to analyze opinion leaders and types of organic foods that will be used as a limitation of the study and the second stage is the research questionnaire. The analysis technique used is qualitative data analysis. Based on the results of the study conclusions can be drawn as follows:\u00a0 adherence to organic food and perceived behavioral control significantly influence buying intention in organic food, while subjective norms are found to have no significant effect on buying intention in organic food. Attitudes toward organic food have the strongest influence among the other two variables. Subjective norms have the lowest influence. Subjective norms or pressure from other people to consume organic food apparently do not have a significant effect because it does not originate from oneself (external) or is not their own volition, in contrast to the attitude and perceived behavior control variables that originate from oneself.\n\n==\n\nSiddique 2012:\n\nTitle: Explaining the Role of Perceived Risk, Knowledge, Price, and Cost in Dry Fish Consumption Within the Theory of Planned Behavior\n\nAbstract: ABSTRACT The general purpose of this study is to explore the influence of perceived risk, knowledge, price, and cost of dry fish consumption in Bangladesh using the general framework from the theory of planned behavior (TPB). The field experiments were performed in Bangladesh with a convenience sample of 558 respondents. Multiple regression analysis was used to investigate the relationship among perceived risk, knowledge, price, and cost and TPB constructs. The findings showed that attitude, norms, and procedural knowledge emerge as the key determinants of intention, whereas intention, attitude, norms, and perceived risk have effects on the dry fish consumption frequency. Perceived behavioral control, price, and cost had no significant effect on intention and consumption behavior. The proposed TPB and the extended model both fit the data well and proved the expectations of this study in a promising way. The study found that people are more concerned about potential long-term risk to their family and others. Therefore, management attention should focus on reducing risks which consumers may face through producing safe and hazard-free dry fish.",
        "final_summary": "The papers suggest that general knowledge and awareness play a role in perceived behavioral control regarding the consumption of unfamiliar low trophic level aquaculture products.\n\nPieniak 2013 found that consumers' knowledge about fish and aquaculture was generally low, and information cues such as quality and food safety were important when buying fish. Labelling and sellers in retail or supermarkets were the most frequently used sources of information, while the Internet was identified as an important source for information about fish products.\n\nBanovi\u0107 2019 highlighted the influence of ethical beliefs on consumer perceptions of environmentally friendly aquaculture products. Ethical beliefs positively affected purchase intention through consumers' trust in the product category. Consumer involvement had a limited moderating effect on these relationships.\n\nVerbeke 2005 investigated individual determinants of fish consumption and found that favorable attitude, subjective norm, and perceived behavioral control positively influenced fish consumption decisions. Taste and health were important drivers for eating fish, while bones and price had a negative impact on attitude. Personal characteristics such as gender, age, children, income, education level, and region also influenced fish consumption.\n\nAitken 2020 examined the role of labelling in consumers' perceived behavioral control and intention to purchase organic food. Labelling played an important role in influencing attitudes and perceived behavioral control, thereby strengthening intentions to purchase organic products. The study suggested that actionable information on labelling, such as health and environmental benefits, could increase consumers' perceived behavioral control.\n\nGempesaw 1995 discussed consumer preferences for seafood products obtained from wild fisheries or aquaculture. The study emphasized the need for effective marketing strategies to compete with other meat products. Stability in the seafood supply, primarily from aquaculture, was seen as crucial for reducing fluctuating retail seafood prices.\n\nSparks 1992 explored the relationship between perceived control, attitude variability, and the consumption of common foods. Higher attitude variability was associated with weaker relationships between components of the theory of reasoned action, and perceived control was negatively related to attitude variability. The study suggested a need for further assessment of attitude variability and perceived control.\n\nHasan 2020 investigated the determinants of intention to buy organic food and found that adherence to organic food and perceived behavioral control significantly influenced buying intention. Attitudes toward organic food had the strongest influence, while subjective norms had the lowest impact.\n\nSiddique 2012 examined the influence of perceived risk, knowledge, price, and cost on dry fish consumption. Attitude, norms, and procedural knowledge were key determinants of intention, while intention, attitude, norms, and perceived risk influenced consumption frequency. Perceived behavioral control, price, and cost had no significant effect on intention and consumption behavior.\n\nIn summary, the papers suggest that general knowledge and awareness, along with factors such as ethical beliefs, labelling, attitudes, subjective norms, and perceived behavioral control, influence consumers' intentions and behaviors regarding the consumption of unfamiliar low trophic level aquaculture products."
    },
    {
        "query": "are questionaire the common method in social science?",
        "paper_list_string": "Yufang 1999:\n\nTitle: Some Considerations of Questionaire Method in Education from Angle of Measurement\n\nAbstract: Educational Questionaire Method is a method in common use.In the process of use,however,there are some problems existing in the method,which affect the scientific nature of it.This essay maintains that it will help to improve the scientific nature of questionaire method if we pay attention to the theoretical conception in its designing,make the process of quantification tally with measuring principles,and make textual research to the reliability and validity of the questionaires.\n\n==\n\nTan\u010di\u0107 2019:\n\nTitle: The application of survey questionnaire in the methodology of social sciences in the example of Kosovo and Metohija\n\nAbstract: In the methodology of social and political sciences it is generally known that without survey researches and survey questionnaires it is possible neither to conduct a lot of researches nor to reach new scientific knowledge. That was determined from one hand, by the scientific field, and from another one, by the theme, project task, and conceptual sketch, draft of the scientific idea and relation of conceptualisation toward the pre-research and toward the interpretation and practicing of the research results. The survey is the technique of the inspection method, and survey questionnaire an instrument so that their application requests a lot of scientific and professional activities starting from the planning of survey inspections until the making of survey questionnaire, formulation and forms of questions etc. In scientific fund there are \"two general approaches of the comprehension of survey. From one hand, there are comprehensions which consider the survey as a scientific method... and from another one, there are comprehensions which consider survey as only one of techniques of scientific method of inspection\" (Tancic, Tancic 2019: 185).\n\n==\n\nManabe 2012:\n\nTitle: Social Research and Sociological Theory : Toward an Innovation of Social Analysis through the Questionnaire Method (\u9ad9\u5742\u5065\u6b21\u6559\u6388\u9000\u8077\u8a18\u5ff5\u53f7)\n\nAbstract: With social research positioned as a technique for building sociological theories, the crux of the problem being discussed here becomes clear. (1) It is the problem of an imbalance between public opinion polls and survey research. On one hand, this is due to the prevalence of public opinion polls, and on the other, the stagnation of survey re- search. This is not a healthy situation for building sociological theories. Here, I distin- guish the former as a technique that was developed based on social needs, focuses on social issues, and is oriented toward the description of people\u2019s subjective conscious- ness, as opposed to the latter, which is a technique that was developed based on aca- demic needs, to address a variety of aspects of social living, and is oriented toward the analysis of people\u2019s subjective consciousness. This paper looks at the commonalities between the two, referring to them both as the \u201cquestionnaire method.\u201d (2) There is a problem involving the heavy use, perhaps overuse, of public opinion polls and survey research, namely, the questionnaire method. Among the social phenomena that exist, there are some that can be approached using the questionnaire method and some that cannot. The current problem is that the questionnaire method is being used with too lit- tle regard to whether it is appropriate for a specific research question. (3) It is almost as if the researcher starts with the method, and then uses that method regardless of the nature of the research. Originally, a study began with observations of a subject; meth- ods for approaching that subject were then developed based on those observations. However, in the case of the questionnaire method, this American-made technique was already available, where the research needed to be conducted before observations of the various aspects of people\u2019s subjective consciousness in various societies were made. In Germany, the German expression \u201cLehne ein bisschen ab\u201d was artificially created as a survey term meant to correspond to the English phrase \u201cDisagree a little.\u201d This is a typical example of this problem. Looking at these problems, one might begin to think that the questionnaire method should be methodologically rejected, but that is not at all the case. As is the case with many kinds of tools, no method is perfect or entirely without fault. Tools must be used appropriately for their particular purposes. This is where the exploration of the poten- tial uses of the questionnaire method begins. This is an effort to \u201cconfirm the dimen- sions\u201d for the description, classification and measurement of a specific social phenome- non, namely subjective reality (people\u2019s subjective consciousness).\n\n==\n\nRobinson 2002:\n\nTitle: Ethics of questionnaires \u2014 again\n\nAbstract: In the 6 years I have been writing this column, one piece stands out as having had the greatest response. It whizzed round university campuses like wildfire. It described simple and obvious concerns about ethics in social science research (Robinson, 1996). Apparently, it came as news to many that asking questions was not necessarily harmless. Everyone, it seemed, was concocting their own little questionnaire, and because they weren\u2019t taking blood or biopsies, assumed there could be no ethical problems. Those who applied for approval to any of the three research committees I sat on soon learned otherwise\u2026 I was simply insisting on the same standards of quality and ethics for social science as for clinical research.\n\n==\n\nSimion 2012:\n\nTitle: THEORETICAL AND APPLIED ASPECTS OF EVALUATION THROUGH STATISTICAL QUESTIONAIRE\n\nAbstract: This paper contains a review of the practice and theory of statistical questionnaires. It points out the increasing distance between the theory of selective research and the practice of using surveys. This distance is being amplified nowadays by the economical and social changes in general and those of the academic environment in particular. In addition, the paper points out the danger of diminishing the scientific reputation of these statistical methods and tools through the excessive contact of individuals with different types of questionnaires suggests that the usage of these tools should be adapted correctly by considering the peculiarities of all the field.\n\n==\n\nSmall 1907:\n\nTitle: Are the Social Sciences Answerable to Common Principles of Method? II\n\nAbstract: Having replied in the first part of this paper' to certain specific criticisms in Dr. Hoxie's \"Rejoinder,\" 2 I shall now attempt to express more positively two or three rather elementary methodological principles. I am not yet sure whether there is a real difference of judgment about them between Dr. Hoxie and myself, or whether the argument amounts only to an incident in the race-hatreds between vocabularies. In either case, no better way of approaching an understanding is in sight than the frankest possible use of the words at command on both sides. In reviewing the scope of this discussion I am impressed with the necessity of appeal to primary principles which the social sciences can hardly repudiate. As I see the situation, the occasion for such a discussion as this arises from the degree of inattention to formulation of these principles which has become habitual. It is hard to believe that responsible scholars would deliberately deny their substantial validity. I have the best of reasons for believing that Dr. Hoxie's own practice, for example, is a salutary object-lesson in consistent respect for the very principles about which we find ourselves disagreeing in the abstract. As I said in the first part of this paper, we seem to be unable to use words in a way that makes us sure of each other's ideas. The removal of the misunderstanding, I repeat, is probably not feasible through a process of logical proof. It must be chiefly through increased attention to conditions or elements of the scientific process which are out of sight when we\n\n==\n\nRoberts 1999:\n\nTitle: In defence of the survey method: An illustration from a study of user information satisfaction\n\nAbstract: The survey method is one of the most common approaches used in the social sciences to empirically study the characteristics and interrelations of sociological and psychological variables. Its impact on research in accounting and related disciplines has been substantial. However, the method has often been criticised. This paper describes and critically assesses a survey undertaken in management information systems. Its purpose is twofold. First, it aims to counter some of the criticisms of the survey method by demonstrating how many of the potential weaknesses of surveys can be overcome. Second, it provides a prescription for rigorous survey research.\n\n==\n\nCoccia 2018:\n\nTitle: An Introduction to the Methods of Inquiry in Social Sciences\n\nAbstract: Abstract. The purpose of this paper is an introduction to methods for conducting inquiries in social sciences. A method of inquiry is an organized and systematic scientific approaches used by scholars for controlled investigations and experiments to efficiently solve theoretical and practical problems, generating discoveries and/or science advances. The paper here presents, briefly, the development of models of inquiry in the philosophy of science. After that, it introduces general methods of inquiry (deduction, induction, abduction and hypothetical-deductive approaches) and specific models of scientific inquiry in social sciences, such as multiple working hypotheses. In general, modern scientific research is multifaceted and requires different approaches for generating new concepts, new hypothesis and theories. Different methods of inquiry directed to solve problems in science can be complementary approaches that foster knowledge creation within and between research fields. Keywords. Methods of inquiry, Philosophy of science, Models of inquiry, Scientific research, Conduct of inquiry. JEL. A20, A29, B40, B49.",
        "final_summary": "Questionnaires are a prevalent method in social science research, but their use comes with certain considerations and criticisms. Yufang (1999) underscores the importance of theoretical conception, quantification principles, and the reliability and validity of questionnaires. Tan\u010di\u0107 (2019) asserts the integral role of survey questionnaires in generating new scientific knowledge in social and political sciences. Manabe (2012) points out an imbalance between public opinion polls and survey research, suggesting a need for differentiation based on social versus academic needs. Ethical concerns surrounding the use of questionnaires are highlighted by Robinson (2002), who advocates for the same ethical standards as in clinical research. Simion (2012) warns of a growing gap between theory and practice in the use of statistical questionnaires. Small (1907) and Coccia (2018) discuss the necessity of adhering to methodological principles in social sciences and the variety of inquiry methods, respectively. Lastly, Roberts (1999) defends the survey method, demonstrating how potential weaknesses can be addressed to ensure rigorous research."
    },
    {
        "query": "what is the relation between cervical disc pathology and cervical posture?",
        "paper_list_string": "C\u00e2mara-Souza 2018:\n\nTitle: Cervical posture analysis in dental students and its correlation with temporomandibular disorder\n\nAbstract: Abstract Objective: To evaluate the relationship between temporomandibular disorders (TMD) and craniocervical posture in the sagittal plane measured from lateral radiographs of the head. Methods: The sample was comprised of 80 randomly selected students of dentistry at the Federal University of Rio Grande do Norte. Research Diagnostic Criteria for TMD (RDC/TMD) was used to evaluate the signs and symptoms of TMD. Lateral radiographs of each individual were used to measure the position of the hyoid bone, the craniocervical angle, and the occiput\u2013atlas distance. A chi-square test was used to evaluate the relationships between craniocervical posture measures and TMD. Results: No relationship was found between TMD and the craniocervical posture measured by the positioning of the hyoid bone, head rotation, and the extension/flexion of the head (p > 0.05). Conclusion: It can be concluded, therefore, that no relationship exists between cervical posture in the sagittal plane and TMD.\n\n==\n\nLaat 1998:\n\nTitle: Correlation between cervical spine and temporomandibular disorders\n\nAbstract: Abstract Neuroanatomical interconnections and neurophysiological relationships between the orofacial area and the cervical spine have been documented earlier. The present single-blind study was aimed at screening possible correlations between clinical signs of temporomandibular disorders (TMD) and cervical spine disorders. Thirty-one consecutive patients with symptoms of TMD and 30 controls underwent a standardised clinical examination of the masticatory system, evaluating range of motion of the mandible, temporomandibular joint (TMJ) function and pain of the TMJ and masticatory muscles. Afterwards subjects were referred for clinical examination of the cervical spine, evaluating segmental limitations, tender points upon palpation of the muscles, hyperalgesia and hypermobility. The results indicated that segmental limitations (especially at the C0\u2013C3 levels) and tender points (especially in the m. sternocleidomastoideus and m. trapezius) are significantly more present in patients than in controls. Hyperalgesia was present only in the patient group (12\u201316%).\n\n==\n\nYoo 2009:\n\nTitle: The relationship between the active cervical range of motion and changes in head and neck posture after continuous VDT work.\n\nAbstract: This study investigated the relationship between the active cervical range of motion (ROM) and changes in the head and neck posture after continuous visual display terminal (VDT) work. Twenty VDT workers were recruited from laboratories. The active cervical ROM of the participants was measured and videotaped to capture the craniocervical and cervicothoracic angles using a single video camera before and after VDT work. Pearson correlation coefficients were used to quantify the linear relationship between active cervical ROM measurements and the changes in the craniocervical and cervicothoracic angles after continuous VDT work. Active neck extension (r=-0.84, p<0.01) was negatively correlated with the mean craniocervical angle, and active neck flexion (r=-0.82, p<0.01) and left lateral flexion (r=-0.67, p<0.01) were negatively correlated with the mean cervicothoracic angle.\n\n==\n\nSonnesen 2007:\n\nTitle: Cervical column morphology related to head posture, cranial base angle, and condylar malformation.\n\nAbstract: The present study describes the cervical column as related to head posture, cranial base, and mandibular condylar hypoplasia. Two groups were included in the study. The 'normal' sample comprised 21 subjects, 15 females aged 23-40 years (mean 29.2 years), and six males aged 25-44 years (mean 32.8 years) with neutral occlusion and normal craniofacial morphology. The condylar hypoplasia group comprised the lateral profile radiographs of 11 patients, eight females, and three males, aged 12-38 years (mean 21.6 years). For each individual, a profile radiograph was taken to perform a visual assessment of the morphology of the cervical column. For the normal group only, the profile radiographs were taken in the standardized head posture to measure the head posture and the cranial base angle. Cervical column: Morphological deviations of the cervical column occurred significantly more often in the subjects with condylar hypoplasia compared with the normal group (P < 0.05 and P < 0.01, respectively). The pattern of morphological deviations was significantly more severe in the subjects with condylar hypoplasia compared with the normal group (P < 0.01). Cervical column related to head posture and cranial base: The cervicohorizontal and cranial base angles were statistically larger in females than in males (P < 0.05 and P < 0.01, respectively). No statistically significant age differences were found. Only in females was the cervical lordosis angle (OPT/CVT, P < 0.01), the inclination of the upper cervical spine (OPT/HOR, P < 0.05), and the cranial base angle (n-s-ba, P < 0.05) significantly positively correlated with fusion of the cervical column. These associations were not due to the effect of age.\n\n==\n\nTonetti 2004:\n\nTitle: Morphological cervical disc analysis applied to traumatic and degenerative lesions\n\nAbstract: Trauma and degenerative pathologies at the lower cervical spine are different from lumbar spine pathologies. However, the description of cervical discs is classically taught similarly to that of the lumbar discs. Recent studies have raised this issue, and in 1999, Mercer and Bogduk described ventral annulus fibrosus as a crescent-shaped interosseous ligament. We propose a metric analysis of the different components of the cervical disc to examine this description. We analyzed 140 sagittal and coronal transections of 35 discs. These discs were taken from seven cervical spines at the five lower levels, C2-C3, C3-C4, C4-C5, C5-C6 and C6-C7. We measured quantitative parameters on sagittal, para-sagittal, ventral coronal and dorsal coronal colored transections: disc length (L), ventral annulus thickness (VAF), lateral annulus thickness (LAF), dorsal annulus thickness (DAF), length of the fibrocartilaginous tissue (FC), sagittal and coronal fibrocartilaginous core ratio (% Core) and intra-disc cleft length (Cleft). We also measured two qualitative parameters: degenerative disease of cartilaginous end plates and total intra-disc cleft. Finally, we examined 114 transections, and 18.5% were ruled out. The results showed thick ventral annulus fibrosus, thin lateral annulus and a very thin dorsal annulus. Fibrocartilaginous tissue filled the dorsal sagittal half of the disc. Intra-disc cleft split the fibrocartilaginous tissue and spread through the ventral annulus only six times. The shape of the ventral annulus at the lower cervical spine is compared to a pivot-hinge device. The aspect is functionally discussed in regard to teardrop fractures, unilateral locked facet syndrome and degenerative changes in the unco-vertebral area.\n\n==\n\nRao 2002:\n\nTitle: Neck pain, cervical radiculopathy, and cervical myelopathy: pathophysiology, natural history, and clinical evaluation.\n\nAbstract: Degenerative cervical disk disease is a ubiquitous condition that is, for the most part, asymptomatic. When symptoms do arise as a result of these degenerative changes, they can be easily grouped into axial pain, radiculopathy and myelopathy. While the pathophysiology of radiculopathy and myelopathy is better understood, the source of neck pain remains somewhat controversial. A discussion of the mechanisms of neck and suboccipital pain, and the chemical and mechanical factors responsible for neurologic symptoms is warranted. Examination of the patient with these symptoms will reveal variations in the clinical presentation. A thorough understanding of the natural history of these conditions will allow appropriate treatment to be carried out. The natural history of these conditions suggests that for the most part patients with axial symptoms are best treated without surgery, while some patients with radiculopathy will continue to be disabled by their pain, and may be candidates for surgery. Myelopathic patients are unlikely to show significant improvement, and in most cases will show stepwise deterioration. Surgical decompression and stabilization should be considered in these patients.\n\n==\n\nNakashima 2015:\n\nTitle: Cervical Disc Protrusion Correlates With the Severity of Cervical Disc Degeneration: A Cross-Sectional Study of 1211 Relatively Healthy Volunteers\n\nAbstract: Study Design. Cross-sectional study. Objective. The purposes of this study were (1) to investigate the frequency and degree of cervical disc degeneration and protrusion on cervical spine magnetic resonance (MR) images and (2) to analyze the correlation between the severity of disc degeneration and disc protrusion. Summary of Background Data. Cervical disc degenerative changes or protrusion is commonly observed on MR images in healthy subjects. However, there are few large-scale studies, and the frequency and range of these findings in healthy subjects have not been clarified. Moreover, there are no reports regarding the correlation between cervical disc degeneration and disc protrusion. Methods. Cervical disc degeneration and protrusion were prospectively measured using magnetic resonance imaging in 1211 relatively healthy volunteers. These included at least 100 males and 100 females in each decade of life between the 20s and the 70s. Cervical disc degeneration was defined according to the modified Pfirrmann classification system, and the amount of disc protrusion was evaluated using the anteroposterior diameter of disc protrusion on sagittal MR image. Results. Mild disc degeneration was very common, including 98.0% of both sexes in their 20s. The severity of cervical disc degeneration significantly increased with age in both sexes at every level. The disc degeneration predominantly occurred at C5\u2013C6 and C6\u2013C7. The difference between sexes was not significant except for individuals in their 50s. The average anteroposterior diameter of disc protrusion increased with aging, especially from the 20s to the 40s. The anteroposterior diameter of disc protrusion increased with a progression in the disc degeneration grade. Conclusion. Cervical disc degeneration and protrusion were frequently observed in healthy subjects even in their 20s and deteriorated with age. Cervical disc protrusion was significantly correlated with cervical disc degeneration, and spatial cervical disc protrusion was affected by biochemical degenerative changes as observed on MR images. Level of Evidence: 2\n\n==\n\nLiu 2015:\n\nTitle: Are the standard parameters of cervical spine alignment and range of motion related to age, sex, and cervical disc degeneration?\n\nAbstract: OBJECT\nThe aims of this study were 1) to establish the standard parameters of alignment and total and segmental range of motion (ROM) of the cervical spine in the asymptomatic population, and 2) to identify factors that influence cervical ROM and alignment.\n\n\nMETHODS\nThe authors measured 636 standard cervical lateral, flexion, and extension plain radiographs of 212 asymptomatic volunteers. The relationship between cervical alignment and total ROM was assessed with simple linear regression. Multivariate linear regression was used to determine the effect of the influential factors on cervical alignment and total and segmental ROM.\n\n\nRESULTS\nThe mean value for C2-7 cervical alignment was 21.40\u00b0 \u00b1 12.15\u00b0, and the mean value for total ROM was 63.59\u00b0 \u00b1 15.37\u00b0. Sex was a significant factor in cervical alignment, total ROM, and segmental ROM for C2-3 and C5-6 (p < 0.05). Age had a significant negative association with both the total ROM and all of the segmental ROM measurements (p < 0.05). Cervical disc degeneration at the level of interest had a significant negative association with C4-5, C5-6, and C6-7 ROM (p < 0.05).\n\n\nCONCLUSIONS\nCervical alignment in female subjects was 2.47\u00b0 lower than that in male subjects. Total ROM was 3.86\u00b0 greater in female than in male subjects and decreased 6.46\u00b0 for each decade of aging. Segmental ROM decreased 1.28\u00b0 for each decade of aging and 2.26\u00b0 for each category increase in disc degeneration at the level of interest.",
        "final_summary": "The relationship between cervical disc pathology and cervical posture is complex. Yoo 2009 found a negative correlation between active cervical range of motion (ROM) and changes in head and neck posture after continuous visual display terminal (VDT) work. Sonnesen 2007 observed morphological deviations in the cervical column in patients with condylar malformation. Tonetti 2004 provided a metric analysis of cervical disc components, describing the shape and distribution of fibrocartilaginous tissue. Rao 2002 discussed the pathophysiology, natural history, and clinical evaluation of neck pain, cervical radiculopathy, and cervical myelopathy. Nakashima 2015 found a high frequency of cervical disc degeneration and protrusion in healthy subjects, with a correlation between the severity of degeneration and protrusion. Liu 2015 established standard parameters for cervical alignment and range of motion (ROM) in the asymptomatic population, with sex, age, and cervical disc degeneration influencing these parameters. (Yoo 2009; Sonnesen 2007; Tonetti 2004; Rao 2002; Nakashima 2015; Liu 2015)"
    },
    {
        "query": "recent advances and therapeutic approache to Adult ADHD",
        "paper_list_string": "Knouse 2008:\n\nTitle: Recent developments in the psychosocial treatment of adult ADHD\n\nAbstract: Adult attention-deficit/hyperactivity disorder (ADHD) is an increasingly recognized Diagnostic and Statistical Manual of Mental Disorders (DSM)-IV psychiatric disorder associated with significant functional impairment in multiple domains. Although stimulant and other pharmacotherapy regimens have the most empirical support as treatments for ADHD in adults, many adults with the disorder continue to experience significant residual symptoms. In the present manuscript, we review the published studies examining group and individual psychosocial treatments for adult ADHD. We include a discussion of coaching interventions and how they differ from cognitive\u2013behavioral therapy. We conclude that the available data support the use of structured, skills-based psychosocial interventions as a viable treatment for adults with residual symptoms of ADHD. Common elements across the various treatment packages include psychoeducation, training in concrete skills (e.g., organization and planning strategies) and emphasis on outside practice and maintenance of these strategies in daily life. These treatments, however, require further study for replication, extension and refinement. Finally, we suggest future directions for the application of psychosocial treatments to the problems of adults with ADHD.\n\n==\n\nAntshel 2011:\n\nTitle: Advances in understanding and treating ADHD\n\nAbstract: Attention deficit hyperactivity disorder (ADHD) is a neurocognitive behavioral developmental disorder most commonly seen in childhood and adolescence, which often extends to the adult years. Relative to a decade ago, there has been extensive research into understanding the factors underlying ADHD, leading to far more treatment options available for both adolescents and adults with this disorder. Novel stimulant formulations have made it possible to tailor treatment to the duration of efficacy required by patients, and to help mitigate the potential for abuse, misuse and diversion. Several new non-stimulant options have also emerged in the past few years. Among these, cognitive behavioral interventions have proven popular in the treatment of adult ADHD, especially within the adult population who cannot or will not use medications, along with the many medication-treated patients who continue to show residual disability.\n\n==\n\nKnouse 2010:\n\nTitle: Current status of cognitive behavioral therapy for adult attention-deficit hyperactivity disorder.\n\nAbstract: Attention-deficit/hyperactivity disorder (ADHD) is a valid and impairing psychological disorder that persists into adulthood in a majority of cases and is associated with chronic functional impairment and increased rates of comorbidity. Cognitive behavioral therapy (CBT) approaches for this disorder have emerged recently, and available evidence from open and randomized controlled trials suggests that these approaches are promising in producing significant symptom reduction. A conceptual model of how CBT may work for ADHD is reviewed along with existing efficacy studies. A preliminary comparison of effect sizes across intervention packages suggests that targeted learning and practice of specific behavioral compensatory strategies may be a critical active ingredient in CBT for adult ADHD. The article concludes with a discussion of future directions and critical questions that must be addressed in this area of clinical research.\n\n==\n\nHorrigan 2001:\n\nTitle: Present and future pharmacotherapeutic options for adult attention deficit/hyperactivity disorder\n\nAbstract: Attention deficit/hyperactivity disorder (ADHD) is often a lifelong condition. When untreated or undertreated, it appears to have a deleterious impact upon the daily functioning of the majority of adults that were diagnosed with this condition during childhood. Effective treatment, under the best circumstances, is multi-modal. The recent MTA study staged by the United States government confirmed the primary role of psychostimulants for children with this condition. The findings from this study have been generalised to adults that also have ADHD, particularly in cases where there is a well-defined longitudinal history dating back to early childhood. Psychostimulants remain a viable first-choice strategy for adults with ADHD. There are idiosyncratic differences in response to the various psychostimulants for any given individual with ADHD. Furthermore, the emergence of long-acting, once daily psychostimulant medications is likely to improve the calibre of care for adults with ADHD. A number of alternative pharmacotherapies have been studied, or are being developed, for adults with ADHD. These pharmacotherapies include antidepressant medications that affect dopaminergic and noradrenergic bioavailability, as well as cholinergic agents. In addition, agents that manipulate histaminergic and glutaminergic receptors are being studied as possible non-stimulant alternatives in the management of adult ADHD. More information is needed before any definitive statements can be made concerning the feasibility and utility of these non-stimulant medication approaches.\n\n==\n\nBuoli 2016:\n\nTitle: Alternative pharmacological strategies for adult ADHD treatment: a systematic review\n\nAbstract: ABSTRACT Adult Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent psychiatric condition associated with high disability and frequent comorbidity. Current standard pharmacotherapy (methylphenidate and atomoxetine) improves ADHD symptoms in the short-term, but poor data were published about long-term treatment. In addition a number of patients present partial or no response to methylphenidate and atomoxetine. Research into the main database sources has been conducted to obtain an overview of alternative pharmacological approaches in adult ADHD patients. Among alternative compounds, amphetamines (mixed amphetamine salts and lisdexamfetamine) have the most robust evidence of efficacy, but they may be associated with serious side effects (e.g. psychotic symptoms or hypertension). Antidepressants, particularly those acting as noradrenaline or dopamine enhancers, have evidence of efficacy, but they should be avoided in patients with comorbid bipolar disorder. Finally metadoxine and lithium may be particularly suitable in case of comorbid alcohol misuse or bipolar disorder.\n\n==\n\nDodson 2005:\n\nTitle: Pharmacotherapy of adult ADHD.\n\nAbstract: Although attention deficit hyperactivity disorder (ADHD) has been officially recognized as persisting into adulthood for more than 25 years, only recently has the condition been studied in adults. There is great syndromatic continuity between childhood and adult ADHD, and thus much of the medication management of adults with ADHD can be based on the experience gained from treating children and adolescents. Stimulant medications remain the treatment of choice and are generally as effective in adults as they are in children. Several extended-release delivery systems that improve convenience and compliance have become available. Several second-line medications are also reviewed. The medications must be fine-tuned to the needs of the individual patient in regard to the dose and timing of dose that achieve optimal therapeutic benefit. Medication adjustment must be done by trial and error because no parameter yet identified predicts the molecule or dose that will provide optimal performance.\n\n==\n\nPrince 2002:\n\nTitle: Pharmacotherapy of Adult ADHD\n\nAbstract: This chapter explains pharmacotherapy of adult Attention-Deficit Hyperactivity Disorder (ADHD). The aggregate literature supports the notion that pharmacotherapy provides an effective treatment for adults with ADHD. Effective pharmacological treatments for ADHD adults include the use of the psychostimulants and antidepressants, with unclear efficacy of cognitive enhancers. Structured psychotherapy may be effective when used adjunctly with medications. Groups focused on coping skills, support, and interpersonal psychotherapy may also be very useful for these adults. For adults considering advanced schooling, educational planning, and alterations in the school environment may be necessary. Further controlled investigations assessing the efficacy of single and combination agents for adults with ADHD are necessary, with careful attention to diagnostics, symptom ,and neuropsychological outcome, long-term tolerability and efficacy, and use in specific ADHD subgroups. The chapter concludes that further studies applying stringent diagnostic criteria and outcome methodology are necessary to enhance the range of pharmacotherapeutic options for adults with ADHD.\n\n==\n\nGeffen 2018:\n\nTitle: Treatment of adult ADHD: a clinical perspective\n\nAbstract: Adult attention deficit/hyperactivity disorder (ADHD) has moved from the blurred edge of clinical focus to clear recognition as a prevalent and significant disorder in its own right. It is a relatively common comorbidity which if identified and treated may open the door to better outcomes for hard-to-treat patients. Conversely, failure to identify and treat adult ADHD is linked to negative outcomes. The recognition of the importance of adult ADHD in a subset of our patients challenges us to overcome our anxiety about this diagnosis and prevent the societal marginalization of vulnerable patients. Adult ADHD responds well to integrated pharmacological and psychotherapeutic intervention. Its treatment responsiveness reduces disability and allows the comorbidity which is typically present to be addressed. Mastering this challenge can make the diagnosis and treatment of adult ADHD a rewarding experience.",
        "final_summary": "The papers suggest that there have been recent advances and therapeutic approaches to adult ADHD. Psychosocial treatments, such as structured, skills-based interventions, have shown promise in reducing residual symptoms of ADHD in adults (Knouse 2008). Cognitive behavioral interventions have also been popular, particularly for adults who cannot or will not use medications (Antshel 2011). Cognitive behavioral therapy (CBT) for adult ADHD may involve targeted learning and practice of specific behavioral compensatory strategies (Knouse 2010). Pharmacotherapy remains a primary treatment option, with stimulant medications being the first choice (Horrigan 2001). Alternative pharmacological strategies, such as amphetamines and certain antidepressants, have also shown efficacy (Buoli 2016). Treatment should be individualized and fine-tuned to optimize therapeutic benefit (Dodson 2005). Integrated pharmacological and psychotherapeutic interventions have been effective in reducing disability and addressing comorbidity in adult ADHD (Geffen 2018)."
    },
    {
        "query": "Does microbial activity affect the surface properties of jarosite precipitate",
        "paper_list_string": "Liu 2009:\n\nTitle: Study of formation of jarosite mediated by thiobacillus ferrooxidans in 9K medium\n\nAbstract: Thiobacillus ferrooxidans, occurring extensively in mine districts, have important effects on the oxidation of metal sulphide and the formation of jarosite. In the coal bio-desulphurization, jarosite mediated by microorganisms decreases the bacterial absorbability of nutrients, occupies the available sites of coal surfaces, ultimately results in residual sulfur, which cannot be removed from coal. The thiobacillus ferrooxidans cultivation conditions with minimal jarosite in 9K medium by varying the initial pH, the initial Fe2+ concentration and the applied potential were studied and determined. Experimental results show that the optimal combination giving the minimal jarosite precipitates (3.73 g/L) and the good growing activity of bacteria is the initial pH of 1.6\u20131.7 with the Fe2+ concentration of 9\u201310 g/L and the applied potential of \u22120.5 V for 7 hours. The results will be of significant importance for the further research on the bacterial cultivation and coal desulphurization.\n\n==\n\nSadowski 1999:\n\nTitle: Adhesion of microorganism cells and jarosite particles on the mineral surface\n\nAbstract: It has been know that during both bioleaching and biomodification of the mineral surface the microbial cell should be tenaciously adhered to the mineral surface. The main goal of this research was evaluate the effect of polysaccharides (dextrine) on both the bacterial cells and precipitated jarosite particles deposition onto the mineral surface. The determination of the free energy of solid surface was med. by means of the thin-layer wicking technique. It was found that the quartz particles which were covered by the dextrine film have value of \u03b3 LW =121.44 mN/m (without dextrin \u03b3 LW =62.18 mN/m). The adsorption of dextrin caused a decrease of the \u03b3 \u2212 component of the free energy from 171.90 to 123.19 mN/m. The treatment of both quartz and gold refractory ore by polysaccharides caused an increase of the adhesion of microbial cells and jarosite colloid particles. The deposition of jarosite on the surface of gold ore stopped by the dispersing reagent addition.\n\n==\n\nWeisener 2008:\n\nTitle: Microbial Dissolution of Silver Jarosite: Examining Its Trace Metal Behaviour in Reduced Environments\n\nAbstract: Iron sulfate minerals such as jarosite-group compounds (e.g., M Fe3(SO 4 ) 2 (OH) 6 ) can be of considerable environmental importance because of their ability to scavenge trace elements and thus contribute to some degree of metal cycling. Jarosite forms in low temperature hydrothermal, acidic, sulfate-rich environments often yielding a range of elemental substituted forms: plumbojarosite (M = Pb), argentojarosite (M = Ag), jarosite (M = K), natrojarosite (M = Na), hydroniumjarosite (M = H3O), and ammoniojarosite (M = NH4). Anthropogenic sources of jarosite are common in mine waste environments, most often associated with the waste products resulting from base metal recovery. Few studies have investigated the effect that dissimilatory metal reducing bacteria may have in the presence of these compounds following the onset of reducing conditions. Jarosite reactivity may differ systematically as a function of its chemical properties. For example, the incorporation of Ag in the mineral lattice may have inhibitory affects on the growth of microbial strains. In this study the reductive dissolution of argentojarosite (M = Ag) in the presence of Shewanella putrefaciens CN32 (10 9 cells ml 1 , pH 7.1) was examined. Using the silver (argento)jarosite, Ag Fe 3 (SO 4 ) 2 (OH) 6 , as a sole terminal electron acceptor we observed the reduction of structural Fe(III) and Ag(I) by CN32 through the release of Fe(II) ions to solution. Aqueous silver concentrations were below instrumental detection. Environmental SEM (ESEM) and TEM micrographs of the microbial clusters revealed the progressive heterogeneous nucleation of Ag(0) nanoparticles within cellular structures and also on adjacent mineral grains. The results of this study are the first presented for the anaerobic dissolution of silver jarosite. This has implications for understanding the processes leading to the mobility or retention of silver in mine waste and industrial landfill environments. It also provides insight into the microbial mechanisms of silver resistance and nanoparticle formation with potential applications for bioleaching and/or biotechnology.\n\n==\n\nSasaki 2006:\n\nTitle: FE-SEM Study of Microbially Formed Jarosites by Acidithiobacillus ferrooxidans\n\nAbstract: Morphological characterization of jarosite groups formed from Fe(III) biologically oxidized with different numbers of Acidithiobacillus ferrooxidans was conducted using FE-SEM. The higher population of A. ferrooxidans resulted in more distinct jarosite mineral shape, and stronger Raman intensities for potassium jarosite, ammoniojarosite and argentojarosite. The morphology of the jarosites might be dependent on iron-oxidizing activity of A. ferrooxidans. The technique was applied to identify jarosite compounds formed during microbially mediated dissolution of arsenopyrite by A. ferrooxidans. It is difficult to identify this jarosite compound by X-ray diffraction and Raman spectroscopy because amounts are typically low and the crystallization is poor in minerals formed by microbially mediated oxidation. However, FE-SEM image provided helpful information for identification of jarosite compounds. The results suggest that morphology would provide useful information for identification and history of jarosite minerals as geochemical samples.\n\n==\n\nDaoud 2006:\n\nTitle: Formation of jarosite during Fe2+ oxidation by Acidithiobacillus ferrooxidans\n\nAbstract: Abstract Jarosite precipitation is a very important phenomenon that is observed in many bacterial cultures. In many applications involving Acidithiobacillus ferrooxidans , like coal desulphurization and bioleaching, it is crucial to minimize jarosite formation in order to increase efficiency. The formation of jarosite during the oxidation of ferrous iron by free suspended cells of A. ferrooxidans was studied. The process was studied as a function of time, pH and temperature. The main parameter affecting the jarosite formation was pH. Several experiments yielded results showing oxidation rates as high as 0.181\u20130.194\u00a0g/L\u00a0h, with low jarosite precipitation of 0.0125\u20130.0209\u00a0g at conditions of pH 1.6\u20131.7 with an operating temperature of 35\u00a0\u00b0C.\n\n==\n\nGao 2019:\n\nTitle: Reductive dissolution of jarosite by a sulfate reducing bacterial community: Secondary mineralization and microflora development.\n\nAbstract: Jarosite is an iron-hydroxysulfate mineral commonly found in acid mine drainage (AMD). Given its strong adsorption capacity and its ability to co-precipitation with heavy metals, jarosite is considered a potent scavenger of contaminants in AMD-impacted environments. Sulfate-reducing bacteria (SRB) play an important role in the reductive dissolution of jarosite; however, the mechanism involved has yet to be elucidated. In this study, an indigenous SRB community enriched from the Dabaoshan mine area (Guangdong, China) was employed to explore the mechanism of the microbial reduction of jarosite. Different cultures, with or without dissolved sulfate and the physical separation of jarosite from bacteria by dialysis bags, were examined. Results indicate that the reduction of jarosite by SRB occurred via an indirect mechanism. In systems with dissolved sulfate, lactate was incompletely oxidized to acetate coupled with the reduction of SO42- to S2-, which subsequently reduced the Fe3+ in jarosite, forming secondary minerals including vivianite, mackinawite and pyrite. In systems without dissolved sulfate, jarosite dissolution occurred prior to reduction, and similar secondary minerals formed as well. Extracellular polymeric substances secreted by SRB appeared to facilitate the release of sulfate from jarosite. Structural sulfate in the solid phase of jarosite may not be available for SRB respiration. Although direct contact between SRB and jarosite is not necessary for mineral reduction, wrapping jarosite into dialysis bags suppressed the reduction to a certain extent. Microbial community composition differed in direct contact treatments and physical separation treatments. Physical separation of the SRB community from jarosite mineral supported the growth of Citrobacter, while Desulfosporosinus dominated in direct contact treatments.\n\n==\n\nJones 2014:\n\nTitle: Synthesis and properties of ternary (K, NH\u2084, H\u2083O)-jarosites precipitated from Acidithiobacillus ferrooxidans cultures in simulated bioleaching solutions.\n\nAbstract: The purpose of this study was to synthesize a series of solid solution jarosites by biological oxidation of ferrous iron at pH2.2-4.4 and ambient temperature in media containing mixtures of K(+) (0, 1, 4, 6, 12, 31 mM) and NH4(+) (6.1, 80, 160, 320 mM). The starting material was a liquid medium for Acidithiobacillus ferrooxidans comprised of 120 mM FeSO4 solution and mineral salts at pH2.2. Following inoculation with A. ferrooxidans, the cultures were incubated in shake flasks at 22\u00b0C. As bacteria oxidized ferrous iron, ferric iron hydrolyzed and precipitated as jarosite-group minerals (AFe3(SO4)2(OH)6) and/or schwertmannite (idealized formula Fe8O8(OH)6(SO4)\u00b7nH2O). The precipitates were characterized by X-ray diffraction (XRD), elemental analysis, and Munsell color. Schwertmannite was the dominant mineral product at low combinations of K(+) (\u2264 4 mM) and NH4(+) (\u2264 80 mM) in the media. At higher single or combined concentrations, yellowish jarosite phases were produced, and Munsell hue provided a sensitive means of detecting minor schwertmannite in the oxidation products. Although the hydrated ionic radii of K(+) and NH4(+) are similar, K(+) greatly facilitated the formation of a jarosite phase compared to NH4(+). Unit cell and cell volume calculations from refinements of the powder XRD patterns indicated that the jarosite phases produced were mostly ternary (K, NH4, H3O)-solid solutions that were also deficient in structural Fe, especially at low NH4 contents. Thus, ferric iron precipitation from the simulated bioleaching systems yielded solid solutions of jarosite with chemical compositions that were dependent on the relative concentrations of K(+) and NH4(+) in the synthesis media. No phase separations involving discrete, end-member K-jarosite or NH4-jarosite were detected in the un-aged precipitates.\n\n==\n\nGrishin 1988:\n\nTitle: Characterization of Jarosite Formed upon Bacterial Oxidation of Ferrous Sulfate in a Packed-Bed Reactor\n\nAbstract: A packed-bed bioreactor with activated-carbon particles as a carrier matrix material inoculated with Thiobacillus ferrooxidans was operated at a pH of 1.35 to 1.5 to convert ferrous sulfate to ferric sulfate. Despite the low operating pH, trace amounts of precipitates were produced in both the reactor and the oxidized effluent. X-ray diffraction and chemical analyses indicated that the precipitates were well-ordered potassium jarosite. The chemical analyses also revealed a relative deficiency of Fe and an excess of S in the reactor sample compared with the theoretical composition of potassium jarosite.",
        "final_summary": "Microbial activity does influence the surface properties of jarosite precipitate. Liu 2009 demonstrated that Thiobacillus ferrooxidans, a microorganism prevalent in mine districts, contributes to jarosite formation. Sadowski 1999 showed that polysaccharides enhance the adhesion of microbial cells and jarosite particles to mineral surfaces. Weisener 2008 observed the dissolution of silver jarosite by dissimilatory metal reducing bacteria, leading to the reduction of structural Fe(III) and Ag(I) and the emergence of Ag(0) nanoparticles. Sasaki 2006 found that the morphology of jarosite, formed by Acidithiobacillus ferrooxidans, is dependent on the bacteria's iron-oxidizing activity. Daoud 2006 identified pH as the main parameter affecting jarosite formation during the oxidation of ferrous iron by Acidithiobacillus ferrooxidans. Gao 2019 reported the formation of secondary minerals and changes in microbial community composition during the reductive dissolution of jarosite by a sulfate-reducing bacterial community. Jones 2014 synthesized ternary jarosites through biological oxidation of ferrous iron, with the composition of the jarosite phases being influenced by the relative concentrations of K(+) and NH4(+). Lastly, Grishin 1988 operated a packed-bed bioreactor with Thiobacillus ferrooxidans, which resulted in the production of well-ordered potassium jarosite, characterized by a relative deficiency of Fe and an excess of S compared to the theoretical composition of potassium jarosite."
    },
    {
        "query": "Corruption and lack of economic reforms threaten Vietnam's economy. Corruption in Vietnam's government and economy leads to inefficient allocation of resources and deters foreign investment (Mai 2013).",
        "paper_list_string": "Salvador 2020:\n\nTitle: Foreign direct investment, corruption, and institutional reforms\n\nAbstract: Corruption impacts the competitive conditions among firms and the flow of foreign investment. Institutional reforms made for fighting against corruption are sometimes useless. We develop a model in which a corrupted government tries to set an optimal institutional level taking into account the cost of this policy on foreign investment, the benefit of a corrupted domestic firm and the benefit of local citizens. A political contribution is made by a corrupted lobby group in order to benefit from a lower institutional level. Our results suggest that the optimal institutional level depends on the degree of efficiency of firms and the level of corruption of the host government.\u00a0Key words: Corruption, Lobbying, Institutional reforms, Foreign direct investment.\u00a0JEL: F21, F30, K42\n\n==\n\nDang 2016:\n\nTitle: The impact of corruption on provincial development performance in Vietnam\n\nAbstract: Corruption has long been considered a national illness in Vietnam but progress in fighting corruption has been modest. In recent years, the Communist Party of Vietnam and the Government of Vietnam have strengthened their efforts to prevent and fight corruption. Despite strong anti-corruption measures being implemented at the national level, provincial authorities have shied away from tackling corruption. One of the reasons for this could be that it is not clear to provincial authorities if and how corruption is affecting local development economically and socially. This article demonstrates that corruption has a negative impact on private sector investment, employment and per capita income at the provincial level in Vietnam. However, corruption is found to have no significant impact on how income is distributed across provinces. The findings demonstrate that more effective anti-corruption measures are necessary to promote the private sector and improve household income. The study provides provincial leaders with empirical evidence and incentives for fighting corruption.\n\n==\n\nMalesky 2012:\n\nTitle: Foreign Investment and Bribery: A Firm-Level Analysis of Corruption in Vietnam\n\nAbstract: Among the concerns faced by countries pondering the costs and benefits of greater economic openness to international capital flows is the worry that new and powerful external actors will exert a corrupting influence on the domestic economy. In this paper, we use a novel empirical strategy, drawn from research in experimental psychology, to test the linkage between foreign direct investment (FDI) and corruption. The prevailing literature has produced confused and contradictory results on this vital relationship due to errors in their measurement of corruption which are correlated with FDI inflows. When a less biased operationalization is employed, we find clear evidence of corruption during both registration and procurement procedures in Vietnam. The prevalence of corruption, however, is not associated with inflows of FDI. On the contrary, one measure of economic openness appears to be the most important driver of reductions in Vietnamese corruption: the wave of domestic legislation, which accompanied the country's bilateral trade liberalization agreement with the United States (US-BTA), significantly reduced bribery during business registration.\n\n==\n\nPhuonga 2020:\n\nTitle: Corruption and long-term investment of businesses in Vietnam\n\nAbstract: Article history: Received: June 13 2020 Received in revised format: June 24 2020 Accepted: June 26 2020 Available online: June 26 2020 This paper investigates the effects of corruption and long-term investment of businesses in Vietnam using the General Least Square (GLS) estimation method for businesses in 63 provinces in Vietnam from 2016-2018. The results show that corruption was an important factor affecting the long-term investment decisions of Vietnamese enterprises. The ability to predict corruption of businesses can explain the phenomenon of part of the cash flow of businesses flowing out of production and business. Informal costs related to low-level administrative procedures act as \u201cgrease\u201d to help businesses reduce time costs, but when the total amount of unofficial expenses exceeds 10% of revenue of businesses, they become a burden for businesses and restrain them from making long-term investments. Corrupted public officials' behavior has led businesses to misallocate resources and prevent them from making long-term investments. The result shows that the East Asia paradox holds only for the case of informal costs related to administrative procedures in Vietnam. \u00a9 2020 by the authors; licensee Growing Science, Canada.\n\n==\n\nNguyen 2017:\n\nTitle: Tax corruption and private sector development in Vietnam\n\nAbstract: This article aims to examine the impact of tax corruption on private sector development in Vietnam. It is motivated by two separate but related considerations. First, despite the seriousness of the phenomenon of corruption, there is a paucity of rigorous empirical research of corruption, particularly tax corruption, in Vietnam. Secondly, ineffective control of corruption is viewed as a cause of Vietnam\u2019s recent total factor productivity (TFP) slowdown or its poor industrial policy, both of which may hamper Vietnam\u2019s progress as a low middle-income country. Without some understanding on the impact of tax corruption on the economy, it may not be possible to devise the most effective anti-corruption policy and measures. After a brief literature review that focuses on tax corruption, various conceptual issues relating to tax corruption are discussed and clarified. The extent of petty tax corruption in Vietnam is then discussed, followed by a review of findings and implications of recent studies on how tax corruption impacts on private sector development in Vietnam. Despite perceptions and evidence of widespread petty tax corruption, Vietnam ranks very highly both in terms of tax collection and tax effort.Not unexpectedly, the impact of tax corruption is mixed in the sense that empirical evidence lends credence to both 'sanding the wheels' and 'greasing the wheels' hypotheses. Finally, some broad policy recommendations for combating tax corruption are offered.\n\n==\n\nThao 2020:\n\nTitle: Current Situation of Corruption Offenses and Measures for Improvement of Anti-Corruption Effectiveness in Vietnam\u2019s Economy\n\nAbstract: Along with the promulgation of 2008 Law on Anti-Corruption, thanking to the comprehensive solutions and determinations of the whole governmental apparatus, there are signals of a positive change in the fight against corruption in Vietnam\u2019s market economy. However, compared to other countries around the world, the corruption in Vietnam is still a national problem. The number of corruption cases may decline, but the scale and severity has been increasing. Many cases has involved high-ranking officials in the government with more than 20 general officers in the armed forces to be sentenced. On the basis of analysing the current situation of corruption in recent years, the author hereby recommends some synchronous solutions to improve the effectiveness of anti-corruption.\n\n==\n\nPhuong 2017:\n\nTitle: Corruption in Vietnam: The current situation and proposed solutions\n\nAbstract: Abstract This chapter introduces an overview of corruption in Vietnam. Although the country\u2019s GDP has maintained a steady growth of around 5%\u20137% per year during 2011\u201315, corruption is widespread throughout Vietnam and remains a substantial issue. The chapter starts by defining corruption according to Vietnamese law and regulations. It then explains the three major characteristics of corruption as perceived in this country. The third section of the chapter lists the 12 acts of corruption and the current state of corruption in Vietnam. The chapter ends with recommendations and solutions to prevent corruption in Vietnam. The critical roles played by ethical government officials, managers, and employees to prevent and punish corruption and bribery, along with effective legislation, will create and contribute toward sustainable economic development in the country.\n\n==\n\nBaum:\n\nTitle: Tax corruption and private sector development in Vietnam *\n\nAbstract: This article aims to examine the impact of tax corruption on private sector development in Vietnam. It is motivated by two separate but related considerations. First, despite the seriousness of the phenomenon of corruption, there is a paucity of rigorous empirical research of corruption, particularly tax corruption, in Vietnam. Secondly, ineffective control of corruption is viewed as a cause of Vietnam\u2019s recent total factor productivity (TFP) slowdown or its poor industrial policy, both of which may hamper Vietnam\u2019s progress as a low middle-income country. Without some understanding on the impact of tax corruption on the economy, it may not be possible to devise the most effective anti-corruption policy and measures. After a brief literature review that focuses on tax corruption, various conceptual issues relating to tax corruption are discussed and clarified. The extent of petty tax corruption in Vietnam is then discussed, followed by a review of findings and implications of recent studies on how tax corruption impacts on private sector development in Vietnam. Despite perceptions and evidence of widespread petty tax corruption, Vietnam ranks very highly both in terms of tax collection and tax effort. Not unexpectedly, the impact of tax corruption is mixed in the sense that empirical evidence lends credence to both \u2018sanding the wheels\u2019 and \u2018greasing the wheels\u2019 hypotheses. Finally, some broad policy recommendations for combating tax corruption are offered.",
        "final_summary": "Corruption in Vietnam impacts competitive conditions among firms and influences the flow of foreign investment, as suggested by Salvador (2020). Dang (2016) provides evidence that corruption negatively impacts private sector investment, employment, and per capita income at the provincial level. Malesky (2012) uncovers corruption during business registration and procurement procedures, but finds no association with foreign direct investment inflows. Phuonga (2020) shows that corruption significantly influences the long-term investment decisions of Vietnamese enterprises, leading to resource misallocation. Nguyen (2017) discusses the mixed effects of tax corruption on economic activity, while Thao (2020) acknowledges corruption as a national issue in Vietnam. Phuong (2017) provides an overview of the widespread nature of corruption in Vietnam."
    },
    {
        "query": "long text classification",
        "paper_list_string": "Weijie 2021:\n\nTitle: Long Text Classification Based on BERT\n\nAbstract: Existing text classification algorithms generally have limitations in terms of text length and yield poor classification results for long texts. To address this problem, we propose a BERT-based long text classification method. First, we slice the long text and use BERT to encode the sliced clauses to obtain the local semantic information. Second, we use BiLSTM to fuse the local semantic information and adopt the attention mechanism to increase the weight of important clauses in the long text, so as to obtain the global semantic information. Finally, the global semantic information is input to the softmax layer for classification. Experimental results show that the proposed method achieves higher accuracy than commonly used models.\n\n==\n\nWeijie 2021:\n\nTitle: Long Text Classification Based on BERT\n\nAbstract: Existing text classification algorithms generally have limitations in terms of text length and yield poor classification results for long texts. To address this problem, we propose a BERT-based long text classification method. First, we slice the long text and use BERT to encode the sliced clauses to obtain the local semantic information. Second, we use BiLSTM to fuse the local semantic information and adopt the attention mechanism to increase the weight of important clauses in the long text, so as to obtain the global semantic information. Finally, the global semantic information is input to the softmax layer for classification. Experimental results show that the proposed method achieves higher accuracy than commonly used models.\n\n==\n\nYue 2022:\n\nTitle: Research on Long Text Classification Model Based on Multi-Feature Weighted Fusion\n\nAbstract: Text classification in the long-text domain has become a development challenge due to the significant increase in text data, complexity enhancement, and feature extraction of long texts in various domains of the Internet. A long text classification model based on multi-feature weighted fusion is proposed for the problems of contextual semantic relations, long-distance global relations, and multi-sense words in long text classification tasks. The BERT model is used to obtain feature representations containing global semantic and contextual feature information of text, convolutional neural networks to obtain features at different levels and combine attention mechanisms to obtain weighted local features, fuse global contextual features with weighted local features, and obtain classification results by equal-length convolutional pooling. The experimental results show that the proposed model outperforms other models in terms of accuracy, precision, recall, F1 value, etc., under the same data set conditions compared with traditional deep learning classification models, and it can be seen that the model has more obvious advantages in long text classification.\n\n==\n\nZang 2023:\n\nTitle: Long Text Multi-label Classification\n\nAbstract: In recent years, the evaluation of non-technical literacy such as sense of worth, outlook on life, and sustainable development concept, has become increasingly important in university education in China. They are often evaluated by questionnaires or interviews. However, the feedback texts from students often contain long paragraphs of students' opinion responses in Chinese, with long text, large amount of information and complex structure, which reduce the accuracy of long Chinese text classification. In this paper, we propose a multi-label classification model named MLformer, which uses the Longformer model to obtain feature representations containing information about the global semantic and local contextual features of the text to effectively classify students' views. The experimental results on our long text dataset in Chinese about engineering sustainability show that our method outperforms the typical deep learning classification model BERT in terms of accuracy, precision, and F1 score, thus, it can provide a powerful reference for students' opinion analysis and evaluation.\n\n==\n\nTang 2023:\n\nTitle: Research on multi-label long text classification algorithm based on transformer-LDA\n\nAbstract: Text classification is an important research area in the field of natural language processing. In view of the low efficiency of the traditional CNN and RNN algorithms for multi-label classification of long text due to timing and spatial displacement problems, this paper proposes a long text classification model with improved Transformer attention mechanism, and combines LDA topic classification algorithm to achieve multi-label classification of document length text. Firstly, the paper introduces the industry's solutions to the problem of multi-classification of long texts. Compared with traditional algorithms such as truncation method and pooling method, the PAPER proposes the LDA topic classification model combined with the improved Transformer-XL text classification model to extract text features with fine granularity, so as to classify texts with higher accuracy. Finally, comparative experiments show that the proposed solution has a significant improvement in P value, R value and F1 value compared with the traditional classification method in the field of long text multi-label classification.\n\n==\n\nFiok 2021:\n\nTitle: Revisiting Text Guide, a Truncation Method for Long Text Classification\n\nAbstract: The quality of text classification has greatly improved with the introduction of deep learning, and more recently, models using attention mechanism. However, to address the problem of classifying text instances that are longer than the length limit adopted by most of the best performing transformer models, the most common method is to naively truncate the text so that it meets the model limit. Researchers have proposed other approaches, but they do not appear to be popular, because of their high computational cost and implementation complexity. Recently, another method called Text Guide has been proposed, which allows for text truncation that outperforms the naive approach and simultaneously is less complex and costly than earlier proposed solutions. Our study revisits Text Guide by testing the influence of certain modifications on the method\u2019s performance. We found that some aspects of the method can be altered to further improve performance and confirmed several assumptions regarding the dependence of the method\u2019s quality on certain factors.\n\n==\n\nFiok 2021:\n\nTitle: Text Guide: Improving the Quality of Long Text Classification by a Text Selection Method Based on Feature Importance\n\nAbstract: The performance of text classification methods has improved greatly over the last decade for text instances of less than 512 tokens. This limit has been adopted by most state-of-the-research transformer models due to the high computational cost of analyzing longer text instances. To mitigate this problem and to improve classification for longer texts, researchers have sought to resolve the underlying causes of the computational cost and have proposed optimizations for the attention mechanism, which is the key element of every transformer model. In our study, we are not pursuing the ultimate goal of long text classification, i.e., the ability to analyze entire text instances at one time while preserving high performance at a reasonable computational cost. Instead, we propose a text truncation method called Text Guide, in which the original text length is reduced to a predefined limit in a manner that improves performance over naive and semi-naive approaches while preserving low computational costs. Text Guide benefits from the concept of feature importance, a notion from the explainable artificial intelligence domain. We demonstrate that Text Guide can be used to improve the performance of recent language models specifically designed for long text classification, such as Longformer. Moreover, we discovered that parameter optimization is the key to Text Guide performance and must be conducted before the method is deployed. Future experiments may reveal additional benefits provided by this new method.\n\n==\n\nHe 2011:\n\nTitle: Research on short text classification for web forum\n\nAbstract: The unique characteristic of short text makes short text classification quite different from traditional long text processing. The feature space of short text is so sparse, which makes it notoriously difficult to extract sufficient and effective features. In this paper, aiming to classify the short text on web forum accurately, a novel short-text-processing method based on semantic extension is introduced to enhance the content of the original short text, which effectively solves the problem of feature sparse. In addition, we put forward the concept of Key-Pattern (KP) and propose a new text feature representation approach based on KP, which extracts phrase with powerful semantic information as the text features. Traditional classifier model are applied to estimate the text's classification, experimental results show that the proposed method is effective to improve the accuracy and recall of short text classification.",
        "final_summary": "Weijie 2021 proposes a BERT-based method for long text classification that slices long texts, encodes the clauses using BERT, fuses local and global semantic information with BiLSTM and attention mechanisms, and achieves higher accuracy than commonly used models.\n\nYue 2022 presents a long text classification model based on multi-feature weighted fusion. It combines BERT for global semantic and contextual feature information, convolutional neural networks for features at different levels, and attention mechanisms to obtain weighted local features. The model outperforms traditional deep learning classification models in terms of accuracy, precision, recall, and F1 value.\n\nZang 2023 focuses on long text multi-label classification. They propose a model called MLformer that utilizes the Longformer model to capture global semantic and local contextual features. The experimental results show that MLformer outperforms BERT in terms of accuracy, precision, and F1 score.\n\nTang 2023 introduces a multi-label long text classification algorithm based on the Transformer-LDA model. It combines the improved Transformer attention mechanism with the LDA topic classification algorithm to achieve accurate multi-label classification of long texts. Comparative experiments demonstrate significant improvements in accuracy, precision, and F1 value compared to traditional methods.\n\nFiok 2021 explores the Text Guide method for truncating long texts and tests modifications to improve its performance. They confirm the method's dependence on certain factors.\n\nHe 2011 focuses on short text classification for web forums. They propose a method based on semantic extension and Key-Pattern (KP) representation to enhance feature extraction and improve accuracy and recall in short text classification."
    },
    {
        "query": "Find a good introduction to quantum computing",
        "paper_list_string": "Hey 1999:\n\nTitle: Quantum computing: an introduction\n\nAbstract: The basic ideas of quantum computation are introduced by a brief discussion of Bennett (1973, 1982) and Fredkin's (1982, 1997) ideas of reversible computation. After some remarks about Deutsch's (1985) pioneering work on quantum complexity and Shor's (1996) factorisation algorithm, quantum logic gates, qubits and registers are discussed. The role of quantum entanglement is stressed and Grover's (1997) quantum search algorithm described in detail. The paper ends with a review of the current experimental status of quantum computers.\n\n==\n\nMilburn 2007:\n\nTitle: An Introduction to Quantum Computing\n\nAbstract: Preface 1. Introduction and background 2. Linear algebra and the Dirac notation 3. Qubits and the framework of quantum mechanics 4. A quantum model of computation 5. Superdense coding and quantum teleportation 6. Introductory quantum algorithms 7. Algorithms with super-polynomial speed-up 8. Algorithms based on amplitude amplification 9. Quantum computational complexity theory and lower bounds 10. Quantum error correction Appendices Bibliography Index\n\n==\n\nPittenger 2000:\n\nTitle: An Introduction to Quantum Computing Algorithms\n\nAbstract: From the Publisher: \nThe purpose of this monograph is to provide the mathematically literate reader with an accessible introduction to the theory of quantum computing algorithms, one component of a fascinating and rapidly developing area which involves topics from physics, mathematics, and computer science. \nThe author briefly describes the historical context of quantum computing and provides the motivation, notation, and assumptions appropriate for quantum statics, a non-dynamical, finite dimensional model of quantum mechanics. This model is then used to define and illustrate quantum logic gates and representative subroutines required for quantum algorithms. A discussion of the basic algorithms of Simon and of Deutsch and Jozsa sets the stage for the presentation of Grover's search algorithm and Shor's factoring algorithm, key algorithms which crystallized interest in the practicality of quantum computers. A group theoretic abstraction of Shor's algorithms completes the discussion of algorithms. \nThe last third of the book briefly elaborates the need for error-correction capabilities and then traces the theory of quantum error-correcting codes from the earliest examples to an abstract formulation in Hilbert space. \nThis text is a good self-contained introductory resource for newcomers to the field of quantum computing algorithms, as well as a useful self-study guide for the more specialized scientist, mathematician, graduate student, or engineer. Readers interested in following the ongoing developments of quantum algorithms will benefit particularly from this presentation of the notation and basic theory.\n\n==\n\nLo 2002:\n\nTitle: Introduction to Quantum Computation Information\n\nAbstract: From the Publisher: \nThis book aims to provide a pedagogical introduction to the subjects of quantum information and computation. Topics include non-locality of quantum mechanics, quantum computation, quantum cryptography, quantum error correction, fault-tolerant quantum computation as well as some experimental aspects of quantum computation and quantum cryptography. Only knowledge of basic quantum mechanics is assumed. Whenever more advanced concepts and techniques are used, they are introduced carefully. This book is meant to be a self-contained overview. While basic concepts are discussed in detail, unnecessary technical details are excluded. It is well-suited for a wide audience ranging from physics graduate students to advanced researchers.\n\n==\n\nRieffel 2011:\n\nTitle: Quantum Computing: A Gentle Introduction\n\nAbstract: The combination of two of the twentieth centurys most influential and revolutionary scientific theories, information theory and quantum mechanics, gave rise to a radically new view of computing and information. Quantum information processing explores the implications of using quantum mechanics instead of classical mechanics to model information and its processing. Quantum computing is not about changing the physical substrate on which computation is done from classical to quantum but about changing the notion of computation itself, at the most basic level. The fundamental unit of computation is no longer the bit but the quantum bit or qubit. This comprehensive introduction to the field offers a thorough exposition of quantum computing and the underlying concepts of quantum physics, explaining all the relevant mathematics and offering numerous examples. With its careful development of concepts and thorough explanations, the book makes quantum computing accessible to students and professionals in mathematics, computer science, and engineering. A reader with no prior knowledge of quantum physics (but with sufficient knowledge of linear algebra) will be able to gain a fluent understanding by working through the book. The text covers the basic building blocks of quantum information processing, quantum bits and quantum gates, showing their relationship to the key quantum concepts of quantum measurement, quantum state transformation, and entanglement between quantum subsystems; it treats quantum algorithms, discussing notions of complexity and describing a number of simple algorithms as well as the most significant algorithms to date; and it explores entanglement and robust quantum computation, investigating such topics as quantifying entanglement, decoherence, quantum error correction, and fault tolerance.\n\n==\n\nNannicini 2017:\n\nTitle: An Introduction to Quantum Computing, Without the Physics\n\nAbstract: This paper is a gentle but rigorous introduction to quantum computing intended for discrete mathematicians. Starting from a small set of assumptions on the behavior of quantum computing devices, we analyze their main characteristics, stressing the differences with classical computers, and finally describe two well-known algorithms (Simon's algorithm and Grover's algorithm) using the formalism developed in previous sections. This paper does not touch on the physics of the devices, and therefore does not require any notion of quantum mechanics. Numerical examples on an implementation of Grover's algorithm using open-source software are provided.\n\n==\n\nChatterjee 2003:\n\nTitle: Introduction to Quantum Computation\n\nAbstract: This is an introductory review on the basic principles of quantum computation. Various important quantum logic gates and algorithms based on them are introduced. Quantum teleportation and decoherence are discussed briefly. Some problems, without solutions, are included.\n\n==\n\nChatterjee 2003:\n\nTitle: Introduction to Quantum Computation\n\nAbstract: This is an introductory review on the basic principles of quantum computation. Various important quantum logic gates and algorithms based on them are introduced. Quantum teleportation and decoherence are discussed briefly. Some problems, without solutions, are included.",
        "final_summary": "Hey 1999 introduces the basic ideas of quantum computation, including reversible computation and quantum logic gates (Hey 1999). Pittenger 2000 provides an accessible introduction to the theory of quantum computing algorithms, discussing Simon's algorithm, Deutsch-Jozsa algorithm, Grover's search algorithm, and Shor's factoring algorithm (Pittenger 2000). Lo 2002 offers a pedagogical introduction to quantum information and computation, covering topics such as non-locality, quantum cryptography, and fault-tolerant quantum computation (Lo 2002). Rieffel 2011 provides a comprehensive introduction to quantum computing and the underlying concepts of quantum physics, explaining quantum bits, quantum gates, quantum algorithms, and entanglement (Rieffel 2011). Nannicini 2017 offers a gentle introduction to quantum computing without the physics, focusing on the behavior of quantum computing devices and discussing Simon's algorithm and Grover's algorithm (Nannicini 2017). Lastly, Chatterjee 2003 introduces the basic principles of quantum computation, including quantum logic gates and algorithms (Chatterjee 2003)."
    },
    {
        "query": "what is good water governance?",
        "paper_list_string": "Xu 2018:\n\nTitle: Good Water Governance for the Sustainable Development of the Arid and Semi-arid Areas of Northwest China\n\nAbstract: Water resources are of great importance for the sustainable development of the Arid and Semi-arid Areas of Northwest China. The theory of good water governance could provide inspirations for properly dealing with the water challenges of these areas. The integrated water resources management, the environmental water flow protection and the public participation are three major requirements of the good water governance. There are three major challenges for promoting the good water governance. The first is related to the clarification of the responsibilities of the water governance. The second is about balancing the diversified water needs. The third is about the optimal approach for improving the public participation. Three policy choices are proposed for promoting the good water governance. The integration of the water governance system, which emphasizes the four principles and the role of the basin management, could be a significant way of clarifying the responsibilities. Improving the water use efficiency could help the water supply for the ecological environment. The capacity building should be enhanced for facilitating the public participation.\n\n==\n\nGrigg 2011:\n\nTitle: Water governance: from ideals to effective strategies\n\nAbstract: Integrated water solutions require effective governance, as well as appropriate technologies and management instruments. While decision scenarios vary across a range of water demands and scales, common patterns of governance are involved. The paper explains these patterns in terms of how policy, empowerment and control are applied in distinct ways in different water management scenarios. Principles of effective water governance emerge from the case discussions, and illustrate how decision makers can identify the actions needed for policy, empowerment and control as well as make progress even while other institutional arrangements continue to evolve.\n\n==\n\nTortajada 2010:\n\nTitle: Water Governance: Some Critical Issues\n\nAbstract: This paper presents an analysis of the issues discussed at a special international workshop on water governance. While it is generally accepted that good governance for the water sector is essential, it is also clear that its implementation requires qualitative and quantitative factors, which may vary from one country to another. In order to objectively assess the opportunities and constraints of implementing good water governance practices, a group of selected international experts were invited to address this complex issue.\n\n==\n\nLautze 2011:\n\nTitle: Putting the cart before the horse: Water governance and IWRM\n\nAbstract: Water governance has emerged as perhaps the most important topic of the international water community in the 21st century, and achieving \u201cgood\u201d water governance is now a focus of both policy discourse and innumerable development projects. Somewhat surprisingly in light of this attention, there is widespread confusion about the meaning of the term \u201cwater governance\u201d. This paper reviews the history of the term's use and misuse to reveal how the concept is frequently inflated to include issues that go well beyond governance. Further, it highlights how calls to improve water governance often espouse predetermined goals that should instead be the very function of water governance to define. To help overcome this confusion, the paper suggests a more refined definition of water governance and related qualities of good water governance that are consistent with broader notions of the concepts. In light of the substantial resources allocated in its name, this paper's findings show there is significant potential to strengthen efforts at improving water governance.\n\n==\n\nSolanes 2006:\n\nTitle: Water governance for development and sustainability\n\nAbstract: Abstract This document aims to identify characteristics of water institutions which promote the sustainable integration of water, both as a resource and as service, into socioeconomic development. As this does not depend only on formal institutional factors, such as legislation and organizational structure, there are also references to dynamic conditions, such as socioeconomic circumstances and the quality of the administration, summarized in the concept of governance, understood as the capability of a social system to mobilize energies, in a coherent manner, for the sustainable development of water resources. As human society becomes ever more complex and the intensity of human impact on natural resources becomes more severe, the need to integrate the different elements of water management becomes imperative. It is for this reason that effective water governance will be more and more closely linked to integrated water resources management. The specific objectives of this paper are: (i); to contribute to focusing the regional debate on those aspects of water institutions and macroeconomic policies which are particularly critical for Latin American and Caribbean countries; (ii); to promote the formulation of a regional position that genuinely reflects its situation, visions, aspirations and problems; (iii); to promote a critical and balanced analysis of legislation, regulatory frameworks and public policies for water resources management and provision of related public services; and (iv); to make available in English a summary of the water-related research carried out by the Division of Natural Resources and Infrastructure of the Economic Commission for Latin America and the Caribbean (ECLAC);.\n\n==\n\nFranks 2007:\n\nTitle: Water governance and poverty\n\nAbstract: This paper engages with policy on meeting development goals for water through interventions, which promote good governance. Addressing an under-researched area, we propose a new analytical framework for understanding water governance, not as a set of abstract principles, but as interlinked processes with variable practical outcomes for poor people. The framework is informed by theories of governance, institutions and structuration, empirical research and field insights. We apply the framework to a case in south-western Tanzania, and we identify a range of issues for further research, particularly relating to water access for the poor.\n\n==\n\nDukhovny 2011:\n\nTitle: Ways to improve water governance\n\nAbstract: Given the growing destabilizing factors (climate change demographic growth financial instability desertification) the water supply for future development of our planet will become even more an issue. In order to guarantee access to water for different countries regions and individuals it is necessary to elaborate and adopt for governance in general a body of legal financial institutional and economic tools and mechanisms that would become an indisputable set of rules as a kind of Bible for modern humankind and as a way to survive. Water governance should prevent any attempt to use water as a means of political or economic pressure or as a good like oil transformed into the subject of profiteering. Water is a holy thing and we all must take care of and save it so that everyone might have access to water in adequate quantity and quality. Copyright \u00a9 2011 John Wiley & Sons Ltd.\n\n==\n\nAraral 2013:\n\nTitle: Water Governance 2.0: A Review and Second Generation Research Agenda\n\nAbstract: Water scholars and practitioners generally agree that improving water governance is the key to addressing water insecurity in developing countries. We review the literature on water governance and argue for a second-generation research agenda, which pays more attention to the study of incentive structures, is multi and inter-disciplinary in orientation and with clear policy implications. We then illustrate how theories drawn from public economics, new institutional economics, political economy and public administration can help diagnose the challenges of integrated water resources management, improving efficiency of water utilities, privatization of utilities and public-private partnerships, water pricing reforms, virtual waters/water trading, among others. We conclude that these tools can help advance the second-generation research agenda on water governance.",
        "final_summary": "The papers collectively provide insights into the concept of good water governance. Xu (2018) emphasizes the importance of integrated water resources management, environmental water flow protection, and public participation in achieving good water governance in arid and semi-arid areas of Northwest China. Grigg (2011) discusses the need for effective governance in water management scenarios, highlighting the role of policy, empowerment, and control. Tortajada (2010) acknowledges the complexity of implementing good water governance practices and the need for qualitative and quantitative factors that may vary across countries. Lautze (2011) addresses the confusion surrounding the term \"water governance\" and proposes a refined definition and qualities of good water governance. Solanes (2006) emphasizes the integration of water management elements and the importance of effective water governance for sustainable development. Franks (2007) focuses on the practical outcomes of water governance for poverty alleviation, proposing an analytical framework to understand its impact on poor communities. Dukhovny (2011) highlights the need for comprehensive legal, financial, institutional, and economic tools to ensure equitable access to water and prevent its misuse. Araral (2013) calls for a second-generation research agenda on water governance, emphasizing the study of incentive structures and interdisciplinary approaches. Overall, the papers underscore the significance of integrated management, stakeholder participation, policy effectiveness, and equitable access in achieving good water governance."
    },
    {
        "query": "Emerging trends in Enterprise DevSecOps",
        "paper_list_string": "Rajapakse 2022:\n\nTitle: Collaborative Application Security Testing for DevSecOps: An Empirical Analysis of Challenges, Best Practices and Tool Support\n\nAbstract: DevSecOps is a software development paradigm that places a high emphasis on the culture of collaboration between developers (Dev), security (Sec) and operations (Ops) teams to deliver secure software continuously and rapidly. Adopting this paradigm effectively, therefore, requires an understanding of the challenges, best practices and available solutions for collaboration among these functional teams. However, collaborative aspects related to these teams have received very little empirical attention in the DevSecOps literature. Hence, we present a study focusing on a key security activity, Application Security Testing (AST), in which practitioners face difficulties performing collaborative work in a DevSecOps environment. Our study made novel use of 48 systematically selected webinars, technical talks and panel discussions as a data source to qualitatively analyse software practitioner discussions on the most recent trends and emerging solutions in this highly evolving field. We find that the lack of features that facilitate collaboration built into the AST tools themselves is a key tool-related challenge in DevSecOps. In addition, the lack of clarity related to role definitions, shared goals, and ownership also hinders Collaborative AST (CoAST). We also captured a range of best practices for collaboration (e.g., Shift-left security), emerging communication methods (e.g., ChatOps), and new team structures (e.g., hybrid teams) for CoAST. Finally, our study identified several requirements for new tool features and specific gap areas for future research to provide better support for CoAST in DevSecOps.\n\n==\n\nBrunnert 2015:\n\nTitle: Performance-oriented DevOps: A Research Agenda\n\nAbstract: DevOps is a trend towards a tighter integration between development (Dev) and operations (Ops) teams. The need for such an integration is driven by the requirement to continuously adapt enterprise applications (EAs) to changes in the business environment. As of today, DevOps concepts have been primarily introduced to ensure a constant flow of features and bug fixes into new releases from a functional perspective. In order to integrate a non-functional perspective into these DevOps concepts this report focuses on tools, activities, and processes to ensure one of the most important quality attributes of a software system, namely performance. \nPerformance describes system properties concerning its timeliness and use of resources. Common metrics are response time, throughput, and resource utilization. Performance goals for EAs are typically defined by setting upper and/or lower bounds for these metrics and specific business transactions. In order to ensure that such performance goals can be met, several activities are required during development and operation of these systems as well as during the transition from Dev to Ops. Activities during development are typically summarized by the term Software Performance Engineering (SPE), whereas activities during operations are called Application Performance Management (APM). SPE and APM were historically tackled independently from each other, but the newly emerging DevOps concepts require and enable a tighter integration between both activity streams. This report presents existing solutions to support this integration as well as open research challenges in this area.\n\n==\n\nMao 2020:\n\nTitle: Preliminary Findings about DevSecOps from Grey Literature\n\nAbstract: Context: Emerging from the agile culture, DevOps particularly emphasizes development and deployment speed to achieve rapid value delivery, which however brings some security risks to the software development process. DevSecOps is an extension of DevOps, which is considered as a means to intertwine development, operation and security. Some companies with security concerns begin to take DevSecOps into consideration when it comes to the application of DevOps. Objective: The goal of this study is to report the state-of-the-practice of DevSecOps as well as calling for academia to pay more attention to DevSecOps. Method: Using Google search engine to collect articles on DevSecOps, we conducted a Grey Literature Review (GLR) on the selected articles. Results: Whilst there exists three major software security risks in DevOps, the establishment of DevOps pipeline provides opportunities for software security activities. Based on the preliminary consensus that DevSecOps is an extension of DevOps, it is observed that the interpretations of DevSecOps can be classified into three core aspects, which are: DevSecOps capabilities, cultural enablers, and technological enablers. Furthermore, to materialize the interpretations into daily software production activities, the recommended DevSecOps practices we obtain from Grey Literature (GL) can be categorized in terms of process, infrastructure and collaboration. Conclusion: Although DevSecOps is getting increasing attention by industry, it is still in its infancy and needs to be promoted by both academia and industry.\n\n==\n\nMohan 2016:\n\nTitle: SecDevOps: Is It a Marketing Buzzword? - Mapping Research on Security in DevOps\n\nAbstract: DevOps is changing the way organizations develop and deploy applications and service customers. Many organizations want to apply DevOps, but they are concerned by the security aspects of the produced software. This has triggered the creation of the terms SecDevOps and DevSecOps. These terms refer to incorporating security practices in a DevOps environment by promoting the collaboration between the development teams, the operations teams, and the security teams. This paper surveys the literature from academia and industry to identify the main aspects of this trend. The main aspects that we found are: definition, security best practices, compliance, process automation, tools for SecDevOps, software configuration, team collaboration, availability of activity data and information secrecy. Although the number of relevant publications is low, we believe that the terms are not buzzwords, they imply important challenges that the security and software communities shall address to help organizations develop secure software while applying DevOps processes.\n\n==\n\nAlawneh 2022:\n\nTitle: Expanding DevSecOps Practices and Clarifying the Concepts within Kubernetes Ecosystem\n\nAbstract: DevSecOps principles and practices come with promising futures, which are related to integrating security by design within organizational processes. These include development, deployment, and operational management. For example, DevSecOps practices help securely speed up the processes of application delivery; resilience; elasticity; availability, and re-liability. Despite the promising future of DevSecOps, it comes with several challenges, and one of these is about establishing robust mechanisms for integrating security by design within the existing DevOps practices. This paper unifies and redefines DevSecOps practices, and then provides several real-life examples clarifying what it means to integrate security by design within each practice. Finally, it clarifies the concepts by illustrating the roles of DevSecOps practices in securing the Kubernetes ecosystem.\n\n==\n\nRajapakse 2022:\n\nTitle: Challenges and solutions when adopting DevSecOps: A systematic review\n\nAbstract: Abstract Context: DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge. Objective: This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future. Method: We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data. Results: We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied. Conclusions: We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.\n\n==\n\nRajapakse 2022:\n\nTitle: Challenges and solutions when adopting DevSecOps: A systematic review\n\nAbstract: Abstract Context: DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge. Objective: This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future. Method: We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data. Results: We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied. Conclusions: We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.\n\n==\n\nYasar 2022:\n\nTitle: DevSecOps In Embedded Systems: An Empirical Study Of Past Literature\n\nAbstract: Over the last decade, DevSecOps principles have gained widespread acceptance, replacing many traditional approaches to software development. DevSecOps has helped developers shorten the overall software development life cycle, and as a result, decreased the time to market. Following the broad success of DevSecOps, the next logical progression is to apply DevSecOps principles to other fields to achieve similar results, such as embedded systems. While embedded systems practices may stand to benefit greatly from the inclusion of DevSecOps principles, the field offers many new and unique challenges that have not been faced with traditional software systems. Existing DevSecOps frameworks cannot simply be applied to embedded systems. It is necessary to adapt current DevSecOps frameworks specifically to embedded systems. This piece will first lay out current DevSecOps principles and their application to software systems. Then, an empirical examination of existing work on DevSecOps in embedded systems will be presented. The required components of a DevSecOps framework that have been excluded from previous research will be highlighted, and from this, future areas of research in DevSecOps for embedded systems will be presented. The goal of this work is to summarize and analyze the current state of knowledge on DevSecOps in embedded systems and outline a path for future research. \u2022 Computer systems organization \u2192 Embedded systems; Redundancy; Robotics; \u2022 Networks \u2192 Network reliability.",
        "final_summary": "Emerging trends in Enterprise DevSecOps focus on collaboration, performance, security, and adaptation to specific contexts. Rajapakse 2022 discusses the challenges and solutions related to adopting DevSecOps, emphasizing tool-related challenges and the need for automation. Brunnert 2015 underscores the importance of integrating performance considerations into DevOps practices. Mao 2020 provides insights into the state-of-the-practice of DevSecOps, including interpretations and recommended practices. Mohan 2016 identifies aspects such as compliance, automation, and team collaboration as key in the integration of security in DevOps. Alawneh 2022 offers a perspective on the application of DevSecOps practices within the Kubernetes ecosystem. Lastly, Yasar 2022 explores the application of DevSecOps principles to embedded systems, highlighting the need for specific frameworks (Rajapakse 2022, Brunnert 2015, Mao 2020, Mohan 2016, Alawneh 2022, Yasar 2022)."
    },
    {
        "query": "\"virginia woolf\" \"lower middle class\"",
        "paper_list_string": "Bailey 1999:\n\nTitle: White Collars, Gray Lives? The Lower Middle Class Revisited\n\nAbstract: The lower middle class has long had a bad press, for in common with other subaltern groups it has been more represented from without than within. Thus Victorian writers faced with the disquieting irruption of a new breed of petty bourgeois shop and office workers devised a parodic discourse of littleness, whose feminized tropes rendered the clerk as socially insignificant as the sequestered Victorian woman. George Grossmith's comic classic, Diary of a Nobody, pilloried the new social type in Mr. Pooter, whose smaller-than-life adventures stood for all that was ineffectual, pretentious, and banal in his class. Social commentators held the lower middle class responsible for the degeneration of civilization itself, stifled by their suburban respectability and addiction to mass culture. In Howard's End, E. M. Forster drew the clerk, Leonard Bast, with some sympathy but made him the book's major casualty, while belittling a class whose education was learned \u201cfrom the outside of books.\u201d In the interwar years the Marxist poet Christopher Caudwell likened the petty bourgeois world to \u201ca terrible stagnant marsh, all mud and bitterness, without even the saving grace of tragedy.\u201d George Orwell's fictional antihero from the same period, the insurance salesman George Bowling, characterizes the men of his class as \u201cTories, yes-men and bumsuckers.\u201d It is still hard to hide a certain relish in repeating such charges, for putting the boot in on the lower middle class has long been the intellectual's blood sport, an exorcism, so we are told, of the guilty secret so many of us share as closet petit bourgeois denying our own class origins.\n\n==\n\nFernald 2005:\n\nTitle: A Feminist Public Sphere? Virginia Woolf's Revisions of the Eighteenth Century\n\nAbstract: Since the historicist turn in scholarship on modernism, critics have focused on the ways modernist writers have marketed themselves. The modernist writers who expressed disdain for mass culture have been shown to have used and depended on the tools of advertising, marketing, professionalization, and self-promotion that was provided by mass culture. Thus, Peter McDonald argues that \u201cthe cultural divisions between the high-class reviews and the illustrated monthlies were not always as rigid as might be supposed,\u201d and Lawrence Rainey asserts that modernist publishing depended upon an intermediate economic stage, such as an early limited or subscription-based edition, during which a text\u2019s cultural value rose, potentially helping create demand for broader sales.1 When this sociohistorical approach focuses on a single writer\u2019s career, it shows how he or she fits into, or, more often, attempts to subvert or challenge the cultural categories of high, low, and middlebrow. Virginia Woolf was, of course, a woman, and thus something of an outsider in the London literary scene; she was also the daughter of Victorian man-of-letters and editor Leslie Stephen, and thus, the ultimate insider. This combination makes her career an ideal case for further study. As one recent critic puts it, \u201cWoolf entered public discourse by the side door.\u201d2\n\n==\n\nDaugherty 2006:\n\nTitle: \"You See You Kind of Belong to Us, and What You Do Matters Enormously\": Letters from Readers to Virginia Woolf\n\nAbstract: Writing to Virginia Woolf about Three Guineas from Springfield, MA in 1938, Agnes K. Potter says her comments are from \"the point of view of this ordinary reader\" (Snaith, \"Three Guineas Letters\" 97). Q. D. Leavis, however, would have us believe that Woolf did not have ordinary readers; Three Guineas \"is a conversation between [Woolf] and her friends\" (272). Woolf's careful and honest description of her class position leads to Leavis's assumption that all Woolf's readers must be of Woolf's own class. Anna Snaith's edition of the letters written to Woolf after the publication of Three Guineas proves that such a narrow construction of Woolf's readership is not accurate. (1) Similarly, Melba Cuddy-Keane's investigations of the readers \"spotting\" The Common Reader (Virginia Woolf 110-14) and of individual readers like John Farrelly (\"From Fan-Mail\" 3-32) succeed in \"break[ing] down categories that have identified high culture with high class\" and show that definitions of Woolf's readership must include respect for \"both the intellectual impulse and the intellectual accomplishment of non-privileged, non-specialist readers\" (\"Imbricated\" 5). Following Q. D. Leavis's lead, however, Jonathan Rose recently argues that Woolf's essay \"Middlebrow\" calls for cultural triage and asserts that modernists made literature difficult to make the common reader \"illiterate once again\" and to preserve \"a body of culture as the exclusive property of a coterie\" (394). He does caution, though, that \"no two individual reading histories [are] alike,\" that generalizations about readers, though not \"completely groundless,\" neglect the \"more complicated and ambiguous\" use of literacy, and that \"[t]he only workable method is to consult the readers themselves\" (367). Exactly. As Anna Snaith and Melba Cuddy-Keane discovered, one of the best ways to consult readers themselves is through their letters to Woolf. (2) Because, as Helen Waddell points out when she encloses someone else's letter about The Waves in her own note to Woolf, \"in spite of all that has already been said in print, there is something in the manuscript word and in the circumstances of the writing that makes it valuable\" (Letter 78). I discovered the same thing in the summer of 2001. At a session called \"Archives in the Age of Mechanical Reproduction\" during the Eleventh Annual Conference on Virginia Woolf at the University of Bangor in Wales, Bet Inglis, the retired Assistant Librarian in the Manuscripts Section at the University of Sussex, talked about the treasures in the Monks House Papers and casually mentioned that someone should take a look at the correspondence between Virginia Woolf and Elizabeth Bowen housed there. When I got to the Sussex archives the next week, planning to spend the entire summer on the drafts of early reviews written by Virginia Stephen, I thought I might take an afternoon to follow up on her suggestion. That digression quickly became an obsession, an archive-junkie race through the correspondence to Virginia Woolf located in Letters III of the Monks House Papers (SxMs 18). But what intrigued me, what I became addicted to, were not the letters from Elizabeth Bowen, engaging as they are. No, although the known correspondents in the authornamed files in Letters III certainly claimed some of my attention, what I could not stop reading and transcribing that crazed summer were the letters in a box tantalizingly called Correspondence of Various Persons re: Books, Articles. (3) Woolf's readers came alive as I read their letters, and I could not turn my back on them. \"[G]iving a voice ... to the silent reader\" (Carr), this collection of letters grows out of and builds on the fine work of Snaith, Cuddy-Keane, and Oldfield, increases the number of actual (as opposed to imagined) readers Woolf scholars can consult, and moves Woolf studies, particularly studies of her reception, another step closer to a full record of letters written to Virginia Woolf about her work and thus to a more accurate view of the \"far wider circle\" Woolf hoped to reach (L6 420). \u2026\n\n==\n\nSquier 1983:\n\nTitle: \"The London Scene\": Gender and Class in Virginia Woolf's London\n\nAbstract: Whether she thought it \"the most beautiful place on the face of the earth\" or \"the very devil,\" to Virginia Woolf the city of London was the focus for an intense, often ambivalent, lifelong scrutiny. Not only did she make her home there for nearly all of her fifty-nine years-first in the narrow streets of Kensington and then in the spacious squares of Bloomsbury-but she found it a powerfully evocative figure in the literary tradition within which she wrote. And one of the most powerful nonfiction representations of Virginia Woolf's response to London was the series of six essays which appeared a little over fifty years ago, from December 1931 to December 1932, in the magazine Good Housekeeping. As their titles indicate, the essays surveyed the highs and lows of the city: \"The Docks of London,\" \"Oxford Street Tide,\" \"Great Men's Houses,\" \"Abbeys and Cathedrals,\" \" 'This is the House of Commons,'\" and \"Portrait of a Londoner.\" Plotless, descriptive, slight as these essays seemed at the time to Woolf, to readers of today the \"London Scene\" essays are fascinating, for as I will show, they reveal Virginia Woolf's ambivalence about identity, social position, and access to material possessions, and they contain the strategies forged to accommodate her changing sense of self and social place without alienating the Good Housekeeping audience. Although the \"London Scene\" essays celebrate a conventionally modernist setting, the city, they are anything but conventionally modernist in their approach. At their best they subvert the often complacent genre of the urban travelogue to portray gender and class relations in the modern city.1 Woolf struggled with conflicting identifications in the \"London Scene\" essays, between insiders (men, the upper classes) and outsiders (women, the working classes), and she used a number of different\n\n==\n\nChilders 2009:\n\nTitle: Virginia Woolf on the Outside Looking Down: Reflections on the Class of Women\n\nAbstract: Sur la contradiction politique/esthetique et les limites du feminisme woolfien. Lecture de Three Guineas et la preface de Life as We Have Known It\n\n==\n\nBesnault-Levita 2012:\n\nTitle: Modernist Short Fiction by Women: The Liminal in Katherine Mansfield, Dorothy Richardson, May Sinclair and Virginia Woolf (review)\n\nAbstract: 214 publishers sought to emulate. Stephen Barkway\u2019s masterly, albeit brisk and businesslike, tour of Vita Sackville-West\u2019s 18-year association with the Press makes a similar point. While the best-selling novel The Edwardians (1930) earned much needed profits, signed limited editions of Sackville-West\u2019s poetry bolstered the firm\u2019s cultural capital. Barkway is a superb guide to the hard-boiled gentility that typified business dealings between Leonard and Sackville-West, severed shortly after Virginia\u2019s death by a poignant rejection letter. Southworth\u2019s contribution to the volume is strong but her characterizations of the \u201cworking-class voices\u201d (229) promoted by the Press prompts me to wonder how reliably she can parse British class distinctions. R. M. Fox, an Oxford educated son of a headmistress, was no more working class than George Orwell. Dealing with the Welsh poet Huw Menai, an ex-miner, Southworth provides a telling observation regarding the \u201cfaddishness of his London supporters, who dropped him when he failed to play up his colliery connections\u201d (224). The \u201cworking-class\u201d credentials paraded by the Hogarth Press were always tinged by an element of patrician patronage.4 It is laudable that these wide-ranging case studies should concentrate on the work of writers and artists who, for the most part, have rarely been examined in relation to the Hogarth Press. Southworth\u2019s intelligently edited collection successfully challenges and complicates received wisdom about the Woolfs as publishers. It also protests too much: Willis\u2019s detailed history of the Press remains indispensable.\n\n==\n\nCuddy\u2010Keane 2003:\n\nTitle: Virginia Woolf, the Intellectual, and the Public Sphere: Democratic highbrow: Woolf and the classless intellectual\n\nAbstract: \u201cWhen I use a word,\u201d Humpty Dumpty said, in rather a scornful tone, \u201cit means just what I choose it to mean \u2013 neither more nor less.\u201d \u201cThe question is,\u201d said Alice, \u201cwhether you can make words mean so many different things.\u201d \u201cThe question is,\u201d said Humpty Dumpty, \u201cwhich is to be master \u2013 that's all.\u201d Lewis Carroll, Alice in Wonderland It is also that the variations and confusions of meaning are not just faults in a system, or errors of feedback, or deficiencies of education. They are in many cases, in my terms, historical and contemporary substance. Indeed they have often, as variations, to be insisted upon, just because they embody different experiences and readings of experience, and this will continue to be true, in active relationships and conflicts, over and above the clarifying exercises of scholars or committees. What can really be contributed is not resolution but perhaps, at times, just that extra edge of consciousness. Raymond Williams, Keywords CULTURAL KEYWORDS To write about Virginia Woolf as a democratic highbrow is to invoke controversy. My approach challenges the notion of the modernist writer as aloof from the public and the idea of intellectuals as an elite; it resists identifications of the popular exclusively with the world of commodities and entertainment; it rejects the notion that declining standards must inevitably follow from \u201cthe mass.\u201d And, as I state in my introduction, my approach contests a number of prevailing constructions of \u201cVirginia Woolf.\n\n==\n\nCuddy\u2010Keane 2003:\n\nTitle: Virginia Woolf, the Intellectual, and the Public Sphere\n\nAbstract: Acknowledgments List of abbreviations Introduction: a wider sphere Part I. Cultural Contexts: 1. Democratic highbrow: Woolf and the classless intellectual 2. Woolf, English studies and the making of the (new) common reader Part II. Critical Practice: 3. Woolf and the theory and pedagogy of reading Postscript: intellectual work today Notes Bibliography Index.",
        "final_summary": "Bailey (1999) provides a historical perspective on the negative portrayal of the lower middle class in Victorian literature. Fernald (2005) delves into Woolf's unique position in the London literary scene, highlighting her dual role as both an insider and outsider. Daugherty (2006) challenges the narrow construction of Woolf's readership, suggesting a broader demographic. Squier (1983) offers an analysis of Woolf's essays on London, revealing her ambivalence about identity and social position. Cuddy-Keane (2003) presents a counter-narrative to the notion of Woolf as an aloof intellectual, emphasizing her engagement with the public sphere."
    },
    {
        "query": "How confidence do student paramedics feel when performing a lifting assessment?",
        "paper_list_string": "Anderson 2019:\n\nTitle: Paramedic student confidence, concerns, learning and experience with resuscitation decision-making and patient death: A pilot survey.\n\nAbstract: BACKGROUND\nAround the world, many paramedics are authorised to withhold or terminate resuscitation. Research indicates this can be a challenging part of their role. Little is known about graduating paramedic student confidence, concerns and learning in this area.\n\n\nMETHODS\nAn online cross-sectional survey of students nearing completion of a paramedic degree in New Zealand, including piloting of a newly-developed confidence scale.\n\n\nRESULTS\nSeventy-two participants reported varying exposure to termination of resuscitation and patient death. Participants felt most confident providing technical procedurally-based care and least confident with non-technical skills. Participants' greatest concerns included making 'the right call', supporting grieving family, controlling emotions and encountering conflict. Clinical exposure with supportive mentoring, clinical simulation, peer reflection and resolved personal experience with death, were considered most useful for professional development.\n\n\nCONCLUSIONS\nExposure to termination of resuscitation and management of the scene of a patient death is variable amongst graduating paramedics. Novice paramedics may benefit from opportunities to observe and rehearse non-technical skills including delivering death notification and communicating with bystanders and bereaved family. The Confidence with Resuscitation Decision-Making and Patient Death Scale has favourable psychometric properties and utility as an outcome measure for future research in this area.\n\n==\n\nSandy 2021:\n\nTitle: Paramedic students\u2019 confidence and satisfaction with clinical simulations of an emergency medical care programme in South Africa: A cross-sectional study\n\nAbstract: Background There has been an increase in the use of clinical simulations as instructional tools in healthcare education. This is because of their role in ensuring patients\u2019 safety and quality-care provision. Aim This study investigated the paramedic students\u2019 satisfaction and self-confidence in the clinical simulation of an emergency medical care programme. Setting The study was conducted at the Durban University of Technology in the KwaZulu-Natal Province of South Africa. The paramedic students\u2019 satisfaction and self-confidence in the clinical simulation of an emergency medical care programme were the focus of the study. Methods The study used a cross-sectional research design. A convenience sampling method was used to select the 83-paramedic students who participated in the study. Data were collected between July and September 2017 using a structured questionnaire. Descriptive statistics (frequencies and percentages and Spearman\u2019s rank-order correlation coefficient) and an inferential test, ordinal logistic regression analysis, were used for data analysis. Results High levels of paramedic students\u2019 satisfaction and self-confidence in simulation activities were reported. Generally, the paramedic students\u2019 demographics were associated with the satisfaction and self-confidence variables with p-values \u2264 0.04. Emergency medical care training undertaken by the paramedic students was significantly associated with self-confidence (p = 0.00). Conclusion Clinical simulation can bridge the theory-practice gap for paramedic students. It is a hands-on approach that promotes students learning of clinical skills through reflection.\n\n==\n\nHolmes 2017:\n\nTitle: Student Paramedic Anticipation, Confidence and Fears: Do Undergraduate Courses Prepare Student Paramedics for the Mental Health Challenges of the Profession?\n\nAbstract: Introduction This study explores the preparedness of undergraduate student paramedics for the mental health challenges of the paramedic profession from the perspective of course coordinators and their students. Methods Two surveys were developed and administered to course coordinators and students of the 16 undergraduate degree paramedicine courses across Australia and New Zealand. Sixteen course coordinators and 302 students responded. Results Results illustrate there was widespread recognition for the need to include preparation for the mental health challenges of the profession within undergraduate courses. Furthermore, most course coordinators and students had a preference for this topic to be taught using multiple teaching modes with particular preference for teaching the topic via discussion and activity based education. Teaching the topic as a standalone unit was supported by more than a third of course coordinators (43%) and a third of students (32%). Conclusion Six themes were identified as positive by anticipants: caring for people, high acuity work, diversity of work and patients, making a difference to patients and their families, using clinical skills and knowledge and engaging with the community. Students were most confident about communicating with patients and using clinical skills and knowledge. Students were least confident about clinical decision making and the most commonly cited fear was making a clinical mistake. A significant proportion of students (16%) feared for their personal mental wellbeing and 14% reported they were least confident about personal mental health within the profession.\n\n==\n\nRoss 2014:\n\nTitle: Perceptions of Student Paramedic Interpersonal Communication Competence: A Cross-Sectional Study\n\nAbstract: Introduction Interpersonal communication skills are essential to the healthcare practitioner aiding in high quality, effective and safe clinical practice. Effective communication exerts a positive influence on the patient's physical and emotional status resulting in better patient outcomes and satisfaction. By identifying strengths and weaknesses, self-assessment of interpersonal communication skills can be used as an intervention tool to inform future curriculum renewal. The objective of this study was to identify paramedic students\u2019 perceptions of their interpersonal communication competence. Methods Second year paramedic students from Monash University (Victoria) were invited to participate in a survey that asked them to record perceptions of their interpersonal communication skills using the Interpersonal Communication Competence Scale (ICCS). The ICCS is a 30-item unipolar questionnaire using a Likert scale ranging from 1 (almost never) to 5 (almost always). Mean and standard deviations (SD) were used to report results. Results Fifty-six second year paramedic students participated in the study. Participants were predominantly aged less than 25 years (85.7%) and male n=36 (64.3%). Students reported \u2018often\u2019 or \u2018almost always\u2019 for the items: \u2018I put myself in others\u2019 shoes\u2019, n=46 (82%), mean=3.98 (SD 0.59); and, \u2018I let others know that I understand what they say\u2019, n=45 (80%), mean=3.96 (SD 0.66). Students reported \u2018sometimes\u2019, \u2018often\u2019 or \u2018almost always\u2019, for the items: \u2018I have trouble convincing others to do what I want them to do\u2019, n=55 (98%), mean=3.5 (SD 0.63); and, \u2018My mind wanders during conversations\u2019, n=41 (73%), mean=3.05 (SD 0.88). Conclusion Preliminary results suggest that student paramedics self-report their interpersonal communication skills highly apart from areas related to assertiveness and listening skills. These results could be indicative of student age, personality or experience level and warrant further research with larger sample sizes.\n\n==\n\nWilliams 2015:\n\nTitle: Are paramedic students ready to be professional? An international comparison study.\n\nAbstract: INTRODUCTION\nThe last decade has seen rapid advancement in Australasian paramedic education, clinical practice, and research. Coupled with the movements towards national registration in Australia and New Zealand, these advancements contribute to the paramedic discipline gaining recognition as a health profession.\n\n\nAIM\nThe aim of this paper was to explore paramedic students' views on paramedic professionalism in Australia and New Zealand.\n\n\nMETHODS\nUsing a convenience sample of paramedic students from Whitireia New Zealand, Charles Sturt University and Monash University, attitudes towards paramedic professionalism were measured using the Professionalism at Work Questionnaire. The 77 item questionnaire uses a combination of binary and unipolar Likert scales (1\u2009=\u2009Strongly disagree/5\u2009=\u2009Strongly agree; Never\u2009=\u20091/Always\u2009=\u20095).\n\n\nRESULTS\nThere were 479 students who participated in the study from Charles Sturt University n\u2009=\u2009272 (56.8%), Monash University n\u2009=\u2009145 (30.3%) and Whitireia New Zealand n\u2009=\u200962 (12.9%). A number of items produced statistically significant differences P\u2009<\u20090.05 between universities, year levels and course type. These included: 'Allow my liking or dislike for patients to affect the way I approach them' and 'Discuss a bad job with family or friends outside work as a way of coping'.\n\n\nCONCLUSIONS\nThese results suggest that paramedic students are strong advocates of paramedic professionalism and support the need for regulation. Data also suggest that the next generation of paramedics can be the agents of change for the paramedic discipline as it attempts to achieve full professional status.\n\n==\n\nBoyle 2008:\n\nTitle: Ambulance clinical placements \u2013 A pilot study of students' experience\n\nAbstract: BackgroundUndergraduate paramedic students undertake clinical placements in a variety of locations. These placements are considered an essential element for paramedic pre-employment education. However, anecdotal evidence suggests some students have not had positive experiences on their emergency ambulance placements. The objective of this study was to identify the type of experiences had by students during ambulance clinical placements and to provide feedback to the ambulance services.MethodsIn this pilot study we employed a cross-sectional study methodology, using a convenience sample of undergraduate paramedic students available in semester one of 2007 to ascertain the students' views on their reception by on-road paramedics and their overall experience on emergency ambulance clinical placements. Ethics approval was granted.ResultsThere were 77 students who participated in the survey, 64% were females, with 92% of students < 25 years of age and 55% < 65 Kg in weight. There was a statistically significant difference in average height between the genders (Male 179 cm vs Female 168 cm, p < 0.001). Clinical instructors were available to 44% of students with 30% of students excluded from patient management. Thirty percent of students felt there was a lot of unproductive down time during the placement. Paramedics remarked to 40% of students that they doubted their ability to perform the physical role of a paramedic, of this group 36% were advised this more than once.ConclusionThis study demonstrates that for a small group of students, emergency ambulance clinical placements were not a positive experience clinically or educationally. Some qualified paramedics doubt if a number of female students can perform the physical role of a paramedic.\n\n==\n\nJensen 2016:\n\nTitle: A Survey to Determine Decision-Making Styles of Working Paramedics and Student Paramedics.\n\nAbstract: OBJECTIVE\nTwo major processes underlie human decision-making: experiential (intuitive) and rational (conscious) thinking. The predominant thinking process used by working paramedics and student paramedics to make clinical decisions is unknown.\n\n\nMETHODS\nA survey was administered to ground ambulance paramedics and to primary care paramedic students. The survey included demographic questions and the Rational Experiential Inventory-40, a validated psychometric tool involving 40 questions. Twenty questions evaluated each thinking style: 10 assessed preference and 10 assessed ability to use that style. Responses were provided on a five-point Likert scale, with higher scores indicating higher affinity for the style in question. Analysis included both descriptive statistics and t tests to evaluate differences in thinking style.\n\n\nRESULTS\nThe response rate was 88.4% (1172/1326). Paramedics (n=904) had a median age of 36 years (IQR 29-42) and most were male (69.5%) and primary or advanced care paramedics (PCP=55.5%; ACP=32.5%). Paramedic students (n=268) had a median age of 23 years (IQR 21-26), most were male (63.1%) and had completed high school (31.7%) or an undergraduate degree (25.4%) prior to paramedic training. Both groups scored their ability to use and favourability toward rational thinking significantly higher than experiential thinking. The mean score for rational thinking was 3.86/5 among paramedics and 3.97/5 among paramedic students (p<0.001). The mean score for experiential thinking was 3.41/5 among paramedics and 3.35/5 among paramedic students (p=0.06).\n\n\nCONCLUSION\nWorking paramedics and student paramedics prefer and perceive that they have the ability to use rational over experiential thinking. This information adds to our current knowledge on paramedic decision-making and is potentially important for developing continuing education and clinical support tools.\n\n==\n\nLowery 2005:\n\nTitle: Role of peer support and emotional expression on posttraumatic stress disorder in student paramedics.\n\nAbstract: This exploratory study contrasted and tested the predictive value of the reverse buffering hypothesis of social support and the information processing model of posttraumatic stress disorder (PTSD) in an investigation of trauma-related symptomatology (TRS) in a single sample of 42 student paramedics. Participants completed several anonymous self-report measures of PTSD symptomatology, peer social support, and attitude toward emotional expression. Regression-based path analyses did not support either theory of PTSD in this population. A path model of PTSD in student paramedics was subsequently developed, indicating that a direct relationship exists between duty-related trauma exposure, dysfunctional peer social support, and students' negative attitudes toward emotional expression. This new model accounted for 30% of the variance in student paramedics' TRS.",
        "final_summary": "The papers suggest that student paramedics have varying levels of confidence when performing lifting assessments. Anderson 2019 found that paramedic students felt most confident in providing technical procedurally-based care but least confident in non-technical skills. Sandy 2021 reported high levels of satisfaction and self-confidence in clinical simulations among paramedic students. Holmes 2017 highlighted the need for undergraduate courses to prepare students for the mental health challenges of the profession. Ross 2014 identified that paramedic students perceived their interpersonal communication competence positively, except in areas related to assertiveness and listening skills. Williams 2015 indicated that paramedic students strongly advocate for professionalism and support the need for regulation. Boyle 2008 revealed that some paramedic students had negative experiences during ambulance clinical placements. Jensen 2016 found that both working paramedics and student paramedics preferred and perceived themselves as having the ability to use rational thinking over experiential thinking. Lowery 2005 suggested a direct relationship between trauma exposure, dysfunctional peer social support, and negative attitudes toward emotional expression in student paramedics."
    },
    {
        "query": "tay-sachs and sulfated glycolipid",
        "paper_list_string": "Hoffman 1977:\n\nTitle: GLYCOSPHINGOLIPIDS IN FETAL TAY\u2010SACHS DISEASE BRAIN AND LUNG CULTURES\n\nAbstract: Abstract\u2014 A study was undertaken of the glycosphingolipids in cell cultures derived from cerebellum of Tay\u2010Sachs disease fetal brain in order to determine the suitability of such cell strains as a model for Tay\u2010Sachs disease. The glycosphingolipids in the Tay\u2010Sachs disease cultured cerebellar cells were compared with those found in normal cultured cerebellar cells, normal and Tay\u2010Sachs cultured lung cells, and normal and Tay\u2010Sachs fetal brain. The glycolipids were separated by TLC, then analyzed by GLC of the trimethylsilyi derivatives of the methylglycosides of the sugar moieties. In the cultured cerebellar lines, the predominant gangliosides were GM2, GM3, and GD3. There was a 4\u2010fold increase of GM2 in the Tay\u2010Sachs as compared with the normal line. Only GM3 and GD3 gangliosides were found in the Tay\u2010Sachs and the normal fetal lung cell cultures. The major neutral glycosphingolipids in all of the cultured cells which were analyzed were glucosylceramide, lactosylceramide, digalactosyl\u2010glucosylceramide, and globoside. When the Tay\u2010Sachs cerebellar cells were labelled with [1\u201014C]gluco\u2010samine, some radioactivity was observed in the trihexosylceramide band, indicating the presence of a small amount of a galactosamine\u2010containing trihexosylceramide which may be asialo\u2010GM2 (GA2). The trihexosylceramide in Tay\u2010Sachs fetal brain was identified as GA2 by GLC. Both Tay\u2010Sachs and normal fetal brain gangliosides were more complex than those found in the cultured cells. Long chain fatty acids (C24:0 and C24;1) predominated in all of the glycosphingolipids of the Tay\u2010Sachs and the normal cultured cerebellar cells. In contrast, the glycosphingolipids of Tay\u2010Sachs and normal fetal brain contained mainly the shorter chain fatty acids (C16:0, C18:0, and C18:1). The cerebrosides in both the Tay\u2010Sachs and normal fetal brains were mainly glucosylceramide with only small amounts of the galactosylceramide which predominates in infant brain. Cultured cells from the fetal Tay\u2010Sachs disease\n\n==\n\nBerman 1963:\n\nTitle: STUDIES ON BRAIN LIPIDS IN TAY\u2010SACHS\u2019DISEASE\u2014I ISOLATION OF TWO SIALIC ACID\u2010FREE GLYCOLIPIDS *\n\nAbstract: A NUMBER of glycolipids (in addition to cerebrosides) containing sphingosine, fatty acids and sugars (glucose, galactose and/or hexosamine) have been isolated from spleen and erythrocyte stroma of various species (KLENK and DEBUCH, 1959). Although lipids of this composition have not been identified with certainty in brain tissue, there is nevertheless some indication that they might be present in small amounts (WEISS, 1956; KLENK et al., 1957; SVENNERHOLM and RAAL, 1961). Two such glycolipids have now been isolated from brain tissue of patients with Infantile Amaurotic Familial Idiocy (Tay-Sach's disease). The main biochemical abnormality which characterizes this disease is an accumulation of gangliosides (KLENK, 1939); the two new compounds which have now been identified in brain tissue of Tay-Sachs' disease belong to the same class of lipids. The isolation and chemical composition of these two glycolipids are described in the present communication. Some of this work has appeared in preliminary reports (GATT and BERMAN, 1961 ; BERMAN and GATT, 1962).\n\n==\n\nToma 2005:\n\nTitle: Impaired sulphated glycosaminoglycan metabolism in a patient with GM-2 gangliosidosis (Tay-Sachs disease)\n\nAbstract: SummaryAn abnormal urinary excretion of sulphated glycosaminoglycans in a patient with GM-2 gangliosidosis (Tay-Sachs disease) is described. Besides the accumulation of GM-2 ganglioside in liver and lack of hexosaminidase A, the patient shows an abnormal urinary excretion of an iduronic acid-rich low molecular weight heparan sulphate. Also, no dermatan sulphate could be detected in the urine, whereas this compound was the main sulphated glycosaminoglycan in the liver of the patient. Heparan sulphate was the main glycosaminoglycan of normal liver. The total amount of sulphated glycosaminoglycans in the urine and liver of the patient did not differ significantly from the amounts found in the liver and urine of normal subjects. Several plasma glycosidases have been assayed and the activities did not differ significantly from the values obtained for the plasma of normal subjects.\n\n==\n\nSandhoff 1969:\n\nTitle: Variation of \u03b2\u2010N\u2010acetylhexosaminidase\u2010pattern in Tay\u2010Sachs disease\n\nAbstract: Tay-Sachs disease is characterized by an accumulation of two types of glycosphingolipids, on the one hand ganglioside GM? (GalNAc-&1,4-(NeuNAc-&2,3) -Gal+1,4-Glc_13-1,1{2-N-acyl) sphingosine), and on the other its asialo residue (GalNAc-/3-1,4-Gal_P1,4GlcQ1,1(2-N-acyl)sphingosine) in nerve tissue [ l-31 . The disease is assumed to involve a lack of the enzyme which catabolizes the stored substances, similar to that present in other sphingolipidoses [4,5]. In a special case of Tay-Sachs disease with visceral storage of kidney globoside (GalNAc-P_1,3-Gal-/3-1,4-Gal-&l,4 GlcQ-1 ,l (2-N-acyl)sphingosine) a general lack of /3-Nacetylhexosaminidase activity was found [6] (fig. Id). The stored glycosphingolipids (ganglioside GM, and its asialo residue in nerve tissue, and kidney globoside in the visceral organs) in this case had in common a terminal /3-glycosidic-bound N-acetylgalactosamine, being partially hydrolized by PN-acetylhexosaminidase-preparations [6,7]. The same substances also accumulate in the various tissues of conventional Tay-Sachs cases, although the storage level of the asialo residue of ganglioside GM, and of the kidney globoside is found to be lower [6]. Accordingly, the lack of some particular /3-N-acetylhexosaminidase was discussed as one of the possible causes for storage in cases of conventional Tay-Sachs disease [6]. In the present study the f3-N-acetylhexosaminidase pattern of 4 cases of conventional Tay-Sachs disease is des cribed. In three of the four cases a lack of the PN-ace-\n\n==\n\nHoffman 1978:\n\nTitle: Fetal tay-sachs disease brain cells in culture: lack of turnover in [14C]glucosamine-labeled GM2\n\n\nAbstract: Abstract The glycosphingolipids in a cultured cell strain derived from Tay-Sachs disease fetal cerebellum were pulse-labeled with radioactive [ 14 C]glucosamine. The turnover of individual gangliosides in the cells were followed during a 10 day period. All of the gangliosides except G M2 had a buildup of counts which peaked between 24 and 96 h, then decreased. In contrast, the counts incorporated into G M2 did not decrease during the entire course of the experiment. The lack of turnover in G M2 ganglioside provides support for the use of these cultured brain cells as a model for Tay-Sachs disease.\n\n==\n\nYu 1983:\n\nTitle: Characterization of some minor gangliosides in Tay\u2014Sachs brains\n\nAbstract: Abstract The ganglioside distribution of Tay\u2014Sachs brain was re-examined in detail. In both the gray and white matter, the levels of lipid-bound sialic acid were increased 6- and 10-fold, respectively, over normal infant brain, and approximately 90% of the total ganglioside was G M2 . The level of G M2 was increased about 90 times in gray matter and 220 times in white matter in comparison with that in normal controls. The level of G D1a -GaINAc was increased 19 times and 10 times in gray and white matter, respectively. The concentration of G D2 was increased about 4-fold in Tay-S-Sachs white matter. In addition, the G M3 level was increased 2.7 and 3.5 times and the G D3 level 2 and 2.4 times over normal gray and white matter, respectively. However, the levels of other complex gangliosides such as G M1 , G D1a , G D1b , G T1b and G Q1b decreased remarkably. Since G M2 , G D2 , G D1a -GalNAc and a recently characterized ganglioside G M1b -GalNAc possess a common N-acetylgalactosaminyl terminal structure, their accumulation in Tay\u2014Sachs brains is therefore consistent with the known hexosaminidase A deficiency. However, the accumulation of hexosamine-free G M3 and G D3 is not. The in vitro incorporation of N-acetylgalactosamine into G M3 to form G M2 was examined in a rat brain microsomal fraction in the presence of large amounts of other glycolipids. Acidic glycolipids were slightly stimulating and then became increasingly inhibitory when the molar ratio of lipid to substrate G M3 exceeded 10 to 1. Neutral glycolipids and the phospholipid, phosphatidylcholine, were inhibitory at all levels tested. The data suggest that the accumulation of G M3 and G D3 in Tay\u2014Sachs brains could be due to an inhibition of N-acetylgalactosaminyl-transferas by high levels of glycolipids, and the inhibition is not due to chelation of the obligate divalent cation necessary for the activity of this enzyme. The inhibition of this enzyme may also be responsible for the decreased levels of other complex gangliosides.\n\n==\n\nPlatt 1997:\n\nTitle: Prevention of lysosomal storage in Tay-Sachs mice treated with N-butyldeoxynojirimycin.\n\nAbstract: The glycosphingolipid (GSL) lysosomal storage diseases result from the inheritance of defects in the genes encoding the enzymes required for catabolism of GSLs within lysosomes. A strategy for the treatment of these diseases, based on an inhibitor of GSL biosynthesis N-butyldeoxynojirimycin, was evaluated in a mouse model of Tay-Sachs disease. When Tay-Sachs mice were treated with N-butyldeoxynojirimycin, the accumulation of GM2 in the brain was prevented, with the number of storage neurons and the quantity of ganglioside stored per cell markedly reduced. Thus, limiting the biosynthesis of the substrate (GM2) for the defective enzyme (beta-hexosaminidase A) prevents GSL accumulation and the neuropathology associated with its lysosomal storage.\n\n==\n\nSchneck 1976:\n\nTitle: Glycolipids in cultured fetal Tay-Sachs disease cerebellar cells.\n\nAbstract: Tay-Sachs disease (TSD) is a fatal, genetically determined disorder of sphingoglycolipid metabolism, associated with the absence of the lysosomal enzyme B-D-N-acetylhexosaminidase A (Hex A) (1). There is a massive accumulation of GM2 ganglioside, and its asialo derivative GA2 in cells of the central nervous system. A cell culture which reproduces these biochemical parameters would permit one to measure the effect of enzyme replacement therapy under controlled conditions that are not easily attainable in vivo. Although skin fibroblasts cultured from TSD patients lack Hex A, these cultured cells do not accumulate GM2 ganglioside (2). Since TSD is a neuronal lipid storage disease, and since fetal TSD brain has the characteristic glycolipid patterns found in infant TSD brain, a cell strain from fetal TSD cerebellum was established and the glycolipid patterns were evaluated by TLC and GLC (3). The cells were labelled with 14C-glucosamine in order to compare the metabolic activity of the gangliosides over an extended time period. The cells were also transformed with the oncogenic DNA SV-40 virus (4) since we wished to establish a permanent cell line for the study of this disease.",
        "final_summary": "The papers suggest that there is an accumulation of sulfated glycolipids, specifically GM2 ganglioside, in Tay-Sachs disease. Hoffman 1977 found increased levels of GM2 ganglioside in Tay-Sachs disease cultured cerebellar cells compared to normal cells. Berman 1963 isolated two sialic acid-free glycolipids from brain tissue of Tay-Sachs patients, which belong to the same class of lipids as the accumulated gangliosides. Toma 2005 described abnormal urinary excretion of iduronic acid-rich low molecular weight heparan sulfate in a patient with Tay-Sachs disease. Sandhoff 1969 discussed the variation of \u03b2-N-acetylhexosaminidase pattern in Tay-Sachs disease, suggesting a lack of this enzyme as a possible cause for storage. Hoffman 1978 observed a lack of turnover in GM2 ganglioside in Tay-Sachs disease cultured brain cells. Yu 1983 found increased levels of GM2 ganglioside and other gangliosides in Tay-Sachs brain tissue, potentially due to inhibition of N-acetylgalactosaminyltransferase. Platt 1997 demonstrated that treatment with N-butyldeoxynojirimycin prevented the accumulation of GM2 ganglioside in Tay-Sachs mice. Schneck 1976 established a cell strain from fetal Tay-Sachs cerebellum that exhibited the characteristic glycolipid patterns found in the disease."
    },
    {
        "query": "\u201cscreen addiction in middle or high school students\u201d",
        "paper_list_string": "Garc\u00eda-Santill\u00e1n 2021:\n\nTitle: Addiction to the Smartphone in High School Students: How It\u2019s in Daily Life?\n\nAbstract: Nowadays, the serious situation that affects the entire world goes beyond the social, cultural, economic problems and other conflicts that occur day by day. These were left aside to move to a global alert; we refer to the pandemic crisis that all the nations of the world are facing. Confinement forced people all over the world to stay at home; therefore, communications through electronic devices became very necessary. This study does not seek to analyze the pandemic crisis; its purpose is to analyze the use that students give to their mobile phone, to determine if this has generated addiction, in addition to identifying if use differs in men and in women. Participants were 184 high school students enrolled in a public sector institution in the Port of Veracruz, Mexico. To obtain the data, the SAS-CV test was used. This contains questions related to the profile of the respondent and 10 items in Likert format. It was distributed via electronic devices for their response. The data were statistically analyzed using polychoric correlation matrices and factor analysis with component extraction. The main findings demonstrate the obtaining of three components: physiological, dependence and distraction, which account for 68% of the total variance, and it was also shown that there are no differences by gender.\n\n==\n\nJohnson 2022:\n\nTitle: Is Screen Addiction a Problem Among Rural School-Going Adolescents? A Cross-Sectional Study in South Karnataka\n\nAbstract: Background: Media, including mobile phones, computers, and social media, is a dominant force in the lives of adolescents, and has now penetrated the rural areas. Screen addiction (SA) is a pathological compulsive use of screen-based devices. Objectives: To estimate SA and its associated factors among school-going adolescents residing in a rural area in South Karnataka. Methods: A cross-sectional study was conducted among adolescents (10-19 years) in 4 schools of Solur Hobli, Ramanagara district, using a structured questionnaire including physical activity, family, and academic stressors. SA was assessed using Tao\u2019s Diagnostic Criteria for Internet Addiction. Depression was screened using Physical Health Questionnaire-9. Chi-square test and logistic regression were done for factors associated with SA. Results: Of the 335 adolescents, 20% felt preoccupied with screens, 8% felt restless, moody, and irritable when not using them, 26% used screen devices as a way of escaping problems or relieving irritable mood, 12% had depression, and 3.9% had SA. SA was significantly higher among depressed subjects (adjusted odds ratio [AOR] = 5.18 [1.48-18.13], P = .010), males (AOR = 12.54 [1.46-109.9], P = .021), adolescents who felt that their parents fought frequently (AOR = 4.21 (1.17-15.09), P = 0.027), and adolescents who did not participate in sports (AOR = 3.82 (confidence interval: 1.04-14.06], P = 0.044). Conclusion: While proportion of rural students with SA is still low, we need to prevent this from developing into a public health issue by recognizing SA and depression among students and develop targeted interventions to manage the same, including increasing awareness among teachers and students about SA and its link with depression.\n\n==\n\nChoi 2021:\n\nTitle: Association Between Screen Overuse and Behavioral and Emotional Problems in Elementary School Children\n\nAbstract: Objectives This study identified the association between excessive exposure to screen media and behavioral and emotional problems in elementary school students. Methods A total of 331 parents of children aged 7\u201310 years were recruited from \u201cThe Kids Cohort for Understanding of Internet Addiction Risk Factors in Early Childhood (K-CURE)\u201d study. Children\u2019s demographics, household media ownership, screen time, and behavioral/emotional problems were assessed using a parental questionnaire. Children\u2019s behavior/emotional problems were measured using the Korean version the of Child Behavior Checklist (K-CBCL) score. Results The total K-CBCL score in the screen overuse group was 51.18\u00b19.55, significantly higher than 47.28\u00b110.09 in the control group (t=2.14, p=0.05). For each subscale, the externalization score (51.65\u00b110.14, 48.33\u00b18.97, respectively; t=2.02, p<0.05), social problem score (55.41\u00b16.11, 53.24\u00b15.19, respectively; t=2.27, p<0.05), and rule breaking behavior score (55.71\u00b16.11, 53.24\u00b15.19, respectively; t=2.27, p<0.05) were significantly higher in the screen overuse group than in the control group. In addition, the screen overuse group also had a significantly higher usage rate than the control group, even if limited to smartphones, not only on weekdays (3.56\u00b12.08, 1.87\u00b12.02, respectively; t=-4.597, p<0.001) but also weekends (1.62\u00b10.74, 1.19\u00b10.83, respectively; t=-3.14, p=0.003). Conclusion The study suggested that screen media overuse patterns in children in Korea are particularly relevant to the excessive use of smartphones and are related to higher risks of emotional and behavioral problems.\n\n==\n\nSasmaz 2014:\n\nTitle: Prevalence and risk factors of Internet addiction in high school students.\n\nAbstract: AIM\nIn this study, the prevalence and risk factors of Internet addiction in high school students was investigated.\n\n\nMATERIAL AND METHOD\nThis cross-sectional study was performed in the Mersin Province in 2012. The study sample consisted of students attending high school in the central district of Mersin. The data were summarized by descriptive statistics and compared by a binary logistic regression.\n\n\nRESULTS\nOur study population included 1156 students, among whom 609 (52.7%) were male. The mean age of the students was 16.1 \u00b1 0.9 years. Seventy-nine percent of the students had a computer at home, and 64.0% had a home Internet connection. In this study, 175 (15.1%) students were defined as Internet addicts. Whereas the addiction rate was 9.3% in girls, it was 20.4% in boys (P < 0.001). In this study, Internet addiction was found to have an independent relationship with gender, grade level, having a hobby, duration of daily computer use, depression and negative self-perception.\n\n\nCONCLUSION\nAccording to our study results, the prevalence of Internet addiction was high among high school students. We recommend preventing Internet addiction among adolescents by building a healthy living environment around them, controlling the computer and Internet use, promoting book reading and providing treatment to those with a psychological problem.\n\n==\n\nAhmadi 2014:\n\nTitle: Prevalence of Addiction to the Internet, Computer Games, DVD, and Video and Its Relationship to Anxiety and Depression in a Sample of Iranian High School Students\n\nAbstract: Objective: The objective of this study was to assess the prevalence of addiction to the Internet, computer games, DVD, and video and its relationship to anxiety and depression in a sample of Iranian high school students. Methods: In this cross-sectional study 1020 high school students (males and females) were selected randomly from different areas of Shiraz city in southern Iran. They were interviewed according to the Diagnostic and Statistical Manual of Mental Disorders, 4th ed (DSM-IV) criteria. Results: About 50% of the students were females, 277 students (27.2%) were studying in the first year of high school, 242 (23.7%) were in the second year, and others in the third year. The prevalence of anxiety was significantly higher in females than in males (p < 0.05). The prevalence of anxiety was lower among students of the third year (p < 0.05). The prevalence of depression was significantly higher in students with lower economic status defined as family monthly income. Internet dependence was seen only in 5 students. The prevalence of anxiety was significantly higher in the students who used internet for chatting, amusement, and reading news (p < 0.05). The prevalence of anxiety was significantly higher in students who were DVD or video CD dependents (p < 0.05). The students who used especial drugs or had especial diseases had higher rates of depression and anxiety (p < 0.05). Conclusion: Internet addiction may cause depression and anxiety in high school students. It seems necessary to develop an Internet addiction prevention program for adolescents taking into account the psychological factors such as depression and Internet use habits.\n\n==\n\nMohammadkhani 2017:\n\nTitle: Internet Addiction in High School Students and Its Relationship With the Symptoms of Mental Disorders\n\nAbstract: Received: 08 Nov. 2016 Accepted: 12 Mar. 2017\n\n==\n\nPremkumar 2019:\n\nTitle: Third Eye Syndrome- a gadget screen addiction among medical professionals in Chennai, Tamilnadu, India.\n\nAbstract: Background: Screen time refers to any and all time spent in front of any device with an electronic screen. Today\u2019s youth log a total of 10 hours and 45 minutes per day on screen. Sufficient severity may result in addiction to today\u2019s digital drug. Staring at electronic screen continuously for long time cause multitude of non-communicable diseases. Objective: 1.To deduce the Screen time among people highly proficient with the usage of electronic screen of gadget regularly 2.To investigate the degree of awareness in taking protective measures among people 3.To ascertain the impacts associated with higher screen time empirically. Methods: A cross sectional study was conducted among medical students, interns, doctor and non-medical staff using a strategically devised questionnaire prioritizing on usage and frequency Results: 1.Mean screen time accounts about 50 minutes sans disjunction 2. Night or Reading mode was used by 47% and remaining were not or used sometimes while some were oblivious 3.More than 50% staffs and students have experienced withdrawal symptoms. Inside of, 14% suffered panic, 15.7% sad, 3.9% stressed, 1% suicidal thoughts, 10% don\u2019t get sleep. Nearly half of them check their mobile up to five times in one hour and 10% check mobile more than 30 times. Conclusion: Screen addiction is at higher risk in this high tech era and the health effects are considered merely conjecture.\n\n==\n\nOng 2014:\n\nTitle: Internet addiction in young people.\n\nAbstract: In our technology-savvy population, mental health professionals are seeing an increasing trend of excessive Internet use or Internet addiction. Researchers in China, Taiwan and Korea have done extensive research in the field of Internet addiction. Screening instruments are available to identify the presence of Internet addiction and its extent. Internet addiction is frequently associated with mental illnesses such as anxiety, depression, conduct disorder and attention deficit hyperactivity disorder (ADHD). Treatment modalities include individual and group therapies, cognitive behavioural therapy (CBT), family therapy and psychotropic medications. A significant proportion of Singapore adolescents engaging in excessive Internet use are also diagnosed to have concomitant Internet addiction. Despite the presence of a variety of treatment options, future research in this area is needed to address its growing trend and to minimise its negative psychological and social impact on the individuals and their families.",
        "final_summary": "Johnson 2022 found that screen addiction was significantly higher among depressed subjects, males, adolescents who felt that their parents fought frequently, and adolescents who did not participate in sports. Choi 2021 found that excessive screen exposure was linked to behavioral and emotional problems in elementary school children. Sasmaz 2014 reported a high prevalence of internet addiction among high school students, with gender, grade level, and psychological factors playing a role. Ahmadi 2014 found that addiction to the internet, computer games, DVD, and video was associated with anxiety and depression in Iranian high school students. Ong 2014 highlighted the increasing trend of excessive internet use and its association with mental illnesses. These papers indicate the need for awareness, prevention, and intervention strategies to address screen addiction in middle and high school students."
    },
    {
        "query": "How do teachers' and scientists' understanding of effective teaching, effective science education, and education improvement differ?",
        "paper_list_string": "Taylor 2008:\n\nTitle: Creativity, inquiry, or accountability? Scientists' and teachers' perceptions of science education\n\nAbstract: Although there have been numerous studies that indicate the benefits of teachers and students working with scientists, there is little research that documents scientists' views of science education, science teacher preparation, and the goals of science education. Furthermore, little is known about how scientists' views of science education may differ from those held by science teachers. Through the use of semistructured interviews, the perceptions of 37 scientists from diverse science domains and 21 middle and high school science teachers were explored. Participating scientists expressed concerns about the variability in quality of teaching, programs, and resources available for science instruction. Scientists expressed a desire for teachers to have more experience conducting science research and developing their own critical thinking skills. When asked what goals are most important for science education, 40% of the scientists emphasized that teachers should make science fun and exciting for their students. Science teachers' perceptions of science education were compared with the scientists' perceptions. Thirty percent of the teachers agreed with the scientists that too much variability in program or instructional quality exists in science education. Seventy-six percent of the science teachers also thought there is a need to teach critical thinking skills, but more importantly there is a need to inspire creativity and a desire to learn science in students. Both teachers and scientists expressed concerns about how high-stakes accountability inhibits efforts to improve science education. \u00a9 2008 Wiley Periodicals, Inc. Sci Ed92:1058\u20131075, 2008\n\n==\n\nSchuster 2009:\n\nTitle: Scientists' teaching orientations in the context of teacher professional development\n\nAbstract: During the past decade science educators have taken steps to identify seminal structures and approaches of science teacher professional development. This literature, which is increasingly informing how and in which contexts professional developers design and implement programs, states that scientists' divergent research interests and knowledge about K-12 science teachers often limit their involvement in these programs. Conversely, concerns persist that there is very little empirical evidence to support these programmatic recommendations made by the science education literature. This embedded case study was bounded by the contextual similarities between seven professional development workshops with the goal of examining the relationship between scientists' views of teachers as professionals and the pedagogical orientations that the scientists used within these professional development contexts. Multiple methods were employed including systematic classroom observation by nine trained observers, as well as analysis of course materials, interviews, and questionnaires. Altogether, the teacher comments and observation data paint a picture of how the instructors implemented specific pedagogical orientations. Participants appear to have realistic views about the practical roles that research scientists can play in professional development. We explore how these findings merit attention and suggest alternatives to current practice and policy. \u00a9 2008 Wiley Periodicals, Inc. Sci Ed93:635\u2013655, 2009\n\n==\n\nUcar 2012:\n\nTitle: How Do Pre-Service Science Teachers\u2019 Views on Science, Scientists, and Science Teaching Change Over Time in a Science Teacher Training Program?\n\nAbstract: Every aspect of teaching, including the instructional method, the course content, and the types of assessments, is influenced by teachers\u2019 attitudes and beliefs. Teacher education programs play an important role in the development of beliefs regarding teaching and learning. The purpose of the study was to document pre-service teachers\u2019 views on science, scientists, and science teaching as well as the relations between these views and the offered courses over several years spent in an elementary science teacher training program. The sample consisted of 145 pre-service elementary science teachers who were being trained to teach general science to students in the 6th through 8th grades. The research design was a cross-sectional study. Three different instruments were used to collect the data, namely, the \u201cDraw a Scientist Test\u201d, \u201cDraw a Science Teacher Test\u201d, and \u201cStudents\u2019 Views about Science\u201d tests. The elementary science teacher training program influenced pre-service science teachers\u2019 views about science, scientists and science teaching to different degrees. The most pronounced impact of the program was on views about science teaching. Participants\u2019 impressions of science teaching changed from teacher-centered views to student-centered ones. In contrast, participants\u2019 views about scientists and science did not change much. This result could be interpreted as indicating that science teacher training programs do not change views about science and scientists but do change beliefs regarding teaching science.\n\n==\n\nHouseal 2014:\n\nTitle: Impact of a Student-Teacher-Scientist Partnership on Students' and Teachers' Content Knowledge, Attitudes toward Science, and Pedagogical Practices.\n\nAbstract: Engaging K-12 students in science-based inquiry is at the center of current science education reform efforts. Inquiry can best be taught through experiential, authentic science experiences, such as those provided by Student\u2013Teacher\u2013Scientist Partnerships (STSPs). However, very little is known about the impact of STSPs on teachers' and students' content knowledge growth or changes in their attitudes about science and scientists. This study addressed these two areas by examining an STSP called \u201cStudents, Teachers, and Rangers and Research Scientists\u201d (STaRRS). STaRRS was incorporated into the existing long-standing education program \u201cExpedition: Yellowstone!\u201d For teachers, a pre-test, intervention, post-test research design was used to assess changes and gains in content knowledge, attitudes, and pedagogical practices. A quasi-experimental, pre-test\u2013post-test, comparison group design was used to gauge gains in students' content knowledge and attitudes. Data analyses showed significant positive shifts in teachers' attitudes regarding science and scientists, and shifts in their pedagogical choices. Students showed significant content knowledge gains and increased positive attitudes regarding their perceptions of scientists. The findings indicate that STSPs might serve as a promising context for providing teachers and students with the sort of experiences that enhance their understandings of and about scientific inquiry, and improve their attitudes toward science and scientists. \u00a9 2013 Wiley Periodicals, Inc. J Res Sci Teach 51: 84\u2013115, 2014\n\n==\n\nCosta 2000:\n\nTitle: SCIENCE TEACHERS\u2019 AWARENESS OF FINDINGS FROM EDUCATION RESEARCH\n\nAbstract: In this paper, we report on a stnall-scale study designed to estimate science teachers' \nawareness of findings derived from research in science education and other branches of educational \nresearch. The study was conducted among experienced science teachers in Portugal who were \nfollowing advanced professional training programmes, usually leading to Masters' degrees in science \neducation. The results indicate that science teachers' knowledge of education research findings is \ngenerally very limited. What teachers regard as sound pedagogical knowledge is usually derived from \npersonal experience and 'common sense' and does tend not to be questioned by them as to its \ncompatibility with the results of research. The outcome of the study provides evidence of the \nexistence of a serious gap between research and the practice of science education. In the light of \nthese findings, the authors propose that to narrow this gap should be a major task to be addressed by \nresearchers and practitioners.\n\n==\n\nPilo 2012:\n\nTitle: Science Education and Teachers' Training: Research in Partnership.\n\nAbstract: International researches put to evidence a worrying decrease in science disciplines\u2019 roll in many countries, especially in European Community and a poor quality in scientific competences, as issues of TIMMS (trends in international mathematics and science study) and PISA (programme for international student assessment) have proved, together with a low interest concerning science knowledge of young people. According to our experience, the most important school-related factor in raising student achievement is the quality of the teacher. We investigated Italian pre-service teachers\u2019 scientific competences and their mind conceptions about teaching models/styles, students\u2019 learning, role of teacher and role of science in daily life. The aim of our research outlined in the present paper is to: (1) improve motivation, learning and pupils\u2019 attitudes in science education; (2) develop a critical thinking, stimulate intuition and creativity; and (3) increase scientific literacy in the community. We designed flexible materials, paths and courses within cooperation involving different institutions. The findings of this work concerning both pre-service and in-service teachers\u2019 training are: increased awareness about conceptual knots related to scientific concepts; more sensitiveness and attention to students\u2019 involvement; reflection about the effectiveness of daily school work-increased awareness about meta-cognition and cooperative learning.\n\n==\n\nLederman 1999:\n\nTitle: Teachers' Understanding of the Nature of Science and Classroom Practice: Factors That Facilitate or Impede the Relationship.\n\nAbstract: The purpose of this multiple case study was to investigate the relationship of teachers' understanding of the nature of science and classroom practice and to delineate factors that facilitate or impede a relationship. Five high school biology teachers, ranging in experience from 2 to 15 years, comprised the sample for this investigation. During one full academic year, multiple data sources were collected and included classroom observations, open-ended questionnaires, semistructured and structured interviews, and instructional plans and materials. In addition, students in each of the teachers' classrooms were interviewed with respect to their understanding of the nature of science. Using analytical induction, multiple data sources were analyzed independently and together to triangulate data while constructing teacher profiles. The results indicated that teachers' conceptions of science do not necessarily influence classroom practice. Of critical importance were teachers' level of experience, intentions, and perceptions of students. The results have importnat implications for teacher education as well as the successful implementation of current reforms. \u00a9 1999 John Wiley & Sons, Inc. J Res Sci Teach 36: 916\u2013929, 1999\n\n==\n\nLustick 2011:\n\nTitle: Experienced secondary science teachers' perceptions of effective professional development while pursuing National Board certification\n\nAbstract: Reports such as Rising Above the Gathering Storm and Europe Needs More Scientists exemplify the international community\u2019s concern about the state of public science education. With a lack of scientific literacy for all students and insufficient numbers of individuals choosing to pursue careers in science, the situation is seen as a threat to the sustainability of a robust national economy. Continuing professional development for science teachers is considered a key reform strategy for addressing the problem. As nations invest considerable funds towards improving the quality of science teachers (and consequently student learning) it is reasonable to explore science teacher perceptions regarding the most effective professional learning opportunities. This descriptive study surveyed 118 candidates for National Board certification in Adolescent and Young Adult Science from 42 states about their professional learning experiences. The survey intended to answer the question: \u2018How do candidates perceive the relative effectiveness of different professional learning experiences?\u2019 \u2018Effectiveness\u2019 here is defined as a professional development's ability to help a teacher foster improved student learning. It is important to note that the survey focused on the approaches to professional development rather than the content or goals of said experiences. Candidates' self\u2010reports reveal that developing science curriculum, reading scientific literature, and pursuing National Board certification are the three most productive approaches to professional development while education courses and in\u2010service workshops were identified as least effective. The results suggest that no one model of professional development explains the top rated activities.",
        "final_summary": "The papers suggest that there are differences in the understanding of effective teaching, effective science education, and education improvement between teachers and scientists.\n\nTaylor 2008 found that scientists expressed concerns about the quality of teaching and resources in science education, emphasizing the importance of making science fun and exciting for students. Teachers shared some of these concerns but also emphasized the need to inspire creativity and a desire to learn science.\n\nSchuster 2009 explored the relationship between scientists' views of teachers as professionals and their pedagogical orientations in professional development contexts. The study found that scientists had realistic views about the roles of research scientists in professional development.\n\nUcar 2012 investigated the changes in pre-service science teachers' views on science, scientists, and science teaching over time. The study found that the teacher training program influenced their views on teaching science, shifting from teacher-centered to student-centered approaches, but had less impact on their views of scientists and science.\n\nHouseal 2014 examined the impact of a Student-Teacher-Scientist Partnership (STSP) on teachers' and students' content knowledge, attitudes toward science, and pedagogical practices. The study found positive shifts in teachers' attitudes toward science and scientists, as well as changes in their pedagogical choices. Students showed content knowledge gains and increased positive attitudes toward scientists.\n\nCosta 2000 highlighted the limited awareness of science teachers regarding findings from education research. The study revealed a gap between research and practice in science education, with teachers relying more on personal experience and common sense rather than research findings.\n\nPilo 2012 focused on pre-service and in-service teachers' training in Italy. The research aimed to improve motivation, learning, and attitudes in science education, emphasizing critical thinking, intuition, creativity, and scientific literacy.\n\nLederman 1999 explored the relationship between teachers' understanding of the nature of science and their classroom practice. The study found that teachers' conceptions of science did not necessarily influence their classroom practice, with factors such as experience, intentions, and perceptions of students playing a critical role.\n\nLustick 2011 surveyed candidates for National Board certification in Adolescent and Young Adult Science. The study revealed that developing science curriculum, reading scientific literature, and pursuing National Board certification were perceived as the most effective professional development approaches, while education courses and in-service workshops were seen as least effective."
    },
    {
        "query": "multi-hop information retrieval  with large language models",
        "paper_list_string": "Lyu 2023:\n\nTitle: Improving Retrieval-Augmented Large Language Models via Data Importance Learning\n\nAbstract: Retrieval augmentation enables large language models to take advantage of external knowledge, for example on tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function. We further proposed an even more efficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental results illustrate that we can enhance the performance of large language models by only pruning or reweighting the retrieval corpus, without requiring further training. For some tasks, this even allows a small model (e.g., GPT-JT), augmented with a search engine API, to outperform GPT-3.5 (without retrieval augmentation). Moreover, we show that weights based on multilinear extension can be computed efficiently in practice (e.g., in less than ten minutes for a corpus with 100 million elements).\n\n==\n\nAwasthi 2022:\n\nTitle: Bootstrapping Multilingual Semantic Parsers using Large Language Models\n\nAbstract: Despite cross-lingual generalization demonstrated by pre-trained multilingual models, the translate-train paradigm of transferring English datasets across multiple languages remains to be a key mechanism for training task-specific multilingual models. However, for many low-resource languages, the availability of a reliable translation service entails significant amounts of costly human-annotated translation pairs. Further, translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models. For multilingual semantic parsing, we demonstrate the effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting. Through extensive comparisons on two public datasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show that our method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages. We study the key design choices that enable more effective multilingual data translation via prompted LLMs.\n\n==\n\nZhu 2023:\n\nTitle: Large Language Models for Information Retrieval: A Survey\n\nAbstract: As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions within this expanding field.\n\n==\n\nBellegarda 2000:\n\nTitle: Large vocabulary speech recognition with multispan statistical language models\n\nAbstract: Multispan language modeling refers to the integration of various constraints, both local and global, present in the language. It was recently proposed to capture global constraints through the use of latent semantic analysis, while taking local constraints into account via the usual n-gram approach. This has led to several families of data-driven, multispan language models for large vocabulary speech recognition. Because of the inherent complementarity in the two types of constraints, the multispan performance, as measured by perplexity, has been shown to compare favorably with the corresponding n-gram performance. The objective of this work is to characterize the behavior of such multispan modeling in actual recognition. Major implementation issues are addressed, including search integration and context scope selection. Experiments are conducted on a subset of the Wall Street Journal (WSJ) speaker-independent, 20000-word vocabulary, continuous speech task. Results show that, compared to standard n-gram, the multispan framework can lead to a reduction in average word error rate of over 20%. The paper concludes with a discussion of intrinsic multi-span tradeoffs, such as the influence of training data selection on the resulting performance.\n\n==\n\nZiems 2023:\n\nTitle: Large Language Models are Built-in Autoregressive Search Engines\n\nAbstract: Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.\n\n==\n\nHiemstra 2001:\n\nTitle: Using language models for information retrieval\n\nAbstract: Because of the world wide web, information retrieval systems are now used by millions of untrained users all over the world. The search engines that perform the information retrieval tasks, often retrieve thousands of potentially interesting documents to a query. The documents should be ranked in decreasing order of relevance in order to be useful to the user. This book describes a mathematical model of information retrieval based on the use of statistical language models. The approach uses simple document-based unigram models to compute for each document the probability that it generates the query. This probability is used to rank the documents. The study makes the following research contributions. \n \n * The development of a model that integrates term weighting, relevance feedback and structured queries. \n * The development of a model that supports multiple representations of a request or information need by integrating a statistical translation model. \n * The development of a model that supports multiple representations of a document, for instance by allowing proximity searches or searches for terms from a particular record field (e.g. a search for terms from the title). \n * A mathematical interpretation of stop word removal and stemming. \n * A mathematical interpretation of operators for mandatory terms, wildcards and synonyms. \n * A practical comparison of a language model-based retrieval system with similar systems that are based on well-established models and term weighting algorithms in a controlled experiment. \n * The application of the model to cross-language information retrieval and adaptive information filtering, and the evaluation of two prototype systems in a controlled experiment. \n \nExperimental results on three standard tasks show that the language model-based algorithms work as well as, or better than, today's top-performing retrieval algorithms. The standard tasks investigated are ad-hoc retrieval (when there are no previously retrieved documents to guide the search), retrospective relevance weighting (find the optimum model for a given set of relevant documents), and ad-hoc retrieval using manually formulated Boolean queries. The application to cross-language retrieval and adaptive filtering shows the practical use of respectively structured queries, and relevance feedback.\n\n==\n\nNing 2020:\n\nTitle: Multi-hop Interactive Cross-Modal Retrieval\n\nAbstract: Conventional representation learning based cross-modal retrieval approaches always represent the sentence with a global embedding feature, which easily neglects the local correlations between objects in the image and phrases in the sentence. In this paper, we present a novel Multi-hop Interactive Cross-modal Retrieval Model (MICRM), which interactively exploits the local correlations between images and words. We design a multi-hop interactive module to infer the high-order relevance between the image and the sentence. Experimental results on two benchmark datasets, MS-COCO and Flickr30K, demonstrate that our multi-hop interactive model performs significantly better than several competitive cross-modal retrieval methods.\n\n==\n\nLafferty 2001:\n\nTitle: Document language models, query models, and risk minimization for information retrieval\n\nAbstract: We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval. A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models. The Markov chain method has connections to algorithms from link analysis and social networks. The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio. Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data.",
        "final_summary": "The papers suggest that large language models (LLMs) can be effectively used for multi-hop information retrieval. Lyu 2023 proposes an algorithm to evaluate the data importance of retrieved points, enhancing the performance of LLMs without additional training. Awasthi 2022 demonstrates the effectiveness of LLMs in translating English datasets into multiple languages for multilingual semantic parsing. Zhu 2023 provides a survey on the integration of LLMs and information retrieval systems, exploring various aspects such as query rewriters, retrievers, rerankers, and readers. Bellegarda 2000 discusses the use of multispan statistical language models for large vocabulary speech recognition, showing improved performance compared to n-gram models. Ziems 2023 discovers that LLMs can generate URLs for document retrieval, outperforming existing retrieval approaches in open-domain question answering. Hiemstra 2001 presents a mathematical model of information retrieval based on statistical language models, demonstrating its effectiveness in various retrieval tasks. Ning 2020 introduces a multi-hop interactive model for cross-modal retrieval, leveraging local correlations between images and words. Lafferty 2001 proposes a framework that combines document models and query models using probabilistic ranking functions, achieving significant improvements in retrieval performance."
    },
    {
        "query": "criticism of Rona moss-morris",
        "paper_list_string": "Miller 1977:\n\nTitle: Conversations with Wright Morris : critical views and responses\n\nAbstract: This book is an attempt to approach the work of a leading American novelist from both sides of the looking-glass--from the opposite, but not necessarily opposing, points of view of the writer/creator and the reader/critic. In 1975, while the author was visiting professor of English at the University of Nebraska-Lincoln, several scholar-critics (among them John W. Aldridge, Wayne C. Booth, and David Madden) were invited to speak about his craft and artistic aims and principles and to record conversations with him about issues growing from their addresses. Since Morris is also an important photographer, facets of his achievement in this field were considered by Peter C. Bunnell. In addition to four conversations, three lectures, and a portfolio of twelve photographs, this volume includes an essay by Wright Morris and a bibliography compiled by Robert L. Boyce.\n\n==\n\nDavis 2000:\n\nTitle: Confessions of an Anacoluthon: Avital Ronell on Writing, Technology, Pedagogy, Politics.\n\nAbstract: In the introduction to a special issue of diacritics devoted to the work of Avital Ronell, Jonathan Culler writes that \"her books are like no others\"; that her sentences \"startle, irritate, illuminate\"; and that her work consti tutes \"one of the most remarkable critical oeuvres of our era.\" RonelPs writing is remarkable, in part, because of the unusual connections it makes, its determination to blur the distinctions between big thought and small talk, philosophy and rumor, literature and headline news?to blur, that is, the very divisions through which academia sustains itself. But Ronell's work is also remarkable in its style: her writing is characteristi cally tough, double entendre intended. It's difficult (because of its enormous scope and depth), and it's also gutsy, rough, edgy, and pushy, with a sort of streetwise candor. Indeed, Ronell herself identifies a kind of \"class struggle\" going down in her texts, a struggle involving her own various compulsions, denials, and voices?including the \"little hood lum,\" the \"high philosophical graduate student,\" and the \"more sophisti cated Parisian.\" Ronell notes, however, that the most discernible and continuous voice in her texts belongs to the \"wise-ass girl,\" an ancestor of the \"buffo\" and every bit an anacoluthon out to disrupt the \"smooth logic of accepted meaning or signification.\" This interruptive force? inasmuch as it does indeed \"startle, irritate, illuminate\"?takes a certain swipe at certitude, prompting rigorous hesitations that open the condi tions of possibility for what Ronell's works are always after: an ethics of decision in a postfoundational whirl(d). RonelPs rigorously deconstructive rereadings of everything?from the telephone, the television, and virtual reality to the Gulf War, AIDS, and Madame Bovary?take up that which has been \"marginalized, minoritized, evicted, persecuted, left out of the picture . . .feminized.\" Operating in the mode of \"irreverent reverence\" and in the service of a posthumanist ethical imperative, Ronell sets out to \"secure the space of academe as a sheltering place of unconditional hospitality for dissidence jac 20.2(2000)\n\n==\n\nMacCarthy 1994:\n\nTitle: William Morris: A Life for Our Time\n\nAbstract: Winner of the Wolfson History Prize, the essential biography of the father of the Arts and Crafts movement. The author, Fiona MacCarthy, is the curator of the National Portrait Gallery's 2014-15 exhibition Anarchy and Beauty: William Morris and His Legacy. \"One of the finest biographies ever published in this country\". (A. S. Byatt). Since his death in 1896, William Morris has come to be regarded as one of the giants of the Victorian era. But his genius was so many-sided and so profound that its full extent has rarely been grasped. Many people may find it hard to believe that the greatest English designer of his time, possibly of all time, could also be internationally renowned as a founder of the socialist movement, and could have been ranked as a poet together with Tennyson and Browning. With penetrating insight, Fiona MacCarthy has managed to encompass all the different facets of Morris' complex character, shedding light on his immense creative powers as artist and designer of furniture, fabrics, wallpaper, stained glass, tapestry and books, and as a poet, novelist and translator; his psychology and his emotional life; his frenetic activities as polemicist and reformer; and his remarkable circle of friends, literary, artistic and political, including Dante Gabriel Rossetti and Edward Burne-Jones. Fiona MacCarthy's skilful drawing together of these disparate elements makes for a comprehensive and compelling biography.\n\n==\n\nSlights 2001:\n\nTitle: Rape and the Romanticization of Shakespeare's Miranda\n\nAbstract: In 1981, Jean Elshtain issued a plea that political philosophy recognize female agency as a valid focus of study: \u201cThe feminist political thinker aims to transform her discipline as well as her social world in important ways. This necessitates locating the woman as subject of political and social inquiry, moving away from the abstracted, disembodied \u2018product\u2019 of social forces featured in much contemporary social science. This female subject, as the object of inquiry, must be approached as an active agent of a life-world of intense personalization and immediacy.\u201d 1 Twenty years later, I am taking up Elshtain\u2019s call in a literary context in order to suggest that the history of Tempest criticism stands as powerful proof that political criticism of Shakespearean drama has yet to devise a solid theoretical basis from which to approach female characters as dynamic participants in the fictional worlds of which they are constitutive members. Specifically, this paper seeks to account for, and to challenge, Miranda\u2019s exclusion from critical discourse. By exploring what happens when Miranda is treated merely as an emblem of a colonialist ruling class rather than understood as an active agent in the life-world of the play, my paper participates in a recent dialogue concerned with evaluating the role a rehabilitated notion of character might play in the development of an ethical\u2014and also historically aware\u2014criticism of Shakespearean drama. These days, \u201ccharacter criticism,\u201d an approach initiated in the eighteenth century and popularized in the early twentieth century by A. C. Bradley, is most often considered synonymous\n\n==\n\nSlights 2001:\n\nTitle: Rape and the Romanticization of Shakespeare's Miranda\n\nAbstract: In 1981, Jean Elshtain issued a plea that political philosophy recognize female agency as a valid focus of study: \u201cThe feminist political thinker aims to transform her discipline as well as her social world in important ways. This necessitates locating the woman as subject of political and social inquiry, moving away from the abstracted, disembodied \u2018product\u2019 of social forces featured in much contemporary social science. This female subject, as the object of inquiry, must be approached as an active agent of a life-world of intense personalization and immediacy.\u201d 1 Twenty years later, I am taking up Elshtain\u2019s call in a literary context in order to suggest that the history of Tempest criticism stands as powerful proof that political criticism of Shakespearean drama has yet to devise a solid theoretical basis from which to approach female characters as dynamic participants in the fictional worlds of which they are constitutive members. Specifically, this paper seeks to account for, and to challenge, Miranda\u2019s exclusion from critical discourse. By exploring what happens when Miranda is treated merely as an emblem of a colonialist ruling class rather than understood as an active agent in the life-world of the play, my paper participates in a recent dialogue concerned with evaluating the role a rehabilitated notion of character might play in the development of an ethical\u2014and also historically aware\u2014criticism of Shakespearean drama. These days, \u201ccharacter criticism,\u201d an approach initiated in the eighteenth century and popularized in the early twentieth century by A. C. Bradley, is most often considered synonymous\n\n==\n\nLeggott 2013:\n\nTitle: No Known Cure: The Comedy of Chris Morris\n\nAbstract: Acknowledgments Notes on Contributors Introduction: Why Bother? - James Leggott 1. Beyond Our Ken: Chris and Armando, Janet and John, and The Road To On the Hour - Ian Greaves 2. Rockarama Newsbanana: Chris Morris - Misbehaviour in Music Radio - Justin Lewis 3. Mocking the News: The Day Today and Brass Eye as Mockumentary News Satire - Craig Hight 4. 'Only This Can Make it a News': The Language of News Media - Dan North 5. Reporting the 'illegalisation of cake': Moral Panics and Brass Eye - Jeremy Collins 6. 'Complaints Were Received': Morris, Comedy, and Broadcasting in Context - Brett Mills 7. 'Handling' The Darkness: Chris Morris as Cultural Capital - Sam Friedman 8. 'Bashed Be Biff By All With Ears': Blue Jam and the Radio - Robert Dean and Richard J. Hand 9. Lost in Techno Trance: Dance Culture, Drugs and the Digital Image in Jam - Jamie Sexton 10. I remain, sir, Disgusted of America. 'Morriucci' and 9/11: 'An Absolute Atrocity Special' - David Walton 11. 'Well Futile': Nathan Barley and Post-Ironic Culture - Adam Whybray 12. 'Dad's Army Side to Terrorism': Chris Morris, Four Lions and Jihad Comedy - Sharon Lockyer 13. 'I don't wanna be dead! There's no future in it!': The US and UK critical reception of Four Lions - Russ Hunter 14. The helium of publicity: mass-mediated 'terrierism' - David Rolinson Appendix: List of Morris Works Index\n\n==\n\nAbram 2014:\n\nTitle: Staging the unsayable: debbie tucker green\u2019s political theatre\n\nAbstract: debbie tucker green is one of the most stylistically innovative and politically engaged playwrights at work in Britain today. Her prolific output is widely recognised in discussions of contemporary black British theatre, where she is often named alongside Kwame Kwei-Armah and Roy Williams as the leading playwrights of their generation. Moreover, she has become a figurehead for new British playwriting more broadly, as evinced by her inclusion in Aleks Sierz\u2019 Rewriting the Nation: British Theatre Today (2011). Yet this energetic acclaim was preceded by a period of critical questioning; early reviewers responded indignantly to her subversion of conventional plot structures, highly stylised use of language and assuredly sparse stage design. Many saw these features as a failure to fulfil the demands of the dramatic medium; some hesitated to use the term \u201cplay\u201d to classify her work. One critic, receiving her drama as an aural rather than visual or embodied experience, claimed that she would \u201cfind a happier home on [BBC] Radio 3\u201d (I. Johns). In another particularly piqued response, reviewer Dominic Cavendish parodied the demotic voice in which tucker green\u2019s characters speak: \u201cI don\u2019t do silence, innit\u201d (251). This invocation of silence \u2013 or, rather, its absence \u2013 is intriguing. Cavendish is right that tucker green\u2019s plays are marked by chaotic dialogue: incessant repetition, overlap and interruption. Yet, amidst this proliferation of speech, much remains unsaid. This article examines tucker\n\n==\n\nWilkes 2001:\n\nTitle: Women and Literature in Britain 1800\u20131900: Remaking the canon\n\nAbstract: In January 1870, a respected British weekly, the Athenaeum , discussed several now-forgotten novels, and gave them short shrift. These included Lady Betty , by Christabel Coleridge, and Herbert Tresham , by the Revd J. M. Neale. Christabel Coleridge's novel is criticised mainly for its plot, which the anonymous reviewer finds badly constructed, tedious and implausible, while the Revd Neale's book is faulted both for being \u2018pedantically written\u2019, and for telling an \u2018insignificant\u2019 tale which the reviewer believes is \u2018a mere pretext for thrusting the author's opinions down the reader's throat\u2019. The review identifies no redeeming features in Herbert Tresham , but acknowledges that Lady Betty does contain \u2018some pleasing writing\u2019, and therefore suggests that Christabel Coleridge is \u2018capable of something better\u2019. What is notable, however, is that Lady Betty's faults are ascribed to the writer's sex, while this is not the case with the Revd Neale's novel. The review of Christabel Coleridge's text begins: \u2018It scarcely needed the author's name affixed to the title page of \u201cLady Betty\u201d, to tell us it was the work of a lady\u2019 \u2013 and goes on to call the novel's story \u2018feeble and badly designed\u2019, as if this were the natural corollary of female authorship. By contrast, the pedantry and offensive didacticism of Herbert Tresham are not linked to the Revd Neale's sex, or even to his clerical status. These reviews represent a minor instance of a practice endemic to nineteenth-century literary criticism \u2013 the ascription of particular characteristics to writing on the basis of its author's sex. Although this happened to texts by writers of both sexes, the practice was more common in discussions of women's publications.",
        "final_summary": "The papers collectively do not directly address the research question regarding criticism of Rona Moss-Morris. None of the papers specifically discuss or critique Rona Moss-Morris."
    },
    {
        "query": "Using the recent and updated references, What is the current state of human rights education in Turkey?",
        "paper_list_string": "\u00c7ay\u0131r 2011:\n\nTitle: \u2018No\u2010one respects them anyway\u2019: secondary school students\u2019 perceptions of human rights education in Turkey\n\nAbstract: The incorporation of compulsory courses on human rights into the secondary school curriculum in 1998 has been an important first step in developing respect for human rights and responsibilities among the younger generation in Turkey. Yet, these courses have many shortcomings in terms of materials, pedagogy and teacher attitudes. This paper explores Grades 7 and 8 (ages 13 and 14) students\u2019 experiences in Citizenship and Human Rights Education courses on the basis of qualitative data collected through focus group discussions in Ankara and Istanbul in the 2006\u20132007 academic year. The responses of the students indicate that these courses have had little impact in empowering students or in facilitating them to consider their own or others\u2019 human rights as an integral part of their lives. Rather, the students perceive the national and the global arena as characterized by mass human rights violations against which they feel powerless. The paper draws attention to the importance of a revised human rights education for students along with a global focus and appropriate methodology.\n\n==\n\nPayaslyoglu 1999:\n\nTitle: Awareness of and Support for Human Rights Among Turkish University Students\n\nAbstract: Human rights have a high place on the agenda of the world today. In the eyes of their defenders they are a sine qua non for the peace and the welfare of mankind and for democratic ideals. On the other hand the human rights records of many countries are still very poor and in some cases even scandalous. Thus the promotion and the reinforcement of human rights, their propagation, cultivation, and protection everywhere, depend upon the ceaseless efforts of all those concerned, including both official authorities and civil societies alike. As part of these efforts, studies on the attitudes of strategic social groups such as youth, women, and minorities with regard to human rights may provide some useful information and clues for both theoretical understanding and practical, preventive, and corrective purposes.' In view of this, the present study attempts to uncover certain facts concerning the awareness of and the support for human rights in one such strategic group in Turkey: university students.2\n\n==\n\n\u015een 2018:\n\nTitle: The Rise and Fall of Citizenship and Human Rights Education in Turkey.\n\nAbstract: Purpose: This article shows the effects of competing political forces on citizenship education in Turkey during the period of commitment to European Union (EU) accession (1999-2005). Methodology: It draws on textbooks, archival documents and interviews. Whilst Turkey had a history of civic education to promote a secular national ethos and identity, the post-Cold War democratisation movement encouraged the Turkish government in 1995 to attempt to internationalise civics by adding human rights themes. Findings: This effort occurred at a time when the hegemony of the secular nationalist establishment was challenged by the electoral rise of an Islamist party. Although Citizenship and Human Rights course suited the purposes of the secular nationalist establishment, after the EU recognised Turkey as a candidate in 1999, a new political Islamist government, elected in 2002, chose first to align the course with its ideology and later decided to repeal it. By exploring the evolution of the curriculum in a crucial period in which political power was switching from the ideology of secular nationalism to that of religious (Islamist) nationalism, the present study illustrates ways in which external and internal influences may affect citizenship education. In particular, it contributes to debates over the role of international agencies in curriculum change in citizenship education.\n\n==\n\n\u015een 2021:\n\nTitle: How are human rights presented in Turkey\u2019s textbooks? Development of an escapist model of human rights education (1950-2020)\n\nAbstract: ABSTRACT Drawing on a critical conceptualisation of human rights education (HRE), this study investigates how human rights were presented in a set of textbooks which were used in Turkey from 1950 to 2020. Findings show that the textbooks avoid human rights issues, struggles, campaigns, and activists by sustaining a de-politicised and de-contextualised narrative. They do not even refer to the Holocaust in explaining the historical development of human rights. These findings suggest that HRE gained a curricular status as a matter of window dressing, an \u2018escapist\u2019 model developed as a result. This model is characterised by a deliberate avoidance of anything that may be considered as \u2018political\u2019. This study concludes that placing socio-political issues at the centre of HRE and encouraging learners to critically reflect on discrepancies between claims and realities of human rights can improve the transformative powers of HRE in nationalist contexts with no strong tradition of rights struggle.\n\n==\n\nKaraman\u2010Kepenek\u00e7i 2005:\n\nTitle: Citizenship and Human Rights Education: A Comparison of Textbooks in Turkey and the United States\n\nAbstract: sues of today\u2019s world (Baehr, 1999; Buergental, 1995; Provost, 2002) and are essential to protect and improve human beings to ensure the national and world peace (Reardon, 2002). One of the important ways to secure human rights is known to be human rights education (Branson & Purta, 1982), which in turn is related to citizenship education. It is undoubtedly essential that human beings make use of the rights they own in order to be responsible citizens. However, making use of the rights primarily depends on humans knowing what rights they own. Are human beings aware and/or making use of the rights they have? It would not be so easy to answer this question as \u201cyes.\u201d Then we are faced with two questions: Should human rights be known by all human beings? Should human rights be utilized at the highest possible level? It is an obvious fact that this problem can be surmounted through education in this field through, briefly expressed, \u201ccitizenship and human rights education\u201d (G\u00fclmez, 2001; KaramanKepenekci, 1999a). There are a number of studies (Bajaj, 2004; Charles, 1991; Cogan, 1999; Goehring, Kurtz, & Rosenthal, 2000; Hornberg, 2002; Karaman-Kepenekci, 1999a; Lenhart & Savolainen, 2002; Lohrenscheit, 2002; Patrick, 1999) focused on citizenship and human rights education. Martin (1987) claims that for citizenship and human rights education to be successful, it is crucial that this education be applied effectively. A number of factors are shown to be important for an effective human rights education: democratic school and class climate (Balton, 1992; Bottery, 1999; Branson & Purta, 1982; Drubay, 1986; G\u00fclmez, 2001; Karaman-Kepenekci, 1999b; Pettman, 1984; Richardson, 1979), positive teacher behavior (Campos, 1989; Charles, 1991; Drubay, 1986; Karaman-Kepenekci, 1998; Pettman, 1984; UNESCO, 1969), contribution from other courses taught at school (Drubay, 1986; Karaman-Kepenekci, 2000; UNESCO, 1985; UNESCO, 1969; Vandenburg, 1984), and a separate course for citizenship and human rights education (Hornberg, 2002; Karaman-Kepenekci, 2000).\n\n==\n\nKepenekci 2005:\n\nTitle: A study of effectiveness of human rights education in Turkey\n\nAbstract: The aim of the research is to examine the effectiveness of Civics and Human Rights Education courses taught in primary schools in Turkey. The criteria for the effectiveness of the courses are determined as \u2018content\u2019, \u2018educational activities\u2019, \u2018teaching methods\u2019, \u2018educational materials\u2019, and \u2018evaluation of students\u2019. A total of 71 teachers teaching these courses participated in the study. The opinions of the teachers for the effectiveness of the courses were gathered by 12 open\u2010ended questions. Content analysis method was used to analyse the views of the teachers. To conclude, more than half of the teachers think that the courses are not effective due to having mainly an informative purpose.\n\n==\n\nMerey 2018:\n\nTitle: A Comparison of Human Rights Education in Social Studies Textbooks in Turkey and the United States\n\nAbstract: The aim of this study was to compare the level of allocation of human rights education issues in social studies textbooks in Turkey and the United States. For this aim, six social studies textbooks from both countries were examined. Textbooks were analyzed in terms of their level of \u201chuman rights education issues,\u201d in accordance with Karaman - Kepenekci\u2019s (1999) subcategories. A content analysis method was used to analyze the textbooks. As a result, it was observed that human rights education issues were included more in Turkish social studies textbooks. The study showed that Turkish and United States textbooks contained the \u201crights\u201d subcategory the most. While the intensity scores of the \u201cdemocracy,\u201d \u201cfreedom,\u201d and \u201cjudgment\u201d subcategories were listed the highest in both countries\u2019 textbooks, on the other hand, the intensity scores of the \u201ctolerance\u201d and \u201cpeace\u201d subcategories were placed last in the textbooks of both countries.\n\n==\n\nAslan 2008:\n\nTitle: Human rights education : a comparison of mother tongue textbooks in Turkey and France\n\nAbstract: Textbooks are major instructional tools playing an important role in education at all grades. The purpose of this study is to compare the level of allocation of human rights issues - rights, freedom, democracy, justice, tolerance and peace - in mother tongue textbooks in Turkey and France. For this aim, ten mother tongue textbooks from both countries were examined. Content analysis method was used to analyse the textbooks. As a result, it was observed that human rights issues are included more in Turkish textbooks when compared to French textbooks. It was found that in the textbooks of both countries the 'rights' subcategory is included the most while the 'justice' subcategory is included the least. While the intensity scores of the 'democracy' and 'freedom' subcategories were listed toward the very end in the textbooks of both countries, the 'democracy' subcategory occupies more places in French textbooks whereas the 'freedom' subcategory occupies more places in Turkish textbooks. Besides, the intensity scores for the subcategories of 'rights', 'peace' and 'tolerance' hold the first three places in the mother tongue textbooks of both countries.",
        "final_summary": "Human rights education in Turkey is marked by significant challenges. \u00c7ay\u0131r (2011) found that secondary school students felt disempowered by human rights education courses, which they perceived as having little impact on their understanding of human rights. Payaslyoglu (1999) focused on the awareness and support for human rights among Turkish university students, a strategic social group. \u015een (2018) discussed the influence of political forces on citizenship education in Turkey, showing how the curriculum can be shaped by both external and internal factors. \u015een (2021) found that textbooks used in Turkey from 1950 to 2020 often avoided discussing human rights issues, leading to a narrative that was both de-politicized and de-contextualized. Karaman\u2010Kepenek\u00e7i (2005) compared citizenship and human rights education textbooks in Turkey and the United States, but did not specifically emphasize the importance of democratic school climate and positive teacher behavior. Merey (2018) found that Turkish social studies textbooks included more human rights content than their United States counterparts. Lastly, Aslan (2008) found that Turkish mother tongue textbooks included more human rights content than French textbooks. These papers collectively suggest a need for improvements in human rights education in Turkey, particularly in terms of curriculum content, pedagogy, and student perception."
    },
    {
        "query": "What is the rate of CO2 emission in percentage per year from peatland deforestation and degradation in Indonesia?",
        "paper_list_string": "Ballhorn 2009:\n\nTitle: Derivation of burn scar depths and estimation of carbon emissions with LIDAR in Indonesian peatlands\n\nAbstract: During the 1997/98 El Ni\u00f1o-induced drought peatland fires in Indonesia may have released 13\u201340% of the mean annual global carbon emissions from fossil fuels. One major unknown in current peatland emission estimations is how much peat is combusted by fire. Using a light detection and ranging data set acquired in Central Kalimantan, Borneo, in 2007, one year after the severe peatland fires of 2006, we determined an average burn scar depth of 0.33 \u00b1 0.18 m. Based on this result and the burned area determined from satellite imagery, we estimate that within the 2.79 million hectare study area 49.15 \u00b1 26.81 megatons of carbon were released during the 2006 El Ni\u00f1o episode. This represents 10\u201333% of all carbon emissions from transport for the European Community in the year 2006. These emissions, originating from a comparatively small area (approximately 13% of the Indonesian peatland area), underline the importance of peat fires in the context of green house gas emissions and global warming. In the past decade severe peat fires occurred during El Ni\u00f1o-induced droughts in 1997, 2002, 2004, 2006, and 2009. Currently, this important source of carbon emissions is not included in IPCC carbon accounting or in regional and global carbon emission models. Precise spatial measurements of peat combusted and potential avoided emissions in tropical peat swamp forests will also be required for future emission trading schemes in the framework of Reduced Emissions from Deforestation and Degradation in developing countries.\n\n==\n\nPage 2002:\n\nTitle: The amount of carbon released from peat and forest fires in Indonesia during 1997\n\nAbstract: Tropical peatlands are one of the largest near-surface reserves of terrestrial organic carbon, and hence their stability has important implications for climate change. In their natural state, lowland tropical peatlands support a luxuriant growth of peat swamp forest overlying peat deposits up to 20 metres thick. Persistent environmental change\u2014in particular, drainage and forest clearing\u2014threatens their stability, and makes them susceptible to fire. This was demonstrated by the occurrence of widespread fires throughout the forested peatlands of Indonesia during the 1997 El Ni\u00f1o event. Here, using satellite images of a 2.5 million hectare study area in Central Kalimantan, Borneo, from before and after the 1997 fires, we calculate that 32% (0.79\u2009Mha) of the area had burned, of which peatland accounted for 91.5% (0.73\u2009Mha). Using ground measurements of the burn depth of peat, we estimate that 0.19\u20130.23\u2009gigatonnes (Gt) of carbon were released to the atmosphere through peat combustion, with a further 0.05\u2009Gt released from burning of the overlying vegetation. Extrapolating these estimates to Indonesia as a whole, we estimate that between 0.81 and 2.57\u2009Gt of carbon were released to the atmosphere in 1997 as a result of burning peat and vegetation in Indonesia. This is equivalent to 13\u201340% of the mean annual global carbon emissions from fossil fuels, and contributed greatly to the largest annual increase in atmospheric CO2 concentration detected since records began in 1957 (ref. 1).\n\n==\n\nPark 2019:\n\nTitle: Estimation of Carbon Dioxide Budget From Peatland In Indonesia With Site-Level Validation\n\nAbstract: Peatland is a natural carbon reservoir in terrestrial ecosystem. The ground water table in peatland is a key factor for carbon exchange through soil decomposition or carbon sink. Especially, Indonesia has the biggest peatland area in Asia. This study focused on the carbon dioxide budget between emissions by ecosystem respiration including fire event and the absorption by photosynthesis. As the result, the annual average of net biome ecosystem carbon dioxide exchange during 12 years were reached to 195.03 MtC/yr. Over the three times of CO2 from fire emissions were emitted by ecosystem respiration from whole peatlands in Indonesia.\n\n==\n\nHusnain 2014:\n\nTitle: CO2 emissions from tropical drained peat in Sumatra, Indonesia\n\nAbstract: With the increasing use of tropical peatland for agricultural development, documentation of the rate of carbon dioxide (CO2) emissions is becoming important for national greenhouse gas inventories. The objective of this study was to evaluate soil-surface CO2 fluxes from drained peat under different land-use systems in Riau and Jambi Provinces, Sumatra, Indonesia. Increase of CO2 concentration was tracked in measurement chambers using an Infrared Gas Analyzer (IRGA, LI-COR 820 model). The results showed that CO2 flux under oil palm (Elaeis guineensis) plantations ranged from 34\u2009\u00b1\u200916 and 45\u2009\u00b1\u200925\u00a0Mg CO2 ha\u20131\u00a0year\u20131 in two locations in Jambi province to 66\u2009\u00b1\u200925\u00a0Mg CO2 ha\u20131\u00a0year\u20131 for a site in Riau. For adjacent plots within 3.2\u00a0km in the Kampar Peninsula, Riau, CO2 fluxes from an oil palm plantation, an Acacia plantation, a secondary forest and a rubber plantation were 66\u2009\u00b1\u200925, 59\u2009\u00b1\u200919, 61\u2009\u00b1\u200925, 52\u2009\u00b1\u200917\u00a0Mg\u00a0ha\u20131\u00a0year\u20131, respectively, while on bare land sites it was between 56\u2009\u00b1\u200930 and 67\u2009\u00b1\u200924\u00a0Mg CO2 ha\u20131\u00a0year\u20131, indicating no significant differences among the different land-use systems in the same landscape. Unexplained site variation seems to dominate over land use in influencing CO2 flux. CO2 fluxes varied with time of day (p\u2009<\u20090.001) with the noon flux as the highest, suggesting an overestimate of the mean flux values with the absence of night-time measurements. In general, CO2 flux increased with the depth of water table, suggesting the importance of keeping the peat as wet as possible.\n\n==\n\nJauhiainen 2011:\n\nTitle: Carbon Dioxide emissions from an Acacia plantation on peatland in Sumatra, Indonesia\n\nAbstract: Abstract. Peat surface CO2 emission, groundwater table depth and peat temperature were monitored for two years along transects in an Acacia plantation on thick tropical peat (>4 m) in Sumatra, Indonesia. A total of 2300 emission measurements were taken at 144 locations, over a 2 year period. The autotrophic root respiration component of CO2 emission was separated from heterotrophic emission caused by peat oxidation in three ways: (i) by comparing CO2 emissions within and beyond the tree rooting zone, (ii) by comparing CO2 emissions with and without peat trenching (i.e. cutting any roots remaining in the peat beyond the tree rooting zone), and (iii) by comparing CO2 emissions before and after Acacia tree harvesting. On average, the contribution of autotrophic respiration to daytime CO2 emission was 21% along transects in mature tree stands. At locations 0.5 m from trees this was up to 80% of the total emissions, but it was negligible at locations more than 1.3 m away. This means that CO2 emission measurements well away from trees were free of any autotrophic respiration contribution and thus represent only heterotrophic emissions. We found daytime mean annual CO2 emission from peat oxidation alone of 94 t ha\u22121 y\u22121 at a mean water table depth of 0.8 m, and a minimum emission value of 80 t ha\u22121 y\u22121 after correction for the effect of diurnal temperature fluctuations, which may result in a 14.5% reduction of the daytime emission. There is a positive correlation between mean long-term water table depth and peat oxidation CO2 emission. However, no such relation is found for instantaneous emission/water table depth within transects and it is clear that factors other than water table depth also affect peat oxidation and total CO2 emissions. The increase in the temperature of the surface peat due to plantation establishment may explain over 50% of peat oxidation emissions. Our study sets a standard for greenhouse gas flux studies from tropical peatlands under different forms of agricultural land management. It is the first to purposefully quantify heterotrophic CO2 emissions resulting from tropical peat decomposition by separating these from autotrophic emissions. It also provides the most scientifically- and statistically-rigorous study to date of CO2 emissions resulting from anthropogenic modification of this globally significant carbon rich ecosystem. Our findings indicate that past studies have underestimated emissions from peatland plantations, with important implications for the scale of greenhouse gas emissions arising from land use change, particularly in the light of current, rapid agricultural conversion of peatlands in the Southeast Asian region.\n\n==\n\nArifanti 2021:\n\nTitle: Mangrove deforestation and CO2 emissions in Indonesia\n\nAbstract: Mangroves are unique intertidal ecosystems that are experiencing high rates of deforestation throughout the world. Indonesia that has the highest mangrove cover in the world has lost its mangroves significantly in 1980s with the expansion of aquaculture development due to the increased shrimp price during the Asian economic crisis. Mangrove loss emits a significant amount of greenhouse gas to the atmosphere that contributes to the global warming. This study aims to estimate the extent of mangrove deforestation in Indonesia and the potential greenhouse gas emissions generated from mangrove deforestation in the last decade. We analyzed the land cover maps produced by the Indonesian Ministry of Environment and Forestry from 2009 to 2019 to calculate mangrove deforested areas as activity data. The average emission factor from mangrove deforestation is 136.9 MgCO2e ha-1yr-1, based on a meta-analysis from mangrove studies in Indonesia. Our result shows the total mangrove loss in the period of 2009 \u2013 2019 covers an estimated area of 182,091 ha. The spatial analysis suggests that the main drivers of mangrove deforestation were derived from conversion of mangroves to low vegetated areas, aquaculture and agriculture practices. Mangrove deforestation potentially generates a significant amount of 182.6 MtonCO2e to the atmosphere within 10 years. If mangrove loss continues, a significant amount of greenhouse gases will be accumulated in the atmosphere and negatively affect their unique biodiversity. Halting mangrove deforestation and conserving the remaining mangrove forests is the most cost effective and efficient measure to reduce CO2 emissions and mitigate climate change.\n\n==\n\nTohir 2019:\n\nTitle: Carbon dioxide (CO2) emissions on highland peatland that has been planted with coffee in Humbang Hasundutan Regency, North Sumatra Province\n\nAbstract: Peatlands in Indonesia are estimated at 26 millions ha. These peatlands are spread across Sumatra on of 8.9 million ha areas. Humbang Hasundutan is one of the area that spread peatlands in North Sumatra with an estimated area of around 1,042 hectares spread in Lintong Nihuta District, Pollung District and Dolok Sanggul District. Peat in this area is unique and rare because the general peat is founded in lowlands adjacent to the coast, but peat in Humbang Hasundutan area is a highland (topogenous) peat which lies at altitude 1000-1450 meters above sea level. The purpose of this study was to determined the amount of CO2 emissions on peatlands cultivated with coffee in Dolok Sanggul District, Humbang Hasundutan Regency. This study was conducted in Huta Bagasan Village, Dolok Sanggul District, Humbang Hasundutan Regency. This research method is a survey research of the amount of CO2 emissions on highland peatlands that are cultivated with coffee. CO2 sampling using a closed hood method. The sampling point is determined 75 cm from the plant. The sampling was taken 3 times a day, namely at 07.00 WIB, 12.00 WIB and 17.00 WIB. Based on the research was known that the highest CO2 emissions were found during the day at 8.12 mg/m2/day, followed by the afternoon at 7.67 mg/m2/day, and the lowest in the morning at 4.21 mg/m2/day.\n\n==\n\nPark 2013:\n\nTitle: Estimation of CO2 budget on peatlands in indonesia by using satellite based data\n\nAbstract: The peatlands are known as the carbon sink in natural. However some disturbances such as fire and drainage are occurred in Indonesian peatlands. The declining of ground water table is the most influential reason of carbon emission from peatlands. For detecting CO2 emission from peatlands in Indonesia, ground water table was estimated by satellite based precipitation and land surface temperature. The CO2 emission is represented by NEE(Net Ecosystem CO2 Exchange) which can be calculated with this equation; NEE = ER - GPP; where ER is ecosystem respiration, and GPP is gross primary productivity. As the result, the ecosystem respiration was larger than GPP in this peat forest. The annual average of ER was about twice of GPP in this region. The GPP of MOD17A2 is underestimated from in-situ observed GPP with 37.2%. The CO2 emission through fire and respiration is increased when GWT was declined.",
        "final_summary": "Peatland deforestation and degradation in Indonesia contribute significantly to CO2 emissions. Ballhorn (2009) estimates that peatland fires in 2006 released 10-33% of the European Community's carbon emissions from transport in that year. Page (2002) estimates that burning of peat and vegetation in Indonesia during the 1997 El Ni\u00f1o event released 13-40% of the mean annual global carbon emissions from fossil fuels. Park (2019) calculates that the net biome ecosystem carbon dioxide exchange in Indonesian peatlands is approximately 195.03 MtC/yr. Husnain (2014) measures CO2 fluxes from drained peat under different land-use systems in Sumatra, Indonesia, finding no significant differences among the systems. Jauhiainen (2011) monitors CO2 emissions from an Acacia plantation on peatland in Sumatra, Indonesia. Arifanti (2021) estimates that mangrove deforestation in Indonesia emitted 182.6 MtonCO2e to the atmosphere within 10 years. Tohir (2019) measures CO2 emissions on highland peatlands cultivated with coffee in Humbang Hasundutan Regency, North Sumatra Province. Park (2013) estimates CO2 emissions from Indonesian peatlands using satellite-based data. These papers collectively demonstrate the significant CO2 emissions resulting from peatland deforestation and degradation in Indonesia."
    },
    {
        "query": "Summarize research on impact of family members on racial attitudes during the civil rights movement",
        "paper_list_string": "Branch 1980:\n\nTitle: Racial Attitudes of Black Preschoolers as Related to Parental Civil Rights Activism.\n\nAbstract: Research on black children's racial attitudes, preference and pride essentially began with the work of Clark and Clark (1939, 1947) and Horowitz (1939). Recently, several authors have sug gested that racial attitudes of black children have changed in the decades since these studies (e.g.. Brand, Ruiz, & Padilla, 1974; Hraba & Grant, 1970). Hraba and Grant found greater numbers of black doll choices than Clark and Clark (1947). However, this result could have been due to regional differences or different amounts of inter racial contact, rather than to historical change. Similarly, McAdoo (1976) found more black choices over time in a longitudinal study, with children tested first in 1969 at 4-5 years and in 1974 at 9-10 years, but the greater age of the children at follow-up could explain the change as well as historical change in attitudes. It is difficult to study historical change unconfounded with age without using complicated cohort-sequential designs. However, if one has hypotheses concerning the active agent in historical change, one can, in a cross-sectional design, compare children who vary in exposure to this active agent. In the present case, if activism in the Black Movement is the relevant variable, as is often suggested, chil dren of activists should show higher levels of black choice than chil dren of nonactivists. Studies suggest that there are durable differences between activ ists and nonactivists. Fendrich (1974, 1976) found that black and white student activists remain politically active as they grew older, and are more likely to work in the knowledge and human services industries, receive graduate education and place low value on ex This article is based on an M.S. thesis conducted by the first author under the direction of the second author. A report of the research was presented at the meetings of the Eastern Psychological Association, Philadelphia, 1979. The\n\n==\n\nTownsend 2007:\n\nTitle: Family Influences on Racial Identity Among African American Youth\n\nAbstract: The purpose of this study was to examine the influence of parental efficacy, family coping, and adaptive family functioning on the development of racial identity among African American youth. Fifty-two African American parent-child dyads were participants. Results of a hierarchical regression revealed family adaptability and family cognitive reframing to be significant predictors of racial identity. Findings suggest that intervention programs targeting the racial identity of African American youth should include a parenting/family component that facilitates adaptive family functioning and that helps parents model culturally specific coping strategies for their children.\n\n==\n\nSmith 2006:\n\nTitle: Environmental and Family Associations With Racism1\n\nAbstract: Environmental and family factors related to racism in college students were investigated. Students (N=114) at a southern liberal-arts college filled out an extensive survey about childhood activities and family of origin, and 40 of their mothers completed a similar survey. The Attitudes Toward Blacks (Brigham, 1993) scale measured racism. Several hypotheses were supported. Students who exhibited less racism also reported that they made more diverse friends in school, came from a more diverse hometown, had diverse encounters and friendships at an earlier age, and had more positive foreign travel experiences. In addition, more racist students perceived their mothers as being more prejudiced while they were growing up, and they perceived their fathers as being more prejudiced currently. Finally, students judged their mothers as less racist than their fathers, and also reported that their mothers exhibited less racist behaviors than did their fathers. The importance of racially diverse experiences and friendships early in life are discussed as means to decrease racism.\n\n==\n\nTaylor 1986:\n\nTitle: Receipt of Support from Family among Black Americans: Demographic and Familial Differences.\n\nAbstract: The impact of familial and demographic factors on the receipt of support from family members was examined among a sample of blacks (National Survey of Black Americans). The findings indicated that income, age, family contact, subjective family closeness, and proximity of relatives were determinants of receiving support. In addition, an interaction was revealed between age and the presence of a child in determining support from family members. The findings substantiate the importance of adult children in the informal social support networks of elderly blacks. The use of multivariate analyses extends previous work in this area and suggests that these relationships are more complex than originally thought.\n\n==\n\nAboud 1996:\n\nTitle: Parental and peer influences on children's racial attitudes\u2606\n\nAbstract: Abstract Two studies examined the hypothesis that children acquire racial attitudes from parents and friends. In the first study, the attitudes of third-grade White children, mean age 9 years, were measured with the Preschool Racial Attitude Measure and with the Multi-response Racial Attitude Measure; the children also predicted the attitudes of their parents on the latter. The perceptual differentiation within-race was assessed. Mothers completed the Katz-Hass attitude scale and the Linville differentiation measure. The results showed that, although children predicted parental attitudes to be similar to their own, the actual parent-child correlation was nonsignificant. However, the mother's differentiation was inversely related to her child's bias score on the PRAM. In the second study, children once again predicted friends' attitudes to be similar to their own. However, only one of several indices of attitude yielded a significant subject-friend correlation; also one subject-non-friend correlation was significantly positive. In each case, the correlations were moderate (between .35 and .40). Thus, there is little support for the widespread assumption that children acquire their racial attitudes from parents and friends. However, parents might indirectly influence their child's attitudes through their view of outgroup members as homogeneous or heterogeneous. Explanations for the children's expectations of similarity to parents and friends are offered.\n\n==\n\nMalaney-Brown 2022:\n\nTitle: The Influence of Familial Relationships: Multiracial Students\u2019 Experiences with Racism at a Historically White Institution\n\nAbstract: Multiracial college students\u2019 enrollment has increased significantly over the past decade. This study examined the experiences of multiracial college students at a historically White institution (HWI) in the Northeast\u2014particularly how student experiences within interracial family relationships\u2014prior to college and while enrolled in college have assisted them in navigating instances of racism. In this exploratory qualitative study, students indicated that their family members can provide support in understanding racism. However, it does depend on the type of relationship and support (e.g., strong, weak, or stressed) they receive from specific family members during their pre-college and college experiences. Multiracial students confirmed experiencing multiracial microaggressions and found that they receive the most family support from their siblings. Implications are provided for students, practitioners, and interracial families to empower multiracial students to confront racism while attending a HWI.\n\n==\n\nCastelli 2009:\n\nTitle: The transmission of racial attitudes within the family.\n\nAbstract: Previous literature based on self-report measures has not found a clear relationship between the ethnic attitudes of White parents and those of their children. In particular, no study has evidenced such a relationship in the case of preschool children. In the present study, the authors measured parents' implicit and explicit racial attitudes as well as the racial attitudes of their 3- to 6-year-old children. They found that parents' explicit attitudes were not related to children's responses. In contrast, mothers' implicit attitudes (but not fathers' implicit attitudes) were significant predictors of children's attitudes. Results demonstrate that early racial attitudes might develop within the family.\n\n==\n\nTaylor 2011:\n\nTitle: Community Influences on White Racial Attitudes: What Matters and Why?\n\nAbstract: Tracing the roots of racial attitudes in historical events and individual biographies has been a long-standing goal of race relations scholars. Recent years have seen a new development in racial attitude research: Local community context has entered the spotlight as a potential influence on racial views. The race composition of the locality has been the most common focus; evidence from earlier decades suggests that white Americans are more likely to hold anti-black attitudes if they live in areas where the African-American population is relatively large. However, an influential 2000 article argued that the socioeconomic composition of the white community is a more powerful influence on white attitudes: In low-socioeconomic status (SES) locales, \u201cstress-inducing\u201d deprivations and hardships in whites' own lives purportedly lead them to disparage blacks. The study reported here reassesses this \u201cscapegoating\u201d claim, using data from the 1998 to 2002 General Social Surveys linked to 2000 census information about communities. Across many dimensions of racial attitudes, there is pronounced influence of both local racial proportions and college completion rates among white residents. However, the economic dimension of SES exerts negligible influence on white racial attitudes, suggesting that local processes other than scapegoating must be at work.",
        "final_summary": "Townsend (2007) found that adaptive family functioning and family coping were significant predictors of racial identity development in African American youth. Smith (2006) suggested that diverse childhood experiences and positive foreign travel experiences were associated with lower levels of racism in college students, and students who perceived their parents as more prejudiced exhibited more racism. Aboud (1996) found little support for the assumption that children acquire racial attitudes from parents and friends, but suggested that parents' views of outgroup members as homogeneous or heterogeneous might indirectly influence their child's attitudes. Malaney-Brown (2022) found that multiracial college students received support in understanding racism from family members, particularly siblings. Castelli (2009) found that mothers' implicit racial attitudes were significant predictors of children's attitudes. Taylor (2011) found that both the racial composition of the community and the socioeconomic status of the white residents influenced white racial attitudes, but the economic dimension of SES had negligible influence."
    },
    {
        "query": "disruptive behavior OR disruption OR aggressive behavior OR misbehavior OR distracting behavior OR troublesome behavior OR violance OR classroom misbehavior AND physical education",
        "paper_list_string": "Cothran 2007:\n\nTitle: Students' Reports of Misbehavior in Physical Education\n\nAbstract: Abstract The purpose of this study was to examine students' reports of misbehavior in physical education class. Secondary school participants (N = 2,309) completed a previously validated instrument designed to measure students' reports of 59 student behaviors occurring in classes that might affect class management (e.g., from talking to bringing weapons to class). Three-way repeated measures analysis of variance results suggested that students' reports of misbehaviors occurring in physical education classes differed by gender, grade level, and ethnic background. Chi-square results also suggested there were differences in participants' views about why students misbehave by gender, grade level, and environment. Descriptive findings indicated most student misbehaviors were mild and moderate in nature.\n\n==\n\nMaddeh 2015:\n\nTitle: Study of Students\u2019 Disruptive Behavior in High School Education in Physical Education Classes\n\nAbstract: This article aims at describing and analyzing students\u2019 disruptive behaviors and comparing them in terms of their degree of intensity in the course of physical education lessons performed by student teachers. The delayed video-scopic analysis was performed using the \u201cDisciplinary Incidents Analysis System\u201d. A total of 1900 disruptive behaviors (DB) were recorded including 1379 seen by student teachers and 521 other behaviors not seen. Results testify that the observed physical education lessons show a very high degree of disruptions with a mean reaching 67.8 DB per session, i.e. 1.2 DB per minute. Besides, a high proportion of DB with a low density (43.7%) and a moderate density (46.7%) was recorded in the classes under investigation. These results presupposed that the learning conditions that prevailed during the observed sessions were not optimal to invite us, consequently, to conduct further investigations in order to determine their reasons.\n\n==\n\nKulinna 2006:\n\nTitle: Teachers' Reports of Student Misbehavior in Physical Education\n\nAbstract: Classroom management, and, more specifically, the management of student behavior, is an essential skill for and central concern of physical education teachers. Surprisingly little information is known, however, about the specific nature of student behaviors that might negatively affect the class environment. The purpose of this study was to examine teachers' reports of student behaviors that may create management issues in physical education classes. Over 300 teachers representing a range of grade levels and school contexts completed a survey related to the frequency of potentially negative student behaviors. Many types of misbehaviors were reported, but the most common were mild. Teachers' reports varied by sex, years of teaching experience, and school context.\n\n==\n\nJim\u00e9nez 2016:\n\nTitle: Disruptive behavior among elementary students in physical education\n\nAbstract: The aim of this study was to determine which disruptive behaviors occur most often in physical education (PE) classes, and to identify the existence of a pattern of behavior that leads to this disruptive behavior. With this in mind, we analyzed five PE sessions taken by pupils at different elementary school in the region of Murcia. The total sample size was 96 students aged between 10 and 13. Data was recorded using an observation instrument (a combination of a field format and a categorical system) and was then analyzed using the \u201cHOISAN\u201d software tool, with a sequential analysis and polar coordinates being conducted. The results of the study revealed that disruptive behaviors (52\u00a0%) occur more frequently than non-relevant behaviors (48\u00a0%), the most common of them being disinterested behavior (29\u00a0%), followed by indiscipline (15\u00a0%), with no statistically significant differences being detected in violent behavior. As regards patterns of behavior, disinterested behavior is stimulated by \u201cno eye contact\u201d, \u201cmiddle distance\u201d, \u201cinside the task\u201d, \u201cno use of material\u201d, \u201cgiving orders\u201d and \u201cregistering of activities\u201d, while indiscipline is stimulated by \u201cno eye contact\u201d, \u201cfar distance\u201d, \u201coutside the task\u201d, \u201cuse of material\u201d, \u201cgrouping in pairs\u201d and \u201cpreparation of material\u201d. In conclusion, it can be stated that disruptiveness is far more common in physical education sessions, affects the development of sessions and has a negative impact on student learning. A solution to this problem should therefore be sought immediately in order to ensure quality education.\n\n==\n\nJim\u00e9nez 2016:\n\nTitle: Erratum to: Disruptive behavior among elementary students in physical education\n\nAbstract: [This corrects the article DOI: 10.1186/s40064-016-2764-6.].\n\n==\n\nSupaporn 1998:\n\nTitle: Middle school student perspectives about misbehavior in physical education classes\n\nAbstract: MIDDLE SCHOOL STUDENT PERSPECTIVES ABOUT MISBEHAVIOR IN PHYSICAL EDUCATION CLASSES FEBRUARY 1998 SALEE SUPAPORN B.Ed., SRI NAKHARINWIROT UNIVERSITY M.Ed., SRI NAKHARINWIROT UNIVERSITY M.S., ILLINOIS STATE UNIVERSITY Ed.D., UNIVERSITY OF MASSACHUSETTS AMHERST Directed by: Professor Patt Dodds and Assistant Professor Linda Griffin Misbehavior is a research topic that most researchers investigate from teachers\u2019 points of view. This study, in contrast, was designed to explore misbehavior from the students\u2019 and the teacher\u2019s perspectives, using Doyle\u2019s ecological approach. A male teacher, Mr. Softball, and 14 seventh and eighth graders (ten males and four females) volunteered to participate during a twoweek basketball unit. Data were collected through shadowing the teacher, critical incidents, student and teacher interviews, videotaped class sessions, and audiotaped stimulated recall sessions with the videotapes. Data from all sources were analyzed using constant comparison to identify common themes. Findings indicated that Mr. Softball\u2019s classes included overlapping instructional, managerial, and social task systems (Doyle, 1986; Siedentop, 1991) and his overall program of action could be classified as casual. Misbehavior was situation specific and occurred throughout these overlapping task systems.\n\n==\n\nKrech 2010:\n\nTitle: Development of a short-form version of the Physical Education Classroom Instrument: measuring secondary pupils' disruptive behaviours\n\nAbstract: Background: Effective classroom management is the cornerstone of successful teaching. Behavioural issues affect the classroom climate as well as the time available for learning. Pupil misbehaviours can also contribute to teacher stress and burn out resulting in teachers leaving the profession. It is important for us to understand more about pupil misbehaviours from teachers' and pupils' perspectives in order to help teachers and schools create effective learning environments. Purpose: This study was designed to shorten an existing survey instrument that addresses pupil misbehaviour patterns. The original instrument was created for pupils, teachers and researchers to describe the spectrum of pupil disruptive behaviours in physical education class settings that impact classroom management. The resulting more parsimonious instrument can be used by teachers to gain insight into pupils' perspectives of pupil misbehaviours and may lead to developing appropriate strategies to minimize disruptions to pupil learning. Participants and setting: Participants were 2309 secondary (junior high and high school boys and girls); along with 303 teachers (men and women) from three Midwestern states using secondary data sets. A new sample of 422 junior high and high school pupils also participated from two school districts in the Southwestern US with 55.2% male and 44.8% female participants, and reported ethnic backgrounds of European American/Caucasian (44.1%), African-American (7.8%), Latino/Hispanic (28.0%), multiracial or other (12.6.%), and 7.6% of non-identified ethnic background. Research design: This study had multiple phases including single and multiple group confirmatory factor analyses (CFA) including: (a) CFA with teacher, junior high, high school pupil secondary data sources; (b) CFA with junior high and high school pupil secondary data only; (c) the development of a shorter and friendlier instrument based on logic and factor loadings; and (d) CFAs with the new pupil sample (n\u2009=\u2009422 secondary pupils) to assess the fit of the data to the shortened 20-item model. Data collection: Pupils completed the surveys using a predefined protocol during physical education class time in both the current and previous data collection procedures. Teachers in the previous study completed surveys during their preparation time. Data analysis: CFA conducted with LISREL version 8.8 and internal consistency reliability assessments using Cronbach's alpha. Findings: Results suggested that a more parsimonious 20-item instrument with five subscales (i.e. Aggressive, Low Engagement, Fails to Follow Directions, Harmful, Distracts) was developed for use with secondary pupils (i.e. junior high and high school) only. Internal consistency reliability was also high for secondary pupils. Conclusions: Future study with another sample of teachers is needed to develop a shortened instrument that can be used by teachers.\n\n==\n\nWhite 1990:\n\nTitle: Reducing disruptive behaviors of elementary physical education students with sit and watch.\n\nAbstract: This study reduced the disruptive behaviors of students in two elementary physical education classes: a regular fourth-grade class comprised of 30 students and an alternative education class containing 14 fourth- and fifth-grade boys with severe behavior problems. Using a multiple baseline design, we introduced a modified time-out procedure called \"Sit and Watch.\" The procedure reduced the frequency of disruptive behaviors by 95%. Sit and Watch proved to be socially acceptable to parents, school personnel, and the physical education teacher.",
        "final_summary": "The papers suggest that disruptive behavior in physical education classes is a prevalent issue. Cothran 2007 found that students reported various misbehaviors, with differences based on gender, grade level, and ethnic background. Maddeh 2015 observed a high degree of disruptive behaviors in physical education lessons, with behaviors ranging from low to moderate intensity. Kulinna 2006 highlighted that teachers reported a variety of misbehaviors, with mild behaviors being the most common. Jim\u00e9nez 2016 identified disruptive behaviors such as disinterested behavior and indiscipline, emphasizing their negative impact on student learning. Supaporn 1998 explored misbehavior from both students' and teachers' perspectives, revealing that misbehavior occurred across instructional, managerial, and social task systems. Krech 2010 developed a shortened instrument to assess pupil misbehaviors, identifying subscales such as aggression, low engagement, failure to follow directions, harmful behavior, and distraction. Finally, White 1990 successfully reduced disruptive behaviors in elementary physical education classes using a modified time-out procedure called \"Sit and Watch.\" These findings collectively highlight the need for effective strategies to address disruptive behaviors in physical education settings."
    },
    {
        "query": "football penalties",
        "paper_list_string": "Leela 2009:\n\nTitle: Modelling Football Penalty Kicks\n\nAbstract: In modern football the penalty kick is considered a golden opportunity for the kicker to register a goal. The kicker is\nvirtually unchallenged by any opposing player except the goalkeeper who stands on the goal-line 12 yards away.\nTherefore, the kicker has an overwhelming advantage. Maximising on this advantage is of paramount importance\nsince penalties in many instances, determine the outcome of games. This paper analyses the variables involved in a\npenalty kick and attempts to devise the best method to kick a penalty to ensure a very high success rate. The two\nfundamental components of a penalty shot are the angle at which the shot is kicked and the velocity of the shot. A\nfeasible range of angles is established using right angled triangles and trigonometric ratios. Also, the sides of these\ntriangles are calculated using Pythagoras theorem. Velocities are calculated using simple projectiles motion equations.\nNumerical methods are employed to find the range of velocities for the respective angles. The penalty kicks modelled\nin this thesis are high velocity shots placed in areas of the goal that are difficult for goal-keepers to reach. These\nresults inform coaches about the techniques used to kick a penalty with the required trajectory. Players can practise\nthese techniques to develop mastery. It is also important to mention the educational impact this project can have on\nthe teaching of calculus to undergraduates. Interest is generated with the use of real world examples that appeal to\nstudents who like sports and provides a foundation for research in Applied Mathematics. This can be described as a\nsimple and stimulating introduction to the technique of Mathematical Modelling.\n\n==\n\nDalton 2015:\n\nTitle: An Analysis of Penalty Kicks in Elite Football Post 1997\n\nAbstract: The penalty kick in football is a seemingly simplistic play; however, it has increased in complexity since 1997 when the rules changed allowing goalkeepers to move laterally along their goal line before the ball was kicked. Prior to 1997 goalkeepers were required to remain still until the ball was struck. The objective of this study was to determine the importance of the penalty kick in the modern game of football. A retrospective study of the 2002, 2006 and 2010 World Cup and the 2000, 2004 and 2008 European Championship tournaments was carried out, assessing the importance of the penalty kick in match play and shootouts and the effect of the time of the game on the shooter's success rate. This study demonstrated the conversion rate of penalties was 73% in shootouts and 68% in match play. Significantly more penalties were awarded late in the game: twice as many penalties in the second half than the first and close to four times as many in the fourth quarter vs. the first. Teams awarded penalty kicks during match play won 52%, drew 30% and lost 18% of the time; chances of winning increased to 61% if the penalty was scored, but decreased to 29% if missed. Teams participating in either the World Cup or European Championship final match had roughly a 50% chance of being involved in a penalty shootout during the tournament. Penalty shots and their outcome significantly impact match results in post 1997 football.\n\n==\n\nPlessner 2001:\n\nTitle: Sequential effects in important referee decisions: the case of penalties in soccer.\n\nAbstract: In a study on penalty decisions in soccer, 115 participants made decisions as referees for each of 20 videotaped scenes from an actual match. In three scenes, defenders committed potential fouls in their penalty area. The first two scenes involved the same team and the third scene occurred in the opposite penalty area. Consistent with the assumption that judges\u2019 initial decisions have an impact on later decisions, we found a negative correlation between participants\u2019 successive penalty decisions concerning the same team, and a positive correlation between successive penalty decisions concerning first one and then the opposing team.\n\n==\n\nSchwarz 2011:\n\nTitle: Compensating tendencies in penalty kick decisions of referees in professional football: Evidence from the German Bundesliga 1963\u20132006\n\nAbstract: Abstract Using a large representative database (12,902 matches from the top professional football league in Germany), I show that the number (441) of two-penalty matches is larger than expected by chance, and that among these 441 matches there are considerably more matches in which each team is awarded one penalty than would be expected on the basis of independent penalty kick decisions (odds ratio = 11.2, relative risk = 6.34). Additional analyses based on the score in the match before a penalty is awarded and on the timing of penalties, suggest that awarding a first penalty to one team raises the referee's penalty evidence criterion for the same team, and lowers the corresponding criterion for the other team.\n\n==\n\nKrenn 2017:\n\nTitle: Do Red and Blue Uniforms Matter in Football and Handball Penalties?\n\nAbstract: Past research has revealed ambiguous results on the impact of red uniforms in sports competition. The current study was aimed at analyzing the role of red and blue uniforms in football and handball penalties. Two experiments were conducted using a within subjects design, where participants rated uniform color-manipulated video clips. In the first study, participants (n = 39) watched footage of football players kicking a penalty, whereas in the second study (n = 118) videos of handball penalty takers, handball goalkeepers and football goalkeepers preparing themselves to score/save a penalty were shown. Participants rated player's/goalkeeper's level of confidence and the expected position of the ball crossing the goal line in the first experiment and additionally the probability of scoring the penalty against the goalkeepers in the second experiment. The videos stopped at the point where the ball was leaving the foot and hand respectively. Results did not show any beneficial impact of red uniforms. Rather, football players wearing blue were rated to kick the ball higher. The study contradicts any positive effect of red versus blue uniforms in the context of football and handball penalties, which emphasizes the need of searching for potential moderators of color's impact on human behavior.\n\n==\n\nMisirlisoy 2014:\n\nTitle: Asymmetric Predictability and Cognitive Competition in Football Penalty Shootouts\n\nAbstract: Sports provide powerful demonstrations of cognitive strategies underlying competitive behavior. Penalty shootouts in football (soccer) involve direct competition between elite players and absorb the attention of millions. The penalty shootout between Germany and England in the 1990 World Cup semifinal was viewed by an estimated 46.49% of the UK population. In a penalty shootout, a goalkeeper must defend their goal without teammate assistance while an opposing series of kickers aim to kick the ball past them into the net. As in many sports, the ball during a penalty kick often approaches too quickly for the goalkeeper to react to its direction of motion; instead, the goalkeeper must guess the likely direction of the kick, and dive in anticipation, if they are to have a chance of saving the shot. We examined all 361 kicks from the 37 penalty shootouts that occurred in World Cup and Euro Cup matches over a 36-year period from 1976 to 2012 and show that goalkeepers displayed a clear sequential bias. Following repeated kicks in the same direction, goalkeepers became increasingly likely to dive in the opposite direction on the next kick. Surprisingly, kickers failed to exploit these goalkeeper biases. Our findings highlight the importance of monitoring and predicting sequential behavior in real-world competition. Penalty shootouts pit one goalkeeper against several kickers in rapid succession. Asymmetries in the cognitive capacities of an individual versus a group could produce significant advantages over opponents.\n\n==\n\nKolbinger 2019:\n\nTitle: Misbehavior During Penalty Kicks and Goalkeepers Holding the Ball Too Long as Trivial Offenses in Football\n\nAbstract: Rule violations occur in every sport and the respective book of rules prescribes how match officials need to sanction them. However, there are some rule violations that are nearly never penalized, even if they are perceived by the match officials. A phenomenon that has been neglected in the scientific community so far, for which we want to introduce the term trivial offenses. This research focuses on two potential trivial offenses in football: rule violations regarding the six-seconds rule, the time a goalkeeper is allowed to control the ball with his hands, and rule violations during the performance of penalty kicks. The aim is to provide empirical proof of the existence of those trivial offenses and describe the respective patterns. For this purpose, two observation systems were constructed; one to investigate 45 games from the German Bundesliga with respect to the six-seconds rule and one to study rule violations during 618 penalty kicks from four European football leagues and one cup event. The following variables were collected: Goalkeeper, MatchLocation, Minute (representing the minute of the game), PreviousAction, CurrentScore, Time (representing the time the goalkeeper controlled the ball with his hands), and Penalization for the six-seconds study; Responsibility for infringement, Decision of the referee, and Outcome for the penalty study. Reliability tests showed almost perfect agreement for the data of both samples. On average, goalkeepers control the ball 6.0 s (SD:4.54) with their hands and the six-second rule was violated in 38.4% of the situations, none of which was penalized. This duration was significantly influenced by CurrentScore (p < 0.001), which indicates a tactical abuse of this situation. None of the investigated penalty kicks was conducted without a rule violation either. In most incidents (96.3%) outfield players from both teams as well as the goalkeeper commit offenses. The umpire only judges 2.8% of these incidents correctly, most of them by approving the scored goal. In total, this research proves the existence of trivial offenses in football and shows how methods and tools of performance analysis can serve to investigate and even solve this issue.\n\n==\n\nLeal 2017:\n\nTitle: What Happens on the Field Stays on the Field: Exploring the Link between Football Player Penalties and Criminal Arrests\n\nAbstract: ABSTRACT Research in organizations has tended to focus only on performance in the workplace and until recently has not paid much attention to behavior outside of the workplace. Conversely, the limited research on crime in the National Football League (NFL), the type of organization we study, has focused largely on misbehavior off-the-field. We confluence these lines of research by focusing on both on-duty and off-duty behaviors. We examine the relationship between on-duty behavior, measured through both the number of penalties and total penalty yards accumulated by an NFL player, as well as their off-duty behavior, measured through criminal arrests. Findings show that a higher number of penalties and a higher total number of penalty yards is associated with more total arrests, more nonviolent arrests, but has no effect on violent arrests. These findings hold in the regular but not post-season.",
        "final_summary": "The papers collectively provide insights into football penalties. Leela 2009 focuses on mathematical modeling of penalty kicks to determine the best method for high success rates. Dalton 2015 examines the importance of penalty kicks in match play and shootouts, showing that penalties awarded late in the game are more common and that scoring a penalty increases the chances of winning. Plessner 2001 suggests that referees' initial penalty decisions can influence subsequent decisions. Schwarz 2011 reveals a tendency for referees to award penalties to both teams in a match, indicating a compensating bias. Krenn 2017 challenges the notion that red uniforms have a positive impact on penalty kicks, finding no significant effect. Misirlisoy 2014 highlights the sequential bias of goalkeepers in penalty shootouts, while kickers fail to exploit this bias. Kolbinger 2019 explores trivial offenses in football, including violations of the six-seconds rule and rule violations during penalty kicks. Leal 2017 investigates the link between on-field penalties and off-field criminal arrests in the NFL."
    },
    {
        "query": "history of advertisement quantitative",
        "paper_list_string": "Mishra 2012:\n\nTitle: History of advertisement and its conservation with special reference to Indian film posters of 20th century\n\nAbstract: The advertisement has been a strong resource of publicity which reached its highest peak in this modern perspective. The advertisement loving people keep on inducing to purchase the mental attitude of new dimensional customers. These advertisements have deep rooted with our activities up to an extent that it is impossible for the consumers to purchase any item without advertisement. The reason of this is that the advertisement are directly linked with the inducing the mental sentiments of the consumers.\n\n==\n\nFalk 1997:\n\nTitle: The Genealogy of Advertising\n\nAbstract: Advertisement, in its modern and proper meaning (Leiss et al., 1986), originated with the massive breakthrough around the turn of the century of consumer society and its huge markets for consumer goods in the major European centres and particularly in the United States (Fraser, 1981; Hayes, 1941). The step from announcement to advertisement came with the recognition that making the product known to people formed an integral part of sales; or, to paraphrase Clausewitz, when this was recognised as a continuation of sales by other means. An early formulation of the idea was presented in 1904 by American advertising guru John E. Kennedy, whose simple but ingenious thesis was: \u2018advertising is salesmanship in print\u2019 (Pope, 1983, 238).\n\n==\n\nVadalkar 2021:\n\nTitle: A critical review of international print advertisements: evolutionary analysis, assessment and elucidations, from 1965 to 2020\n\nAbstract: PurposeAmidst the plethora of mass communication methods that technology bestowed business with, print advertisements still remain an effective and widely utilized advertising tool, and retain a diachronically venerable position in international marketing practice. Bar and transcending mere academic fascination or curiosity, this research provides insights into the past, an understanding of the present and an outlook into the future. In this vein, through a methodical and comprehensive critical review of extant literature on print advertisements since 1965, this research aims to identify gaps in extant knowledge, to map its trends and divergences, to trace its paradigm shifts and to ultimately develop agendas for truly significant future research.Design/methodology/approachThis spatial-temporal study reviews 256 methodically selected articles, using VantagePoint software, and adopts a novel methodology through natural language processing (NLP), text mining, auto-correlation maps, and bubble maps to conduct and present a robust analysis and explicit findings.FindingsUsing also the VOSviewer for density and network visualization, the results identify the predominant literature themes and, conversely, the relatively under-researched areas, and provide a more insightful collective interpretation of extant works, while laying the foundation for future research of greater value and significance to academia and industry.Originality/valueThis study transcends the partial and/or limited analyses and perspectives of extant literature to present scholars with the first comprehensive and long term meta-analysis or systematic study of print advertising, with explicit findings of both scholarly and executive worth.\n\n==\n\nFord 2021:\n\nTitle: A decade (2008\u20132019) of advertising research productivity: A bibliometric review\n\nAbstract: Abstract Technological advancements and adoption among consumers have changed the way various agents in the market communicate. Due to the recent growth and interest in the advertising area, the present study investigates the intellectual body of research built over the last decade, by utilizing a bibliometric approach. The analysis consists of two different studies. The first study examines the top three academic journals specifically focused on advertising (JA, JAR and JCIRA) as the top benchmark for advertising research. The second is more comprehensive with multiple sources of advertising scholarship, ranging across a variety of communications, management and marketing journals. The studies taken together provide a holistic and in-depth analysis of authors, institutions, designations and productivity in the advertising area, thus further adding to the intellectual body of knowledge. In addition we compare and contrast the last decade\u2019s growth with the two previous decades of advertising scholarship.\n\n==\n\nRogers 1995:\n\nTitle: The economics of advertising: Where's the data?\n\nAbstract: Economists accept the importance of advertising to firm rivalry and economic performance, but data limitations have frustrated empirical research. This paper addresses that frustration and compares sources of advertising data. The paper concludes that data provided by a private vendor on measured-media consumer advertising represents the best choice, but involves substantial effort to link it to the Census industrial classification system. The authors do this for 284 manufacturing industries for Census years 1967 and 1982. Comparisons of industry advertising levels and advertising-to-sales ratios are given. Relative advertising levels and intensities have remained remarkably stable over the 15 year period.\n\n==\n\nEechambadi 1994:\n\nTitle: Does advertising work\n\nAbstract: Advertising's effectiveness is notoriously difficult to measure -- but accurately valuing its contribution is more important than ever DESPITE THEIR HEIGHTENED ATTENTION to advertising spend, managers still have a hard time getting it precisely right. This should occasion no surprise: the task is extremely -- perhaps even impossibly -- difficult. There are problems everywhere: data are often unreliable, internal accounting and control systems irrelevant, performance criteria inappropriate, budgets unpredictable, time horizons for decisions unclear or unreasonable, and managerial incentives unfocused. These problems do not remove the burden of getting it right. But they do redefine a bit the yardstick by which \"right\" gets measured. Greater accuracy and precision are, of course, always welcome. But the primary merit of any new approach to making decisions about advertising spend is to embed throughout an organization a consistent, shared discipline for systematically thinking through the complex dynamics of value creation and capture. Such consistent discipline is especially important given the relatively fragile state of knowledge about the true economic value of advertising. There are, for example, a couple of facts about advertising that almost everyone knows. For much of the 1980s, ad spending boomed at a double-digit clip. Then, in the 1990s, expenditure began to decline in real terms. Among managers as well as students of advertising, two competing theories have developed to explain these conflicting trends: 1.Advertisers were overspending during the 1980s, or they are underspending today, or both. 2.Either (or both) rate of spending is right -- it is simply that the nature of competition, the overall performance of the economy, and the role of advertising have changed from one decade to the next. Both theories are plausible and popular. Both are wrong. In fact, there is a third explanation: ad spending has become so disconnected from economic reality and from an understanding of how best to evaluate an ad's \"quality\" -- that is, its creativity -- that these numbers do not reflect much other than management's willingness to open its wallet. New approaches to thinking about advertising that are anchored in effective metrics of creativity and in the economic reality of value creation are therefore needed. The one described here focuses on the economic half of this equation. Based on existing market research techniques and information technology, which have eased tremendously the execution of such approaches, it provides managers with a rational, dependable tool for setting advertising budgets and monitoring advertising effectiveness. It also allows them to see how their products deliver value to individual market segments and learn how advertising can communicate this value to the consumer. Most of all, it helps to clarify the relationship between a dollar of media spend and a dollar of economic value. Many companies are adopting bits and pieces of this approach, although to date no large advertisers use it systematically. They tend to direct their approach toward the market as a whole and not toward the multiple segments they want to reach. Even so, they have all found genuine benefit in the disciplined thought process it helps create and institutionalize. A different model In most companies, senior management gives each brand manager his or her ad budget for the year. The budget is generally based on last year's, plus or minus a few percent. The brand manager then works with ad agencies and is \"free\" to spend the budget more or less as he or she pleases -- within, of course, the sharp financial constraints imposed by the need to deliver against tough, year-to-year P&Ls. This new approach works differently. It should be familiar to senior managers because it resembles the standard capital budgeting process that operates outside advertising. \u2026\n\n==\n\nSwayne 1987:\n\nTitle: Comparative advertising in horizontal business publications\n\nAbstract: Abstract Comparative advertising, the practice of comparing two or more named or unnamed products in an advertisement, is well known to industrial marketers. This article focuses on the use of comparative advertising in the three leading business publications: Business Week , Forbes , and Fortune . The authors studied approximately 2900 full-page advertisements from 1970 through 1985. It was found that there was a significant increase in the use of comparative advertising during this period; in fact, the use of comparative advertising nearly tripled from 1970 to 1985. Although the predominant use of comparative advertising involves implied comparisons, there has been a continuing increase in the use of the more aggressive strictly comparative format.\n\n==\n\nStevenson 1984:\n\nTitle: Comparative industrial advertising: The content and frequency\n\nAbstract: Abstract Comparative advertising is the pratice of comparing two or more named or unnamed products in an advertisement. The purpose of this study was to examine the use of comparative advertising in widely circulated industrial publications. The study examined approximately 2100 full-page advertisements in major trade publications for the years 1970, 1975, and 1980. It was found that comparative advertising is not the dominant format for industrial journal advertising. Further, there was a significant decrease in the use of comparative advertising from 1975 to 1980. Nevertheless, when industrial advertisers use comparative advertising, they tend to rely more frequently on implied comparisons rather than on the more aggressive, strictly comparative format. Also, industrial marketers seem to be more inclined to stress product features rather than price, distribution, or promotion when using comparative advertising.",
        "final_summary": "The papers collectively provide insights into the history and impact of quantitative advertising.\n\nFalk (1997) traces the origins of modern advertising to the turn of the century when it became recognized as an integral part of sales. Mishra (2012) highlights the deep-rooted influence of advertisements on consumer behavior, emphasizing their role in inducing mental sentiments. Vadalkar (2021) conducts a comprehensive review of print advertisements since 1965, identifying trends, paradigm shifts, and gaps in knowledge. Ford (2021) employs a bibliometric approach to analyze the intellectual body of advertising research over the past decade, examining authors, institutions, and productivity. Rogers (1995) addresses the challenge of data limitations in studying advertising's economic impact, suggesting private vendor data as the best choice for analysis. Eechambadi (1994) discusses the difficulty of measuring advertising effectiveness and emphasizes the importance of embedding a consistent discipline for decision-making. Swayne (1987) and Stevenson (1984) focus on comparative advertising, with Swayne noting a significant increase in its use in business publications and Stevenson examining its content and frequency in industrial advertising.\n\nIn summary, these papers shed light on the historical development, effectiveness measurement challenges, and various aspects of advertising, including print advertisements, comparative advertising, and research productivity."
    },
    {
        "query": "waterfall methodology; case study of Jordan's construction industry",
        "paper_list_string": "Hatamleh 2021:\n\nTitle: Risk assessment and ranking in the developing countries\u2019 construction industry: the case of Jordan\n\nAbstract: PurposeRisk impedes the success of construction projects in developing countries due to planning in an unpredictable and poorly resourced environment. Hence, the literature suggests that practitioners are not fully aware of how important the risk identification process is. Some of the prior studies identified risks in developing countries without highlighting how they can be beneficial to the practitioners in the industry. Therefore, this study highlights this process and identifies the key risks that affect the Jordanian construction industry.Design/methodology/approachThis study adopted an exploratory sequential mixed approach, two rounds of face-to-face interviews that were conducted in Jordan among 12 experts followed by a questionnaire randomly distributed to 122 practitioners. This study utilized the relative importance index, coefficient of variation, and Mann\u2013Whitney (U) to analyze the data. Also, the factor analysis technique was used to identify and regroup the risk factors to further understand the correlation among the risks.FindingsThe result revealed an agreement among contractors\u2019 and consultants\u2019 responses toward allocating risks. Furthermore, several risks can be traced back to the project communication management process, highlighting a deficiency in the process. Also, four-factor groups were established, the first group includes the risk of defective design, late decisions making by other project participants and poor coordination with the subcontractor. The second group has only the risk of corruption, including bribery at sites. The third group includes stakeholders\u2019 financial instability and inadequate distribution of responsibilities and risks. The fourth group includes adverse weather conditions and the use of illegal foreign labor.Originality/valueSome of the prior studies identified risks in developing countries without highlighting how they can be beneficial to the practitioners in the industry.\n\n==\n\nAbuezhayeh 2021:\n\nTitle: Integration between knowledge management and business process management and its impact on the decision making process in the construction sector: a case study of Jordan\n\nAbstract: \nPurpose\nThe purpose of this paper is to investigate and explain how organizations in the construction sector can enhance their decision-making process (DMP) by practising knowledge management (KM) and business process management (BPM) activities. A conceptual framework is developed that recognises the elements that impact DMP in terms of KM and BPM. The development of this framework goes beyond current empirical work on KM in addition to BPM as it investigates a wider variety of variables that impact DMP.\n\n\nDesign/methodology/approach\nA case study is undertaken in the context of the construction industry in Jordan. A theoretical framework is developed and assessment of the proposed framework was undertaken through a questionnaire survey of decision-makers in the construction sector and expert interviews.\n\n\nFindings\nThe outcomes of this research provide several contributions to aid decision-makers in construction organizations. Growth in the usage of KM and BPM, in addition to the integration between them, can provide employees with task-related knowledge in the organization\u2019s operative business processes, improve process performance, promote core competence and maximise and optimise business performance.\n\n\nOriginality/value\nThrough the production of a framework, this study provides a tool to enable improved decision-making. The framework generates a strong operational as well as theoretical approach to the organizational utilization of knowledge and business processes.\n\n\n==\n\nAlkilani 2012:\n\nTitle: READYING A DEVELOPING ECONOMY FOR NATIONAL PERFORMANCE MEASUREMENT AND BENCHMARKING: A CASE STUDY OF THE JORDANIAN CONSTRUCTION INDUSTRY\n\nAbstract: In many developing countries, national construction performance measures and benchmarking processes are yet to be formulated, implemented and tracked. This inherent weakness of an economic sector has negative impacts on, productivity, efficiency and performance. This paper takes the Jordanian construction industry as a case study of a developing economy and highlights the significant challenges it faces in implementing performance measurement. Findings from this study reveal a number of specific and general characteristics, and the extent and severity of industry-based barriers. The paper identifies a range of requirements at the national level that must be met to move from a state of relative disarray to an industry ready for the successful implementation of performance measurement and benchmarking. The authors anticipate that the findings of this paper will be of interest to academics and professionals involved in other developing countries\u2019 construction industries.\n\n==\n\nAlkilani 2013:\n\nTitle: Paving the Road for Sustainable Construction in Developing Countries: A Study of the Jordanian Construction Industry\n\nAbstract: There is an increasing pressure on governments of developing economies to support sustainable construction procurement. In exploring this issue, this research asks three related questions: (1) how is sustainable procurement perceived in developing economies, (2) how can government regulation influence its application, and (3) what is the role of performance measurement in stimulating sustainable procurement practices? Answers are sought via an investigation exploring the underlying issues and challenges facing the construction industry of a developing country looking to address sustainable procurement. Using primary and secondary data sources, this research presents a case study of the Jordanian construction industry. Findings show that whilst sustainable procurement practices are promoted, it is still in its infancy - in part due to ineffective procurement frameworks and a lack of performance measurement. Current government regulations and policies are identified as an underlying cause, discouraging the development and adoption of sustainable procurement methods. As the Jordanian construction industry shares characteristics with other developing economies, it is expected that the findings of this paper will be of interest to professionals in those construction industries attempting to initiate sustainable procurement via performance measurement.\n\n==\n\nHiyassat 2016:\n\nTitle: Factors affecting construction labour productivity: a case study of Jordan\n\nAbstract: Improving productivity in construction is a way of increasing profit with little or no increase in cost. Labour productivity in construction in developing countries, including Jordan, is relatively low and needs to be improved significantly. The objective of this paper is to describe and analyse the factors that affect construction labour productivity. To achieve this goal, a questionnaire survey containing 27 questions (variables) was conducted. It was sent to 200 engineers and foremen who work for contractors. The 90 returned responses were statistically analysed by calculating the average, standard deviation, and RII of each variable. The results of a Spearman correlation\u2012rank\u2012coefficient test showed that no significant differences existed between the responses of the engineers and foremen. It was concluded that the top three ranked dimensions were \u2018Productivity increases as experience increases\u2019, \u2018Financial incentives increase productivity\u2019, and \u2018Trust and communications between management and workers increase productivity\u2019. Furthermore, the data was analysed using the principal components method of factor analysis. Of the 11 extracted factors, only five were considered, while the others were dropped since the observed variables had low loadings on these factors. Contractors and officials can benefit from this study in understanding factors affecting productivity, and act upon that understanding.\n\n==\n\nSweis 2008:\n\nTitle: Delays in construction projects: The case of Jordan\n\nAbstract: Abstract The construction industry is a major player in the economy, generating both, employment and wealth. However many projects experience extensive delays and thereby exceed initial time and cost estimates. A host of causes of construction delays in residential projects were identified and classified according to Drewin\u2019s Open Conversion System. The most common causes were evaluated by using both, the data collected in a survey conducted to residential projects consultant engineers, contractors, and owners, and interviews with senior professionals in the field. Most correspondents agreed that, financial difficulties faced by the contractor and too many change orders by the owner are the leading causes of construction delay. Severe weather conditions and changes in government regulations and laws ranked among the least important causes.\n\n==\n\nBalkhy 2021:\n\nTitle: Barriers to Adopting Lean Construction in the Construction Industry\u2014The Case of Jordan\n\nAbstract: During the last few decades, there has been a greater understanding of the role of lean construction (LC) in achieving better management of construction projects and enabling the application of the fourth industrial revolution in the construction industry (Construction 4.0), which in turn, has the potential to improve the practices and outcomes in the construction industry. This paper aims to identify the barriers to adopt LC practices by taking a case of the Kingdom of Jordan. A questionnaire that included 30 barriers that were resulted from a comprehensive literature review was distributed and filled by 326 respondents from the Jordanian construction industry. The findings showed that the absence of support from the top management, low awareness toward LC, lack of training, and the absence of transparency are amongst the most serious factors that hinder the adoption of LC. The findings in this paper might be helpful to improve the knowledge about LC and support the adoption of new techniques that might improve the performance in the construction industry.\n\n==\n\nEl-Mashaleh 2007:\n\nTitle: Benchmarking information technology utilization in the construction industry in Jordan\n\nAbstract: This paper reports the findings of conducting a modified version of the IT barometer survey in the construction industry in Jordan. The study collects data from 207 firms. Thirty-four percent of these firms are general contractors, 37% are architecture and engineering firms, 19% are consulting and architecture and engineering firms, and 10% are consulting firms. The participating firms are involved in the construction of buildings, roads and bridges, water and sewage projects, and electromechanical projects. The size of firms in terms of number of employees varies widely with some firms have only one employee, while other firms have over 150 employees. The study shows that there is a 0.46 desktop computer and a 0.024 laptop computer per employee. Among the top software utilized by the industry are Word, Excel, web browsers, and e-mail. AutoCad dominates the production of engineering drawings with most firms utilize the software almost 100% of the time. Eighty-two percent of firms have increased their IT investment in the last two years. Eighty-five percent are committing more dollars to invest in IT in the next two years. According to 92% of respondents, demands from customers are a very important motivator for new IT investment. The perceived benefits for IT adoption according to the respondents are better quality of work, work done more quickly, better financial control, better communications, faster and simpler access to common data, greater felxibility to satisfy customers, possibility of sharing common information, easier to use lots of data, and possibility of telecommuting. The main obstacles for IT use are investment costs too high and greater know-how required from staff. In conclusion, this paper benchmarks the current IT usage, availability, and perceived impact in the construction industry in Jordan. Such benchmarking is of prime importance to both construction education and practice to understand current trends, forecast future directions, and conduct international comparisons.",
        "final_summary": "Hatamleh (2021) identifies key risks in the Jordanian construction industry, with several risks traced back to the project communication management process. Abuezhayeh (2021) underscores the potential of integrating knowledge management and business process management for improved decision-making and business outcomes in the construction sector. Alkilani (2012) highlights the challenges and national-level requirements necessary for implementing performance measurement and benchmarking in the Jordanian construction industry. Alkilani (2013) discusses the barriers to sustainable construction procurement, emphasizing the role of performance measurement in promoting sustainable practices. Hiyassat (2016) investigates factors affecting construction labor productivity, emphasizing the importance of experience, financial incentives, and trust and communication between management and workers. Sweis (2008) identifies causes of construction delays, including financial difficulties faced by contractors and excessive change orders by owners. Balkhy (2021) explores barriers to adopting lean construction practices, highlighting the lack of top management support and low awareness. El-Mashaleh (2007) benchmarks the utilization of information technology in the construction industry, revealing the prevalence of desktop and laptop computers and the perceived benefits of IT adoption."
    },
    {
        "query": "This is in line with the UK Corporate Governance Code, which recommends that at least one-third of board members should be women.",
        "paper_list_string": "Singh 2001:\n\nTitle: Women Directors on Top UK Boards\n\nAbstract: Research suggests that more diversity in board membership could improve overall performance. This paper considers the business case for increased numbers of female directors, and the lack of female representation on UK FTSE 100 company boards in 1999 and 2000. It also offers a comparison to US data. In 1999, almost two-thirds of FTSE 100 companies had at least one female director, but numbers had dropped by July 2000 from 64 per cent to 58 per cent, paralleling the levelling-off at top level reported in North America. More firms having female directors are to be found amongst those with the highest turnover, profit and number of employees in the FTSE 100, again paralleling the findings from the US.\n\n==\n\nWearing 2004:\n\nTitle: Between glass ceilings: Female non-executive directors in UK quoted companies\n\nAbstract: The Higgs Review and the Tyson Report have drawn attention to board independence, the lack of female directors and the potential positive contribution of female participation to UK corporate governance. This paper examines the situation of female directors in the top 350 UK quoted companies in the context of recent debates about occupational gender differences and corporate governance. In particular, female non-executive directors account for just over 6 per cent of the non-executive directors and receive about two-thirds the remuneration of their male counterparts. Although female participation increases with firm size, the gender wage gap widens. More importantly, it was found that female nonexecutive directors are at a disadvantage in gaining promotions to positions such as chair of important board sub-committees (ie, the audit committee, the nomination committee and the remuneration committee) and as chairman of the board itself. Given that, in most companies, non-executive remuneration is related to the holding of such posts, this appears to be an important factor in explaining differences in non-executive remuneration. Exclusion of females from these powerful positions also suggests that women face a \u2018second glass ceiling\u2019 even after reaching board level. However, the latest version of the Combined Code on Corporate Governance now provides a valuable opportunity for UK company boards to address the issue of gender imbalance and independence on boards of directors.\n\n==\n\nHolton 1995:\n\nTitle: Corporate Governance Report Surveying the Situation for Women Directors in the UK\n\nAbstract: A survey \u2018Women on the Board of Britain's Top 200 Companies\u2019 published by Ashridge Management Research Group, 1993.\n\n==\n\nVilliers 2010:\n\nTitle: Achieving gender balance in the boardroom: is it time for legislative action in the UK?\n\nAbstract: In the UK and across the globe, women struggle to get a place on the boards of large public companies and still take home less pay than their male counterparts. At a time of financial crisis and corporate governance failures, this lack of equality is considered especially problematic because the talents of half the workforce are not being utilised fully. This paper explores the possibility of introducing legislative gender quotas for company boardrooms in the UK. Such laws have been passed in Norway and Spain with dramatic results. Other countries, such as France and the Netherlands, look set to follow the examples of Norway and Spain. Has that time arrived in the UK?\n\n==\n\nBrammer 2007:\n\nTitle: Gender and Ethnic Diversity Among UK Corporate Boards\n\nAbstract: This paper investigates the ethnic and gender diversity of the corporate board of UK companies, placing particular emphasis on links to board size and industry characteristics. We employ a novel dataset that covers a large sample of UK PLCs and describes a director\u2019s gender, ethnicity and position held. We find both ethnic and gender diversity to be very limited, and that diversity is somewhat less pronounced among executive positions. We find significant cross\u2010sector variation in gender diversity, with an above average prevalence of women in Retail, Utilities, Media and Banking, while such variation in ethnic diversity is considerably less pronounced. Our evidence suggests that a close proximity to final consumers plays a more significant role in shaping board diversity than does the female presence among the industry\u2019s workforce. We argue that this shows that board diversity is influenced by a firm\u2019s external business environment and particularly an imperative to reflect corresponding diversity among its customers.\n\n==\n\nGregory\ufffd?Smith 2014:\n\nTitle: Appointments, Pay and Performance in UK Boardrooms by Gender\n\nAbstract: This article uses UK data to examine issues regarding the scarcity of women in boardroom positions. The article examines appointments, pay and any associated productivity effects deriving from increased diversity. Evidence of gender-bias in the appointment of women as non-executive directors is found together with mixed evidence of discrimination in wages or fees paid. However, the article finds no support for the argument that gender diverse boards enhance corporate performance. Proposals in favour of greater board diversity may be best structured around the moral value of diversity, rather than with reference to an expectation of improved company performance.\n\n==\n\nMartin 2008:\n\nTitle: Boards of directors and gender diversity in UK companies\n\nAbstract: Purpose \u2013 This paper is an exploratory quantitative study aimed at providing the first overview of the incidence of female directors in UK companies, mapped against types of firms. It provides a unique quantitative perspective on the types of companies with boards on which female directors serve.Design/methodology/approach \u2013 A quantitative analysis of a newly constructed database based on data for all UK companies (using Companies House Financial Analysis Made Easy data) was carried out to explore overall data for board membership related to gender, resulting in a new typology to describe firms with female directors.Findings \u2013 The data supports earlier partial studies suggesting male dominance continues at senior levels. Although female directors represented one in four directors in UK firms, most companies remain male dominated. Women directors are generally found in smaller firms and only one in 226 of larger firms have a majority of female directors. The service sector remains the main focus for female...\n\n==\n\nFrancoeur 2008:\n\nTitle: Gender Diversity in Corporate Governance and Top Management\n\nAbstract: This article examines whether and how the participation of women in the firm\u2019s board of directors and senior management enhances financial performance. We use the Fama and French (1992, 1993) valuation framework to take the level of risk into consideration, when comparing firm performances, whereas previous studies used either raw stock returns or accounting ratios. Our results indicate that firms operating in complex environments do generate positive and significant abnormal returns when they have a high proportion of women officers. Although the participation of women as directors does not seem to make a difference in this regard, firms with a high proportion of women in both their management and governance systems generate enough value to keep up with normal stock-market returns. These findings tend to support the policies currently being discussed or implemented in some countries and organizations to foster the advancement of women in business.",
        "final_summary": "The papers collectively indicate a gender imbalance on UK corporate boards. Singh 2001 observed a decrease in female representation on FTSE 100 company boards from 64% to 58% between 1999 and 2000. Wearing 2004 underscored the underrepresentation of female non-executive directors and their limited opportunities for promotion to influential positions. Villiers 2010 explored the potential of implementing legislative gender quotas in UK boardrooms, referencing the positive outcomes of such laws in Norway and Spain. Brammer 2007 reported limited ethnic and gender diversity on UK corporate boards, with diversity varying across sectors. Gregory-Smith 2014 identified gender bias in the appointment of women as non-executive directors but found no correlation between gender-diverse boards and enhanced corporate performance. Lastly, Martin 2008 found that female directors are more common in smaller firms."
    },
    {
        "query": "temporal manipulation and pacing in thriller novels",
        "paper_list_string": "Hepburn 2023:\n\nTitle: \"To Come into the Story as Late as Possible, and To Tell It as Fast as You Can\": Pace in Tinker, Tailor, Soldier, Spy\n\nAbstract: Abstract:Why do some novels have a fast pace and others slow? Pace in narrative gauges the movement of a story in a specific direction with respect to time. Dialogue, descriptive summaries, chapter breaks, and other formal elements speed up or slow down the perceived tempo of a narrative. In Tinker, Tailor, Soldier, Spy, John le Carr\u00e9 creates pace by exploiting the discrepancies between fabula and syuzhet. By his own admission, he prefers to begin a story as late as possible to induce pressure between the content of a story and the manner of its telling, a pressure that sets and regulates narrative pace.\n\n==\n\nBentley 2021:\n\nTitle: Narrative Manipulations of Time Travel: Character Knowledge and Temporal Disruption in Fiction and Film\n\nAbstract: This thesis explores the way time travel narratives experiment with story structure to convey the estrangement of travelling in time. It considers the relationship of character and audience knowledge and its role in comprehension in a range of film, television, and literary texts across a variety of genres. It concludes that time travel narratives function as a form of covert metafiction, in their blending of history and fiction, their use of popular culture and intertextual references, and the way a time travelling character\u2019s awareness of the order of events leads an audience to question the structure of the narrative itself.\n\n==\n\nNaz 2021:\n\nTitle: Temporal Structure in the Novel \u201cThe Killer of Hamza (Qaatil Hamza)\u201d by Najeeb Gillani\n\nAbstract: Temporal Structure is an essential element of the narrative works as well as a component of narratives. It has become a prominent issue in literary criticism in 20th century. Because time structure plays an enormously important role in the construction of story/novel, the other elements of the novel/story cannot be viewed without temporal space. It is temporal space/structure on which the events of the story are based and rhythm, flow and continuity of the story can be maintained by temporal space. Events cannot exist out of the temporal structure of the story. \nThis study deals with the temporal structure and its role in the novel \u201cKiller of Hamza\u201d by Najeeb Gillani and attempts to reveal the method of constructing time in the novel by standing on temporal order and temporal paradoxes both types of retrieval and anticipation, as well as the structure of the rhythm of time such as the technique of speeding narration with its movements (The dialogue scene, the descriptive position) and the frequency of the three types (solitary, repetitive, author).\u00a0 In addition, the study concludes the discussion based on the findings.\n\n==\n\nAbdulrraziq 2021:\n\nTitle: Narratological Analysis of Temporality in Novel\n\nAbstract: This paper definitely attempts to intermingle them with regard to structural analysis. It adopted the theory of Gerard Genette,\u00a0 the analysis of the famous novel\u00a0 Great Expectations\u00a0 by Charles Dickens, the main discussion concerns order and duration whereas frequency is ruled out. Consequently, order can be presented by two different narrative techniques; through the profound analysis of analepsis and prolepsis to show the chronological and anachronological order of the novel. A further analysis of the novel also includes the alternation of the four narrative movements of duration which are divided into techniques of deceleration; descriptive pause and scene whereas techniques of accelerations; ellipsis and summary. The diagrams and tables of the study aim to reach semi-statistical deduction for the chronological techniques of anachronism and its effect on the structure of the narrative time of the novel as whole, which gives a general idea about the system of the time in novel.\n\n==\n\nAbdulrraziq 2021:\n\nTitle: Narratological Analysis of Temporality in Novel\n\nAbstract: This paper definitely attempts to intermingle them with regard to structural analysis. It adopted the theory of Gerard Genette,\u00a0 the analysis of the famous novel\u00a0 Great Expectations\u00a0 by Charles Dickens, the main discussion concerns order and duration whereas frequency is ruled out. Consequently, order can be presented by two different narrative techniques; through the profound analysis of analepsis and prolepsis to show the chronological and anachronological order of the novel. A further analysis of the novel also includes the alternation of the four narrative movements of duration which are divided into techniques of deceleration; descriptive pause and scene whereas techniques of accelerations; ellipsis and summary. The diagrams and tables of the study aim to reach semi-statistical deduction for the chronological techniques of anachronism and its effect on the structure of the narrative time of the novel as whole, which gives a general idea about the system of the time in novel.\n\n==\n\nJones 2010:\n\nTitle: Time is wasting: con/sequence and s/pace in the Saw series\n\nAbstract: Horror film sequels have not received as much serious critical attention as they deserve this is especially true of the Saw franchise, which has suffered a general dismissal under the derogatory banner Torture Porn. In this article I use detailed textual analysis of the Saw series to expound how film sequels employ and complicate expected temporal and spatial relations in particular, I investigate how the Saw sequels tie space and time into their narrative, methodological and moral sensibilities. Far from being a gimmick or a means of ensuring loyalty to the franchise (one has to be familiar with the events of previous episodes to ascertain what is happening), it is my contention that the Saw cycle directly requests that we examine the nature of space and time, in terms of both cinematic technique and our lived, off-screen temporal/spatial orientations.\n\n==\n\nVorderer 2009:\n\nTitle: Suspense : Conceptualizations, Theoretical Analyses, and Empirical Explorations\n\nAbstract: Contents: Preface. H.J. Wulff, Suspense and the Influence of Cataphora on Viewers' Expectations. G. Leonard, Keeping Our Selves in Suspense: The Imagined Gaze and Fictional Constructions of the Self in Alfred Hitchcock and Edgar Allan Poe. L. Mikos, The Experience of Suspense: Between Fear and Pleasure. P. Wuss, Narrative Tension in Antonioni. N. Carroll, The Paradox of Suspense. R.J. Gerrig, The Resiliency of Suspense. W.F. Brewer, The Nature of Narrative Suspense and the Problem of Rereading. P. Ohler, G. Nieding, Cognitive Modeling of Suspense-Inducing Structures in Narrative Films. E. Tan, G. Diteweg, Suspense, Predictive Inference, and Emotion in Film Viewing. G.C. Cupchik, Suspense and Disorientation: Two Poles of Emotionally Charged Literary Uncertainty. D. Zillmann, The Psychology of Suspense in Dramatic Exposition. P. Vorderer, Toward a Psychological Theory of Suspense. M. de Wied, D. Zillmann, The Utility of Various Research Approaches in the Empirical Exploration of Suspenseful Drama. A. Mattenklott, On the Methodology of Empirical Research on Suspense. G. Nieding, P. Ohler, C. Thussbas, The Cognitive Development of Temporal Structures: How Do Children Make Inferences with Temporal Ellipses in Films? M. Friedrichsen, Problems of Measuring Suspense.\n\n==\n\nKim 2020:\n\nTitle: What Time Is It? Temporal Analysis of Novels\n\nAbstract: Recognizing the flow of time in a story is a crucial aspect of understanding it. Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in the absence of explicit time-descriptive phrases. To do so, we construct a data set of hourly time phrases from 52,183 fictional books. We then construct a time-of-day classification model that achieves an average error of 2.27 hours. Furthermore, we show that by analyzing a book in whole using dynamic programming of breakpoints, we can roughly partition a book into segments that each correspond to a particular time-of-day. This approach improves upon baselines by over two hours. Finally, we apply our model to a corpus of literature categorized by different periods in history, to show interesting trends of hourly activity throughout the past. Among several observations we find that the fraction of events taking place past 10 P.M jumps past 1880 - coincident with the advent of the electric light bulb and city lights.",
        "final_summary": "The papers suggest that temporal manipulation and pacing play significant roles in thriller novels. Hepburn 2023 explores how John le Carr\u00e9 creates pace in \"Tinker, Tailor, Soldier, Spy\" by exploiting the discrepancies between the story's content and its manner of telling. Bentley 2021 discusses how time travel narratives experiment with story structure to convey temporal estrangement and function as covert metafiction. Naz 2021 emphasizes the importance of temporal structure in constructing a story, specifically focusing on the novel \"The Killer of Hamza\" by Najeeb Gillani. Abdulrraziq 2021 analyzes the narratological aspects of temporality in the novel \"Great Expectations\" by Charles Dickens, examining techniques such as analepsis, prolepsis, deceleration, and acceleration. Jones 2010 explores how the Saw film series manipulates space and time to engage viewers and provoke reflection. Vorderer 2009 delves into the conceptualizations, theoretical analyses, and empirical explorations of suspense in narratives. Kim 2020 proposes a computational approach to annotating novels with wall clock times, enabling the analysis of temporal flow and trends in literature. Overall, these papers highlight the intricate relationship between temporal manipulation, pacing, and the impact on the reader's engagement and understanding in thriller novels."
    },
    {
        "query": "How was Bach's music received in terms of modernism?",
        "paper_list_string": "Marissen 2021:\n\nTitle: Bach against Modernity\n\nAbstract: By key standards of what in the eighteenth century and later was considered to be forward-looking and modern\u2014namely to exalt reason (above revelation, whatever the flaws of reason) as arbiter of truth, to exalt human autonomy and achievement, to exalt religious tolerance, to exalt cosmopolitanism, and to exalt social and political progressiveness\u2014Bach and his music reflected and forcefully promoted a premodern world and life view. While we are arguably free to make use of Bach and his music in whatever historically informed or uninformed ways we find fitting, we ought also to be on the ethical alert for a kind of cultural narcissism in which we end up miscasting Bach in our own ideological image and proclaiming the authenticity of that image, and hence its prestige value, in support of our own agendas.\n\n==\n\nVarwig 2012:\n\nTitle: Metaphors of Time and Modernity in Bach\n\nAbstract: Narratives of music and modernity have been prominent in musicological writings of the past decade, and the place of Johann Sebastian Bach within these narratives has formed the subject of stimulating debates. Recent studies by Karol Berger and John Butt have aimed to integrate Bach9s Passion compositions into broadly conceived philosophical frameworks, in Berger9s case focusing specifically on changing perceptions of time from a premodern sense of circular stasis to a modern linear idea of progress. This article proposes an alternative model of historical inquiry into these issues by presenting a detailed look at attitudes to time in early eighteenth-century Protestant Leipzig. My approach reveals a complex constellation of conflicting ideas and metaphors that encompass notions of time as both circular and linear and evince a particular concern for the question of how to fill the time of one9s earthly existence productively. In this light, pieces like Bach9s St. Matthew Passion and Georg Philipp Telemann9s Brockes Passion can be understood to have offered a range of different temporal experiences, which depended on individual listening attitudes, performance decisions, and surrounding social conventions. I argue that only through paying close attention to these fluid and often incongruous discourses can we gain a sufficiently nuanced picture of how music may have reflected and shaped early eighteenth-century conceptions of time, history, and eternity.\n\n==\n\nButt 2010:\n\nTitle: Bach's Dialogue with Modernity: Perspectives on the Passions\n\nAbstract: Preface Introduction 1. Bach's Passions and the construction of early modern subjectivities 2. Bach's Passions and the textures of time 3. The hermeneutic perspective: negotiating the poles of faith and suspicion 4. The voices we hear and the construction of narrative authority 5. Between rhetoric and dialectic: Bach's inventive stance Afterword.\n\n==\n\nStinson 2006:\n\nTitle: The reception of Bach's organ works from Mendelssohn to Brahms\n\nAbstract: In this penetrating study, Russell Stinson explores how four of the greatest composers of the nineteenth century-Felix Mendelssohn, Robert Schumann, Franz Liszt, and Johannes Brahms-responded to the model of Bach's organ music. The author shows that this quadrumvirate not only borrowed from Bach's organ works in creating their own masterpieces, whether for keyboard, voice, orchestra, or chamber ensemble, but that they also reacted significantly to the music as performers, editors, theorists, and teachers. Furthermore, the book reveals how these four titans influenced one another as \"receptors\" of this repertory and how their mutual acquaintances-especially Clara Schumann-contributed as well. As the first comprehensive discussion of this topic ever attempted, Stinson's book represents a major step forward in the literature on the so-called Bach revival. He considers biographical as well as musical evidence to arrive at a host of new and sometimes startling conclusions. Filled with fascinating anecdotes, the study also includes detailed observations on how these composers annotated their personal copies of Bach's organ works. Stinson's book is entirely up-to-date and offers much material previously unavailable in English. It is meticulously annotated and indexed, and it features numerous musical examples and facsimile plates as well as an exhaustive bibliography. Included in an appendix is Brahms's hitherto unpublished study score of the Fantasy in G Major, BWV 572. Engagingly written, this study should be read by anyone interested in the music of Bach or the music of the nineteenth century.\n\n==\n\nBerry 2008:\n\nTitle: Romantic Modernism: Bach, Furtwa\u0308ngler, and Adorno\n\nAbstract: In very different ways, Wilhelm Furtw?ngler and Theodor W. Adorno stand as two of the twentieth century's most important and most controversial musi cians. Both were profoundly marked by the experience and aftermath of the Third Reich. Moreover, both were composers, and, although neither is remem bered primarily for this, their understanding of composition enabled them to understand musical works \"from the inside.\" Furtw?ngler remains for many the greatest conductor ofthat century, unsurpassed?perhaps never equaled? in the central German Romantic repertoire of Ludwig van Beethoven, Johannes Brahms, Anton Bruckner, and Richard Wagner. His so-called subjectivism may be contrasted with the more \"timely\" and \"objective\" anti-Romanticism of the Neue Sachlichkeit movement associated with such musicians as Paul Hindemith and Otto Klemperer and, most celebratedly, with the score-bound literalism of Arturo Toscanini. Adorno, having studied composition with Alban Berg, became the preeminent theorist of musical modernism and its dialectical adversary, the \"culture industry.\" He provided much of the intellectual under pinning for the postwar serialism of the Darmstadt school of Pierre Boulez, Karlheinz Stockhausen, and others, and his unabashed modernist elitism has inspired and infuriated ever since.\n\n==\n\nAshby 2010:\n\nTitle: The pleasure of modernist music : listening, meaning, intention, ideology\n\nAbstract: The debate over modernist music has continued for almost a century: from Strauss's Elektra and Webern's Symphony Op.21 to John Cage's renegotiation of musical control, the unusual musical practices of the Velvet Underground, and Stanley Kubrick's use of Ligeti's Lux Aeterna in the epic film 2001. The composers discussed in these pages -- including Bartok, Stockhausen, Bernard Herrmann, Steve Reich, and many others -- are modernists in that they are defined by their individualism, whether covert or overt, and share a basic urge toward redesigning musical discourse. The aim of this volume is to negotiate a varied and open middle ground between polemical extremes of reception. The contributors sketch out the possible significance of a repertory that in past discussions has been deemed either meaningless or beyond describable meaning. With an emphasis on recent aesthetics and contexts -- including film music, sexuality, metaphor, and ideas of a listening grammar -- they trace the meanings that such works and composers have held for listeners of different kinds. None of them takes up the usual mandate of \"educated listening\" to modernist works: the notion that a person can appreciate \"difficult\" music if given enough time and schooling. Instead the book defines novel but meaningful avenues of significance for modernist music, avenues beyond those deemed appropriate or acceptable by the academy. While some contributors offer new listening strategies, most interpret the listening premise more loosely: as a metaphor for any manner of personal and immediate connection with music. In addition to a previously untranslated article by Pierre Boulez, the volume contains articles (all but one previously unpublished) by twelve distinctive and prominent composers, music critics, and music theorists from America, Europe, Australia, and South Africa: Arved Ashby, Amy Bauer, William Bolcom, Jonathan Bernard, Judy Lochhead, Fred Maus, Andrew Mead, Greg Sandow, Martin Scherzinger, Jeremy Tambling, Richard Toop, and Lloyd Whitesell Arved Ashby is associate professor of music at the Ohio State University.\n\n==\n\nSchleifer 2011:\n\nTitle: Modernism and Popular Music: Contents\n\nAbstract: Traditionally, ideas about twentieth-century modernism \u2013 whether focused on literature, music, or the visual arts \u2013 have made a distinction between \u201chigh\u201d art and the \u201cpopular\u201d arts of best-selling fiction, jazz and other forms of popular music, and commercial art of one form or another. In Modernism and Popular Music, Ronald Schleifer instead shows how the music of George and Ira Gershwin, Cole Porter, Thomas \u201cFats\u201d Waller, and Billie Holiday can be considered as artistic expressions equal to those of the traditional high modernist art practices in music and literature. Combining detailed attention to the language and aesthetics of popular music with an examination of its early twentieth-century performance and dissemination through the new technologies of the radio and phonograph, Schleifer explores the \u201cpopularity\u201d of popular music in order to reconsider received and seemingly self-evident truths about the differences between high art and popular art and, indeed, about twentieth-century modernism altogether.\n\n==\n\nKramer 2023:\n\nTitle: Classical Music and Postmodern Knowledge\n\nAbstract: Classical Music and Postmodern Knowledge. By Lawrence Kramer. (Berkeley: University of California Press, 1995.) (xvii, 297 p. ISBN 0-520-08820-4. $35.00.] In this, his third book on eighteenth- and nineteenth-century music as \"cultural practice,\" Lawrence Kramer takes on one of the thorniest problems facing musicology today: resistance to the idea that music means anything other than itself. He disagrees with those who believe that masterpieces are works which \"transcend all social utility\" and \"into which the social as such disappears without a trace\" (p. 235). Kramer wants to show the contrary. He is concerned about the future of \"classical\" music, which he finds \"losing both its prestige and its appeal.\" In a country that has reduced it to endless repetition on the radio and entertainment for an aging population, ho hopes to put this music back at the center of public discourse, where it was at the end of the last century. To this end, Kramer posits what he calls a number of \"postmodernist strategies of understanding.\" Not that this is a study about postmodernism in music: Jean-Francois Lyotard is cited as an authority only in passing and American postmodernist theorists like Frederic Jameson appear not at all. In his first chapter, Kramer questions the insistence on impartial rationality, the tendency to universalize and desire unity, the notion of subjectivity as alienated, and the communicative paradigms of modernist thought. But what really interests him is the \"musical pertinence\" (p. 13) of psychoanalysis, for it is the reception of music, rather than its composition or performance, that he sees as critical in determining its meaning. Lacanian psychoanalysis, he explains, is \"a theory of how certain articulations of identification and alienation, desire and law, continually 'mark' the field of communicative action\" (p. 12). Taking off from Theodor Adorno and Michel Foucault and bringing Ins understanding of Jacques Lacan's \"imaginary\" and \"symbolic,\" Julia Kristeva's \"abject,\" and Jacques Derrida's \"dissemination\" to his inquiry about the listening process, Kramer argues that listeners continually interact with what they hear. They listen (as they act), he tells us, \"not as a radiation from a central core of being, but as a circulation among positions to be taken in discourse and society.\" They respond in diverse ways contingent on their own situations. Their perspective is always \"partial,\" like Donna Haraway's description of the \"never finished, constituted\" self who \"does not seek partiality for its own sake, but for the sake of the connections and unexpected openings [its] situated knowledges make possible. The only way to find a larger vision is to be somewhere in particular\" (Simians, Cyborgs, and Women: The Reinvention of Nature [New York: Routledge, 1991], 196: quoted by Kramer on p. 7). Of course, this raises the problem of self and other: How does one interact with what is outside oneself? In other words, In the process of hearing music, what are the boundaries between the listener and the work? In the second chapter, Kramer points to the binary opposition, masculine/feminine, to explain how the self has come to be associated with \"reason, activity, progression, unity, and the integrity of boundaries, the other with irrationality, passivity, stasis or regression, fragmentation, and the crossing or dissolution of boundaries\" (p. 38). This, he argues, is a historical idea, \"not just a conceptual phenomenon, the consequences of which have too often been inhumane or worse ... [it] always has a moral dimension\" (p. 39). Kramer is particularly interested in \"the rhetoric of abjection\" in critical thinking about music, the attempt not to fuse with the other, but rather to reject it violently to preserve one's \"intactness\" (p. 58). To illustrate this, he examines Friedrich Nietzsche's rejection of Richard Wagner, the present-day dismissal of Felix Mendelssohn, and Carl Dahlhaus's categorization of Gioacchino Rossini. \u2026",
        "final_summary": "Marissen (2021) posits that Bach's music reflected and promoted a premodern worldview, contrasting with modern values. Varwig (2012) suggests that Bach's music offered a range of temporal experiences, reflecting the complex attitudes towards time in early eighteenth-century Leipzig. Stinson (2006) explores how nineteenth-century composers Mendelssohn, Schumann, Liszt, and Brahms were influenced by and borrowed from Bach's organ music. Berry (2008) discusses the different approaches to composition of Furtw\u00e4ngler and Adorno, though the relationship to Bach is not explicitly mentioned. Ashby (2010) challenges traditional interpretations of modernist music, proposing alternative avenues of significance. Schleifer (2011) argues for the artistic value of popular music, challenging the distinction between high art and popular art. Kramer (2023) addresses the resistance to the idea that music has meaning beyond itself, though the abstract does not explicitly mention exploring the moral dimensions of musical categorization."
    },
    {
        "query": "more than 40 session of cbt-e for resistant patients with eating disorders",
        "paper_list_string": "Byrne 2011:\n\nTitle: The effectiveness of enhanced cognitive behavioural therapy for eating disorders: an open trial.\n\nAbstract: The aim of this study was to examine the effectiveness of Enhanced Cognitive Behaviour Therapy (CBT-E) for eating disorders in an open trial for adults with the full range of eating disorders found in the community. The only previously published trial of CBT-E for eating disorders was a randomised controlled trial (RCT) conducted in the U.K. for patients with a BMI \u2265 17.5. The current study represents the first published trial of CBT-E to include patients with a BMI<17.5. The study involved 125 patients referred to a public outpatient clinic in Perth, Western Australia. Patients attended, on average, 20-40 individual sessions with a clinical psychologist. Of those who entered the trial, 53% completed treatment. Longer waiting time for treatment was significantly associated with drop out. By the end of treatment full remission (cessation of all key eating disorder behaviours, BMI \u2265 18.5 kg/m(2), not meeting DSM-IV criteria for an eating disorder) or partial remission (meeting at least 2 these criteria) was achieved by two thirds of the patients who completed treatment and 40% of the total sample. The results compared favourably to those reported in the previous RCT of CBT-E, with one exception being the higher drop-out rate in the current study. Overall, the findings indicated that CBT-E results in significant improvements, in both eating and more general psychopathology, in patients with all eating disorders attending an outpatient clinic.\n\n==\n\nSignorini 2018:\n\nTitle: The Effectiveness of Enhanced Cognitive Behavioural Therapy (CBT-E): A Naturalistic Study within an Out-Patient Eating Disorder Service.\n\nAbstract: BACKGROUND\nThe effectiveness of enhanced cognitive behavioural Therapy (CBT-E) for adults with a range of eating disorder presentations within routine clinical settings has been examined in only two known published studies, neither of which included a follow-up assessment period.\n\n\nAIM\nThe current study aimed to evaluate the effectiveness of CBT-E within an out-patient eating disorder service in Brisbane, Queensland, Australia, and incorporated a follow-up assessment period of approximately 20 weeks post-treatment.\n\n\nMETHOD\nThe study involved 114 adult females with a diagnosed eating disorder, who attended an average of 20-40 individual CBT-E sessions with a psychologist or a psychiatry registrar between 2009 and 2013.\n\n\nRESULTS\nOf those who began treatment, 50% did not complete treatment, and the presence of psychosocial and environmental problems predicted drop-out. Amongst treatment completers, statistically and clinically significant improvements in eating disorder and general psychopathology were observed at post-treatment, which were generally maintained at the 20-week follow-up. Statistically significant improvements in eating disorder and general psychopathology were observed amongst the total sample.\n\n\nCONCLUSIONS\nThe findings, which were comparable to the previous Australian effectiveness study of CBT-E, indicate that CBT-E is an effective treatment for adults with all eating disorders within out-patient settings. Given the high attrition rate, however, minimizing drop-out appears to be an important consideration when implementing CBT-E within clinical settings.\n\n==\n\nFairburn 2015:\n\nTitle: A transdiagnostic comparison of enhanced cognitive behaviour therapy (CBT-E) and interpersonal psychotherapy in the treatment of eating disorders\n\nAbstract: Eating disorders may be viewed from a transdiagnostic perspective and there is evidence supporting a transdiagnostic form of cognitive behaviour therapy (CBT-E). The aim of the present study was to compare CBT-E with interpersonal psychotherapy (IPT), a leading alternative treatment for adults with an eating disorder. One hundred and thirty patients with any form of eating disorder (body mass index >17.5 to <40.0) were randomized to either CBT-E or IPT. Both treatments involved 20 sessions over 20 weeks followed by a 60-week closed follow-up period. Outcome was measured by independent blinded assessors. Twenty-nine participants (22.3%) did not complete treatment or were withdrawn. At post-treatment 65.5% of the CBT-E participants met criteria for remission compared with 33.3% of the IPT participants (p < 0.001). Over follow-up the proportion of participants meeting criteria for remission increased, particularly in the IPT condition, but the CBT-E remission rate remained higher (CBT-E 69.4%, IPT 49.0%; p = 0.028). The response to CBT-E was very similar to that observed in an earlier study. The findings indicate that CBT-E is potent treatment for the majority of outpatients with an eating disorder. IPT remains an alternative to CBT-E, but the response is less pronounced and slower to be expressed. Current controlled trials ISRCTN 15562271.\n\n==\n\nFrostad 2018:\n\nTitle: Implementation of enhanced cognitive behaviour therapy (CBT-E) for adults with anorexia nervosa in an outpatient eating-disorder unit at a public hospital\n\nAbstract: BackgroundAnorexia nervosa (AN) in adults is difficult to treat, and no current treatment is supported by robust evidence. A few studies, most of which were performed by highly specialized research units, have indicated that enhanced cognitive behaviour therapy (CBT-E) for eating disorders can be effective. However, the dropout rate is high and the evidence from non-research clinical units is sparse.MethodsThis quality assessment project implemented CBT-E in an outpatient setting at a public hospital. Forty-four patients with AN started therapy. Each patient received at least 40 sessions of CBT-E over a 12-month period. Their body mass index (BMI) was recorded at baseline and after 3, 6 and 12\u00a0months. Reasons for not starting therapy or for leaving therapy prematurely were recorded.ResultsHalf (n\u2009=\u200922) of the 44 patients who started outpatient CBT-E did not complete the treatment. In the remaining sample there was a large (and statistically significant) weight gain after 12\u00a0months. The percentage of patients achieving the target BMI of >\u200918.5\u00a0kg/m2 was 36.4, 50.0 and 77.3% after 3, 6 and 12\u00a0months, respectively.ConclusionsThis quality assessment project shows that it is possible to establish effective CBT-E in an outpatient eating-disorder unit at a public hospital. Although half of the patients did not complete CBT-E, the remaining patients achieved a significant increase in BMI at 1\u00a0year after the start of therapy.\n\n==\n\nKarbasi 2010:\n\nTitle: Enhanced Cognitive-Behavioral Therapy (CBT-E) for Eating Disorders: Case Study of a Client With Anorexia Nervosa\n\nAbstract: This case study involved the treatment of a young adult female, referred to as \u201cMarie,\u201d who presented for treatment seeking help with her eating disorder. Marie evinced symptoms of dietary restriction, amenorrhea, low weight, and low body mass index (BMI); she was subsequently diagnosed with anorexia nervosa. She was treated with 20 sessions of enhanced cognitive-behavioral therapy (CBT-E), a relatively new, transdiagnostic treatment for eating disorders. Marie\u2019s progress throughout treatment is detailed in this case study report. Results supported the promising but limited body of research available on this treatment, which was highly effective for this client and, therefore, may be more generally useful for clients with anorexia nervosa.\n\n==\n\nJong 2016:\n\nTitle: Effectiveness of enhanced cognitive behavioral therapy (CBT-E) for eating disorders: study protocol for a randomized controlled trial\n\nAbstract: BackgroundWhile eating disorder not otherwise specified (EDNOS) is the most common eating disorder (ED) diagnosis in routine clinical practice, no specific treatment methods for this diagnosis have yet been developed and studied. Enhanced cognitive behavioral therapy (CBT-E) has been described and put to the test as a transdiagnostic treatment protocol for all EDs, including EDNOS. Initial research in the UK suggests that CBT-E is more effective for EDs, especially bulimia nervosa (BN) and EDNOS, than the earlier version of CBT. These positive results of CBT-E have to be replicated in more detail, preferably by independent researchers in different countries. Being the first Dutch study into CBT-E, the results from this national multicenter study \u2013 on three sites specialized in EDs \u2013 will deliver important information about the effectiveness of CBT-E in several domains of ED pathology, while providing input for the upcoming update of the Dutch Multidisciplinary Guideline for the Treatment of Eating Disorders.Methods/designA multicenter randomized controlled trial will be conducted. One hundred and thirty-two adult outpatients (aged 18\u00a0years and older) with an ED diagnosis and a Body Mass index (BMI) of between 17.5 and 40 will be randomly allocated to the control or the intervention group. Subjects in the control group will receive Treatment as Usual (standard outpatient treatment provided at the participating sites). Subjects in the intervention group will receive 20 sessions of CBT-E in 20\u00a0weeks. The design is a 2 (group)\u2009\u00d7\u20095 (time) repeated measures factorial design in which neither therapists nor patients will be blinded for treatment allocation. The primary outcome measure is recovery from the ED. Secondary outcome measures include ED psychopathology, common mental disorders, anxiety and depressive symptoms, health-related quality of life, health care use and productivity loss. Self-esteem, perfectionism and interpersonal problems will be examined as putative predictors and mediators of the effect of treatment. Also, an economic evaluation from a societal perspective will be undertaken. All relevant effects, direct and indirect costs will be included. Utility scores will measure the effects. Measurements will take place at pretreatment, 6\u00a0weeks, 20\u00a0weeks, 40\u00a0weeks and 80\u00a0weeks.DiscussionThis effectiveness study into CBT-E has the aim of broadening the scope and generalizability of former studies. If CBT-E appears to be at least as effective as traditional diagnosis-specific treatments for a broad range of ED patients, training in one protocol would be sufficient for clinicians to treat patients with different kinds of EDs. It gives the opportunity to offer treatment for a severe mental disorder with fewer resources, thereby increasing the accessibility of specialized care for patients with an ED.Trial registrationNetherlands Trial Register, NTR4485. Registered on 2 April 2014.\n\n==\n\nGarte 2015:\n\nTitle: Implementation of a day hospital treatment programme based on CBT-E for severe eating disorders in adults: an open trial\n\nAbstract: More intense forms of enhanced cognitive-behaviour therapy (CBT-E) may be necessary for patients who do not benefit from outpatient CBT-E. A novel application of the transdiagnostic approach for eating disorders (ED) was implemented. A mixed sample of 62 patients, 13 anorexia nervosa (AN) (21%); 29 bulimia nervosa (BN) (47%) and 20 eating disorders not otherwise specified (EDNOS) (32%) were admitted to a CBT-E day treatment. Forty-seven patients (75.8%) completed the treatment as defined by completing two-thirds of treatment and the discharge assessment. A total of 31.9% (15/47) achieved recovery as defined by post-treatment body mass index (BMI)\u2009\u2265\u200918.5 and global Eating Disorder Examination Questionnaire (EDE-Q) +1 SD above community mean. An increase of 1.2 BMI units (15.8\u201317.0, p\u2009<\u2009.02) was observed for very underweight patients (BMI\u2009<\u200917.5). Patients with BN showed significantly greater improvement than AN or EDNOS for ED symptomology, state anxiety and self-esteem. Findings demonstrated high treatment acceptability of the CBT-E day treatment programme and significant, yet differential, improvement across diagnostic groups.\n\n==\n\nHamadi 2021:\n\nTitle: Delivering CBT-E In an Online Group Format: A Pilot Study in a Child and Adolescent Eating Disorder Service\n\nAbstract: \n Background: The increased prevalence of eating disorders during the COVID-19 pandemic has placed services for children and adolescents under immense pressure. The high number of people at medical risk has led to longer waiting lists for psychological support for those who are physically stable. A pilot study was conducted to evaluate the feasibility and effectiveness of providing group enhanced cognitive behavioral therapy for eating disorders (CBT-E), in a virtual setting, as a way of increasing the provision of evidence-based treatment during the pandemic.Method: Clinicians in a child and adolescent eating disorder service were invited to refer patients to take part in a six-session course of therapy comprising the CBT-E Stage Three Body Image module. Primary outcomes were acceptance rates, completion rates, qualitative feedback and quantitative data from routine measures of eating disorder psychopathology and psychosocial impairment.Results: From 22 eligible referrals, 12 participants accepted and enrolled in therapy. Eight completed all six sessions. Qualitative feedback was positive. Both the content and group nature of the intervention were described as helpful. There was an improvement in all scores on the psychometric tests. Conclusions: This pilot study demonstrated that online group CBT-E was a feasible method of providing psychological therapy within the service. A larger trial is recommended to robustly test the effectiveness of the intervention compared to one-to-one in-person CBT-E, and to test whether other modules of the CBT-E protocol can be similarly delivered in this population.Trial registration: This study was pre-registered and approved as a clinical service evaluation by the Oxford Health NHS Foundation Trust, United Kingdom",
        "final_summary": "More than 40 sessions of CBT-E can be effective for treating resistant patients with eating disorders. Byrne 2011 and Signorini 2018 both found that CBT-E led to significant improvements in eating disorder and general psychopathology. Fairburn 2015 found that CBT-E had higher remission rates than interpersonal psychotherapy at post-treatment and follow-up. Frostad 2018 reported a significant increase in BMI after 12 months of CBT-E treatment in an outpatient setting. Karbasi 2010 presented a case study showing the effectiveness of CBT-E for a client with anorexia nervosa. Jong 2016 proposed that CBT-E could be a potent treatment for a broad range of eating disorders. Garte 2015 implemented a day hospital treatment program based on CBT-E and observed significant improvement in eating disorder symptomology across diagnostic groups. Hamadi 2021 demonstrated the feasibility and positive feedback of delivering CBT-E in an online group format. (Byrne 2011; Signorini 2018; Fairburn 2015; Frostad 2018; Karbasi 2010; Jong 2016; Garte 2015; Hamadi 2021)"
    },
    {
        "query": "What is generative AI?",
        "paper_list_string": "Nalini 2023:\n\nTitle: Generative AI: A Comprehensive Study of Advancements and Application\n\nAbstract: -A branch of artificial intelligence called \"generative AI\" is concerned with creating models and algorithms that can create new data that is like a given dataset. Generative AI seeks to imitate human creativity by learning patterns and distributions from existing data to produce wholly new content, contrast to standard AI systems that are built for specific tasks. Learning a model that captures the underlying patterns and structures of the data is the fundamental idea behind generative AI. The new samples produced by this model can then be compared to the old dataset. The creation of images, texts, music, videos, and other things is one of the many tasks that generative AI is frequently utilised for.\n\n==\n\nHoude 2020:\n\nTitle: Business (mis)Use Cases of Generative AI\n\nAbstract: Generative AI is a class of machine learning technology that learns to generate new data from training data. While deep fakes and media-and art-related generative AI breakthroughs have recently caught people's attention and imagination, the overall area is in its infancy for business use. Further, little is known about generative AI's potential for malicious misuse at large scale. Using co-creation design fictions with AI engineers, we explore the plausibility and severity of business misuse cases.\n\n==\n\nZant 2011:\n\nTitle: Generative Artificial Intelligence\n\nAbstract: The closed systems of contemporary Artificial Intelligence do not seem to lead to intelligent machines in the near future. What is needed are open-ended systems with non-linear properties in order to create interesting properties for the scaffolding of an artificial mind. Using post-structuralistic theories of possibility spaces combined with neo-cybernetic mechanisms such as feedback allows to actively manipulate the phase space of possibilities. This is the field of Generative Artificial Intelligence and it is implementing mechanisms and setting up experiments with the goal of the creation of open-ended systems. It sidesteps the traditional argumentation of top-down versus bottom-up by using both mechanisms. Bottom-up procedures are used to generate possibility spaces and top-down methods sort out the structures that are functioning the worst. Top-down mechanisms can be the environment, but also humans who steer the development processes.\n\n==\n\nInie 2023:\n\nTitle: Designing Participatory AI: Creative Professionals\u2019 Worries and Expectations about Generative AI\n\nAbstract: Generative AI, i.e., the group of technologies that automatically generate visual or written content based on text prompts, has undergone a leap in complexity and become widely available within just a few years. Such technologies potentially introduce a massive disruption to creative fields. This paper presents the results of a qualitative survey (N = 23) investigating how creative professionals think about generative AI. The results show that the advancement of these AI models prompts important reflections on what defines creativity and how creatives imagine using AI to support their workflows. Based on these reflections, we discuss how we might design participatory AI in the domain of creative expertise with the goal of empowering creative professionals in their present and future coexistence with AI.\n\n==\n\nGoodfellow 2014:\n\nTitle: Generative adversarial networks\n\nAbstract: Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.\n\n==\n\nEpstein 2023:\n\nTitle: Art and the science of generative AI: A deeper dive\n\nAbstract: A new class of tools, colloquially called generative AI, can produce high-quality artistic media for visual arts, concept art, music, fiction, literature, video, and animation. The generative capabilities of these tools are likely to fundamentally alter the creative processes by which creators formulate ideas and put them into production. As creativity is reimagined, so too may be many sectors of society. Understanding the impact of generative AI - and making policy decisions around it - requires new interdisciplinary scientific inquiry into culture, economics, law, algorithms, and the interaction of technology and creativity. We argue that generative AI is not the harbinger of art's demise, but rather is a new medium with its own distinct affordances. In this vein, we consider the impacts of this new medium on creators across four themes: aesthetics and culture, legal questions of ownership and credit, the future of creative work, and impacts on the contemporary media ecosystem. Across these themes, we highlight key research questions and directions to inform policy and beneficial uses of the technology.\n\n==\n\nBilgram 2023:\n\nTitle: Accelerating Innovation With Generative AI: AI-Augmented Digital Prototyping and Innovation Methods\n\nAbstract: Easy-to-use generative artificial intelligence (AI) is democratizing the use of AI in innovation management and may significantly change the way how we work and innovate. In this article, we show how large language models (LLMs), such as generative pretrained transformer (GPT), can augment the early phases of innovation, in particular, exploration, ideation, and digital prototyping. Drawing on six months of experimenting with LLMs in internal and client innovation projects, we share first-hand experiences and concrete examples of AI-assisted approaches. The article highlights a large variety of use cases for generative AI ranging from user journey mapping to idea generation and prototyping and foreshadows the promising role LLMs may play in future knowledge management systems. Moreover, we argue that generative AI may become a game changer in early prototyping as the delegation of tasks to an artificial agent can result in faster iterations and reduced costs. Our experiences also provide insights into how human innovation teams purposively and effectively interact with AIs and integrate them into their workflows.\n\n==\n\nTzirides 2023:\n\nTitle: Generative AI: Implications and Applications for Education\n\nAbstract: The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.",
        "final_summary": "Generative AI, a branch of artificial intelligence, is designed to create new data that mirrors a given dataset, emulating human creativity by learning patterns from existing data to produce novel content (Nalini 2023). Its applications span from generating images, texts, music, and videos to potential business uses, though concerns about large-scale misuse persist (Houde 2020). Generative AI explores the creation of open-ended systems with non-linear properties, using both bottom-up and top-down mechanisms (Zant 2011). The advancement of these technologies has prompted creative professionals to reflect on the definition of creativity and the design of participatory AI (Inie 2023). Generative adversarial networks (GANs), a type of generative model, have been successful in generating realistic high-resolution images, and their game theory-based approach presents unique challenges and research opportunities (Goodfellow 2014). Generative AI's impact extends to innovation management, education, and knowledge management systems, indicating its transformative potential across various industries (Epstein 2023; Bilgram 2023; Tzirides 2023)."
    },
    {
        "query": "why SMEs' Access to Financial Services in Nigeria through Fintech is good for Nigeria",
        "paper_list_string": "Adegboye 2018:\n\nTitle: Does Access to Finance Enhance SME Innovation and Productivity in Nigeria? Evidence from the World Bank Enterprise Survey\n\nAbstract: The primary link between financial institutions and economic performance is the provision of resources by these institutions to businesses in order to drive enterprise development. In this study, the role of access to finance in enhancing innovation and productivity among Nigerian small and medium\u2010sized enterprises (SMEs) is investigated using the World Bank Enterprise Survey (ES) dataset. Access to finance is categorized as external and internal to the firm. Using the logit estimation technique, the study finds that ease of accessing bank credit is the strongest positive force in driving all types of innovation among SMEs in Nigeria. In the same vein, the source of investment financing matters in terms of how it affects innovation: both internal and external sources improve investment in product, process, and organizational innovation, but only external financing has a significant effect on R&D spending and use of foreign licensed technology. Overall spending on R&D is only driven by access to external finance by the SMEs. The study also shows that increased access to finance may actually lead to productivity decline among SMEs in Nigeria.\n\n==\n\nBabajide 2020:\n\nTitle: Financial Technology Financial Inclusion and MSMEs Financing in The South West of Nigeria\n\nAbstract: The Strategic role Micro, Small and Medium Enterprises (MSMEs) play in achieving sustainable development cannot be over emphasized in any given economy. Regrettably, the difficulty in accessing finance from the formal financial institutions remains the greatest problem to development and survival of MSMEs sector in Nigeria. In the light of this, this paper provided functional solutions by examining developments, innovations and new financing windows available to MSMEs viz-a-viz Financial Technology that is capable of enhancing the goal of financial inclusion as a strategic tool of achieving sustainable development in the MSMEs sector in Nigeria. We classify the methodologies and approaches the researchers have adopted to predict the impact of Financial Technology on financial inclusion in respect to MSMEs financing in the South West States of Nigeria. We notice that MSMEs financing has the potential to be the fourth largest sector to be disrupted by Financial Technology in the next five years after consumer banking, payments and investment/wealth management in the financial sector. We identify that financial institutions need to collaborate and capitalize on the Financial Technology ecosystems by leveraging on artificial intelligence, big data, data analytics and MSMEs friendly applications in effectively lending to MSMEs sector. We observe that Financial Technology has the potential to drive financial inclusion, with high internet, electricity and mobile phone penetration more states will achieve the 20% financial exclusion target faster by the year 2020. This study also provided recommendations for financial service providers, government and regulatory authorities for this emerging sector going forward.\n\n==\n\nAminu 2015:\n\nTitle: Influence of Strategic Orientation on SMEs Access to Finance in Nigeria\n\nAbstract: The paper examined the influence of EO, MO, LO, and TO on SMEs financial capital accessibility in Nigeria. The purpose of this paper is to establish the role of firm strategic orientation in helping SMEs improve their financial access. A total of 362 questionnaires from SMEs in North Western Nigeria were used in this study. Partial Least Squares Structural Equation Modeling (PLS-SEM) was used to test the study hypotheses. Using SmartPLS 3.0 the findings indicates that strategic orientations are important drivers of firm success to finance. The result further suggests that SMEs who configured and utilized their strategic activities are more likely to get more cash flow, profit and retained earnings and will obtain a loan from external sources. To get an adequate financial capital SMEs need to improve their marketing activities, learn more from their experience and environment and lastly produce product with high technological improvement. At the same time they should avoid too much emphasis on taking risky business decision and investments.\n\n==\n\nDada 2014:\n\nTitle: Commercial Banks\u2019 Credit and SMEs Development in Nigeria: An Empirical Review\n\nAbstract: Access to credit is crucial for the growth and survival of Small and Medium-sized Enterprises (SMEs), this is because the ability of SMEs to grow depend highly on their potentials to invest in restructuring, innovation, etc. And all of these investments need capital, and therefore access to finance.\n\n==\n\nOaya 2017:\n\nTitle: The Impact of SMEs Financing on Business Growth in Nigeria: A Study of Keffi and Mararaba Metropolis\n\nAbstract: The impact of SMEs financing on business growth in Nigeria using Keffi and Mararaba Metropolis as a case study was assessed in this work. Descriptive research designs as well t-test statistics for the test of hypotheses were utilized. Hypotheses applied for the study includes: banks credits to SMEs have no significant impact on growth of Nigeria economy as well as interest rates charged on credits has no effect on SMEs business expansion in Nigeria. Access to finance was found to be sine qua non for successful entrepreneurial development while in respect of interest rate charged on SMEs loans and advances; the entrepreneurs ability to borrow was not hindered. The study therefore recommends strong availability to finance for successful growth of SMEs through different channels like microfinance banks and institutions in addition to formal and informal financial institutions. In addition, the scheme could be further expanded vide various tax palliatives. It is also recommended that the government should provide suitable incentives for the growth and effective performance of SMEs. There should also be close liaison between Nigeria Stock Exchange and family business to raise cheap funds for the growth of SMEs.\n\n==\n\nAhmed-Ishmel 2018:\n\nTitle: The Impact of Financial Technology in the Operations (Payments/Collections) of SMEs in Nigeria\n\nAbstract: This study examines the impact of Financial Technology in the Operations (Payments/Collections) of SMEs in Nigeria. The study conducted a survey of 120 Small and Medium Scale Enterprises across the four (4) identified geo-political zones in Lagos state. These SMEs with employment ranging from 2-10 employees in the fashion, educational, online merchants, pharmaceuticals, automobile, cosmetics, agro-allied, printing, bakery, eatery, I.T. firms and retail enterprises. This was done in such a way that the four axes were represented, each axis having thirty (30) SMEs. One hundred (100) Questionnaires were found useful for the purpose of the study representing 83% of the total questionnaire distributed. The data was analyzed using inferential statistics. The study reveals that Financial Technology (FinTech) has great impact on the economy, and therefore contributing positively to national development. It also discussed the benefits and risks of embracing and investing in FinTech. In order to remain sustainable and profitable in business in this 21st century, it was recommended that the financial authorities and government embark on massive campaign and support of emerging FinTech organizations as well as embracing FinTech in its operations. Also, the SMEs are highly recommended to adopt FinTech payment channels in their operations.\n\n==\n\nAbereijo 2005:\n\nTitle: Innovative Approach to SME Financing in Nigeria: A Review of Small and Medium Industries Equity Investment Scheme (SMIEIS)\n\nAbstract: Abstract ABSTRACT The need for SME growth in Nigeria is beyond question, but access to finance is one of the factors standing in the way of its growth. As a financing tool, equity has distinct advantages over debt for the development of SMEs, but equity financing in Nigeria presently comes with significant challenges as can be seen from the low response from the SME industrialists to apply for the fund, and the foot-dragging of the banks in seeking for and investing in SME. The paper tried to review the innovative approach to SME financing all over the world, especially the private equity financing, so as to identify the best practices and lessons to be learnt. It also reviews the small and medium industries equity investment scheme (SMIEIS) in Nigeria. It was discovered that there are still many challenges that the banks in Nigeria need to tackle before a successful implementation of the scheme. These include those challenges that relate to deal flow, investment structuring, monitoring/value enhancement, and liquidity and exit strategies.\n\n==\n\nRasheed 2019:\n\nTitle: Financial Inclusion for SMEs: Role of Digital Micro-financial Services\n\nAbstract: SMEs paly major role in poverty reduction and employment generation, therefore experts considered this sector as engine of economic growth. However, access to finance in developing countries is one of major issue in development of SME sector as well as hurdle in economic growth. Financial institutions banking and non-banking shows reluctant behaviour in providing financing to SMEs and the issue is more severe in emerging economies. Bank financing has been found as main source of funds for SMEs in Pakistan, however, to obtain these funds not easy for small and medium firms. Recently digital micro financial services have been introduced by a number of micro finance banks.\u00a0 Current study examines the role of digital micro financial services in enhancing SMEs\u2019 access to finance and thereby enabling a more inclusive financial market for SMEs especially in context of emerging and developing economies. By digging out the existing literature and secondary data, the study discusses that digital financial services have greatly helped owner managers of SMEs in smooth management of their transactions and finances. The study concludes that to strengthen SME sector for economic growth, it is important to further reduce the cost of using digital financial services and increase the financial product portfolio on digital platforms.",
        "final_summary": "Fintech enhances SMEs' access to financial services in Nigeria, driving innovation and growth. Adegboye (2018) found that easy access to bank credit is a strong positive force for innovation among SMEs. Babajide (2020) highlighted fintech's potential to enhance financial inclusion, supporting sustainable development in the MSME sector. Aminu (2015) emphasized that strategic orientation improves SMEs' financial access, leading to increased cash flow and profit. Dada (2014) stressed the importance of credit access for SMEs' survival and growth. Oaya (2017) found that access to finance is essential for SME growth and recommended various financing channels, including microfinance banks. Ahmed-Ishmel (2018) revealed that fintech positively impacts SME operations, particularly in payments and collections. Abereijo (2005) discussed the potential of equity financing schemes for SMEs, despite existing challenges. Rasheed (2019) underscored the role of digital micro-financial services in enhancing SMEs' access to finance and promoting financial inclusion."
    },
    {
        "query": "spectral bias of neural networks",
        "paper_list_string": "Choraria 2022:\n\nTitle: The Spectral Bias of Polynomial Neural Networks\n\nAbstract: Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a $\\textit{spectral bias}$ towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the $\\Pi$-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. We verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials.\n\n==\n\nCao 2019:\n\nTitle: Towards Understanding the Spectral Bias of Deep Learning\n\nAbstract: An intriguing phenomenon observed during training neural networks is the spectral bias, which states that neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining the generalization ability of neural networks, and certain efforts have been made to provide a theoretical explanation for spectral bias. However, there is still no satisfying theoretical result justifying the underlying mechanism of spectral bias. In this paper, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along different directions defined by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit sphere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks. Finally, we provide numerical experiments to demonstrate the correctness of our theory. Our experimental results also show that our theory can tolerate certain model misspecification in terms of the input data distribution.\n\n==\n\nCao 2021:\n\nTitle: Towards Understanding the Spectral Bias of Deep Learning\n\nAbstract: An intriguing phenomenon observed during training neural networks is the spectral bias, which states that neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining the generalization ability of neural networks, and certain efforts have been made to provide a theoretical explanation for spectral bias. However, there is still no satisfying theoretical result justifying the underlying mechanism of spectral bias. In this paper, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along different directions defined by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit sphere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks. Finally, we provide numerical experiments to demonstrate the correctness of our theory. Our experimental results also show that our theory can tolerate certain model misspecification in terms of the input data distribution.\n\n==\n\nHong 2022:\n\nTitle: On the Activation Function Dependence of the Spectral Bias of Neural Networks\n\nAbstract: Neural networks are universal function approximators which are known to general-ize well despite being dramatically overparameterized. We study this phenomenon from the point of view of the spectral bias of neural networks. Our contributions are two-fold. First, we provide a theoretical explanation for the spectral bias of ReLU neural networks by leveraging connections with the theory of \ufb01nite element methods. Second, based upon this theory we predict that switching the activation function to a piecewise linear B-spline, namely the Hat function, will remove this spectral bias, which we verify empirically in a variety of settings. Our empirical studies also show that neural networks with the Hat activation function are trained signi\ufb01cantly faster using stochastic gradient descent and ADAM. Combined with previous work showing that the Hat activation function also improves generalization accuracy on image classi\ufb01cation tasks, this indicates that using the Hat activation provides signi\ufb01cant advantages over the ReLU on certain problems.\n\n==\n\nHong 2022:\n\nTitle: On the Activation Function Dependence of the Spectral Bias of Neural Networks\n\nAbstract: Neural networks are universal function approximators which are known to generalize well despite being dramatically overparameterized. We study this phenomenon from the point of view of the spectral bias of neural networks. Our contributions are two-fold. First, we provide a theoretical explanation for the spectral bias of ReLU neural networks by leveraging connections with the theory of finite element methods. Second, based upon this theory we predict that switching the activation function to a piecewise linear B-spline, namely the Hat function, will remove this spectral bias, which we verify empirically in a variety of settings. Our empirical studies also show that neural networks with the Hat activation function are trained significantly faster using stochastic gradient descent and ADAM. Combined with previous work showing that the Hat activation function also improves generalization accuracy on image classification tasks, this indicates that using the Hat activation provides significant advantages over the ReLU on certain problems.\n\n==\n\nRahaman 2018:\n\nTitle: On the Spectral Bias of Neural Networks\n\nAbstract: Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with $100\\%$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets \\emph{easier} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.\n\n==\n\nLazzari 2023:\n\nTitle: Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics\n\nAbstract: Spectral bias is an important observation of neural network training, stating that the network will learn a low frequency representation of the target function before converging to higher frequency components. This property is interesting due to its link to good generalization in over-parameterized networks. However, in low dimensional settings, a severe spectral bias occurs that obstructs convergence to high frequency components entirely. In order to overcome this limitation, one can encode the inputs using a high frequency sinusoidal encoding. Previous works attempted to explain this phenomenon using Neural Tangent Kernel (NTK) and Fourier analysis. However, NTK does not capture real network dynamics, and Fourier analysis only offers a global perspective on the network properties that induce this bias. In this paper, we provide a novel approach towards understanding spectral bias by directly studying ReLU MLP training dynamics. Specifically, we focus on the connection between the computations of ReLU networks (activation regions), and the speed of gradient descent convergence. We study these dynamics in relation to the spatial information of the signal to understand how they influence spectral bias. We then use this formulation to study the severity of spectral bias in low dimensional settings, and how positional encoding overcomes this.\n\n==\n\nYang 2022:\n\nTitle: Overcoming the Spectral Bias of Neural Value Approximation\n\nAbstract: Value approximation using deep neural networks is at the heart of off-policy deep reinforcement learning, and is often the primary module that provides learning signals to the rest of the algorithm. While multi-layer perceptron networks are universal function approximators, recent works in neural kernel regression suggest the presence of a spectral bias, where fitting high-frequency components of the value function requires exponentially more gradient update steps than the low-frequency ones. In this work, we re-examine off-policy reinforcement learning through the lens of kernel regression and propose to overcome such bias via a composite neural tangent kernel. With just a single line-change, our approach, the Fourier feature networks (FFN) produce state-of-the-art performance on challenging continuous control domains with only a fraction of the compute. Faster convergence and better off-policy stability also make it possible to remove the target network without suffering catastrophic divergences, which further reduces TD}(0)'s estimation bias on a few tasks.",
        "final_summary": "The papers suggest that neural networks exhibit a spectral bias, favoring the learning of low-frequency functions over high-frequency ones. Choraria 2022 analyzes polynomial neural networks (PNNs) and finds that a specific parametrization of PNNs, called the $\\Pi$-Net family, accelerates the learning of higher frequencies. Cao 2019 and Cao 2021 provide theoretical explanations for the spectral bias and relate it to the neural tangent kernel function. They show that the training process of neural networks can be decomposed along different directions defined by the eigenfunctions of the neural tangent kernel, with each direction having its own convergence rate. Hong 2022 and Hong 2022 propose using the Hat activation function instead of ReLU to remove the spectral bias and improve training speed and generalization accuracy. Rahaman 2018 demonstrates that deep ReLU networks are biased towards low-frequency functions and that learning high frequencies becomes easier with increasing manifold complexity. Lazzari 2023 studies the training dynamics of ReLU MLPs and their connection to spectral bias, while Yang 2022 focuses on overcoming the spectral bias in value approximation for reinforcement learning."
    },
    {
        "query": "What is the relationship between the rates oxygen evolution in the chloroplasts and the productivity of crops",
        "paper_list_string": "Morita 1975:\n\nTitle: RELATIONSHIP BETWEEN THE CHANGE IN THE CHLOROPLASTIC NITROGEN FRACTIONS AND THE RATE OF OXYGEN EVOLUTION IN RICE PLANTS\n\nAbstract: Abstract Chloroplasts and the lamellae systems were isolated from rice leaves and the change with leaf age in the nitrogen fractions localized in the lamellae systems and the stroma region was examined. The pattern of the change in these two chloroplastic nitrogen fractions varied according to the leaf position. In order to find out the relationship between the change in the chloroplastic nitrogen fractions and the photosynthetic activity of a leaf, the rate of oxygen evolution of leaf sections on a total leaf nitrogen basia was also followed. The rate of oxygen evolution/total leaf nitrogen was affected by two factors. i.e., lamellae nitrogen/total leaf nitrogen and the lamellae-stroma composition of the chloroplast. An increase in lamellae nitrogen/total leaf nitrogen caused an increase in oxygen evolution, whereas an increaae in the proportion of stroma nitrogen in chloroplast brought about the negative effect. Consequently, oxygen evolution/total leaf nitrogen had a close correlation with an index of ...\n\n==\n\nHudock 1964:\n\nTitle: The Relation of Oxygen Evolution to Chlorophyll and Protein Synthesis in a Mutant Strain of Chlamydomonas reinhardi.\n\nAbstract: The preceding paper (2) has described some of the changes that occur in the photosynthetic activity of the mutant strain y-2 of the unicellular green alga Chlamydomonas reinhardi accompanying changes in chlorophyll content. These results indicated that photosynthetic activity might be regulated by a mechanism which is related to either a critical chlorophyll content or structural configuration within the chloroplast. In this paper, the postulated regulatory mechanism will be given further consideration by studying the effects of chloramphenicol on chlorophyll synthesis, protein synthesis, and photosynthetic 02 evolution. Electron micrographs of cells of regreening cultures were prepared to determine the relationship between the rate of photosynthesis and the structural configuration of the chloroplast.\n\n==\n\nHajibagheri 1984:\n\nTitle: Photosynthetic oxygen evolution in relation to ion contents in the chloroplasts of Suaeda maritima\n\nAbstract: The effects of sodium chloride on photosynthetic oxygen evolution by chloroplasts isolated from the leaves of the halophyte Suaeda maritima L. (Dum) are reported. When plants were grown in concentrations of salt increasing to 340 mol m\u22123 the rate of oxygen evolution by isolated chloro-plasts increased; beyond this concentration of salt it declined. Maximum rates of oxygen evolution (coupled and uncoupled) were found when the plants were grown in 340 mol m\u22123 NaCl. Under these conditions mean ion concentrations in the chloroplasts measured in vivo by X-ray microanalysis were Na+, 84\u2013257 mol m\u22123; Cl\u2212, 86\u2013212 mol m\u22123; K+, 23\u201336 mol m\u22123. \n \nA comparative study was also made of the in vitro effects of sodium chloride on oxygen evolution. Here again maximum rates were found in the presence of 340 mol m\u22123 NaCl.\n\n==\n\nBoyer 1970:\n\nTitle: Inhibition of oxygen evolution in chloroplasts isolated from leaves with low water potentials.\n\nAbstract: Chloroplasts were isolated from pea and sunflower leaves having various water potentials. Oxygen evolution by the chloroplasts was measured under identical conditions for all treatments with saturating light and with dichloroindophenol as oxidant. Evolution was inhibited when leaf water potentials were below -12 bars in pea and -8 bars in sunflower and the inhibition was proportional to leaf water potential below these limits. Inhibition was more severe in sunflower than in pea chloroplasts. In sunflower, it could be detected after 5 minutes of leaf desiccation, and, up to 1 hour, the effect was independent of the duration of low leaf water potential.In high light, the reduction in activity of sunflower chloroplasts paralleled the reduction in CO(2) fixation by intact sunflower plants having low leaf water potentials. Stomatal apertures and transpiration rates were also reduced under these conditions and were probably limiting. In low light, intact sunflowers required more light per unit of CO(2) fixed when leaf water potentials were low than when they were high. This increased light requirement in the intact system was of a magnitude which could be predicted from the reduced oxygen evolution by the isolated chloroplasts. It was concluded that moderately low leaf water potential affects photosynthesis in at least two ways: first, through an inhibition of oxygen evolution by chloroplasts and, second, by closure of stomata in intact leaves.\n\n==\n\nArnason 1976:\n\nTitle: Studies on the rate-limiting reaction of photosynthetic oxygen evolution in spinach chloroplasts.\n\nAbstract: The modulated oxygen polarography has been used to study the rate-determining steps of photosynthetic oxygen evolution in spinach chloroplasts. The rate constant, k, of the reaction has a value of 218 +/- 10 (S.E.) s-1 at 23 degrees C and an activation energy of 7 +/- 2 (S.E.) kcal-mol-1. A kinetic isotope experiment indicated that this step is probably not the water-splitting reaction. These findings resemble previous results with the unicellular alga Chlorella (Sinclair, J. and Arnason, T (1974) Biochim. Biophys. Acta 368, 393-400). In other experiments we changed the pH, O2 concentration and osmolarity of the medium, and treated the chloroplasts with 1 mM NH4Cl without detecting any significant change in k. These results suggest that the step is irreversible. However, a significantly lower value of k, 110 +/- 20 (S.E.) s-1 was obtained when all salts except 1 mM MgCl2 were removed from the medium bathing the chloroplasts.\n\n==\n\nAllen 1995:\n\nTitle: Dissection of Oxidative Stress Tolerance Using Transgenic Plants\n\nAbstract: Environmental stress is the major limiting factor in plant productivity. Much of the injury to plants caused by stress exposure is associated with oxidative damage at the cellular level. Widespread losses of forests and crops due to ozone pollution provide a highly visible example of oxidative stress (see Tingey et al., 1993, for a review), but less obvious losses caused by oxidative damage associated with periods of cold or drought also take their toll in the accumulation of incremental setbacks during a growing season. The role of ROIs in plant stress damage is indicated by the increased production of ROIs and the increased oxidative damage in tissues during stress. In plants, the highly energetic reactions of photosynthesis and an abundant oxygen supply make the chloroplast a particularly rich source of ROIs. High light intensity can lead to excess reduction of PSI so that CO2 fixation cannot keep pace and NADP+ pools are reduced. Under these conditions, 02 can compete for electrons from PSI, leading to the generation of ROIs through the Mehler reaction. When CO2 fixation is limited by environmental conditions such as cold temperatures or low CO2 availability (closed stomata), excess PSI reduction and increased ROI production can occur even at moderate light intensities. Efficient removal of ROIs from chloroplasts is critical, since H202 concentrations as low as 10 ptM can inhibit photosynthesis by 50% (Kaiser, 1979). Although the toxicity of *?2and H202 themselves is relatively low, their metal-dependent conversion to the highly toxic -OH via the Haber-Weiss reaction is thought to be responsible for the majority of the biological damage associated with these molecules. Antioxidant systems of plant chloroplasts include enzymes such as SOD and APX, and nonenzymatic components such as ascorbic acid and glutathione. The proposed ROI scavenging pathway of chloroplasts is shown in Figure 1 (Asada, 1994). Superoxide radicals are produced by the reduction of molecular oxygen at PSI via the Mehler reaction. This ?2- is rapidly dismuted to H202 by SOD that is associated with the thylakoid. The H202 produced is\n\n==\n\nWalker 1976:\n\nTitle: Photosynthesis in a reconstituted chloroplast system from spinach. Some factors affecting CO2-dependent oxygen evolution with fructose-1,6-bisphosphate as substrate.\n\nAbstract: When envelope-free spinach chloroplasts are incubated with stromal protein, catalytic NADP, catalytic ADP, radioactive bicarbonate and fructose 1,6-bisphosphate, 14CO2 fixation starts immediately upon illumination but oxygen evolution is delayed. The delay is increased by the addition of fructose 6-phosphate and by a variety of factors known (or believed) to increase fructose bisphosphatase activity (such as dithiothreitol, more alkaline pH, higher [Mg] and antimycin A). Conversely, the lag can be decreased or eliminated by the addition of an ATP-generating system. Bearing in mind the known inhibition, by ADP, of sn-phospho-3-glycerate (3-phosphoglycerate) reduction it is concluded that the lag in O2 evolution results from the production of ribulose 5-phosphate from fructose bisphosphate and that this in turn inhibits the reoxidation of NADPH by adversely affecting the ADP/ATP ratio. The results are discussed in their relation to the mode of action of antimycin A and to regulation of the reductive pentose phosphate pathway.\n\n==\n\nTOLMACH 1951:\n\nTitle: Effects of Triphosphopyridine Nucleotide upon Oxygen Evolution and Carbon Dioxide Fixation by Illuminated Chloroplasts\n\nAbstract: THE illumination of suspensions of chloroplasts always causes a very small and transient evolution of oxygen. The rate curve for this process has previously been measured1 under anaerobic conditions by means of the phosphorescence quenching method of Franck and Pringsheim2. In continuation of this work, the influence on oxygen production of certain biologically important substances added to the chloroplast suspensions has been studied.",
        "final_summary": "The rate of oxygen evolution in chloroplasts, a key process in photosynthesis, is influenced by various factors. Morita (1975) found that in rice plants, an increase in lamellae nitrogen led to increased oxygen evolution. Hudock (1964) observed that changes in chlorophyll content and protein synthesis in a mutant strain of Chlamydomonas reinhardi affected oxygen evolution. Hajibagheri (1984) reported that the rate of oxygen evolution in Suaeda maritima increased up to a certain concentration of sodium chloride. Boyer (1970) demonstrated that oxygen evolution was inhibited in chloroplasts isolated from leaves with low water potentials. Arnason (1976) identified a rate-limiting step in photosynthetic oxygen evolution in spinach chloroplasts that was not the water-splitting reaction. Allen (1995) highlighted the importance of efficient removal of reactive oxygen intermediates from chloroplasts to prevent oxidative stress damage. Walker (1976) found a delay in oxygen evolution in a reconstituted chloroplast system from spinach, related to fructose bisphosphate production. Lastly, Tolmach (1951) examined the effects of triphosphopyridine nucleotide on oxygen evolution and carbon dioxide fixation in illuminated chloroplasts. These findings collectively suggest that the rate of oxygen evolution in chloroplasts, and thus potentially crop productivity, is influenced by a range of biological and environmental factors."
    },
    {
        "query": "Why the USSR was an imperialist state?",
        "paper_list_string": "Slezkine 2000:\n\nTitle: Imperialism As the Highest Stage of Socialism\n\nAbstract: The Soviet Union was an empire-in the sense of being very big, bad, asymmetrical, hierarchical, heterogeneous, and doomed. It was also Utopia in power and a prison of the peoples (sentenced to life without parole or death through eventual fusion). But was it a modern colonial empire? Does it belong on the same trash heap as the Dutch, French, and British imperial states that consisted of a national core and overseas dependencies? Paula Michaels seems to think that it does, but does not explain why. Her essay describes the Soviet cleansing of Kazakh persons and practices, the \"subjugation of Kazakh bodies to state medical authority,\" and the denigration of \"traditional Kazakh social, cultural, and economic structures\" by a state that \"knew better than Kazakhs themselves how to care for their physical selves, their domiciles, and their villages.\" Since this sounds very much like the progress and health care that Foucault would bemoan and most modern states would promote, the question is whether there was anything specifically colonial or specifically Soviet about the way the campaign was conducted. On the colonial score, Michaels argues her case indirectly, by suggesting that the attempt by the modernizing Soviet state to civilize its citizens was also a \"Russian effort to reshape Kazakhs in their own image,\" a crosscultural encounter between \"Russian civilization\" and Kazakh life. One could quibble that the anonymous doctor from the essay's opening lines might have been just as likely to pine for the Belorussian marshes, Caucasian peaks, or Ukrainian steppes as for the \"dense birch forests\" of Central Russia, or that the sinister-named agents of Russian cleanliness Minlos and Akodus were ably assisted by Nurpeisov, Tleugabylov, and Salim, but the general point seems valid and potentially very productive. Michaels does not pursue it, however, stating elsewhere that \"the experiences of Kazakhstan and Central Asia mirror that of other regions, including rural Russia.\" This weakens her argument somewhat, it seems to me-for if medical authorities used the same \"tools of empire\" to sanitize Kazakh nomads and Slavic peasants, then what is the utility of ''colonial empire\" as a comparative framework? One way to answer this question is to argue that mutual perceptions are as good a gauge of colonialism as the behavior of the medical missionaries or the shape of the state that bred them. Would Minlos and Akodus have described Tambov backwardness differently? Did most Kazakhs regard the alien healers as \"Russian\"? Would a \"yes\" on both counts strengthen the colonial argument? I expect so, but I would like to see more evidence.\n\n==\n\n\u0415\u043a\u0435\u043b\u044c\u0447\u0438\u043a 2015:\n\nTitle: \u0423\u043a\u0440\u0430\u0438\u043d\u0441\u043a\u0430\u044f \u0418\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u041f\u0430\u043c\u044f\u0442\u044c \u0438 \u0421\u043e\u0432\u0435\u0442\u0441\u043a\u0438\u0439 \u041a\u0430\u043d\u043e\u043d: \u041a\u0430\u043a \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u043b\u043e\u0441\u044c \u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0435 \u041d\u0430\u0441\u043b\u0435\u0434\u0438\u0435 \u0423\u043a\u0440\u0430\u0438\u043d\u044b \u0432 \u0421\u0442\u0430\u043b\u0438\u043d\u0441\u043a\u0443\u044e \u042d\u043f\u043e\u0445\u0443\n\nAbstract: SUMMARY: Serhy Yekelchyk\u2019s article proceeds from the assumption that an imperial state creates room for the articulation of ethnic difference rather than pursues some kind of an assimilationist project. Moreover, the imperial rule necessitates the development of homogenizing and essentializing devices such as \u201cIndia\u201d or \u201cUkraine\u201d that are useful both for the imperial definitions of what or who is being ruled and for the indigenous elites. The author argues that understanding Stalinist historical memory as a subspecies of an imperial discourse allows us to make sense of the hierarchy of many national pasts within it. Moreover, such an approach also throws some new light on the question of agency in the Stalinist cultural production. In the article, bureaucrats and intellectuals in the republics who interpreted the vague yet powerful signals from the Kremlin emerge as major players in the shaping of the Stalinist historical imagination. In his assessment of the role of this social group, Yekelchyk relies on the insights from post-colonial theory, which, in his view, is particularly helpful for making sense of the limits and possibilities in the promotion of non-Russian historical memory under Stalinism. Yekelchyk shows that the Stalinist project of memory was disadvantaged from the beginning by the state\u2019s inability to control individual interpretations of historical narratives. But this was not its only problem. When in the 1930s the Stalinist USSR became the self-conscious successor of the Russian Empire, it had to incorporate into its narrative the story of tsarist conquests and territorial acquisitions. However, it never quite reconciled this narrative with the previous notion of \u201cclass history\u201d or with the separate historical mythologies of the non-Russian peoples. In addition, residual counter-memories of the pre-Bolshevik nationalist historical narratives survived in Ukraine well after World War II. The German occupation further undermined the Soviet authorities\u2019 control over public memory. The Kremlin sought to prescribe and homogenize social memory, but internal tensions within the Stalinist historical narrative and their inability to prescribe only one possible reading of cultural products undermined their efforts. The authorities could not fix the meaning of the past, from which the Soviet nations supposedly got their sense of orientation for the future. In the end, the Stalinist empire of memory was kept together by state intimidation \u2013 and began disintegrating as soon as the threat of political violence was removed. Yekelchyk develops these arguments in the sections of the article dedicated to the post-war pantheon of \u201cUkrainian classics\u201d, Sovietization of Ukranian museums and the mnemo-projects of monuments and memorials.\n\n==\n\nGrachev 2022:\n\nTitle: The Soviet Union as an Empire: On the Issue of the Political and Legal Nature of the Territorial Structure of the USSR\n\nAbstract: Introduction: despite the fact that the modern Russian Federation is the legal successor of the USSR and historically has grown out of it, the necessary theoretical analysis of the Soviet statehood does not occur in modern social sciences. This also applies to the problem of the political and territorial structure of the Soviet Union, about the form of which there is a significant range of opinions in the literature, often diametrically opposed. The subject of the paper is to find the most accurate methods of studying the nature of the territorial organization of the Soviet state, and its purpose is to determine on this basis an adequate state form of the USSR. Methods: the methodological framework for the work is a set of methods of scientific cognition, among which the dialectical and historical-legal methods and the civilizational approach are of key importance in the systematic analysis of the basic conditions of education and trends in the evolution of the Soviet statehood. The results of the study show that when searching for the optimal model of the state form, especially in a revolutionary situation, it is impossible to dogmatically oppose democracy to dictatorship, unitarianism to federalism, centralization to autonomy, etc., which was well demonstrated by the leadership of the Bolshevik Party by creating an updated version of the giant Eurasian (Russian) continental empire of the socialist type. Conclusions: the Soviet Union possessed a sufficient set of political and legal features that allow it to be attributed to empires. Therefore, looking in many external parameters as a historical break with the St. Petersburg Empire, the Soviet state that replaced it, from the point of view of the logic of Russian history and its political and legal content, was a necessary link in the evolution of the Russian statehood, representing a completely natural phase of its development and having at its core an imperial geopolitical nature, genetically characteristic of the Russian civilization.\n\n==\n\nRieber 1994:\n\nTitle: Russian Imperialism: Popular, Emblematic, Ambiguous\n\nAbstract: T hese three richly suggestive essays give strong and hopeful promise that future studies of Russian imperialism will be informed as much by culture as by diplomatic and military history. By their substance and eloquence they challenge the traditional reliance on geopolitics as the dominant explanatory model of Russian foreign policy. At the very least they contribute to the modest body of scholarly literature that proposes \"geoculture\" as an alternative way of analyzing the dramatic expansion and the no less dramatic contraction of the Russian state over the past five centuries. In addition, they explore and contrast two dimensions of imperialism, the universalistor European-and the particularist-or Russian. Finally, they remind us of the importance of making analytical distinctions between the self-perception of the imperialists and the critical perception of the scholar. The overall effect, subtle but compelling, is to question once again the hoary dualism of Westernizer and Slavophile as a useful intellectual framework in which to compose cultural history. In every modern state imperialism shares a commonality of motives: the search for markets and raw materials, the assumption of greator world-power status, the consolidation of real or imagined security interests and the fulfillment of a national mission. But in each case the definition of the national mission must differ, reflecting the peculiarities of the dominant culture of the metropolitan center. Manifest destiny, the white man's burden, la mission civilisatrice, the contest between Kultur and Zivilization-yes, and even the Greater East Asian Co-Prosperity Sphere-were so many forms of shorthand, slogans, to legitimize continental and overseas expansion, simplified like all shorthand, but capturing essentials of the real phenomenon. Russia lacked a popular and catchy phrase to capture the essence of its imperialism. Pushkin's narodnost' might have worked if the term had not been expropriated and vulgarized by Uvarov and Nicholas I. But this absence of a defining term should not blind us, as these essays demonstrate, to the complex cultural traits that informed Russian expansion. Modern Russian imperialism was no more or less particularist than that of the United States, Great Britain, France, Germany or Japan. It was cast in a different cultural mold; the problem is to explain the difference.\n\n==\n\nShamakhov 2021:\n\nTitle: Why the Reasons for the Collapse of the USSR Should Be Sought Before the Formation of the USSR. Article Two. The Strategic Error of Soviet Federalization as a Reflection on the Imperial Structure\n\nAbstract: After the victory in the civil war, the question of the organization of power arose not only in the political, but also in the territorial-political sphere. For a long time, the Bolsheviks positioned the empire as a \u201cprison of peoples\u201d. After 1920 (Crimea), an adequate answer was needed from the authorities. The old party slogan influenced constitutional construction, and, to the Gorbachev era, the country came with the \u201csleeping norm\u201d of the right to leave the Union Republic from the USSR. \u043a \u043d\u0435\u0436\u0435\u043b\u0430\u0442\u0435\u043b\u044c-\u043d\u044b\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u00bb. (\u0418\u0437\u0432\u0435\u0441\u0442\u0438\u044f \u0412\u0426\u0418\u041a \u0421\u043e\u0432\u0435\u0442\u043e\u0432 \u043a\u0440\u0435\u0441\u0442\u044c\u044f\u043d\u0441\u043a\u0438\u0445, \u0440\u0430\u0431\u043e\u0447\u0438\u0445, \u0441\u043e\u043b\u0434\u0430\u0442\u0441\u043a\u0438\u0445 \u043a\u0430\u0437\u0430\u0447\u044c\u0438\u0445 \u0434\u0435\u043f\u0443\u0442\u0430\u0442\u043e\u0432, 1918, 29 \u043c\u0430\u0440\u0442\u0430). \u041a \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u043c \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430\u043c\n\n==\n\nLieven 1995:\n\nTitle: The Russian Empire and the Soviet Union as Imperial Polities\n\nAbstract: trate on the external aspects of empire, investigating the roots of expansionism, for instance, or the military and economic sinews of power.2 Others have looked at the domestic constitutions of empires, analysing, for example, the management of multi-ethnicity.3 One major problem is that 'empire' in the contemporary world is a word with very strong negative connotations. A century ago, European countries and their rulers welcomed the term. It implied not only that a country was powerful but also, probably, that it was in the forefront of progress, one of that very small group of great powers entrusted, in Hegelian terms, with leading mankind towards higher levels of culture, wealth and freedom. Flattering comparisons were made with the great civilizations of the past, almost all of them embodied in political terms in empires. In the late twentieth century, however, empire implies exploitation of weak communities by stronger ones, as well, particularly, as the suppression of the Third World by Western power and culture. Empire is seen, moreover, not merely as wicked but also as anachronistic and doomed to disappear. In the post-1945 global economy, autarchy and physical control of territory are viewed as largely irrelevant: peaceful access to food, raw materials and markets seems assured, and the skills, unity and motivation of a country's workforce are regarded as the keys to prosperity and even power. Countries are unwilling to take upon themselves\n\n==\n\nRisch 2015:\n\nTitle: A Soviet West: nationhood, regionalism, and empire in the annexed western borderlands\n\nAbstract: This article considers the role the Soviet Union's western borderlands annexed during World War II played in the evolution of Soviet politics of empire. Using the Baltic Republics and Western Ukraine as case studies, it argues that Sovietization had a profound impact on these borderlands, integrating them into a larger Soviet polity. However, guerrilla warfare and Soviet policy-making indirectly led to these regions becoming perceived as more Western and nationalist than other parts of the Soviet Union. The Baltic Republics and Western Ukraine differed in their engagement with the Western capitalist world. Different experiences of World War II and late Stalinism and contacts with the West ultimately led to this region becoming Soviet, yet different from the rest of the Soviet Union. While the Soviet West was far from uniform, perceived differences between it and the rest of the Soviet Union justified claims at the end of the 1980s that the Soviet Union was an empire rather than a family of nations.\n\n==\n\nMartin 2001:\n\nTitle: The Affirmative Action Empire: Nations and Nationalism in the Soviet Union, 1923-1939\n\nAbstract: The Soviet Union was the first of Europe's multiethnic states to confront the rising tide of nationalism by systematically promoting the national consciousness of its ethnic minorities and establishing for them many of the institutional forms characteristic of the modern nation-state. In the 1920s, the Bolshevik government, seeking to defuse nationalist sentiment, created tens of thousands of national territories. It trained new national leaders, established national languages, and financed the production of national-language cultural products.This was a massive and fascinating historical experiment in governing a multiethnic state. Terry Martin provides a comprehensive survey and interpretation, based on newly available archival sources, of the Soviet management of the nationalities question. He traces the conflicts and tensions created by the geographic definition of national territories, the establishment of dozens of official national languages, and the world's first mass \"affirmative action\" programs. Martin examines the contradictions inherent in the Soviet nationality policy, which sought simultaneously to foster the growth of national consciousness among its minority populations while dictating the exact content of their cultures; to sponsor national liberation movements in neighboring countries, while eliminating all foreign influence on the Soviet Union's many diaspora nationalities. Martin explores the political logic of Stalin's policies as he responded to a perceived threat to Soviet unity in the 1930s by re-establishing the Russians as the state's leading nationality and deporting numerous \"enemy nations.\"",
        "final_summary": "The USSR's imperialist nature is explored through various lenses in these papers. Slezkine (2000) posits the USSR as an empire, emphasizing the state's dominance over Kazakh culture. \u0415\u043a\u0435\u043b\u044c\u0447\u0438\u043a (2015) delves into the role of Stalinist historical memory as an imperial discourse, highlighting the influence of bureaucrats and intellectuals in its formation. Grachev (2022) asserts that the USSR exhibited political and legal characteristics of an empire, marking a natural phase in Russian statehood's evolution. Rieber (1994) challenges the conventional geopolitical interpretation of Russian imperialism, advocating for a cultural perspective. Lieven (1995) interrogates the negative connotations associated with \"empire\" and proposes a geocultural analysis of Russian expansion. Risch (2015) examines the impact of Sovietization on the western borderlands, which were perceived as more Western and nationalist than other parts of the USSR. Lastly, Martin (2001) investigates the USSR's affirmative action policies aimed at fostering national consciousness among ethnic minorities. These papers collectively provide a multifaceted understanding of the USSR's imperialist state."
    },
    {
        "query": "teaching strategies for economics and engineering at the tertiary level",
        "paper_list_string": "Chu 2017:\n\nTitle: Teaching strategies for English-medium instruction economics courses\n\nAbstract: This paper presents teaching pedagogies to help students for whom English is a foreign language learn economics in an English medium instruction (EMI) environment. We applied these strategies to introductory-level and intermediate-level economics courses at a public university in southern Taiwan. The questionnaire results indicate that innovative pedagogical methods can compensate for students' inadequate language proficiency. Tertiary students who may not have an excellent command of English can still have pleasant EMI experiences in a content-based classroom. A by-product of implementing our teaching strategies is that students also experienced statistically significant improvement in their English skills.\n\n==\n\nLi 2020:\n\nTitle: Teaching Reform of Technical Economics for Engineering Postgraduates\n\nAbstract: The core content of \"technical economics\" course teaching is the decision-making analysis and evaluation of the project, and the course teaching is the main channel to cultivate students' innovation ability. Based on the analysis of the connotation and implementation path of engineering postgraduate education, this paper points out that it is one of the best practical ways for students to obtain \"application innovation\" education which integrates innovative knowledge with practical ability training into professional curriculum teaching.\n\n==\n\nZhang 2019:\n\nTitle: Teaching Reform and Design of Engineering Economics\n\nAbstract: Engineering economics is a course with strong application, in order to improve students' comprehensive application ability and realize the training goal of applied talents. In this paper, based on the idea of collaborative education, a model of multi-dimensional collaborative teaching model is constructed, and the model is improved by applying the model to the process of teaching practice. In the teaching practice, the teaching methods and means have been reformed, and the teaching design has been systematically combed. Finally, the problems and solutions in the application practice are put forward. Through the study of this paper, in order to promote the reform of personnel training model, promote the construction of the curriculum system.\n\n==\n\nKorez-Vide 2020:\n\nTitle: Teaching Methods for Economics and Business Sciences: Proceedings of the 3rd International Scientific Conference\n\nAbstract: The author of the first paper explores how students perceive various types of active learning techniques. The second paper identifies the teaching methods that are accepted by Generation Y students. The author of the third paper addresses the characteristics of contemporary students and their implications for teaching strategies. The fourth paper explores the effects of interdisciplinary problem-based learning on the development of students' knowledge, skills, and competences. Gamification as a teaching strategy is discussed in the fifth paper. The sixth paper addresses the challenge of merging disciplines of entrepreneurship and finance in one study program. The approaches to increasing students\u2019 focus in learning finance-related subjects are discussed in the seventh paper. In the eighth paper authors expose the value of practical higher education as a response to the perceived phenomena of financial illiteracy and financial exclusion. The role of research results transfer into university teaching of transport economics discusses the author of the ninth paper. The authors of tenth paper explore the relevance of mastering Spanish for students of economics and business disciplines. The last paper discusses the authors' experiences with e-learning in higher education from the perspective of its extent and importance.\n\n==\n\nKrotenko 2023:\n\nTitle: Engineering economics and engineering economics education strategy\n\nAbstract: \u00a0 \u00a0Acceleration of scientific and technological progress requires a new approach to the training and retraining of talent engaged in the real sector of the economy, which is particularly true of the economy\u2019s elite \u2013 engineering personnel. Globalization imposes new requirements with regard to the quality of modern engineers\u2019 education. Russian engineers are confronted with the task of carrying out technological modernisation of the industry and entering new markets with new high-tech products. This necessitates a conceptualization of the cultural traditions, philosophical and historical foundations of modern engineering education, which makes the need to create a new type of educational environment extremely obvious. The article substantiates the creation and implementation of a new strategy for continuous engineering education based on transdisciplinarity and the combination of engineering education with economic and linguistic knowledge as well as knowledge in the field of information technology.\n\n==\n\nBurtner 1998:\n\nTitle: Using Collaborative Learning Techniques To Integrate Economics And Engineering Economy At The Freshman Level\n\nAbstract: Mercer University\u2019s School of Engineering has taken the opportunity provided by the transition from the quarter system to the semester system to engage in a two-year-long curriculum renewal effort. As a result of this effort, the industrial engineering department is responsible for designing an integrated economics/engineering economy course to be taught at the freshman level. In keeping with the school\u2019s philosophy of an integrated curriculum, the course also emphasizes students\u2019 computer and communication skills. To teach such an integrated course, especially at the freshman level, we needed to redesign our traditional engineering economy course. We decided to incorporate collaborative learning strategies into our revised course; thus, the course features a mix of traditional lectures and group learning assignments. This paper describes the development of this new course . INTRODUCTION As part of the redesigned engineering core curriculum, the engineering economics course, traditionally taught at the junior level, has become a part of the freshman year curriculum. Furthermore, the microeconomics that had been a separate quarter-long required course taught by the business school has been integrated into the new semester-long course. These two changes in the engineering curriculum have led to the need to seriously reexamine the way the engineering economics course is taught. We have made three major changes to the course structure. First, the new course is team-taught. Business school faculty teach the microeconomics portion and engineering school faculty teach the engineering economy portion. Second, the new semester-long course deals with subject matter previously included in two quarter-long courses; therefore we eliminated some topics that were included in the original courses. Third, we designed the new course around the needs of the typical college freshman. Many freshmen have not yet learned the benefits of studying together as a group. Unlike juniors, freshman students are typically still developing their time management and study habit skills. We believe that designing the course based on collaborative learning principles will increase the students\u2019 chances for success. Studies conducted over a number of years have shown that the cooperative learning approach is very effective in a wide variety of educational environments. 1 A recent Prism article 2 describes the following benefits of cooperative learning at the higher education level: \u2022 Higher levels of achievement \u2022 Greater level of academic self-esteem \u2022 Creation of more sophisticated solutions to complex problems \u2022 More positive relationships P ge 309.1 Two recent books discuss general ways to apply collaborative learning principles in the engineering classroom. 3,4 In addition, several authors have written articles which describe the use of collaborative learning principles in specific engineering courses. Felder\u2019s 5 articles detail his successful experiences over a number of years. Yokomoto and Ware 6 present data on the effective use of group quizzes in a sophomore level electronics course. Sears 7 gives a detailed description of the implementation of collaborative learning in a freshman level electronics class. Jones and Brickner 8 report on the use of cooperative learning in a large-enrollment statics class; Howell 9 gives a detailed description of the implementation of cooperative learning in a dynamics class. However, there is some indication that a number of faculty members have not yet incorporated these active learning and group techniques in their engineering economy classrooms. In a recent survey, 10 fewer than half of the respondents indicated that they use groups (formal or informal) in their engineering economy classes. On the other hand, many of us have used some collaborative learning techniques in the engineering economy classroom and would like to spend time developing more such exercises. Since the literature indicates that the potential benefits of teaching in a cooperative learning environment are worth the substantial time investment, we will use these techniques extensively in our new course. IMPLEMENTATION PLAN Having decided to use cooperative techniques in this course, we needed an implementation plan; we adopted a three-phase implementation procedure. Phase I Develop a departmental library of collaborative learning books and articles Motivate other instructors as to benefits of collaborative learning Incorporate collaborative learning principles into the basic course structure Prepare collaborative learning exercises for a small number of classes Develop preliminary assessment/feedback forms for students and faculty Develop a long-term assessment plan Initiate documentation procedures Explore possible funding sources for curricular development\n\n==\n\nGrasman 2002:\n\nTitle: Teaching Engineering Economics Via Distance Education\n\nAbstract: Driven by demand for distance education, the instructor lecturing to a room of students is replaced by a \u201cvirtual classroom\u201d. This new method of delivery, coupled with the necessity for unique course administration procedures, requires a teaching approach that differs significantly from that of a traditional classroom. This paper outlines demonstrated challenges with learning and teaching via streaming video over the internet as they relate to teaching a technical class such as engineering economics. Effective communication both in and out of the classroom is discussed, along with administrative issues such as remote submittal of assignments and administration of exams. Finally, the paper presents a recommended approach to managing such issues so that instructors may provide a student-friendly learning environment.\n\n==\n\nGlebova 2020:\n\nTitle: Features of Teaching Economic Disciplines to Students of Engineering Specialties\n\nAbstract: the authors raise the problem of teaching economic disciplines to students of engineering specialties; prove the necessity and effectiveness of using modern interactive teaching methods, namely, business computer games. The article proposes five formats for conducting training sessions using business computer games, and evaluates the effectiveness of such classes.",
        "final_summary": "The papers collectively provide insights into teaching strategies for economics and engineering at the tertiary level. Chu (2017) suggests that innovative pedagogical methods can compensate for students' inadequate language proficiency in English-medium instruction economics courses. Li (2020) emphasizes the integration of innovative knowledge and practical ability training in technical economics teaching for engineering postgraduates. Zhang (2019) proposes a multi-dimensional collaborative teaching model to enhance students' comprehensive application ability in engineering economics. Korez-Vide (2020) discusses a variety of teaching methods for economics and business sciences, including but not limited to active learning techniques, gamification, and practical higher education. Krotenko (2023) highlights the need for a new strategy for continuous engineering education that combines engineering, economic, linguistic, and information technology knowledge. Burtner (1998) explores the use of collaborative learning techniques to integrate economics and engineering economy at the freshman level. Grasman (2002) addresses the challenges and recommended approaches for teaching engineering economics via distance education. Glebova (2020) advocates for the use of modern interactive teaching methods, such as business computer games, in teaching economic disciplines to engineering students."
    },
    {
        "query": "I want a list of Flour recommendations in terms of nutritional composition and labeling Answe until 2019r up",
        "paper_list_string": "Babarinde 2020:\n\nTitle: Nutritional composition of gluten-free flour from blend of fonio (Digitaria iburua) and pigeon pea (Cajanus cajan) and its suitability for breakfast food\n\nAbstract: The promotion and enrichment of underutilized cereal based foods with legumes and oilseeds are receiving considerable attention in order to reduce the menace of protein and micronutrients malnutrition. This research therefore investigated the quality of flour produced from fonio (Digitaria iburua) and pigeon pea (Cajanus cajan) blend. Fonio and pigeon pea flour blends (100:0, 95:5, 90:10, 85:15 and 80:20 of fonio to pigeon pea) were analyzed for proximate, vitamin, mineral elements and amino acids. The flour blend with highest level of fiber, protein, ash and some essential amino acids (80:20 fonio to pigeon pea) and 100% fonio were developed into breakfast food and sensory attributes such as colour, taste, flavour and overall acceptability were evaluated. The results obtained were moisture (6.74\u20137.78%), protein (12.19\u201324.85%), fat (0.98\u20131.25%), crude fibre (1.03\u20131.20%), ash (0.58\u20131.03%), carbohydrates (63.69\u201377.77%) and energy (363.09\u2013371.53 kcal/100 g). Eighteen amino acids comprising essential and non essential amino acids were identified in the flour samples. The essential amino acids were phenylalanine, histidine, isoleucine, leucine, lysine, methionine, threonine, tryptophan and valine. Vitamins identified in the samples were A, B1, B2, B5, B6, B9, C, D, E and K. Significant amount of mineral elements were also recorded. The result of this study revealed that substitution of fonio grain with pigeon pea increased the protein, ash, some amino acids and vitamins of the flour blends. Sensory evaluation of all the attributes of the breakfast food ranked above like-moderately on the 9-point hedonic scale. The flour mixes can be used in the production of breakfast food.\n\n==\n\nHager 2012:\n\nTitle: Nutritional properties and ultra-structure of commercial gluten free flours from different botanical sources compared to wheat flours\n\nAbstract: Coeliac patients suffer from an immune mediated disease, triggered by the ingestion of a protein composite (gluten) found in wheat, rye and barley. Consequently, there is a need for products such as bread or pasta, made from alternative cereal grains or pseudocereals. A fair proportion of the gluten free products currently on the market are nutritionally inadequate. Hence, it was the aim of this study to investigate the nutrient composition of seven commonly used commercial gluten free flours (oat, rice, sorghum, maize, teff, buckwheat and quinoa) and compare them to wheat and wholemeal wheat flour. In addition to the levels of all major compounds, also mineral composition, fatty acid profile, phytate, polyphenols and folate content were determined. Furthermore, properties of carbohydrates were studied in greater detail, looking at total and damaged starch levels; total, soluble and insoluble dietary fibre content as well as amylose/amylopectin ratio. Proteins were further investigated by means of capillary electrophoreses. Additionally, the ultra-structure of these materials was explored using scanning electron microscopy. The results show that maize and rice flour are poor regarding their nutritional value (low protein, fibre, folate contents). In contrast, teff as well as the pseudocereals quinoa and buckwheat show a favourable fatty acid composition and are high in protein and folate. In particular, quinoa and teff are characterised by high fibre content and are high in calcium, magnesium and iron. Therefore these flours represent nutrient-dense raw materials for the production of gluten free foods.\n\n==\n\nCule\u021bu 2021:\n\nTitle: Nutritional and Functional Properties of Gluten-Free Flours\n\nAbstract: This study characterized and compared 13 gluten-free (GF) flours (rice, brown rice, maize, oat, millet, teff, amaranth, buckwheat, quinoa, chickpea, gram, tiger nut, and plantain) for their nutritional and functional properties. For all GF flours investigated, starch was the major component, except for gram, chickpea, and tiger nut flours with lower starch content (<45%), but higher fiber content (8.8\u201335.4%). The higher amount of calcium, magnesium, zinc, potassium, phosphorus, similar values for iron and lower content of sodium in gram, makes this flour a good alternative to chickpea or other GF flour to develop healthier food products. Amaranth flour had a high protein digestibility, while tiger nut and millet flours were less digestible. Gram, chickpea, quinoa, buckwheat, and oat flours fulfilled amino acids recommendation for daily adult intake showing no limiting amino acid. Total polyphenolic content and antioxidant capacity showed higher values for buckwheat, followed by quinoa and maize flours. Gram, chickpea, maize, and quinoa flours are good candidates to improve health conditions due to lower saturated fatty acid content. The findings of this study provide useful insights into GF flours and may contribute to the development of novel gluten-free products like bread, cookies, or pasta.\n\n==\n\nCule\u021bu 2021:\n\nTitle: Nutritional and Functional Properties of Gluten-Free Flours\n\nAbstract: This study characterized and compared 13 gluten-free (GF) flours (rice, brown rice, maize, oat, millet, teff, amaranth, buckwheat, quinoa, chickpea, gram, tiger nut, and plantain) for their nutritional and functional properties. For all GF flours investigated, starch was the major component, except for gram, chickpea, and tiger nut flours with lower starch content (<45%), but higher fiber content (8.8\u201335.4%). The higher amount of calcium, magnesium, zinc, potassium, phosphorus, similar values for iron and lower content of sodium in gram, makes this flour a good alternative to chickpea or other GF flour to develop healthier food products. Amaranth flour had a high protein digestibility, while tiger nut and millet flours were less digestible. Gram, chickpea, quinoa, buckwheat, and oat flours fulfilled amino acids recommendation for daily adult intake showing no limiting amino acid. Total polyphenolic content and antioxidant capacity showed higher values for buckwheat, followed by quinoa and maize flours. Gram, chickpea, maize, and quinoa flours are good candidates to improve health conditions due to lower saturated fatty acid content. The findings of this study provide useful insights into GF flours and may contribute to the development of novel gluten-free products like bread, cookies, or pasta.\n\n==\n\nKadam 2011:\n\nTitle: Development and evaluation of composite flour for missi roti/chapatti.\n\nAbstract: The present research work on development and evaluation of composite flour for missi roti / chapati were carried out to develop the nutritious flours from various food commodities (wheat flour, chickpea, and soybean and methi leaves powder) to make good quality of chapaties. Four types of bends were acceptable. They were made from wheat flour, chickpea, full fat soy flour and methi powder in different ratio viz; \u2018A\u2019 wheat flour: chickpea flour (80:20). \u2018B\u2019 wheat flour: fullfat soy flour (90:10) \u2018C\u2019 wheat flour: chickpea flour: soy flour (80:10:10) and \u2018D\u2019 wheat flour: chickpea flour: soy flour: methi leaves powder (75: 10: 10: 05). The proximate composition of blends used for preparation of various products contained higher amount of protein and other nutrients. They contained proteins (11.8 to 15.37%), fat (1.53 to 3.45%), fibre (1.24 to 2.05%), ash (2.08 to 2.70%) and carbohydrates (65.99 to 74.2%). These results showed that soy flour / chickpea flour alone or in combination, both increased the amount of protein significantly. Calcium, phosphorus and iron were found to increase on supplementation of chickpea, soy flour and methi powder. Iron was high in methi supplemented blend. All these blended flours were found to have good sensory quality characteristics of products as control. All these blended flours could be well stored in polyethylene bags or tin boxes for the period of 3 months without any deterioration of quality. The supplementation of 5% methi powder increased the nutritional quality of flour particularly in minerals (calcium and iron) and fibres.\n\n==\n\nSaeid 2015:\n\nTitle: Comparative studies on nutritional quality of commercial wheat flour in Bangladesh\n\nAbstract: Cereal industry and its derived products have a large economic and social importance worldwide. Baking industries are extensively using wheat flour to produce various baked products. Therefore, in this investigation it is intended to study and determine proximate, physicochemical, functional properties of six brand of wheat flour in Bangladesh. The entire brand had moisture contents: 9.90-12.48%, protein: 8.67-12.47%, fat: 0.893-1.387% and ash content: 0.387-0.707%. The ACI brand had the highest amount of protein of 12.47% whereas Romoni brand had highest fat of 1.387%, fiber: 0.26%, gluten content: 15.34% and water absorption index of 1.9 g/g as compared to other brands. In addition, sodium and calcium content were higher in ACI brand whereas potassium content was higher in Romoni brand. Results showed that wheat flour from all brand could be used to make various type of baked products. Bangladesh J. Sci. Ind. Res. 50(3), 181-188, 2015\n\n==\n\nVr\u010dek 2014:\n\nTitle: A comparison of the nutritional value and food safety of organically and conventionally produced wheat flours.\n\nAbstract: Growing interest in organic agriculture has prompted this study aiming to evaluate nutritional content of wheat flours originating from organic and conventional production systems. Obtained results showed that organic samples had significantly lower protein content and lower levels of Ca, Mn and Fe compared to conventional samples. Protein digestibility and levels of K, Zn and Mo were significantly higher in organic than in conventional wheat flours. Regarding undesirable metals, significantly higher levels of As and Cd were found in conventional compared to organic wheat flours. Although the mean concentrations of zearalenone and ochratoxin A were higher in conventional than in organic flours, this difference was not significant. This study revealed that organic agriculture has the potential to yield products with some relevant improvements in terms of high quality proteins and microelements contents, while the reduction in contamination with toxic elements and mycotoxins may be accomplished.\n\n==\n\nPatil 2017:\n\nTitle: Nutritional, functional, phytochemical and structural characterization of gluten-free flours\n\nAbstract: Different gluten-free flours from various sources may vary in composition and properties, thereby affecting applications in food systems. Various gluten-free flours viz. sorghum, rice, moong, water chestnut flour (WCF) and unripe banana flour (UBF) were studied in comparison with wheat flour. These were studied for their proximate composition, pH, iodine affinity value, leaching, functional properties, phytochemical analysis, color analysis, and X-ray diffraction analysis. Protein content in these flours ranged from 5.31 to 24.07%moong flour having highest protein content. Variations were observed with fat percentage from 1.01 to 3.45%. All flours were high in carbohydrate content i.e. ranged between 61.63 to 81.19%. pH values indicated slight acidic nature of all flours. UBF was found to have maximum iodine affinity value (46086\u00a0ppm). Leaching study indicated the extent of leaching with respect to time in these flours. Light transmittance was decreased with the duration of cooking for all flours. Color analysis values showed the difference in the L*, a, b values. X-ray diffractograms revealed the crystallinity patterns of the flours with percentage crystallinity which ranged from 22.6 to 34.9.This study concludes that wheat can be substituted by these flours in various applications.",
        "final_summary": "The papers collectively provide insights into the nutritional composition and labeling recommendations for various flours.\n\nBabarinde 2020 found that blending fonio and pigeon pea flour increased fiber, protein, ash, and essential amino acids in the flour. The sensory evaluation of breakfast food made from these blends ranked positively.\n\nHager 2012 compared the nutrient composition of commercial gluten-free flours from different botanical sources to wheat flour. They found that maize and rice flour had lower nutritional value, while teff, quinoa, and buckwheat had favorable fatty acid composition, high protein, and folate content.\n\nCule\u021bu 2021 characterized and compared 13 gluten-free flours for their nutritional and functional properties. They found that gram, chickpea, quinoa, buckwheat, and oat flours fulfilled amino acid recommendations and had high polyphenolic content and antioxidant capacity.\n\nKadam 2011 developed composite flours for missi roti/chapatti and found that blends containing soy flour, chickpea flour, and methi leaves powder increased protein, fiber, and mineral content.\n\nSaeid 2015 studied the nutritional quality of commercial wheat flour in Bangladesh and found variations in protein, fat, fiber, and mineral content among different brands.\n\nVr\u010dek 2014 compared the nutritional value and food safety of organically and conventionally produced wheat flours. Organic flours had lower protein content but higher protein digestibility and levels of potassium, zinc, and molybdenum. Conventional flours had higher levels of toxic elements.\n\nPatil 2017 characterized gluten-free flours from various sources and found variations in protein, fat, carbohydrate, pH, iodine affinity value, functional properties, color, and crystallinity patterns.\n\nIn summary, the papers provide recommendations for flour selection based on nutritional composition, including protein, fiber, essential amino acids, minerals, and functional properties."
    },
    {
        "query": "NEED FOR INSTITUTIONAL MEDICAL RESEARCH JOURNALS",
        "paper_list_string": "Mufti 2015:\n\nTitle: NEED FOR INSTITUTIONAL MEDICAL RESEARCH JOURNALS\n\nAbstract: Research publications in Pakistan have grown by 54% from year 2008 to 2012.\u00a0Because of increased research interest related to increased postgraduate programs in medical universities, the pressure on existing medical journals has also grown leading to long waiting queues for publication. The online submissions to journals has created further difficulty where the novice writer does not have an off the record chance to reform his manuscript. The institutions publishing their own journals have the relevant human resource, who in addition to improving the capacity building of medical writing, motivate their institutional colleagues. The statistical and bibliographic help that is ever so important in write-up may also be available readily. In case of submission to the journals outside institutions, researchers in Pakistan may face high rejection or deferral rate.\n\n==\n\nSteynberg 1993:\n\nTitle: The availability of research journals in South African academic medical libraries.\n\nAbstract: Biomedical researchers depend on the journals of science as a primary source of information. The spiralling cost of journals threatens the ability of libraries to provide their users' information needs. In this study the availability of a representative sample of journals used by South African biomedical researchers was determined at each of the seven medical faculty libraries. The application of a standardised document delivery test is described and the results are interpreted in terms of: (i) the capability index, which includes material obtainable through inter-library loans; and (ii) an availability index, or measure of the probability that a user will find an item without delay in his/her own library. The current status of availability of biomedical journals was found to be high at all the libraries; indeed, the scores compare favourably with results obtained at academic libraries overseas. There is, however, real concern that the financial crisis in tertiary education will cause the situation to deteriorate. Repetition of this test is recommended to monitor the (probably declining) level of journal availability. The information would support efforts to rationalise and subsequently strengthen academic medical libraries' collective holdings as a strategic national resource.\n\n==\n\nAminpour 2012:\n\nTitle: The influence of Iranian scientific journals in disseminating medical information\n\nAbstract: Background: Scientific journals are the most credible and updated information resources for valid information in the various fields of science and technology. The present study investigates the status of Iranian scientific journals in disseminating medical information to the world of science. Materials and Methods: Total 163 Iranian medical journals accredited by national medical journals commission of Iranian ministry of health and medical education were evaluated through a cross-sectional study. The results were represented in descriptive statistics in the form of table and chart. Results: The study showed that 89.6% of Iranian medical journals were covered by regional information databases. Web of Science database indexed 22 (13.5%) Iranian journals in the field of medical science. Only six (6.7%) journals were indexed by Medline. Fifty-eight (35.6%) journals were in English, 102 (62.6%) in Persian, and three (1.8%) were bilingual which published their articles both in Persian and English languages. The highest Impact factor belonged to Iranian Journal of Allergy Asthma and Immunology. Conclusions: Improving scientific credibility of Iranian scholarly journals and their influence in disseminating medical information calls for a precise scientific and executive administration in publishing standards and also in the quality of content.\n\n==\n\nB 2018:\n\nTitle: [History, purposes and characteristics of medical journals].\n\nAbstract: Medical journals are fundamental elements in medical education, in professional life, in research and in academic careers. They communicate new scientific and clinical findings, legitimate and give credibility to biomedical and clinical investigation, are guarantors of the authors' intellectual property, are a useful forum for the discussion of relevant topics, distribute selected information to readers and are archives of their publications. Out of the numerous medical journals appeared before and during the XIXth Century, only 24 of them remain active until today and Revista M\u00e9dica de Chile appears as number 20 in the list. Nowadays thousands of medical journals are published worldwide but only a small fraction are qualified as \"mainstream journals\", because they strictly apply an independent editorial process with clear criteria in manuscript selection based in external peer review, format requisites according to ICMJE Recommendations, are included in major databases, publish a high proportion of research articles, and other reasons. English language predominates in mainstream journals and abstracts in English become available in some databases. Electronic publications and open access are important tools to disseminate new information but the appearance of \"predatory journals\" is a threatening menace to the integrity of medical publications. Authors should identify and reject them. Some reflections may help those who intend to create new medical journals.\n\n==\n\nIcmje 2016:\n\nTitle: International Committee of Medical Journal Editors (ICMJE): uniform requirements for journals\n\nAbstract: and\n\n==\n\nRivas-Ruiz 2019:\n\nTitle: [The importance of national medical journals in a global world].\n\nAbstract: National medical journals are important because they focus on local clinical problems, peer communication, and fostering critical reading among healthcare workers. The actors involved in the development of the journals must fulfill their purposes to improve the quality of the publications of the national journals and contribute to improve the quality of care.\n\n==\n\nAltman 2002:\n\nTitle: Poor-quality medical research: what can journals do?\n\nAbstract: The aim of medical research is to advance scientific knowledge and hence--directly or indirectly--lead to improvements in the treatment and prevention of disease. Each research project should continue systematically from previous research and feed into future research. Each project should contribute beneficially to a slowly evolving body of research. A study should not mislead; otherwise it could adversely affect clinical practice and future research. In 1994 I observed that research papers commonly contain methodological errors, report results selectively, and draw unjustified conclusions. Here I revisit the topic and suggest how journal editors can help.\n\n==\n\nHumbertoReyes 2018:\n\nTitle: [History, purposes and characteristics of medical journals].\n\nAbstract: : Medical journals are fundamental elements in medical education, in professional life, in research and in academic careers. They communicate new scientific and clinical findings, legitimate and give credibility to biomedical and clinical investigation, are guarantors of the authors' intellectual property, are a useful forum for the discussion of relevant topics, distribute selected information to readers and are archives of their publications. Out of the numerous medical journals appeared before and during the XIXth Century, only 24 of them remain active until today and Revista Medica de Chile appears as number 20 in the list. Nowadays thousands of medical journals are published worldwide but only a small fraction are qualified as \"mainstream journals\", because they strictly apply an independent editorial process with clear criteria in manuscript selection based in external peer review, format requisites according to ICMJE Recommendations, are included in major databases, publish a high proportion of research articles, and other reasons. English language predominates in mainstream journals and abstracts in English become available in some databases. Electronic publications and open access are important tools to disseminate new information but the appearance of \"predatory journals\" is a threatening menace to the integrity of medical publications. Authors should identify and reject them. Some reflections may help those who intend to create new medical journals.",
        "final_summary": "The papers suggest the need for institutional medical research journals. Mufti 2015 highlights the growing pressure on existing medical journals due to increased research interest, leading to long waiting queues for publication. Steynberg 1993 emphasizes the importance of journal availability in academic medical libraries and expresses concern about the financial crisis potentially affecting access to journals. Aminpour 2012 discusses the influence of Iranian scientific journals in disseminating medical information, calling for improvements in credibility and quality. B 2018 provides a historical perspective on the significance of medical journals in research and academic careers. Rivas-Ruiz 2019 emphasizes the importance of national medical journals in addressing local clinical problems and fostering critical reading. Altman 2002 discusses the role of journal editors in addressing poor-quality medical research. HumbertoReyes 2018 further emphasizes the importance of mainstream medical journals that adhere to rigorous editorial processes and publish high-quality research. Overall, these papers highlight the need for institutional medical research journals to address publication pressures, ensure access to information, improve credibility, and support local healthcare needs."
    },
    {
        "query": "epidermal growth factor receptors overexpression abdomen",
        "paper_list_string": "Takehana 2003:\n\nTitle: Expression of epidermal growth factor receptor in gastric carcinomas.\n\nAbstract: BACKGROUND & AIMS\nEpidermal growth factor receptor belongs to the family of type I receptor tyrosine kinases. Overexpression of epidermal growth factor receptor has been observed in a variety of cancers with or without amplification of the gene. Novel chemotherapies targeting receptor tyrosine kinases might be effective for the treatment of cancers in which overexpression of this protein is a feature. The aim of this study was to assess the potential efficacy of epidermal growth factor receptor-targeted therapy in gastric cancer. This was achieved by determining the frequency of increased epidermal growth factor receptor expression in gastric cancers and investigating the relationship between protein overexpression and gene amplification.\n\n\nMETHODS\nImmunohistochemical evaluation of 413 gastric cancers was carried out by using a monoclonal antibody to the epidermal growth factor receptor. The intensity of reactivity was scored by using a 4-tier system (negative, 1+, 2+, and 3+). All positive staining (>1+) tumors overexpressing the protein were then analyzed for gene amplification by fluorescence in situ hybridization by using a gene-specific probe.\n\n\nRESULTS\nHigh levels of overexpression (2+ or 3+ staining) were found in 9 of 413 (2.2%) patients, whereas low levels of overexpression (1+) were found in 34 (8.2%) of the study cohort. Fluorescence in situ hybridization analysis showed that more than 10 copies of the gene were recognized in all 5 cancers with 3+ staining and in 2 of the 4 tumors with 2+ staining.\n\n\nCONCLUSIONS\nAlthough a high level of overexpression of epidermal growth factor receptor is uncommon in gastric carcinomas, it almost exclusively occurs by gene amplification.\n\n==\n\nItakura 1994:\n\nTitle: Epidermal growth factor receptor overexpression in esophageal carcinoma. An immunohistochemical study correlated with clinicopathologic findings and DNA amplification\n\nAbstract: Background. Many studies have reported the increased expression of epidermal growth factor receptor (EGFR) in various human malignancies and its association with the biologic behavior of the tumors.\n\n==\n\nFriedrich 2015:\n\nTitle: Epidermal growth factor receptor overexpression is common and not correlated to gene copy number in ependymoma\n\nAbstract: PurposeThe aim of this study was to investigate the epidermal growth factor receptor (EGFR) status in ependymoma specimens, as there is a need for new prognostic and druggable targets in this disease.MethodsEpendymomas (WHO grade II, n\u00a0=\u00a040; WHO grade III, n\u00a0=\u00a015) located spinal (n\u00a0=\u00a035), infratentorial (n\u00a0=\u00a014), and supratentorial (n\u00a0=\u00a06) of 53 patients with a median age of 40 (range, 2\u201379) years were analyzed for Ki-67, p53, and EGFR expression by immunohistochemistry using a tissue microarray and for EGFR gene copy number alterations/mutations. Results were correlated to clinical data.ResultsEGFR overexpression was found in 30/60\u00a0% of ependymomas depending on the antibody used and was more pronounced in WHO grade III. High EGFR gene copy number gains were found in 6 (11\u00a0%) ependymomas with half of them being amplifications. EGFR amplified ependymomas displayed an EGFR overexpression with both antibodies in two of three cases. A missense mutation in exon 20 of EGFR (S768I) was detected in one amplified case.ConclusionsEGFR is frequently overexpressed in ependymomas. Other mechanisms than amplification of the EGFR gene appear to contribute to EGFR overexpression in most cases. EGFR mutations may be present in a small subset of ependymomas.\n\n==\n\nKhalifa 1994:\n\nTitle: Prognostic utility of epidermal growth factor receptor overexpression in endometrial adenocarcinoma\n\nAbstract: Background. Overexpression of epidermal growth factor receptor (EGFR) has been reported in endometrial adenocarcinoma.\n\n==\n\nSakai 1986:\n\nTitle: Expression of epidermal growth factor receptors on normal human gastric epithelia and gastric carcinomas.\n\nAbstract: Tissues of normal human gastric mucosae and 15 advanced gastric carcinomas were studied immunohistologically for the presence of receptors for epidermal growth factor (EGF) by use of a murine monoclonal antibody (528IgG), which reacts with the binding domain of human EGF receptor. On normal gastric mucosae, only parietal cells showed positive staining. On cancer tissues, definite staining was observed in 9 of 15 cases. Their staining intensities were variable and weaker in general compared to those of either gastric parietal cells or normal tonsilar squamous epithelium. No apparent correlation of EGF receptor staining with the grade of histologic differentiation or lymph node metastases of these gastric carcinomas was noted.\n\n==\n\nKalyankrishna 2006:\n\nTitle: Epidermal growth factor receptor biology in head and neck cancer.\n\nAbstract: Epidermal growth factor receptor (EGFR) is overexpressed in several epithelial malignancies, including head and neck squamous cell carcinoma (HNSCC), which exhibits EGFR overexpression in up to 90% of tumors. EGFR ligands such as transforming growth factor alpha are also overexpressed in HNSCC. EGFR plays a critical role in HNSCC growth, invasion, metastasis and angiogenesis. However, EGFR inhibitors as monotherapy have yielded only modest clinical outcomes. Potential mechanisms for lack of response to EGFR inhibition in HNSCC include constitutive activation of signaling pathways independent of EGFR, as well as genetic aberrations causing dysregulation of the cell cycle. EGFR-directed therapy may be optimized by identifying and selecting those HNSCC patients most likely to benefit from EGFR inhibition. Resistance to EGFR inhibition may be circumvented by combination therapy employing EGFR inhibitors together with other treatment modalities.\n\n==\n\nGroves 1992:\n\nTitle: Abnormal expression of epidermal growth factor receptor in cutaneous epithelial tumours\n\nAbstract: Epidermal growth factor (EGF) and transforming growth factor alpha (TGF\u03b1) are important keratinocyte mitogens. Their effects are mediated by a cell membrane receptor (EGFR), quantitative and qualitative abnormalities of which may be responsible for deranged keratinocyte proliferation and differentiation. We have therefore examined EGFR expression immunohistochemically in a variety of benign and malignant epithelial neoplasms using monoclonal antibodies to the extracellular and intracellular receptor domains. In benign tumours (virus wart, seborrhoeic keratosis, keratoacanthoma), there was an ordered pattern of EGFR expression. In malignant tumours (basal and squamous cell carcinoma), there was loss of membrane labelling and cytoplasmic accumulation of the receptor. In premalignant proliferations, there was loss of membrane receptor with either absent cytoplasmic EGFR (actinic keratosis) or cytoplasmic receptor accumulation (Bowen's disease). Evidence of truncated receptors was not found. We suggest that dysregulation of the EGFR may be important in the development of cutaneous epithelial malignancies but that grossly abnormal forms of the receptor do not occur.\n\n==\n\nHerbst 2004:\n\nTitle: Review of epidermal growth factor receptor biology.\n\nAbstract: The epidermal growth factor receptor (EGFR) is a transmembrane glycoprotein that constitutes one of four members of the erbB family of tyrosine kinase receptors. Binding of EGFR to its cognate ligands leads to autophosphorylation of receptor tyrosine kinase and subsequent activation of signal transduction pathways that are involved in regulating cellular proliferation, differentiation, and survival. Although present in normal cells, EGFR is overexpressed in a variety of tumor cell lines and has been associated with poor prognosis and decreased survival. EGFR activation also plays a role in resistance to chemotherapy and radiation treatment in tumor cells. Over the past two decades, much effort has been directed at developing anticancer agents that can interfere with EGFR activity. The most common pharmacologic approaches to inhibiting EGFR have been to develop monoclonal antibodies and small-molecule inhibitors. Monoclonal antibodies block ligand binding to the extracellular domain, whereas the small-molecule inhibitors exert their effects at the intracellular portion of the receptor to prevent tyrosine kinase phosphorylation and subsequent activation of signal transduction pathways. A number of EGFR inhibitors have been developed that can arrest tumor growth and, in some cases, cause tumor regression. When used in combination with cytotoxic treatments, chemotherapy, and radiation, EGFR inhibitors have been able to potentiate their anticancer activity.",
        "final_summary": "The papers suggest that overexpression of epidermal growth factor receptors (EGFR) is observed in various types of cancers, including gastric carcinomas (Takehana 2003), esophageal carcinoma (Itakura 1994), ependymoma (Friedrich 2015), endometrial adenocarcinoma (Khalifa 1994), and cutaneous epithelial tumors (Groves 1992). However, the frequency of EGFR overexpression varies among different cancer types. Additionally, the papers highlight the potential of EGFR-targeted therapies for the treatment of cancers with EGFR overexpression (Takehana 2003). The role of EGFR in tumor growth, invasion, metastasis, and angiogenesis is emphasized (Kalyankrishna 2006). The development of EGFR inhibitors as anticancer agents is also discussed (Herbst 2004)."
    },
    {
        "query": "technological solutions, tools, or best practices for enhancing digital accessibility in the Metaverse",
        "paper_list_string": "Fernandes 2022:\n\nTitle: Accessibility in the Metaverse: Are We Prepared?\n\nAbstract: Metaverse is a new paradigm and is under construction where social, immersive Virtual Reality platforms will be compatible with several kinds of applications. However, solutions must be developed to ensure that people with disabilities access the Metaverse. This work aims to present an overview of research on accessibility of immersive systems and outline challenges and opportunities so that the Human-Computer Interaction community can reflect and intensify research in this area.\n\n==\n\nXu 2022:\n\nTitle: A Full Dive into Realizing the Edge-enabled Metaverse: Visions, Enabling Technologies, and Challenges\n\nAbstract: Dubbed \u201cthe successor to the mobile Internet\", the concept of the Metaverse has grown in popularity. While there exist lite versions of the Metaverse today, they are still far from realizing the full vision of an immersive, embodied, and interoperable Metaverse. Without addressing the issues of implementation from the communication and networking, as well as computation perspectives, the Metaverse is difficult to succeed the Internet, especially in terms of its accessibility to billions of users today. In this survey, we focus on the edge-enabled Metaverse to realize its ultimate vision. We first provide readers with a succinct tutorial of the Metaverse, an introduction to the architecture, as well as current developments. To enable the ubiquitous, seamless, and embodied access to the Metaverse, we discuss the communication and networking challenges and survey cutting-edge solutions and concepts that leverage next-generation communication systems for users to be telepresent and teleoperate in the Metaverse. Moreover, given the high computation costs required, e.g., to render immersive 3D worlds and run data hungry artificial intelligence (AI) driven applications, we discuss the computation challenges and cloud-edge-end computation framework driven solutions to realize the Metaverse on resource-constrained edge devices. Next, we explore how blockchain technologies can aid in the interoperable development of the Metaverse, not just in terms of empowering the economic circulation of virtual usergenerated contents, but also to manage physical edge resources in a decentralized, transparent, and tamper-proof manner. Finally, we discuss the future research directions towards realizing the true vision of the edge-enabled Metaverse.\n\n==\n\nXu 2022:\n\nTitle: A Full Dive Into Realizing the Edge-Enabled Metaverse: Visions, Enabling Technologies, and Challenges\n\nAbstract: Dubbed \u201cthe successor to the mobile Internet,\u201d the concept of the Metaverse has grown in popularity. While there exist lite versions of the Metaverse today, they are still far from realizing the full vision of an immersive, embodied, and interoperable Metaverse. Without addressing the issues of implementation from the communication and networking, as well as computation perspectives, the Metaverse is difficult to succeed the Internet, especially in terms of its accessibility to billions of users today. In this survey, we focus on the edge-enabled Metaverse to realize its ultimate vision. We first provide readers with a succinct tutorial of the Metaverse, an introduction to the architecture, as well as current developments. To enable ubiquitous, seamless, and embodied access to the Metaverse, we discuss the communication and networking challenges and survey cutting-edge solutions and concepts that leverage next-generation communication systems for users to immerse as and interact with embodied avatars in the Metaverse. Moreover, given the high computation costs required, e.g., to render 3D virtual worlds and run data-hungry artificial intelligence-driven avatars, we discuss the computation challenges and cloud-edge-end computation framework-driven solutions to realize the Metaverse on resource-constrained edge devices. Next, we explore how blockchain technologies can aid in the interoperable development of the Metaverse, not just in terms of empowering the economic circulation of virtual user-generated content but also to manage physical edge resources in a decentralized, transparent, and immutable manner. Finally, we discuss the future research directions towards realizing the true vision of the edge-enabled Metaverse.\n\n==\n\nJeong 2023:\n\nTitle: Establishment of Production Standards for Web-based Metaverse Content: Focusing on Accessibility and HCI\n\nAbstract: Metaverse technology is expanding to industries in various fields, such as medical, national defense, and education, and training simulation programs have been mainstream so far.\nHowever, there have been increasing attempts to apply metaverse content to web-based platforms linked to social media services and, as a result, we face the problem of access to web-based metaverse content. Unlike traditional content, metaverse content interacts with many users, so content accessibility is the first important part to consider. In other words, to maximize the quality of metaverse content, it is essential to pull out the optimal UX through a detailed HCI (human computer interaction) design. Metaverse content development methodologies have effective methods proposed by many researchers. However, they are limited to web-based metaverse content that limits the use of high-end hardware. They are ineffective for platforms such as PCs and VR devices, as most studies focus on improving the visual performance of PCs or high-performance VR devices. Therefore, unlike existing research, the key theme of our research is to study optimized development standards that can be applied to web-based metaverse content and find out their effects through experiments. We created a development standard to be applied to a Web-based platform based on the existing metaverse content development methodology. Then, we redeveloped the VR content into the metaverse content and named them the VR build and the metaverse build. We had 25 people play virtual reality builds and metaverse builds simultaneously. Then, we measured the overall experience with an evaluation tool called the Game Experience Questionnaire (GEQ); the GEQ is a proven tool for evaluating content experiences by dividing them into positive/negative scales. When comparing the results measured from the two builds, the metaverse build showed consistent results with a higher positive scale, and a lower negative scale, than the VR build. The results showed that users indeed rated metaverse content positively. The bottom line is that the web-based metaverse content development standards that we have produced are practical. However, since generalization is limited, continuous research will be needed in more experimental groups in the future.\n\n==\n\nParker 2023:\n\nTitle: Towards an Inclusive and Accessible Metaverse\n\nAbstract: The push towards a Metaverse is growing, with companies such as Meta developing their own interpretation of what it should look like. The Metaverse at its conceptual core promises to remove boundaries and borders, becoming a decentralised entity for everyone to use - forming a digital virtual layer over our own \u201creal\u201d world. However, creation of a Metaverse or \u201cnew world\u201d presents the opportunity to create one which is inclusive and accessible to all. This challenge is explored and discussed in this workshop, with an aim of understanding how to create a Metaverse which is open and inclusive to people with physical and intellectual disabilities, and how interactions can be designed in a way to minimise disadvantage. The key outcomes of this workshop outline new opportunities for improving accessibility in the Metaverse, methodologies for designing and evaluating accessibility, and key considerations for designing accessible Metaverse environments and interactions.\n\n==\n\nShpak 2015:\n\nTitle: The Metaverse as Mediator between Technology , Trends , and the Digital Transformation of Society and Business\n\nAbstract: In this reflective article we discuss the potential of recent developments in virtual worlds for offering novel value propositions for the digital transformation of society and business. In particular, we consider the role of a Metaverse, understood as a globally accessible 3D virtual space and computing infrastructure\u2014and today still a conceptual vision\u2014as a mediator between technology trends and societal and business applications. We outline how current technology trends can be linked with high-value added application scenarios through the Metaverse as a mediating design space. Our insights project both a push effect, i.e. novel technologies fostering radical shifts in society and business, as well as a pull effect, i.e. radical ideas stimulating technology developments. Leveraging both effects for creating high-value added applications however, requires an integrated, mediating design space, which can potentially be obtained through advances of virtual worlds towards a Metaverse.\n\n==\n\nChang 2022:\n\nTitle: 6G-enabled Edge AI for Metaverse: Challenges, Methods, and Future Research Directions\n\nAbstract: 6G-enabled edge intelligence opens up a new era of Internet of Everything and makes it possible to interconnect people-devices-cloud anytime, anywhere. More and more next-generation wireless network smart service applications are changing our way of life and improving our quality of life. As the hottest new form of next-generation Internet applications, Metaverse is striving to connect billions of users and create a shared world where virtual and reality merge. However, limited by resources, computing power, and sensory devices, Metaverse is still far from realizing its full vision of immersion, materialization, and interoperability. To this end, this survey aims to realize this vision through the organic integration of 6G-enabled edge AI and Metaverse. Specifically, we first introduce three new types of edge-Metaverse architectures that use 6G-enabled edge AI to solve resource and computing constraints in Metaverse. Then we summarize technical challenges that these architectures face in Metaverse and the existing solutions. Furthermore, we explore how the edge-Metaverse architecture technology helps Metaverse to interact and share digital data. Finally, we discuss future research directions to realize the true vision of Metaverse with 6G-enabled edge AI.\n\n==\n\nBouachir 2022:\n\nTitle: AI-based Blockchain for the Metaverse: Approaches and Challenges\n\nAbstract: The recent advances in AI have enabled novel approaches and remarkable enhancements in the provision of existing services and applications. The metaverse is one of the emerging applications that got the advantages of this technological progression. Due to the cooperation between several cutting-edge technologies including IoT, digital twin (DT), and big data, the metaverse highly relies on the manipulation of an immense amount of gathered data. In this context, security is a major challenge, and as such, blockchain brings several advantages to the metaverse ecosystem as it allows secure and transparent data storage. However, with the dynamic nature of such an ecosystem and the fast variation in its AI-based services, the traditional blockchain features (e.g. smart contracts and consensus protocols) will face big challenges to meet the needed dynamic service requirements and scalability. This positioning paper proposes the concept of using a blockchain governed by AI to leverage the various features and advantages of blockchain services. The paper provides a review of the role of blockchain in the metaverse, challenges that arise, and then the role of AI in building intelligent blockchain features and their impact on the metaverse ecosystem.",
        "final_summary": "The papers collectively emphasize the need for technological solutions and best practices to enhance digital accessibility in the Metaverse. Fernandes (2022) underscores the necessity of solutions for people with disabilities. Xu (2022) highlights the importance of addressing communication, networking, and computation challenges to improve accessibility. Jeong (2023) advocates for the establishment of production standards for web-based Metaverse content, focusing on accessibility and HCI. Parker (2023) explores the creation of an inclusive and accessible Metaverse, particularly for individuals with disabilities. Shpak (2015) discusses the Metaverse's potential as a mediator between technology, trends, and digital transformation, implying the need for accessibility considerations."
    },
    {
        "query": "looking for papers on which has applied portable or consumer grade EEG device and Machine learning techniques together.",
        "paper_list_string": "Fabietti 2021:\n\nTitle: On-Chip Machine Learning for Portable Systems: Application to Electroencephalography-based Brain-Computer Interfaces\n\nAbstract: The improvement of hardware for the acquisition and processing of electroencephalography (EEG) has made its portability become a reality. This allows for studies to be carried outside lab settings, as well as many commercial applications. As recordings are done over extended periods, these devices generate large volumes of data, mainly if the neuronal activity is recorded through multiple channels. Machine learning (ML) techniques allow to effectively analyse and use this data for a wide range of applications. However the portability of these techniques can be challenging. In this article, we set out to review over 40 relevant articles where ML techniques in a diverse set of EEG applications that have successfully been incorporated into portable systems.\n\n==\n\nSabio 2022:\n\nTitle: A scoping review on the use of consumer-grade EEG devices for research\n\nAbstract: BACKGROUND Commercial electroencephalography (EEG) devices have become increasingly available over the last decade. These devices have been used in a wide variety of fields ranging from engineering to cognitive neuroscience. PURPOSE The aim of this study was to chart peer-review articles that used currently available consumer-grade EEG devices to collect neural data. We provide an overview of the research conducted with these relatively more affordable and user-friendly devices. We also inform future research by exploring the current and potential scope of consumer-grade EEG. METHODS We followed a five-stage methodological framework for a scoping review that included a systematic search using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews (PRISMA-ScR) guidelines. We searched the following electronic databases: PsycINFO, MEDLINE, Embase, Web of Science, and IEEE Xplore. We charted study data according to application (BCI, experimental research, validation, signal processing, and clinical) and location of use as indexed by the first author\u2019s country. RESULTS We identified 916 studies that used data recorded with consumer-grade EEG: 531 were reported in journal articles and 385 in conference papers. Emotiv devices were most used, followed by the NeuroSky MindWave, OpenBCI, interaXon Muse, and MyndPlay Mindband. The most common use was for brain-computer interfaces, followed by experimental research, signal processing, validation, and clinical purposes. CONCLUSIONS Consumer-grade EEG has proven to be a useful tool for neuroscientific research and will likely continue to be used well into the future. Our study provides a comprehensive review of their application, as well as future directions for researchers who wish to use these devices.\n\n==\n\nOmelianenko 2017:\n\nTitle: Applying advanced machine learning models to classify electro-physiological activity of human brain for use in biometric identification\n\nAbstract: In this article we present the results of our research related to the study of correlations between specific visual stimulation and the elicited brain's electro-physiological response collected by EEG sensors from a group of participants. We will look at how the various characteristics of visual stimulation affect the measured electro-physiological response of the brain and describe the optimal parameters found that elicit a steady-state visually evoked potential (SSVEP) in certain parts of the cerebral cortex where it can be reliably perceived by the electrode of the EEG device. After that, we continue with a description of the advanced machine learning pipeline model that can perform confident classification of the collected EEG data in order to (a) reliably distinguish signal from noise (about 85% validation score) and (b) reliably distinguish between EEG records collected from different human participants (about 80% validation score). Finally, we demonstrate that the proposed method works reliably even with an inexpensive (less than $100) consumer-grade EEG sensing device and with participants who do not have previous experience with EEG technology (EEG illiterate). All this in combination opens up broad prospects for the development of new types of consumer devices, [e.g.] based on virtual reality helmets or augmented reality glasses where EEG sensor can be easily integrated. The proposed method can be used to improve an online user experience by providing [e.g.] password-less user identification for VR / AR applications. It can also find a more advanced application in intensive care units where collected EEG data can be used to classify the level of conscious awareness of patients during anesthesia or to automatically detect hardware failures by classifying the input signal as noise.\n\n==\n\nPathirana 2018:\n\nTitle: A Critical Evaluation on Low-Cost Consumer-Grade Electroencephalographic Devices\n\nAbstract: Electroencephalography (EEG) has been recognized as one the finest cost-effective techniques to measure the electrical activity of the human brain. Since the electrical activity measured at the scalp is an abstract representation of the regional brain activity of the human brain, the EEG readings (EEG data) are often useful in many aspects. The value of an EEG data is proportionate to an electrical potential measured at a specific position on the scalp using an electrode. One of the major uses of EEG data from the medical domain is detecting the brain disorders such as Epileptic Seizure. Brain Computer Interfacing (BCI) is another use of EEG data. The paper discusses the analytical procedures of EEG data acquired using consumer-grade low-cost EEG devices in order to achieve the objective of implementation of a cost-effective BCI system. Furthermore, the paper presents a critical evaluation of some selected consumer-grade EEG devices from the point of view of cost-effectiveness against the expected accuracy. We have obtained a maximum of 76.6% for user intention detection with a low-cost EEG device Emotiv's \u201cInsight\u201d. This is a remarkable achievement in a context that, the maximum accuracy reported for the EEG devices in the class was only 60.57%. We emphasize the key impact behind this achievement is the employment of the technique Multi-Agent Systems (MAS), which is an Artificial Intelligence (AI) approach, for EEG data analysis and intention classification.\n\n==\n\nHosseini 2020:\n\nTitle: A Review on Machine Learning for EEG Signal Processing in Bioengineering\n\nAbstract: Electroencephalography (EEG) has been a staple method for identifying certain health conditions in patients since its discovery. Due to the many different types of classifiers available to use, the analysis methods are also equally numerous. In this review, we will be examining specifically machine learning methods that have been developed for EEG analysis with bioengineering applications. We reviewed literature from 1988 to 2018 to capture previous and current classification methods for EEG in multiple applications. From this information, we are able to determine the overall effectiveness of each machine learning method as well as the key characteristics. We have found that all the primary methods used in machine learning have been applied in some form in EEG classification. This ranges from Naive-Bayes to Decision Tree/Random Forest, to Support Vector Machine (SVM). Supervised learning methods are on average of higher accuracy than their unsupervised counterparts. This includes SVM and KNN. While each of the methods individually is limited in their accuracy in their respective applications, there is hope that the combination of methods when implemented properly has a higher overall classification accuracy. This paper provides a comprehensive overview of Machine Learning applications used in EEG analysis. It also gives an overview of each of the methods and general applications that each is best suited to.\n\n==\n\nGoldberg 2012:\n\nTitle: The Feasibility of Consumer Grade EEG Devices as Computer Input\n\nAbstract: Electroencephalography (EEG) is a technology that records electrical brain activity through electrodes, and outputs signals that can then be used for a variety of applications. Although EEG has been used in research and clinically for decades, user friendly, consumer-affordable (~$100), non-invasive EEG devices have only been on the market for the last two to three years. Many applications have been developed using consumer-grade EEG, mostly along the lines of \"biofeedback,\u201d where the goal is to achieve and maintain brainwave activity in a desirable state for an extended period; for example, success is often marked by the shifting of a user's brain activity towards a relaxed or meditative state and holding it there for as long as possible. Such unidirectional control is useful for cognitive and emotional response training, but falls short of allowing user- directed control analogous to devices such as knobs and joysticks, e.g., allowing a user to intentionally manipulate and regulate such devices bi- or multi-directionally via thought control. Such a manipulable EEG control would be highly beneficial for medical applications, such as improved computer interfaces and/or control of prostheses for people with disabilities, in addition to lifestyle applications, such as gaming and music performance and production. This study serves as a proof-of-concept and preliminary exploration of the viability of various techniques for the use of consumer-grade EEG devices as a control input that can be manipulated by the user, having concluded that a significant majority of users report being able to control the cursor, and appear to be able to move the cursor in the direction indicated by the test software.\n\n==\n\nCi 2015:\n\nTitle: The key techniques research on portable EEG examination expert system\n\nAbstract: In this paper we present an EEG detection system, which is a low-cost, portable, popularization. It is realized the low cost, mass brain health examination through the mobile phone software and networking in the \"cloud\" of the EEG pattern recognition expert database. The main functions of the system include: (1) through the mobile phone to control the EEG data acquisition; (2) established the expert system database of EEG patterns; (3) the EEG patterns, connected to the Internet matching. The key technologies of portable EEG health examination expert system such as structured storage of EEG data, feature extraction, construction of knowledge database and pattern matching are also discussed and researched in the paper.\n\n==\n\nYan-ke 2015:\n\nTitle: The key techniques research on portable EEG examination expert system\n\nAbstract: In this paper we present an EEG detection system, which is a low-cost, portable, popularization. It is realized the low cost, mass brain health examination through the mobile phone software and networking in the \"cloud\" of the EEG pattern recognition expert database. The main functions of the system include: (1) through the mobile phone to control the EEG data acquisition; (2) established the expert system database of EEG patterns; (3) the EEG patterns, connected to the Internet matching. The key technologies of portable EEG health examination expert system such as structured storage of EEG data, feature extraction, construction of knowledge database and pattern matching are also discussed and researched in the paper.",
        "final_summary": "Portable or consumer-grade EEG devices combined with machine learning techniques have been successfully applied in various research areas. Fabietti (2021) reviews over 40 articles where machine learning techniques were incorporated into portable EEG systems for diverse applications. Sabio (2022) provides an overview of research conducted using consumer-grade EEG devices, highlighting their affordability and user-friendliness. Omelianenko (2017) demonstrates the use of advanced machine learning models to classify EEG data collected from inexpensive consumer-grade devices, showing potential for applications in virtual reality and healthcare. Pathirana (2018) critically evaluates low-cost consumer-grade EEG devices for implementing cost-effective brain-computer interfaces, achieving promising accuracy. Hosseini (2020) reviews machine learning methods applied to EEG analysis, highlighting the effectiveness of supervised learning approaches such as Support Vector Machine and K-Nearest Neighbors. Goldberg (2012) explores the feasibility of consumer-grade EEG devices as computer input, demonstrating that users can manipulate cursor movement through thought control. Lastly, Ci (2015) presents a portable EEG examination expert system that utilizes mobile phone software and cloud-based databases for low-cost brain health examinations."
    },
    {
        "query": "Neuroblastoma and baicalein",
        "paper_list_string": "Zhao 2017:\n\nTitle: Neuroprotective Effects of Baicalein on Acrolein-induced Neurotoxicity in the Nigrostriatal Dopaminergic System of Rat Brain\n\nAbstract: Elevated levels of acrolein, an \u03b1,\u03b2-unsaturated aldehyde are detected in the brain of patients with Parkinson\u2019s disease (PD). In the present study, the neuroprotective effect of baicalein (a phenolic flavonoid in the dried root of Scutellaria baicalensis Georgi) on acrolein-induced neurodegeneration of nigrostriatal dopaminergic system was investigated using local infusion of acrolein in the substantia nigra (SN) of rat brain. Systemic administration of baicalein (30\u00a0mg/kg, i.p.) significantly attenuated acrolein-induced elevations in 4-hydroxy-2-noneal (a product of lipid peroxidation), N-(3-formyl-3,4-dehydropiperidino)lysine (a biomarker of acrolein-conjugated proteins), and heme-oxygenase-1 levels (a redox-regulated protein) in the infused SN, indicating that baicalein inhibited acrolein-induced oxidative stress and protein conjugation. Furthermore, baicalein reduced acrolein-induced elevations in glial fibrillary acidic protein (a biomarker of activated astrocytes), ED-1 (a biomarker of activated microglia), and mature cathepsin B levels (a cysteine lysosomal protease), suggesting that baicalein attenuated acrolein-induced neuroinflammation. Moreover, baicalein attenuated acrolein-induced caspase 1 activation (a pro-inflammatory caspase) and interleukin-1\u03b2 levels, indicating that baicalein prevented acrolein-induced inflammasome activation. In addition, baicalein significantly attenuated acrolein-induced caspase 3 activation (a biomarker of apoptosis) as well as acrolein-induced elevation in receptor interacting protein kinase (RIPK) 3 levels (an initiator of necroptosis), indicating that baicalein attenuated apoptosis and necroptosis. At the same time, baicalein mitigated acrolein-induced reduction in dopamine levels in the striatum ipsilateral to acrolein-infused SN. In conclusion, our data suggest that baicalein is neuroprotective via inhibiting oxidative stress, protein conjugation, and inflammation. Furthermore, baicalein prevents acrolein-induced program cell deaths, suggesting that baicalein is therapeutically useful for slowing PD progression.\n\n==\n\nJiang 2010:\n\nTitle: Baicalein reduces E46K \u03b1\u2010synuclein aggregation in vitro and protects cells against E46K \u03b1\u2010synuclein toxicity in cell models of familiar Parkinsonism\n\nAbstract: J. Neurochem. (2010) 114, 419\u2013429.\n\n==\n\nJason 2018:\n\nTitle: Neuroblastoma\n\nAbstract: Neuroblastoma is a type of cancer that most often affects children. Neuroblastoma occurs when immature nerve cells called neuroblasts become abnormal and multiply uncontrollably to form a tumor. Most commonly, the tumor originates in the nerve tissue of the adrenal gland located above each kidney. Other common sites for tumors to form include the nerve tissue in the abdomen, chest, neck, or pelvis. Neuroblastoma can spread (metastasize) to other parts of the body such as the bones, liver, or skin.\n\n==\n\nJason 2018:\n\nTitle: Neuroblastoma\n\nAbstract: Neuroblastoma is a type of cancer that most often affects children. Neuroblastoma occurs when immature nerve cells called neuroblasts become abnormal and multiply uncontrollably to form a tumor. Most commonly, the tumor originates in the nerve tissue of the adrenal gland located above each kidney. Other common sites for tumors to form include the nerve tissue in the abdomen, chest, neck, or pelvis. Neuroblastoma can spread (metastasize) to other parts of the body such as the bones, liver, or skin.\n\n==\n\nAnderson 2002:\n\nTitle: Current Clinical Trials in Neuroblastoma\n\nAbstract: Neuroblastoma is a pediatric malignant tumor of the postganglionic sympathetic nervous system that usually develops in the adrenal gland or in nonadrenal abdominal or thoracic sites.[1] It is the most common malignancy in infants and the most common extracranial solid tumor of childhood, with approximately 650 cases diagnosed annually in the United States.[2] The dramatic age-related survival differences among neuroblastoma patients with a similar tumor stage emphasize the heterogeneity of neuroblastoma pathobiology. Early research efforts to understand the pathobiology of neuroblastoma[3-5] and the significant progress made in neuroblastoma molecular biology[6] have informed the clinical treatment of neuroblastoma.\n\n==\n\nRatner 2016:\n\nTitle: The \u201cneuro\u201d of neuroblastoma: Neuroblastoma as a neurodevelopmental disorder\n\nAbstract: Neuroblastoma is a childhood cancer derived from cells of neural crest origin. The hallmarks of its enigmatic character include its propensity for spontaneous regression under some circumstances and its association with paraneoplastic opsoclonus, myoclonus, and ataxia. The neurodevelopmental underpinnings of its origins may provide important clues for development of novel therapeutic and preventive agents for this frequently fatal malignancy and for the associated paraneoplastic syndromes. Ann Neurol 2016;80:13\u201323\n\n==\n\nLey 2011:\n\nTitle: Neuroblastoma imaging.\n\nAbstract: Neuroblastoma is an embryonic tumor of the sympathetic nervous system which represents one of the most common malignancies in early childhood. Its clinical and biological behavior show a remarkable heterogeneity, ranging from spontaneous regression to inexorable progression with a fatal outcome. This review summarizes the clinical risk stratification and treatment options. An extensive overview of the role of imaging during the course of the disease and typical imaging findings in all imaging modalities are demonstrated.\n\n==\n\nCheah 1989:\n\nTitle: Adult neuroblastoma: a case report.\n\nAbstract: Neuroblastoma is an uncommon malignancy in adult life as compared with childhood. A case of Stage IV neuroblastoma in a 24-year-old Chinese man seen at the University Hospital, Kuala Lumpur is reported. The diagnosis was based on characteristic histological features seen in an excised lymph node with metastatic involvement and an elevated urinary metanephrine level. The finding of a suprarenal mass on computerised axial tomographic scan supported an adrenal origin. Other diagnostic aids which help to distinguish this tumour, especially the less differentiated forms, from other \"small, dark, round cell tumours\" are discussed.",
        "final_summary": "The papers suggest that baicalein may have neuroprotective effects. Specifically, Zhao 2017 found that baicalein protected against acrolein-induced neurodegeneration in the nigrostriatal dopaminergic system of rat brains, reducing oxidative stress, inflammation, apoptosis, and necroptosis. However, the papers do not directly address the specific relationship between baicalein and neuroblastoma."
    },
    {
        "query": "a survey on transfer learning methods for 'large language models'",
        "paper_list_string": "Alyafeai 2020:\n\nTitle: A Survey on Transfer Learning in Natural Language Processing\n\nAbstract: Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging NLP tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of NLP. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.\n\n==\n\nAlyafeai 2020:\n\nTitle: A Survey on Transfer Learning in Natural Language Processing\n\nAbstract: Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging NLP tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of NLP. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.\n\n==\n\nChronopoulou 2019:\n\nTitle: An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models\n\nAbstract: A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.\n\n==\n\nZhao 2023:\n\nTitle: A Survey of Large Language Models\n\nAbstract: Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.\n\n==\n\nYoon 2017:\n\nTitle: Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network\n\nAbstract: In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects.\n\n==\n\nWang 2021:\n\nTitle: Meta Distant Transfer Learning for Pre-trained Language Models\n\nAbstract: With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from negative transfer. Inspired by meta-learning, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, it trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each task with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.\n\n==\n\nSchioppa 2023:\n\nTitle: Cross-Lingual Supervision improves Large Language Models Pre-training\n\nAbstract: The recent rapid progress in pre-training Large Language Models has relied on using self-supervised language modeling objectives like next token prediction or span corruption. On the other hand, Machine Translation Systems are mostly trained using cross-lingual supervision that requires aligned data between source and target languages. We demonstrate that pre-training Large Language Models on a mixture of a self-supervised Language Modeling objective and the supervised Machine Translation objective, therefore including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities. As pre-training is a very resource-intensive process and a grid search on the best mixing ratio between the two objectives is prohibitively expensive, we propose a simple yet effective strategy to learn it during pre-training.\n\n==\n\nRadford 2019:\n\nTitle: Language Models are Unsupervised Multitask Learners\n\nAbstract: Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
        "final_summary": "The papers collectively provide insights into transfer learning methods for large language models. Alyafeai (2020) highlights the increasing demand for transfer learning in natural language processing (NLP) due to the challenges of acquiring large datasets and computational resources. Chronopoulou (2019) proposes a simple yet effective transfer learning approach that combines task-specific optimization with an auxiliary language model objective, achieving superior results in affective and text classification tasks. Wang (2021) introduces the Meta Distant Transfer Learning (Meta-DTL) framework, which leverages meta-learning to capture transferable knowledge across tasks and improve performance, particularly when limited data is available for the target task. Schioppa (2023) demonstrates that pre-training large language models with a mixture of self-supervised language modeling and supervised machine translation objectives enhances in-context learning abilities. Radford (2019) explores the remarkable capabilities of language models trained on a large dataset, showcasing their ability to perform various NLP tasks without explicit supervision. These findings collectively emphasize the potential and effectiveness of transfer learning methods for large language models in improving performance and addressing challenges in NLP."
    },
    {
        "query": "monetary policy, bank lending",
        "paper_list_string": "Borio 2017:\n\nTitle: Monetary Policy and Bank Lending in a Low Interest Rate Environment: Diminishing Effectiveness?\n\nAbstract: This paper analyses the effectiveness of monetary policy on bank lending in a low interest rate environment. Based on a sample of 108 large international banks, our empirical analysis suggests that reductions in short-term interest rates are less effective in stimulating bank lending growth when rates reach a very low level. This result holds after controlling for business and financial cycle conditions and different bank-specific characteristics such as liquidity, capitalisation, funding costs, bank risk and income diversification. We find that the impact of low rates on the profitability of banks' traditional intermediation activity helps explain the subdued evolution of lending in the period 2010-14.\n\n==\n\nHe 2013:\n\nTitle: Monetary Policy and Bank Lending in China \u2014 Evidence from Loan-Level Data\n\nAbstract: We investigate how monetary policy in a mixed financial system such as that of China, which is characterized by a juxtaposition of quantity- and price-based policy instruments and the co-existence of regulated and market-determined interest rates, affects bank lending. Using a newly constructed loan-level dataset, we find that loan rates but not loan size are affected by both the regulated and the market-determined interest rates and that loan size is instead affected by an implicit quota that is imposed on aggregate bank lending through window guidance. We interpret this finding to be evidence of credit rationing.\n\n==\n\nKashyap 1993:\n\nTitle: Monetary Policy and Bank Lending\n\nAbstract: This paper surveys recent work that relates to the \"lending\" view of monetary policy transmission. It has three main goals: 1) to explain why it is important to distinguish between the lending and \"money\" views of policy transmission; 2) to outline the microeconomic conditions that are needed to generate a lending channel; and 3) to review the empirical evidence that bears on the lending view.\n\n==\n\nKishan 2012:\n\nTitle: Monetary Policy, Bank Lending, and the Risk\u2010Pricing Channel\n\nAbstract: THIS PAPER IDENTIFIES A monetary policy channel that operates through the risk pricing of uninsured bank debt. Testing for the endogeneity of risk pricing to policy shocks is the main focus of this paper. We test for the relation between monetary policy shocks and changes in risk pricing in the market for jumbo certificates of deposit (jumbo CDs) 1 .\n\n==\n\nVithessonthi 2017:\n\nTitle: Monetary policy, bank lending and corporate investment\n\nAbstract: The purpose of this study is to shed light on the chain of causality from macroeconomic financial policy to the microeconomic investment function. Concretely, we aim to provide an in-depth analysis of the relationships between the monetary policy of central banks, the loan policy of commercial banks, and the investment behavior of firms. We focus on countries that conduct their monetary policy under the inflation-targeting framework. Our empirical analysis with data from Germany, Switzerland and Thailand provides several new insights. First, after controlling for the US monetary policy, the monetary policy in Germany and Thailand appears to influence the banks' lending rate in the short run (i.e. within two months), whereas the monetary policy in Switzerland seems to be ineffective at influencing the banks' lending rate in the short run. Second, our results show that the banks' lending rate has a negative effect on their loans and that this negative effect is weakened by their growth opportunities. Third, we find that the supply of bank loans plays a more pivotal role in determining firms' investment than the lending rate. Last but not least, we document that neither the lending rate nor the loan-to-assets ratio moderates the sensitivity of the firms' investment to growth opportunities.\n\n==\n\nBenetton 2021:\n\nTitle: Targeted monetary policy and bank lending behavior\n\nAbstract: Abstract We exploit an allocation rule set by the ECB for the first series of TLTROs to study the effects of targeted monetary policy on banks\u2019 credit supply to firms. Combining transaction-level data from the Italian credit register and an instrumental variable identification strategy, we find that targeted longer-term central bank liquidity decreased rates and increased loan amounts, also avoiding some unintended consequences of untargeted measures, such as carry-trade strategies and risk shifting. We show that the outward shift in banks\u2019 credit supply was heterogeneous, taking place only in more competitive banking markets, with stronger effects for smaller and safer firms.\n\n==\n\nPeek 1995:\n\nTitle: Bank lending and the transmission of monetary policy\n\nAbstract: A resurgence of interest in the role of banks in the transmission of monetary policy has resulted in a spate of theoretical and empirical studies. These studies have established that, under certain conditions, the traditional transmission mechanism for monetary policy (\"the money view\") may be augmented through changes in the supply of bank loans (\"the lending view\"). Because both the money view and the lending view operate through the banking sector, the health of the banking system, insofar as it affects bank behavior, is an important factor in the transmission of monetary policy. It affects both the nature and the size of bank responses to shifts in monetary policy, with particular relevance for the bank lending channel. The traditional description of monetary policy generally emphasizes the reserve requirement constraint on banks. In this story, banks are an important link in the transmission of monetary policy because changes in bank reserves influence the quantity of reservable deposits held by banks. Because banks rarely hold significant excess reserves, the resel~ce requirement constraint typically is considered to be binding at all times. However, a second constraint on banks, the capital constraint, may be more important in accounting for the variability in the magnitude of the effect of monetary policy over time. The extent to which a capital constraint is binding, unlike the reserve requirement, is likely to vary\n\n==\n\nMatemilola 2015:\n\nTitle: The impact of monetary policy on bank lending rate in South Africa\n\nAbstract: The pass-through of the policy rates to bank lending rate is an important subject matter because it measures the effectiveness of monetary policy to control inflation or stabilize the economy. This study investigates the long-run interest rate pass-through of the money market rate to the bank lending rate and asymmetric adjustment of the bank lending rate. The study applies the momentum threshold autoregressive and asymmetric error correction models. The asymmetric error correction results reveal that bank lending rate adjusts to a decrease in the money market rate in South Africa. The findings suggest that the South African commercial banks adjust their lending rate downward but the lending rate appears rigid upward, which supports the customer reaction hypothesis.",
        "final_summary": "The papers suggest that the effectiveness of monetary policy on bank lending depends on various factors. Borio (2017) finds that reductions in short-term interest rates have diminishing effectiveness in stimulating bank lending growth when rates are very low. He (2013) shows that in China's mixed financial system, loan rates are affected by both regulated and market-determined interest rates, while loan size is influenced by an implicit quota on aggregate bank lending. Kashyap (1993) emphasizes the importance of distinguishing between the lending and \"money\" views of policy transmission and reviews empirical evidence related to the lending view. Kishan (2012) identifies a monetary policy channel that operates through the risk pricing of uninsured bank debt. Vithessonthi (2017) highlights the relationships between central bank monetary policy, commercial bank loan policy, and firm investment behavior, with the supply of bank loans playing a pivotal role in determining investment. Benetton (2021) explores the effects of targeted monetary policy on banks' credit supply to firms, finding that targeted longer-term central bank liquidity decreased rates and increased loan amounts. Peek (1995) discusses the role of banks in the transmission of monetary policy, particularly through the lending view, and the importance of the health of the banking system. Matemilola (2015) investigates the pass-through of policy rates to bank lending rates in South Africa, finding that the lending rate adjusts downward to a decrease in the money market rate but appears rigid upward."
    },
    {
        "query": "cassava effluent from cassava mill",
        "paper_list_string": "Lawal 2019:\n\nTitle: Degrading cassava mill effluent using aerated sequencing batch reactor with palm kernel shell as medium\n\nAbstract: Local cassava agro-processing industries in Nigeria generate toxic organic effluent with negative environmental impact if disposed without adequate treatment. This study examines the performance of a lab-scale aerated sequencing batch reactor (SBR) in degrading cassava mill effluent using palm kernel ( Elaeis guineensis ) shell (PKS) as biofilter media. Wastewater samples were collected before and after flowing through each compartment at hydraulic retention times of 3, 5 and 7 hours. Continuous aeration and nature-based degradation of the effluent recorded overall removal efficiencies of 73.5% (Hydrogen cyanide), 70.59% (BOD), 69.18% (COD), 29.93% (Turbidity), 4.92% (Sodium), 25% (Magnesium) and 14.32% (Calcium) respectively. Effluent electrical conductivity (EC) slightly increased by 7.84%. The Sodium Adsorption Ratio (SAR) of the treated wastewater ranged from 6.9 to 7.3 while the final pH ranged from 4.5 to 4.6. The values of EC, BOD and COD were significantly different ( P <0.05) along the treatment sequence, confirming the effectiveness of the chambers in reducing these pollutants. Despite achieving high removal efficiencies, the final values of most parameters still fall short of the local permissible limit signifying operational limitations and the need to optimize the system to reduce key contaminants to safe disposal limits\n\n==\n\nOkoye 2023:\n\nTitle: Evaluation of the impact of untreated cassava mill effluent on the physicochemical properties of the soil in Aba, Abia State, Nigeria.\n\nAbstract: Samples of fresh cassava effluent, cassava effluent polluted soil and unpolluted soil were collected during the rainy season to evaluate the effect on the physicochemical characteristics of the soil in Aba, Abia State. The physical and chemical parameters of the untreated cassava mill effluent, untreated cassava mill effluent-polluted soil and unpolluted soil were determined using standard laboratory methods. It was observed that addition of cassava mill effluent to the soil resulted to changes in the physicochemical parameters. The cyanide content, conductivity, turbidity, moisture content, TDS, TSS, TS, total acidity, total alkalinity, total chloride, and magnesium were higher in the contaminated soil samples than the unpolluted soil. The values of pH, Total hardness, calcium hardness, phosphorus, organic carbon, COD and BOD of the contaminated soil samples were lower than those of the unpolluted soil due to high content of hydrogen cyanide present in the contaminated soil. The high cynogenic potential had been attributed to the high cyanogenic glucoside (linamarin and lotaustralin) contained in cassava. The pH range of the untreated cassava mill effluent and the untreated cassava mill effluent polluted soil were completely acidic while the unpolluted soil was neutral indicating that the effluent impacted acidic properties to the soil. Phosphorus buildup in the unpolluted soil can be caused by excessive use of inorganic fertilizer or use of composts and manures high in phosphorus. The effluent from cassava plant when discharged on soil causes physicochemical changes in the soil, which calls for serious concern if the soil will be used for agricultural and other purposes. Therefore, cassava mills must be owned and managed by individuals who have basic knowledge of environmental protection.\n\n==\n\nIzah 2015:\n\nTitle: Bioethanol Production from Cassava Mill Effluents Supplemented with Solid Agricultural Residues Using Bakers' Yeast (Saccharomyces cerevisiae)\n\nAbstract: Nigeria is the leading cassava and fifth largest oil palm producer, accounting for 54 and 0.93 million metric tonnes respectively. As such during processing large wastes are generated including chaff, palm press fiber, palm kernel shell, empty fruit bunch (EFB) and palm oil mill effluents from oil palm and cassava peels and cassava mill effluents (CME). These wastes are discharged into the ecosystem without treatment where they cause attendant environmental impacts. This study evaluated bioethanol production from CME supplemented with chaff, EFB and cassava peels through separate hydrolysis and fermentation (SHF) techniques. CME with initial pH of 3.6 were used for hydrolysis and results showed a total reducing sugar of 0.31\u00b10.006 mg/ml (CME + EFB -Treatment A), 0.25\u00b10.002 mg/ml (CME +Chaff - Treatment B), 0.43\u00b10.007 mg/ml CME + Cassava peels Treatment C) and 0.20\u00b10.002mg/ml (CME i.e not hydrolyzed \u2013 Treatment D). Fermentation was carried out with Bakers\u2019 yeast (Saccharomyces cerevisiae) under candle jar and highest ethanol volume, weight, concentration, yield and fermentation efficiency was respectively observed as 54.00\u00b10.289ml, 49.56\u00b10.058g, 1.25\u00b10.001%, 0.16 Yp/s and 31.34% at fermentation period of 120 hours (Treatment A), 92.00\u00b10.2889ml, 87.42\u00b10.557g, 0.94\u00b10.010%, 0.35 Yp/s and 68.56% at incubation period of 72 hours (Treatment B), 66.00\u00b10.404ml, 62.84\u00b10.557g, 1.25\u00b10.012%, 0.15 Yp/s and 28.65% at fermentation time of 120 hours (Treatment C), 54.00\u00b10.153ml, 49.47\u00b10.199g, 0.60\u00b10.006%, 0.25 Yp/s and 48.55% at fermentation duration of 144 hours (Treatment D) was observed. The specific gravity of the ethanol produced from the various Treatment ranged from 0.8875 \u2013 0.9673. The study concluded that CME supplemented with EFB, chaff and cassava peels could generate ethanol and thus can be used as a lignocellulosic ethanol feedstock.\n\n==\n\nKaewkannetra 2011:\n\nTitle: Treatment of cassava mill wastewater and production of electricity through microbial fuel cell technology\n\nAbstract: Abstract Cassava mill wastewater has a high organic content and is an important economic product of traditional and rural low technology agro-industry in many parts of the world. This study explores the utilization of agro-industrial wastewater collected from cassava mills as a resource for electricity generation by microbial fuel cells (MFCs). Mixed culture sludge was used to inoculate the bottom chamber of the MFCs whilst cassava mill wastewater was used in the MFCs. Experimental results showed that the MFCs could generate electricity from full-strength cyanide laden wastewater (16000\u00a0mg-COD/L, 86\u00a0mg/L cyanide) with a maximum power density of 1771\u00a0mW/m 2 . The results from this study demonstrate the feasibility of using MFC technology to generate electricity whilst simultaneously treating cyanide laden cassava mill wastewater effectively. Using MFCs for cassava mill wastewater treatment provides an attractive way to reduce the cost of wastewater treatment in addition to generating electricity.\n\n==\n\nOsakwe 2012:\n\nTitle: Effect of Cassava Processing Mill Effluent on Physical and Chemical Properties of Soils in Abraka and Environs, Delta State, Nigeria.\n\nAbstract: Selected heavy metals and physicochemical characteristics of the soils around some cassava processing mills in Abraka and environs were analyzed in order to assess the impact of the cassava mill effluent on the soils. The results of the physicochemical analysis showed overall decrease in pH values and corresponding increase in the other parameters. The pH values indicated that the soils were acidic which suggests that the effluents imparted acidic property on the soils. The elevated levels of Total Organic Carbon and Total Nitrogen are suggestive of increased organic matter and microbial activities in the effluent residues. Electrical conductivity values indicate presence of dissolved inorganic salts while the phosphorus values confirm the rich source of phosphorus in cassava tuber. The result of heavy metal analysis shows elevated levels of heavy metals in the soil receiving cassava mill effluent. The relative potential index and enrichment coefficient values of the metals in the soils coupled with the observed physiochemical characteristics revealed that there are some levels of heavy metal enrichment, contamination and bioavailability in the soils studied. Key words : cassava mill effluent, heavy metals, physicochemical characteristics soil pollution, Abraka\n\n==\n\nIzah 2017:\n\nTitle: Changes in the Treatment of Some Physico-Chemical Properties of Cassava Mill Effluents Using Saccharomyces cerevisiae\n\nAbstract: Cassava is majorly processed into gari by smallholders in Southern Nigeria. During processing, large volume of effluents are produced in the pressing stage of cassava tuber processing. The cassava mill effluents are discharged into the soil directly and it drain into nearby pits, surface water, and canals without treatment. Cassava mill effluents is known to alter the receiving soil and water characteristics and affects the biota in such environments, such as fishes (water), domestic animals, and vegetation (soil). This study investigated the potential of Saccharomyces cerevisiae to be used for the treatment of some physicochemical properties of cassava mill effluents. S. cerevisiae was isolated from palm wine and identified based on conventional microbiological techniques, viz. morphological, cultural, and physiological/biochemical characteristics. The S. cerevisiae was inoculated into sterile cassava mill effluents and incubated for 15 days. Triplicate samples were withdrawn from the setup after the fifth day of treatment. Portable equipment was used to analyze the in-situ parameters, viz. total dissolved solids (TDS), pH, dissolved oxygen (DO), conductivity, salinity, and turbidity. Anions (nitrate, sulphate, and phosphate) and chemical oxygen demand (COD) were analyzed using spectrophotometric and open reflux methods respectively. Results showed a decline of 37.62%, 22.96%, 29.63%, 20.49%, 21.44%, 1.70%, 53.48%, 68.00%, 100%, and 74.48% in pH, conductivity, DO, TDS, salinity, sulphate, nitrate, phosphate, and COD levels respectively, and elevation of 17.17% by turbidity. The study showed that S. cerevisiae could be used for the treatment of cassava mill effluents prior to being discharged into the environment so as to reduce the pollution or contamination and toxicity levels.\n\n==\n\nOrhue 2014:\n\nTitle: Effect of Cassava mill effluent on some soil chemical properties and the growth of fluted pumpkin (Telfairia occidentalis Hook F.)\n\nAbstract: In the trials, Cassava mill Effluent was used for fluted pumpkin (Telfairia occidentalis) cultivation in order to verify the influence of the effluent on the growth and some soil chemical properties. In this regard, a completely randomized and randomized complete block designs were used in the greenhouse and field trials respectively with 6 treatments replicated 3 times. In the greenhouse, the following rates of 0, 100, 200, 300, 400 and 500 ml per 5 kg topsoil were used while in the field trial, 0, 40000, 80000, 120000, 160000 and 200000 litres/ha were utilized. The rates used in the field were equivalent to those of greenhouse. In both trials, the cassava mill effluent was applied 2 weeks prior to transplanting the seedlings. Results indicated that the cassava mill effluent significantly (P < 0.05) increased soil pH, organic carbon, N, P, K, Ca, Mg, Na, Fe, Cu and Zn whereas the exchangeable acidity decreased significantly (P < 0.05) with corresponding increase in cassava mill effluent treatments. Except N and Na, which declined with corresponding increase in the cassava mill effluent treatments, an improved P, K, Mg, Ca, Fe, Cu and Zn components was achieved in cassava mill effluent polluted plants compared to control. The plant height, significantly (P < 0.05) decreased with increased cassava mill effluent treatment in the greenhouse trial while in the field trial, 120000 litres/ha was significantly (P < 0.05) higher than other treatments. In the greenhouse trial, significantly (P < 0.05) higher number of leaves was attained in 100 ml treatment compared to other treatments whereas in the field trial, the 120000 and 200000 litres/ha were significantly (P < 0.05) higher compared to other treatments\n\n==\n\nIzah 2018:\n\nTitle: Impacts of Cassava Mill Effluents in Nigeria\n\nAbstract: Cassava is the fourth largest staple food after rice, wheat and maize. Cassava is produced in the tropical and sub-tropical countries. Currently, the global production of cassava is about 215,436,496 tons. Out of these, Nigeria accounts for 20.3%, being the largest producing nation. During processing of cassava flour (Gari), three main wastes are generated including cassava mill effluents (CME), solid and gaseous emission. This paper reviews the impacts of CME in Nigeria. The study found that CME\u2019s physicochemical quality often exceeds the limit for effluents discharge onto land and surface water as recommended by Federal Environmental Protection Agency (FEPA), Nigeria. CME alters the quality of soil and water with regard to physicochemical, heavy metal and microbial characteristics. CME can induce toxicological effects on the environments and its biota including humans, fisheries, flora and fauna. The impacts are mostly associated with physicochemical (viz: odour, cyanide, acidic, dissolved oxygen, biological and chemical oxygen demand, conductivity) and heavy metals characteristics. Therefore, there is the need for treatment and sustainable management strategies of CME through biotechnological advancement. DOI : 10.14302/issn.2637-6075.jpae-17-1890 Corresponding Author: Sylvester Chibueze Izah, Department of Biological Sciences, Faculty of Science, Niger Delta University, Wilberforce Island, Bayelsa State, Nigeria, Tel: +234 703 0192 466, E-mail: chivestizah@gmail.com",
        "final_summary": "Cassava mill effluent has been found to have significant environmental impacts and requires effective treatment. Lawal 2019 demonstrated that a lab-scale aerated sequencing batch reactor could degrade cassava mill effluent, but the treated effluent did not meet local permissible limits. Okoye 2023 reported that soil polluted with untreated cassava mill effluent had elevated levels of cyanide, conductivity, turbidity, and other parameters compared to unpolluted soil. Izah 2015 and Kaewkannetra 2011 explored the potential of cassava mill effluent as a resource for bioethanol production and electricity generation, respectively. Osakwe 2012 and Izah 2017 highlighted the detrimental impact of cassava mill effluent on soil properties, including increased acidity and elevated levels of heavy metals. Orhue 2014 found that cassava mill effluent increased soil pH and nutrient levels and, in the field trial, the treatment of 120000 litres/ha resulted in significantly higher fluted pumpkin plant height compared to other treatments. Izah 2018 emphasized the potential environmental contamination from cassava mill effluent in Nigeria and the need for its treatment and sustainable management."
    },
    {
        "query": "The year Unilever Plc Uk was established, what did they do and what is their staff capacity? ",
        "paper_list_string": "Doubleday 2004:\n\nTitle: Institutionalising non-governmental organisation dialogue at Unilever: framing the public as \u2018consumer-citizens\u2019\n\nAbstract: Between 1994 and 2001, Unilever engaged in dialogue with environmental and consumer non-governmental organisations (NGOs) in the UK over questions raised by the commercialisation of genetically modified foods. This initiative took the form of a \u2018Contact Group\u2019, which allowed members from the company and the NGOs to develop close working relations. Through this group, Unilever sponsored academic social science research into public attitudes to genetically modified foods. This paper explores how Unilever sought to reframe its relations with consumers using the concept of \u2018consumer-citizens\u2019, which had emerged from the work of the Contact Group. This term acknowledges that consumers can reflect on their relationship with companies such as Unilever in citizen-like ways. Copyright , Beech Tree Publishing.\n\n==\n\nCherono 2017:\n\nTitle: Employee development and organizational performance of Unilever Tea Kenya ltd in Kericho County\n\nAbstract: A research project submitted to the school of business in partial fulfillment of the requirement for degree in master of business administration (human resource management) of Kenyatta University November, 2017\n\n==\n\nYehoshua 2020:\n\nTitle: Prediksi Penjualan Produk Promo PT. Unilever, Tbk Menggunakan Metode Fuzzy Time Series\n\nAbstract: PT. Unilever is a multinational company headquartered in Rotterdam, the Netherlands (under the name Unilever N.V.), London, England (under the name Unilever pic.) And in Indonesia has a subsidiary, PT. Unilever, Tbk was established on December 5, 1933. Unilever produces food, drinks, cleaners, and also body care. Unilever is the third largest producer of household goods in the world, if based on the amount of revenue in 2012, behind P & G and Nestle. In forecasting products, it is often influenced by the sale of these products because there are also changes in sales for each period. Usually there is an increase in sales of these products which, among other things, is caused by price discounts, new products, one free one to buy promo, or a saving package from Unilever or from a rival company. Data collection method used by the author is a method of observation or directly observing the process of transmission, interview methods and literature study methods. While the method for processing data uses fuzzy time series algorithms, context diagrams, data flow diagrams, HIPO, relational diagram entities, data dictionary design, input design, output design, relation diagrams between tables, system implementation and testing. The method for implementation uses vb.net and Mysql. The results of this thesis are a system for calculating the forecasting amount of sales or sales of promo products for the following year. From this system, information on store data, item data, sales year history data, and forecasting data from fuzzy time series data will be displayed.. From rinso goods promotion data which have been calculated using fuzzy time series method which get MAPE value equal to 3,2%, so sales data for category of goods will experience increase based on calculation equal to 3,2%.\n\n==\n\nJones 2005:\n\nTitle: European Integration and Corporate Restructuring: The Strategy of Unilever, C. 1957-C. 1990\n\nAbstract: European integration and corporate restructuring: the strategy of Unilever, c.1957-c.1990. While much has been written about the politics of European integration, discussion about the role of business in this process has been largely confined to lobbying activities. This article focuses on the business reaction to European integration. It highlights the constraints facing one of Europe's largest firms in building a regional detergents business. These included divergences in market demand and political obstacles to rationalization, but more serious was a corporate culture based around local decision making and consensus. The study demonstrates that a full understanding of the European integration process must incorporate a firm-level analysis of how Europe-wide businesses were built after 1957.\n\n==\n\nGorman 2006:\n\nTitle: Unilever (C)\n\nAbstract: Jan-Kees Vis, a staff worker in the middle echelons of the company, attempts to integrate sustainable development into existing environmental policy in hopes of providing a plan of action for achieving sustainability. Once the new policy is adopted, the case leaves the student to decide how the company should begin fulfilling its commitments. The C case continues the story of Vis's attempts to integrate sustainable development into Unilever's existing environmental policy in hopes of providing a plan of action for achieving sustainability, and reveals more of the company's efforts to fulfill its commitments. See also the A, B, and D cases (UVA-E-0153, UVA-E-0154, and UVA-E-0156) and \"Unilever: Corporate Venturing and Environmental Sustainability (A)\" (UVA-E-0152).\n\n==\n\nFatyandri 2023:\n\nTitle: Internal Organization Analysis Of Unilever\n\nAbstract: The outbreak of Covid-19 in 2020 has brought downturn to the global economy and many companies went bankrupt during the epidemic. However, according to Unilever's financial report for the first half year, its sales fell only by 1.6% and its financial situation is still stable. The reason for it is not only for Unilever's action of taking Covid-19 as normalization, but also for its differentiation strategies, which make Unilever a competitive enterprise when the consumer products are in demand in Covid-19. Through the literature research method, this article uses the combination of SWOT (strengths, weaknesses, opportunities and threats) and PEST (political, economic, social and technological factors) models to explore the internal and external environments of Unilever in 2020, and concludes that under the multiple impacts of the external environment, Unilever still has great opportunities to make development relying on its own excellent internal conditions.\n\n==\n\nPretty 2008:\n\nTitle: Multi-year assessment of Unilever's progress towards agricultural sustainability II: outcomes for peas (UK), spinach (Germany, Italy), tomatoes (Australia, Brazil, Greece, USA), tea (Kenya, Tanzania, India) and oil palm (Ghana)\n\nAbstract: The foods, home and personal care company, Unilever, is a large user of raw materials from agriculture, and a major buyer of goods on world markets. The continued supply of these materials is seen as an important component in the business's long-term success. The company has a long history of seeking to farm responsibly on company farms and for directly contracted raw materials, but it became clear that an approach based solely on suppliers' good agricultural practice would not safeguard supplies where increasing social and environmental pressures on agriculture were growing, or where increasing consumer concerns about the food chain could undermine markets and brands. Both threats suggested the need for a more radical approach. This resulted in the development of a mission statement, the agreement of four principles for sustainable agriculture, the identification of ten sustainable agriculture indicators (later 11), and the selection of five key crops the sustainable supply of which was significant to the company. This paper summarizes progress towards the sustainable supply of these crops by reporting on selected sustainability indicators for the crops (peas, spinach, tomatoes, tea and oil palm) in 11 countries. Some of the businesses using these products have been subsequently sold, but these are reported here because the aim is to explore how responsive are different indicators of sustainability to management action in different crops in widely differing locations. This paper focuses on a selection of findings for each of the 10 indicators, in order to illustrate the extent of changes that have been observed over time. These also indicate some of the difficulties faced in making improvements on the ground. The gathering of data on sustainability indicators is closely tied to the development of alternative practices that should quickly deliver improvements in a variety of outcomes. An assessment is also made of the key changes that have occurred for each of the main five crops as a result of adopting the sustainability indicator system and associated new management practices. Multi-year assessments were conceived as the way to understand and demonstrate progress towards more sustainable agriculture. The important developments were of systems that combined ensuring that agricultural suppliers performed to an acceptable set of criteria, and then had the capacity and willingness to identify the most critical areas where further progress was required. The challenge for the company is now to encourage others to adopt their approach to making supply chains more sustainable, both for their customers and the consumers of their branded goods.\n\n==\n\nGorman 2006:\n\nTitle: Unilever (B)\n\nAbstract: Jan-Kees Vis, a staff worker in the middle echelons of the company, attempts to integrate sustainable development into existing environmental policy in hopes of providing a plan of action for achieving sustainability. Once the new policy is adopted, the case leaves the student to decide how the company should begin fulfilling its commitments. The B case continues the story of Vis's attempts to integrate sustainable development into Unilever's existing environmental policy in hopes of providing a plan of action for achieving sustainability, and describes how the company begins to fulfill its commitments. See also the A, C, and D cases (UVA-E-0153, UVA-E-0155, and UVA-E-0156) and \"Unilever: Corporate Venturing and Environmental Sustainability (A)\" (UVA-E-0152).",
        "final_summary": "Unilever, a multinational company with a subsidiary in Indonesia established on December 5, 1933, produces a range of products including food, drinks, cleaners, and body care items (Yehoshua 2020). The company has engaged in dialogue with environmental and consumer NGOs in the UK, particularly concerning genetically modified foods (Doubleday 2004). While the company's internal organization is not explicitly detailed in the papers, there is evidence of Unilever's commitment to sustainability, with efforts to integrate sustainable development into their environmental policy (Gorman 2006). However, specific information about Unilever Plc UK's staff capacity is not provided in the papers."
    },
    {
        "query": "state of healthcare management information in Nigeria",
        "paper_list_string": "Ojo 2018:\n\nTitle: Repositioning health information management practice in Nigeria: Suggestions for Africa\n\nAbstract: The significance of health information management practice to the effectiveness of a healthcare delivery system cannot be overemphasised. A well-structured and coordinated health information management system has been known to generate the information needed for decision-making at all levels of healthcare delivery. However, the state of health information management in Nigeria, as is the case in most African countries, is a cause for concern. Observation and past studies have highlighted challenges facing the practice of health information management in Africa to be centred around the quality of professional training, inadequately qualified practitioners, disgruntled practitioners, government\u2019s indifference towards the practice, lack of policies and inadequate technological infrastructure among others. This article examines some of the challenges facing health information management practice in Nigeria and makes recommendations that may uplift the profession.\n\n==\n\nOmole 2015:\n\nTitle: Health Management Information System for Decision-Making in Nigeria: Challenges and Resolutions\n\nAbstract: Health Information System (HIS) refers to any organized effort to systematically collect, maintain and disseminate data relevant to the performance of a health system or any of its component parts. As such, any health system has potentially many health applications functioning within it. Not only are the health challenges facing developing countries (Nigeria) enormous, the health systems that addresses those challenges are struggling with limited resources, capability, technology etc. The need to find ways to strengthen health systems overall is therefore vital. The use and knowledge of HIS in Nigeria hospitals is still below what it should be. As technology seem to be advancing each day, information generation behind the health sector seem not to. Applications of IT in health are so rampant now, making it easier to treat patients and manage information about them. Information is critical in making healthrelated decisions. The Health Management Information System (HMIS) aims at developing a culture of information use amongst health care workers through the development of knowledge and skills in data handling in order to create locally relevant information for use in the management of district level health and service programs. HMIS does not inform policy alone but also improve care at the point of service. It involves better information management tools at the local level, data capture through routine business operations, identification of the minimum essential data sets, growth of specialized technology and providers. Etc. Challenges and reasons for the failure of HIS in Nigeria will be discussed. This study will look at one of the software tools used in health care analysis of data and how it is used for decision-making. In addition, it will look at measures that need to be set in place for HMIS to achieve its goal.\n\n==\n\nOluwaseun 2015:\n\nTitle: Strategies for Managing Information Flow in Nigeria Healthcare System\n\nAbstract: The healthcare system is an information intensive domain that is responsible for capturing, processing and storing large volumes of health information which could be clinical, research or administrative in nature. Healthcare information is primarily used for making decisions, improving the efficiency and quality of healthcare systems as well as conducting research to enhance medical science in both developed and developing countries. Healthcare information is usually stored in paper form, physical files or digital formats. However, developing countries such as Nigeria are faced with the challenge of moving from paper based health information system towards the digital formats that facilitates an integrated computerized health information system. Nevertheless, healthcare information is usually transmitted among disparate healthcare providers within or across different healthcare institutions. However, healthcare entities in developing countries are plagued with the difficulty of how to successfully and effectively manage information as it flows across the continuum of care. Consequently, this results in inappropriate decision making, ineffective planning, increase in medical errors and cost as well as a decline in the quality of patients\u2019 care. Based on this background, this paper appraises how information flows within and across diverse healthcare organizations in developing countries with a particular reference to Nigeria. The paper also recommends ways of managing information flow within the Nigeria healthcare system.\n\n==\n\nAdeleke 2015:\n\nTitle: Information Technology Skills and Training Needs of Health Information Management Professionals in Nigeria: A Nationwide Study\n\nAbstract: There is a lack of effective health information management systems in Nigeria due to the prevalence of cumbersome paper-based and disjointed health data management systems. This can make informed healthcare decision making difficult. This study examined the information technology (IT) skills, utilisation and training needs of Nigerian health information management professionals. We deployed a cross-sectional structured questionnaire to determine the IT skills and training needs of health information management professionals who have leadership roles in the nation's healthcare information systems (n=374). It was found that ownership of a computer, level of education and age were associated with knowledge and perception of IT. The vast majority of participants (98.8%) acknowledged the importance and relevance of IT in healthcare information systems and many expressed a desire for further IT training, especially in statistical analysis. Despite this, few (8.1 %) worked in settings where such systems operate and there exists an IT skill gap among these professionals which is not compatible with their roles in healthcare information systems. To rectify this anomaly they require continuing professional development education, especially in the areas of health IT. Government intervention in the provision of IT infrastructure in order to put into practice a computerised healthcare information system would therefore be a worthwhile undertaking.\n\n==\n\nAdeleke 2015:\n\nTitle: Information technology skills and training needs of health information management professionals in Nigeria: a nationwide study.\n\nAbstract: There is a lack of effective health information management systems in Nigeria due to the prevalence of cumbersome paper-based and disjointed health data management systems. This can make informed healthcare decision making difficult. This study examined the information technology (IT) skills, utilisation and training needs of Nigerian health information management professionals. We deployed a cross-sectional structured questionnaire to determine the IT skills and training needs of health information management professionals who have leadership roles in the nation's healthcare information systems (n=374). It was found that ownership of a computer, level of education and age were associated with knowledge and perception of IT. The vast majority of participants (98.8%) acknowledged the importance and relevance of IT in healthcare information systems and many expressed a desire for further IT training, especially in statistical analysis. Despite this, few (8.1 %) worked in settings where such systems operate and there exists an IT skill gap among these professionals which is not compatible with their roles in healthcare information systems. To rectify this anomaly they require continuing professional development education, especially in the areas of health IT. Government intervention in the provision of IT infrastructure in order to put into practice a computerised healthcare information system would therefore be a worthwhile undertaking.\n\n==\n\nAdeleke 2015:\n\nTitle: Information technology skills and training needs of health information management professionals in Nigeria: a nationwide study.\n\nAbstract: There is a lack of effective health information management systems in Nigeria due to the prevalence of cumbersome paper-based and disjointed health data management systems. This can make informed healthcare decision making difficult. This study examined the information technology (IT) skills, utilisation and training needs of Nigerian health information management professionals. We deployed a cross-sectional structured questionnaire to determine the IT skills and training needs of health information management professionals who have leadership roles in the nation's healthcare information systems (n=374). It was found that ownership of a computer, level of education and age were associated with knowledge and perception of IT. The vast majority of participants (98.8%) acknowledged the importance and relevance of IT in healthcare information systems and many expressed a desire for further IT training, especially in statistical analysis. Despite this, few (8.1 %) worked in settings where such systems operate and there exists an IT skill gap among these professionals which is not compatible with their roles in healthcare information systems. To rectify this anomaly they require continuing professional development education, especially in the areas of health IT. Government intervention in the provision of IT infrastructure in order to put into practice a computerised healthcare information system would therefore be a worthwhile undertaking.\n\n==\n\nBenson 2011:\n\nTitle: Hospital Information Systems in Nigeria: A Review of Literature\n\nAbstract: This literature review was developed to examine empirically the factors hindering adoption of hospital information systems in Nigeria. The study was focused on the perceived paucity of health information technology policy in Nigeria and the causes of poor implementation of hospital information systems in the country. The findings of the literature review highlighted hindrances to the adoption of hospital information systems to include; the high cost of full implementation of a hospital information system, inadequate human capital, corruption, and problems associated with poor infrastructure in Nigeria. The recommendations were that, the Nigerian government needs to provide stable electricity, basic communication infrastructures, and Internet access to boost private initiatives in the adoption of health information technology across the country.\n\n==\n\nIdowu 2008:\n\nTitle: Health informatics deployment in Nigeria\n\nAbstract: Information and Communication Technology (ICT) has become a major tool in delivery of health services and has had a revolutionary impact on how we live and perceive the world. ICT has given birth to the contemporary \u201cEs\u201d such as e-learning, e-commerce, e-governance, e-banking, e-shopping and e-health, the primary focus of this paper. In this paper, we discuss ICT in Nigeria with focus on three common ICT indicators: Internet, computing and telephony. We review the past and present state of health informatics in Nigeria, in comparison to the United Kingdom as examples of less developed and developed nations. We also analyse the problems facing successful implementation of health informatics in Nigeria and suggest possible solutions.",
        "final_summary": "The papers collectively highlight the challenges and recommendations regarding the state of healthcare management information in Nigeria. Ojo (2018) emphasizes the need to address challenges such as inadequate training, disgruntled practitioners, and lack of policies to uplift the profession of health information management. Omole (2015) discusses the importance of Health Management Information Systems (HMIS) in decision-making and suggests measures to strengthen the use of IT in healthcare. Oluwaseun (2015) focuses on managing information flow within the Nigerian healthcare system to improve decision-making, planning, and quality of care. Adeleke (2015) identifies the lack of effective health information management systems in Nigeria and emphasizes the need for IT skills and training among professionals to bridge the gap. Benson (2011) highlights hindrances to the adoption of hospital information systems, including high costs, inadequate human capital, corruption, and poor infrastructure, and recommends government intervention to improve health information technology adoption. Lastly, Idowu (2008) discusses the state of health informatics in Nigeria, comparing it to the United Kingdom, and suggests solutions to overcome implementation challenges."
    },
    {
        "query": "what geographical findings emerged from real estate crash of 2007 in united states?",
        "paper_list_string": "Wang 2020:\n\nTitle: A geographically weighted regression approach to investigating local built-environment effects on home prices in the housing downturn, recovery, and subsequent increases\n\nAbstract: The 2007 financial crisis profoundly affected most American metropolitan areas. Over the past 10\u00a0years, Columbus, Ohio, has experienced a housing downturn, recovery, and subsequent increases. This allows to investigate the response of housing market in different periods of the recession. Ordinary and geographically-weighted regression (GWR) models were developed to examine global and local built-environment effects on home-price appreciations for the three periods while controlling for other physical and socioeconomic variables. The results found that home buyers showed an unchanged preference for residential privacy and amenity and avoided those features that might attract negative external effects from a period to another. The home-price appreciation rates showed different spatial patterns across the study region in the three periods. Nevertheless, the results suggested that suburban areas, particularly those in northern Columbus, better resisted, recovered from, and adapted to the recession. In the wake of the recession, a smaller house was preferred by home buyers. GWR models also provided some interesting findings. In the downturn, accessibility to a park or library helped sustain home prices in the northwest. Bus stop density had a positive effect in eastern Columbus in the recovery, most likely due to the high fuel price at that time. Neighborhoods with a higher income better retained their home value in the downturn, especially those in southern Columbus. Finally, this study found that the recession hit harder on minority neighborhoods in all three periods. This finding suggests that housing policies should focus on these neighborhoods with other social support.\n\n==\n\nAubuchon 2010:\n\nTitle: The geographic distribution and characteristics of U.S. bank failures, 2007-2010: do bank failures still reflect local economic conditions?\n\nAbstract: The financial crisis and recession that began in 2007 brought a sharp increase in the number of bank failures in the United States. This article investigates characteristics of banks that failed and regional patterns in bank failure rates during 2007-10. The article compares the recent experience with that of 1987-92, when the United States last experienced a high number of bank failures. As during the 1987-92 and prior episodes, bank failures during 2007-10 were concentrated in regions of the country that experienced the most serious distress in real estate markets and the largest declines in economic activity. Although most legal restrictions on branch banking were eliminated in the 1990s, the authors find that many banks continue to operate in a small number of markets and are vulnerable to localized economic shocks.\n\n==\n\nAalbers 2009:\n\nTitle: Geographies of the financial crisis\n\nAbstract: Real estate is, by definition, local as it is spatially fixed. Mortgage lending, however, has developed from a local to a national market and is increasingly a global market today. An understanding of the financial crisis is ultimately a spatialised understanding of the linkages between local and global. This article looks at the geographies of the mortgage crisis and credit crunch and asks the question: how are different places affected by the crisis? The article looks at different states, different cities, different neighbourhoods and different financial centres. Investors in many places had invested in residential mortgage backed securities and have seen their value drop. Housing bubbles, faltering economies and regulation together have shaped the geography of the financial crisis on the state and city level in the US. Subprime and predatory lending have affected low-income and minority communities more than others and we therefore not only see a concentration of foreclosures in certain cities, but also in certain neighbourhoods. On an international level, the long-term economical and political consequences of this are still mostly unknown, but it is clear that some financial centres in Asia (including the Middle East) will become more important now that globalisation is coming full circle. This article does not present new empirical research, but brings together work from different literatures that all in some way have a specific angle on the financial crisis. The aim of this article is to make the geographical dimensions of the financial crisis understandable to geographers that are not specialists in all - or even any - of these literatures, so that they can comprehend the spatialisation of this crisis.\n\n==\n\nCohen 2016:\n\nTitle: Spatial effects and house price dynamics in the USA\n\nAbstract: While an understanding of spatial spillovers and feedbacks in housing markets could provide valuable information for location decisions, little known research has examined this issue for the US Metropolitan Statistical Areas (MSAs). Also, it is unknown whether there can be differences in the spatial effects before and after a major housing \u201cbust\u201d. In this paper we examine spatial effects in house price dynamics. Using panel data from 363 US MSAs for 1996 to 2013, we find that there are significant spatial diffusion patterns in the growth rates of urban house prices. Lagged price changes of neighboring areas show greater effects after the 2007\u201308 housing crash than over the entire sample period of 1996\u20132013. In general, the findings are robust to controlling for potential endogeneity, and for various spatial weights specifications (including contiguity weights and migration flows). These results underscore the importance of considering spatial spillovers in MSA-level studies of housing price growth.\n\n==\n\nMartin 2011:\n\nTitle: The local geographies of the financial crisis: from the housing bubble to economic recession and beyond\n\nAbstract: The recent financial crisis, with its origins in the collapse of the sub-prime mortgage boom and house price bubble in the USA, is a shown to have been a striking example of \u2018glocalisation\u2019, with distinctly locally varying origins and global consequences and feedbacks. The shift from a \u2018locally originate and locally-hold\u2019 model of mortgage provision to a securitised \u2018locally originate and globally distribute\u2019 model meant that when local subprime mortgage markets collapsed in the USA, the repercussions were felt globally. At the same time, the global credit crunch and the deep recession the global financial crisis precipitated have had locally varying impacts and consequences. Not only does a geographical perspective throw important light on the nature and dynamics of the recent financial meltdown, the latter in turn should give impetus for a more general research effort into the economic geography of bubbles and crashes.\n\n==\n\nCohen 2012:\n\nTitle: The boom and bust of U.S. housing prices from various geographic perspectives\n\nAbstract: This paper summarizes changes in housing prices during the recent U.S. boom and bust from various geographic perspectives. Nationally, the Standard & Poor\u2019s/Case-Shiller house price index more than doubled in nominal terms during the boom and has fallen by roughly a third subsequently. During the boom, housing prices tended to rise much faster in metropolitan areas in the East and West Coast regions than in the country\u2019s interior. After adjusting for inflation, 7 of 19 metropolitan areas have experienced real declines in housing prices from the start of the boom to the present. Although lower-priced houses showed a larger percentage increase during the boom, higher-priced houses fared relatively better over the boom and bust. Changes in land prices, which are not easily measured, appear to have driven housing prices to a greater extent than changes in the prices of housing structures. Internationally, seven countries experienced housing booms and busts; however, these countries tended to have larger booms and smaller absolute busts than the United States.\n\n==\n\nMoscone 2014:\n\nTitle: Real estate market and financial stability in US metropolitan areas: A dynamic model with spatial effects\n\nAbstract: This paper investigates spatio-temporal variations in ex-post credit risk in the United States, as a function of real estate prices, loan purchases made by government sponsored enterprises, and a set of local characteristics during the recent housing boom and bust.\n\n==\n\nBrown 2013:\n\nTitle: Housing Foreclosure as A Geographically Contingent Event: Columbus Ohio 2003\u20132007\n\nAbstract: This article seeks to better understand geographic manifestations of housing foreclosure, moving beyond the usual portrayal that highlights, e.g., race/ethnicity and income. We depart from the usual analytical strategy which centers on factors that subsume high proportions of variance. Instead, this is the starting point for considering constellations and idiosyncratic but formative characteristics\u2014contingencies\u2014that further understanding of, e.g., why two households with identical attributes experience different outcomes. Empirical focus is on Columbus Ohio, 2003\u20132007. Regression analysis identifies central tendencies, followed by regression tree procedures that reveal variable combinations which alter correlational expectations. Unique areas are examined by neighborhood reconnaissance, exploratory data analysis, interviews, and archival research. Relevant factors include race/ethnicity and socio-economic characteristics. Beyond that, differing variable combinations lead to different outcomes, as do processes such as neighborhood life cycle, institutional actions/involvement, and year of home purchase/construction relative to housing de/inflation and mortgage market characteristics.",
        "final_summary": "The 2007 real estate crash in the United States had varied geographical impacts. Wang (2020) found that suburban areas in northern Columbus were more resilient to the recession. Aubuchon (2010) noted that bank failures were concentrated in regions experiencing real estate distress and economic decline. Aalbers (2009) highlighted the disproportionate impact of the crisis on low-income and minority communities across different states, cities, and neighborhoods. Cohen (2016) revealed significant spatial diffusion patterns in urban house price growth rates, with lagged price changes of neighboring areas showing greater effects after the 2007\u201308 housing crash. Martin (2011) discussed the local-global interplay of the crisis, with locally varying origins leading to global consequences. Cohen (2012) observed regional disparities in housing price changes during the boom and bust, with land prices playing a significant role. Moscone (2014) investigated variations in ex-post credit risk as a function of real estate prices and local characteristics. Lastly, Brown (2013) explored the geographic manifestations of housing foreclosure, identifying unique variable combinations that led to different outcomes."
    },
    {
        "query": "Does using novel methods (vs. older methods) of intubation in the emergency department influence the impact of hospital-, provider-, and patient-level characteristics on patient outcomes?",
        "paper_list_string": "Lee 2022:\n\nTitle: Experiences of healthcare providers with a novel emergency response intubation team during COVID-19\n\nAbstract: Objectives In the early stages of the COVID-19 pandemic, there were significant concerns about the infectious risks of intubation to healthcare providers. In response, a dedicated emergency response intubation team (ERIT) consisting of anesthesiologists and allied health providers was instituted for our emergency department (ED). Given the high-risk nature of intubations and the new interprofessional team dynamics, we sought to assess health-care provider experiences and potential areas of improvement. Methods Surveys were distributed to healthcare providers at the University Health Network, a quaternary healthcare centre in Toronto, Canada, which includes two urban EDs seeing over 128,000 patients per year. Participants included ED physicians and nurses, anesthesiologists, anesthesia assistants, and operating room nurses. The survey included free-text questions. Responses underwent thematic analysis using grounded theory and were independently coded by two authors to generate descriptive themes. Discrepancies were resolved with a third author. Descriptive themes were distilled through an inductive, iterative process until fewer main themes emerged. Results A total of 178 surveys were collected (68.2% response rate). Of these, 123 (69%) participated in one or more ERIT activations. Positive aspects included increased numbers of staff to assist, increased intubation expertise, improved safety, and good team dynamics within the ERIT team. Challenges included a loss of scope (primarily ED physicians and nurses) and unfamiliar workflows, perceived delays to ERIT team arrival or patient intubation, role confusion, handover concerns, and communication challenges between ED and ERIT teams. Perceived opportunities for improvement included interprofessional training, developing clear guidelines on activation, inter-team role clarification, and guidelines on handover processes post-intubation. Conclusions Healthcare providers perceived that a novel interprofessional collaboration for intubations of COVID-19 patients presented both benefits and challenges. Opportunities for improvement centred around interprofessional training, shared decision making between teams, and structured handoff processes.\n\n==\n\nSakles 2019:\n\nTitle: Improvement in the Safety of Rapid Sequence Intubation in the Emergency Department with the Use of an Airway Continuous Quality Improvement Program\n\nAbstract: Introduction Airway management in the critically ill is associated with a high prevalence of failed first attempts and adverse events which negatively impacts patient care. The purpose of this investigation is to describe an airway continuous quality improvement (CQI) program and its effect on the safety of rapid sequence intubation (RSI) in the emergency department (ED) over a 10-year period. Methods An airway CQI program with an ongoing airway registry was initiated in our ED on July 1, 2007 (Academic Year 1) and continued through June 30, 2017 (Academic Year 10). Data were prospectively collected on all patients intubated in the ED during this period using a structured airway data collection form. Key data points included method of intubation, drugs and devices used for intubation, operator specialty and level of training, number of intubation attempts, and adverse events. Adult patients who underwent RSI in the ED with an initial intubation attempt by emergency medicine (EM) resident were included in the analysis. The primary outcome was first pass success which was defined as successful tracheal intubation with a single laryngoscope insertion. The secondary outcome was the prevalence of adverse events associated with intubation. Educational and clinical interventions were introduced throughout the study period with the goal of optimizing these outcomes. Data were analyzed by academic year and are reported descriptively with 95% confidence intervals (CI) of the difference of means. Results EM residents performed RSI on 342 adult patients during Academic Year 1 and on 445 adult patients during Academic Year 10. Over the 10-year study period, first pass success increased from 73.1% to 92.4% (difference = 19.3%, 95% CI 14.0% to 24.6%). The percentage of patients who experienced an adverse event associated with intubation decreased from 22.5% to 14.4% (difference = \u22127.9%, 95% CI \u221213.4% to \u22122.4%). The percentage of patients with first pass success without an adverse event increased from 64.0% to 80.9% (difference = 16.9%, 95% CI 10.6% to 23.1%). Conclusion The use of an airway CQI program with an ongoing airway registry resulted in a substantial improvement in the overall safety of RSI in the ED as evidenced by an increase in first pass success and a decrease in adverse events.\n\n==\n\nDriver 2016:\n\nTitle: Direct Versus Video Laryngoscopy Using the C-MAC for Tracheal Intubation in the Emergency Department, a Randomized Controlled Trial.\n\nAbstract: BACKGROUND\nDirect laryngoscopy (DL) has long been the most common approach for emergency endotracheal intubation, although the use of video laryngoscopy (VL) is becoming more widespread. Current observational data suggest that VL has higher first-pass success, although randomized trials are lacking.\n\n\nOBJECTIVES\nThe objective was to compare first-pass success in patients undergoing emergency intubation with DL or VL using a C-MAC device.\n\n\nMETHODS\nThis was an open-label, prospective, randomized, controlled trial in an academic emergency department of patients undergoing emergency intubation with a plan of DL for the first attempt. Patients were randomly assigned in a 1:1 ratio to either DL or VL using a C-MAC device for the first intubation attempt. The primary outcome was first-pass success. Secondary outcomes included time to intubation, development of aspiration pneumonia, and hospital length of stay (LOS). The study was registered at Clinicaltrials.gov, number NCT01710891.\n\n\nRESULTS\nA total of 198 patients were enrolled and intubated with either DL (n\u00a0=\u00a095) or VL (n\u00a0=\u00a0103). First-attempt success was 86 and 92% for the DL and VL groups, respectively (difference\u00a0=\u00a0-5.9%, 95% confidence interval\u00a0=\u00a0-14.5% to 2.7%, p\u00a0=\u00a00.18). Time to intubation, rates of aspiration pneumonia, and hospital LOS were not different between the two groups.\n\n\nCONCLUSIONS\nIn patients undergoing emergency intubation in whom DL was planned for the first attempt, we did not detect a difference between VL or DL using the C-MAC device in first-pass success, duration of intubation attempt, aspiration pneumonia, or hospital LOS.\n\n==\n\nBrown 2015:\n\nTitle: Techniques, success, and adverse events of emergency department adult intubations.\n\nAbstract: STUDY OBJECTIVE\nWe describe the operators, techniques, success, and adverse event rates of adult emergency department (ED) intubation through multicenter prospective surveillance.\n\n\nMETHODS\nEighteen EDs in the United States, Canada, and Australia recorded intubation data onto a Web-based data collection tool, with a greater than or equal to 90% reporting compliance requirement. We report proportions with binomial 95% confidence intervals (CIs) and regression, with year as the dependent variable, to model change over time.\n\n\nRESULTS\nOf 18 participating centers, 5 were excluded for failing to meet compliance standards. From the remaining 13\u00a0centers, we report data on 17,583 emergency intubations of patients aged 15 years or older from 2002 to 2012. Indications were medical in 65% of patients and trauma in 31%. Rapid sequence intubation was the first method attempted in 85% of encounters. Emergency physicians managed 95% of intubations and most (79%) were physician trainees. Direct laryngoscopy was used in 84% of first attempts. Video laryngoscopy use increased from less than 1% in the first 3 years to 27% in the last 3 years (risk difference 27%; 95% CI 25% to 28%; mean odds ratio increase per year [ie,\u00a0slope] 1.7; 95% CI 1.6 to 1.8). Etomidate was used in 91% and succinylcholine in 75% of rapid sequence intubations. Among rapid sequence intubations, rocuronium use increased from 8.2% in the first 3 years to 42% in the last 3 years (mean odds ratio increase per year 1.3; 95% CI 1.3 to 1.3). The first-attempt intubation success rate was 83% (95% CI 83% to 84%) and was higher in the last 3 years than in the first 3 (86% versus 80%; risk difference 6.2%; 95% CI 4.2% to 7.8%). The airway was successfully secured in 99.4% of encounters (95% CI 99.3% to 99.6%).\n\n\nCONCLUSION\nIn the EDs we studied, emergency intubation has a high and increasing success rate. Both drug and device selection evolved significantly during the study period.\n\n==\n\nAdnet 1998:\n\nTitle: Survey of out-of-hospital emergency intubations in the French prehospital medical system: a multicenter study.\n\nAbstract: STUDY OBJECTIVE\nTo determine the clinical characteristics of endotracheal intubation in the French emergency prehospital medical system and compare these data with those of other systems.\n\n\nMETHODS\nThis study was performed in lle de France (Paris Region) in mobile ICUs staffed by physicians. This prospective, descriptive study involved completion of a questionnaire by the operator just after endotracheal intubation was performed.\n\n\nRESULTS\nSix hundred eighty-five (99.1%) of 691 consecutive prehospital intubations were performed successfully in the field. The orotracheal route was used in 96.0%, and no surgical approaches such as cricothyroidotomy were used. Mechanical complications occurred in 84 patients, at a rate of 15.9% for nonarrest patients and 8.1% for arrest patients. A wide variety of sedation protocols were used. Difficult intubations (10.8%) were comparable in incidence to the number seen in US emergency departments, not US prehospital systems. By the same token, intubation success rates (99.1%) were comparable to US EDs and much higher than US prehospital results.\n\n\nCONCLUSION\nThe characteristics of French prehospital airway management differ significantly from those of other countries. These differences may be explained by differences in approach to prehospital management rather than differences of skill.\n\n==\n\nRoppolo 1999:\n\nTitle: Nasotracheal intubation in the emergency department, revisited.\n\nAbstract: This retrospective study was designed to investigate the current practice of nasotracheal intubation (NTI) in the Emergency Department (ED) at the University of California, San Diego Medical Center. Over a 5-year period, 21% (105/501) of patients intubated in the ED had at least one NTI attempt. The most frequent primary diagnoses in these patients included drug overdose, congestive heart failure, and chronic obstructive pulmonary disease. We report an overall NTI success rate of 79% (83/105). Sixty-one percent (64/105) of the patients were nasally intubated on the first NTI attempt. Nasal dilators, topical neosynephrine, and sedation improved NTI success rates. Epistaxis and improper tube position were the most common immediate complications. Sinusitis, pneumonia, and sepsis were the most frequent late complications. Patients receiving thrombolytic therapy were at risk of developing severe epistaxis. A prior history of sinus disease may predispose a nasally intubated patient to sinusitis. The complication rates reported here are similar to those of previous studies. A survey of emergency medicine (EM) residency programs found that EM residents throughout the country perform an average of 2.8 NTIs during their residency training. Thus, there is limited exposure to this intubation technique in EM residency programs. Nasotracheal intubation is a useful alternative to oral intubation, particularly when oral access is compromised. While not the optimal approach, we conclude that NTI is still a valuable method for establishing an airway and should remain among the emergency physician's arsenal of intubation techniques.\n\n==\n\nWalls 2011:\n\nTitle: Emergency airway management: a multi-center report of 8937 emergency department intubations.\n\nAbstract: OBJECTIVE\nEmergency department (ED) intubation personnel and practices have changed dramatically in recent decades, but have been described only in single-center studies. We sought to better describe ED intubations by using a multi-center registry.\n\n\nMETHODS\nWe established a multi-center registry and initiated surveillance of a longitudinal, prospective convenience sample of intubations at 31 EDs. Clinicians filled out a data form after each intubation. Our main outcome measures were descriptive. We characterized indications, methods, medications, success rates, intubator characteristics, and associated event rates. We report proportions with 95% confidence intervals and chi-squared testing; p-values < 0.05 were considered significant.\n\n\nRESULTS\nThere were 8937 encounters recorded from September 1997 to June 2002. The intubation was performed for medical emergencies in 5951 encounters (67%) and for trauma in 2337 (26%); 649 (7%) did not have a recorded mechanism or indication. Rapid sequence intubation was the initial method chosen in 6138 of 8937 intubations (69%) and in 84% of encounters that involved any intubation medication. The first method chosen was successful in 95%, and intubation was ultimately successful in 99%. Emergency physicians performed 87% of intubations and anesthesiologists 3%. Several other specialties comprised the remaining 10%. One or more associated events were reported in 779 (9%) encounters, with an average of 12 per 100 encounters. No medication errors were reported in 6138 rapid sequence intubations. Surgical airways were performed in 0.84% of all cases and 1.7% of trauma cases.\n\n\nCONCLUSION\nEmergency physicians perform the vast majority of ED intubations. ED intubation is performed more commonly for medical than traumatic indications. Rapid sequence intubation is the most common method of ED intubation.\n\n==\n\nOkubo 2017:\n\nTitle: The effectiveness of rapid sequence intubation (RSI) versus non-RSI in emergency department: an analysis of multicenter prospective observational study\n\nAbstract: BackgroundAlthough rapid sequence intubation (RSI) is the method of choice in emergency department (ED) airway management, data to support the use of RSI remain scarce. We sought to compare the effectiveness of airway management between RSI and non-RSI (intubation with sedative agents only or without medications) in the ED.MethodsSecondary analysis of the data from a multicenter prospective observational registry at 13 Japanese EDs. All non-cardiac-arrest patients who underwent intubation with RSI or non-RSI were included for the analysis. Outcomes of interest were the success rate of intubation and intubation-related complications.ResultsOf 2365 eligible patients, 761 (32%) underwent intubations with RSI and 1,604 (68%) with non-RSI. Intubations with RSI had a higher success rate on the first attempt compared to those with non-RSI (73 vs. 63%; P\u2009<\u20090.0001). By contrast, the complication rates did not differ significantly between RSI and non-RSI groups (12 vs. 13%; P\u2009=\u20090.59). After adjusting for age, sex, estimated weight, principal indication, device, specialties and training level of the intubator, and clustering of patients within EDs, intubation with RSI was associated with a significantly higher success rate on the first attempt (OR, 2.3; 95% CI, 1.8\u20132.9; P\u2009<\u20090.0001) while that with RSI was not associated with the risk of complications (OR, 0.9; 95% CI, 0.6\u20131.2; P\u2009=\u20090.31).ConclusionsIn this large multicenter study of ED airway management, we found that intubation with RSI was independently associated with a higher success rate on the first attempt but not with the risk of complications.",
        "final_summary": "The papers suggest that the use of novel methods of intubation in the emergency department can influence patient outcomes. Lee 2022 found that a dedicated emergency response intubation team improved safety and team dynamics during intubations. Sakles 2019 demonstrated that the implementation of an airway continuous quality improvement program led to increased first-pass success and decreased adverse events during rapid sequence intubation. Driver 2016 compared direct laryngoscopy (DL) and video laryngoscopy (VL) and found no significant difference in first-pass success between the two methods. Brown 2015 reported a high and increasing success rate of emergency department intubations over a 10-year period. Adnet 1998 highlighted differences in airway management practices in the French prehospital medical system compared to other countries. Roppolo 1999 discussed the success and complications of nasotracheal intubation in the emergency department. Walls 2011 described the characteristics of emergency department intubations, including high success rates and low complication rates. Okubo 2017 concluded that rapid sequence intubation was associated with a higher first-pass success rate compared to non-RSI methods."
    },
    {
        "query": "what is the Satir Change Model and its importance for organizational change",
        "paper_list_string": "Sayles 2002:\n\nTitle: Transformational Change\u2014Based on the Model of Virginia Satir\n\nAbstract: Transformational change using the basic constructs of the Satir Process Model provides a road map for assisting individuals, couples, and families through a transformational shift in their basic belief or understanding of themselves. The process of change is a way to assist people in making choices that increase self-esteem, provide self-accountability, and move a person toward a more congruent sense of self. The transformation that is experienced is an internal process where the universal yearnings to be loved, acknowledged, and accepted can be met by the individual \u201cself.\u201d\n\n==\n\nHussain 2018:\n\nTitle: Kurt Lewin's change model: A critical review of the role of leadership and employee involvement in organizational change\n\nAbstract: Abstract Change is crucial for organizations in growing, highly competitive business environments. Theories of change describe the effectiveness with which organizations are able to modify their strategies, processes, and structures. The action research model, the positive model, and Lewin's change model indicate the stages of organizational change. This study examined the three stages of Lewin's model: unfreezing, movement, and refreezing. Although this model establishes general steps, additional information must be considered to adapt these steps to specific situations. This article presents a critical review of change theories for different stages of organizational change. In this critical review, change management offers a constructive framework for managing organizational change throughout different stages of the process. This review has theoretical and practical implications, which are discussed in this article. Immunity to change is also discussed.\n\n==\n\nMendes 2003:\n\nTitle: Applying Business Process Modeling to Organizational Change\n\nAbstract: Organizational change can be regarded as a process that changes the state of the organization. This simple yet powerful idea is the ground basis for the work being presented in this paper. This original work brings a new dimension to business modeling: time. Looking at the organization as a dynamic entity that changes through time, allows us to reason over the processes that make the organization evolve, in a formal and objective way. The purpose of this research is to develop tools that help managers to drive business more effectively through time, by keeping in perspective all the issues regarding change in the organization. The first step towards this goal is to create an adequate modeling environment that can be used to represent (1) the organization (at different time frames) and (2) the change processes (that make the organization evolve between time frames). The modeling framework being developed uses UML and, more specifically, the CEO Framework, a business process oriented UML modeling framework. This research is based on a vast theoretical study of process modeling techniques and application domains, as well as on hands-on industry experience in several change processes in real-life companies. The paper presents a formal definition of all the concepts involved in this research, the first version of the UML Profile for Business Change Modeling that is under development and the preliminary results of the application of this research in the Portuguese Army.\n\n==\n\nAppelbaum 1998:\n\nTitle: Strategic organizational change: the role of leadership, learning, motivation and productivity\n\nAbstract: Presents an overview of strategic organizational change (SOC) and its managerial impact on leadership, learning, motivation and productivity. Theoretical and empirical data presented are: the sources and determinants of strategic organizational change; the management implications of SOC; organizational leadership within the context of SOC; learning aspects of SOC; the impact of SOC on organizational and individual productivity; a model that explains the relationships between SOC, leadership, learning, motivation and productivity. Depicts strategic organizational change as an integrative process with all organizational elements such as human resources, systems and technologies being considered for successful change to occur. The proposed model for strategic organizational change is an attempt to link the software and hardware components of organizations. In view of the pressures being expected from the external environment and the critical vision of organizations, research suggests that top management needs to establish a flexible and adaptive infrastructure that should lead contemporary and complex organizations to optimum levels of performance. The largest barrier to \u201cchange\u201d is not changes to technologies and work processes but changes involving people.\n\n==\n\nProchaska 2004:\n\nTitle: A Transtheoretical Approach to Changing Organizations\n\nAbstract: Several authors have reflected on the underdeveloped state of organizational change theory, research, and practice. This field needs an integrative framework that can synthesize major approaches to change. The Transtheoretical Model has the potential to do for organizational change what it has done for individual behavior change. Conceptually the stages-of-change dimension can be used to integrate principles and processes of change from divergent models of change. Practically, the stages-of-change dimension can be applied by leaders to reduce resistance, increase participation, reduce dropout, and increase change progress among employees.\n\n==\n\nHussain 2016:\n\nTitle: Kurt Lewin\\'s process model for organizational change: The role of leadership and employee involvement: A critical review\n\nAbstract: Change is crucial for organizations in continuous growing and high competition in business environment. Different theories of change describe the effectiveness of modification of strategies, processes and structures for organizations. The action research model, the positive model and the Lewin's change model submit the views of the phases for change in organization. This study views the Lewin's model as three steps process (unfreezing, movement and refreezing) for change in organization. Although this model sets a general steps to be followed, more information is considered to guide these steps in specific situations. This article is critically reviewed for change theories in different phases of organizational change. In this critical review the change management has constructive framework for managing the organizational change through different phases of the process. This review gives theoretical and practical implications and somehow the immunity to change has been discussed.\n\n==\n\nBold 2011:\n\nTitle: A Perspective on Organizational Change Issue \u2013 Case Study on the Application of Two Change Models\n\nAbstract: The organization is a social system in which people interact to achieve common goals. We observe an acceleration of change in the value systems worldwide, an adoption of new attitudes, new goals, and aspects leading to rapid change of the society. To survive and develop, an organization must first know how to anticipate and capitalize on the opportunities and to attenuate or eliminate the vulnerabilities. Second, the organization must demonstrate the ability to effectively manage the change process, from detailed investigation of its past and present. These are the fundamental premises on which the change management is based. The research and practice of the change management models summarize a basis for understanding the change processes and mechanisms at both organizational and individual levels. In the following we will present some change management models that may be useful in managing change in economic organizations and whose applicability and implementation proved to be effective for identifying outcomes of change in organizations.\n\n==\n\nParry 2014:\n\nTitle: Empirical Development of a Model of Performance Drivers in Organizational Change Projects\n\nAbstract: In order to investigate the drivers that impact on the success of change projects, a major research project was commenced in 1998. Extensive quantitative studies of organizational change processes were conducted across industries, countries and companies, in order to establish the determinants of successful change projects. This article presents the initial step in this project, the development of a descriptive model that provides the basis for the assessment of change projects and the analysis of interactions between key success factors and performance outcomes. Forty-four survey questions were put to 117,355 employees involved in or affected by organizational change. The results of factor analyses were used to guide the development of a descriptive model of the drivers of performance in organizational change projects. This article presents the results of the factor analyses, and the ChangeTracking model that has been developed. The ChangeTracking model consists of: two outcome variables, realizing business benefits and business performance; and six key drivers, the amount of change and turbulence, available resources, alignment with the company's vision and direction, quality of change management, work roles and emotional energy. The ChangeTracking model provides an empirically derived model of change management to guide future research and practice.",
        "final_summary": "The papers collectively provide insights into the Satir Change Model and its importance for organizational change. Sayles (2002) highlights that the Satir Process Model facilitates transformational change by assisting individuals, couples, and families in shifting their beliefs and understanding of themselves. The model focuses on increasing self-esteem, self-accountability, and congruence. On the other hand, Hussain (2018) discusses Kurt Lewin's change model, which includes the stages of unfreezing, movement, and refreezing. This model emphasizes the need for leadership and employee involvement in organizational change. Mendes (2003) introduces the concept of applying business process modeling to organizational change, emphasizing the importance of considering time and formal modeling techniques to drive effective change. Appelbaum (1998) explores the role of leadership, learning, motivation, and productivity in strategic organizational change, emphasizing the need for an integrative approach that considers all organizational elements. Prochaska (2004) suggests that the Transtheoretical Model, commonly used for individual behavior change, has the potential to be applied to organizational change by integrating principles and processes from different change models. Hussain (2016) further discusses Lewin's change model, highlighting the three-step process of unfreezing, movement, and refreezing, while also acknowledging the need for additional information to guide these steps in specific situations. Bold (2011) presents change management models that can be useful in managing change in economic organizations, emphasizing the importance of effective change management and understanding outcomes. Lastly, Parry (2014) presents a descriptive model, the ChangeTracking model, which identifies key drivers of performance in organizational change projects, including the amount of change, available resources, alignment with the company's vision, quality of change management, work roles, and emotional energy. \n\nIn summary, the papers collectively emphasize the importance of various change models, such as the Satir Process Model, Lewin's change model, and the Transtheoretical Model, in facilitating organizational change. They highlight the significance of leadership, employee involvement, effective change management, and understanding outcomes for successful change initiatives."
    },
    {
        "query": "Rent-seeking behaviour, where companies seek preferential treatment, further diverts resources to less productive uses. The cumulative impact of these distortions significantly reduces overall productivity. Government intervention and ownership contribute to resource allocation distortions by favouring politically connected firms and sectors over more productive ones (Zhang 2021). Consequently, resources tend to flow towards less efficient state-owned enterprises and politically motivated projects, hindering productivity and hindering economic growth (Nicola 2020).",
        "paper_list_string": "Deacon 2015:\n\nTitle: Rent seeking and the resource curse\n\nAbstract: Many countries receiving natural resource windfalls suffer from slow growth, low incomes and weak political institutions, an empirical regularity dubbed the resource curse. Patterns in the data suggest a political link is involved: some countries escape this fate, those that succumb generally have weak institutions initially, and a curse is most likely when the resource is spatially concentrated. Rentseeking for a resource prize is a prominent theme in theoretical explanations of the curse. While the precise mechanisms vary, several postulate that rent-seeking diverts activity or resources away from productive employment, e.g., private capital may be shifted to a less productive but secure sector, potential entrepreneurs may be attracted into rent-seeking rather than wealth creation, labor may be diverted away from producing output and toward competing for a resource rent prize. While empirical results remain somewhat controversial, there is now extensive evidence supporting both the curse and a political transmission channel. Three aspects of this work are particularly important for rent-seeking: (i) political theories of the resource curse consistently predict more than 100 percent dissipation of the resource windfall, a finding at odds with theoretical treatments of rentseeking; (ii) variations in pre-windfall political institutions can magnify, moderate of overturn the resource curse effect; and (iii) a resource windfall can alter the quality of political institutions.\n\n==\n\nMurphy 1993:\n\nTitle: Why Is Rent-Seeking So Costly to Growth?\n\nAbstract: Economists from Adam Smith (1776) to Douglass C. North (1981) agree that poor protection of property rights is bad for growth. But why is this problem so severe? Why do Peru (Hernando De Soto, 1989) and Equatorial Guinea (Robert Klitgaard, 1990) fail to grow at all when public and private rent-seeking make property insecure? In this paper, we explore two reasons why rent-seeking, meaning any redistributive activity that takes up resources, is so costly to growth. First, rent-seeking activities exhibit very natural increasing returns. That is, an increase in rent-seeking activity may make rent-seeking more (rather than less) attractive relative to productive activity. This condition can lead to multiple equilibria in the economy, with \"bad\" equilibria exhibiting very high levels of rent-seeking and low output.l Second, rent-seeking, particularly public rent-seeking by government officials, is likely to hurt innovative activities more than everyday production. Since innovation drives economic growth, public rent-seeking hampers growth more severely than production.\n\n==\n\nEsteban 2006:\n\nTitle: Inequality, Lobbying and Resource Allocation\n\nAbstract: This paper describes how wealth inequality may distort public resource allocation. A government seeks to allocate limited resources to productive sectors, but sectoral productivity is privately known by agents with vested interests in those sectors. They lobby the government for preferential treatment. The government\u2014even if it honestly seeks to maximize economic efficiency\u2014may be confounded by the possibility that both high wealth and true economic desirability create loud lobbies. Broadly speaking, both poorer economies and unequal economies display greater public misallocation. The paper warns against the conventional wisdom that this is so because such governments are more \"corrupt.\"\n\n==\n\nKurer 1993:\n\nTitle: Clientelism, corruption, and the allocation of resources\n\nAbstract: ConclusionCorruption has been shown to have more serious effects upon allocation than simply drawing resources into rent-seeking activities. If politicians anticipate gains from corruption in the policy making process, sub-optimal policies will be the result. Typical examples include excessive nationalization of industry, excessive regulation of the private sector through practices such as licensing and quotas with a view to extract the rent created by these measures. In addition, further misallocation will result at the execution state. It has been shown that the revisionists were wrong in assuming that licenses and contracts will be allocated to the most efficient producers, that rent will be eliminated in government employment, and that corruption increases the efficiency of the bureaucracy. Moreover, differential access to government services leads to additional allocative losses.\n\n==\n\nNgo 2008:\n\nTitle: Rent-seeking and economic governance in the structural nexus of corruption in China\n\nAbstract: This article examines the sources of widespread rent-seeking practices and their relations to corruption in China. It argues that rent-creation and seeking are difficult to eliminate because they have become institutionalized as the constitutive parts of economic governance. Using case studies drawn from a number of industrial sectors, this study shows that the creation and allocation of economic rents has become a major policy instrument used by various levels of governments to manoeuvre industrial plans and development priorities. At the same time, the discretionary power exercised by state agents in implementing development priorities has created mounting opportunities for exchanging rents with bribes. The result is a structural nexus between economic governance, rent-seeking and corruption that defy administrative measures aiming at achieving a clean government in China.\n\n==\n\nScharfstein 1997:\n\nTitle: The Dark Side of Internal Capital Markets: Divisional Rent-Seeking and Inefficient Investment\n\nAbstract: We develop a two-tiered agency model that shows how rent-seeking behavior on the part of division managers can subvert the workings of an internal capital market. By rent-seeking, division managers can raise their bargaining power and extract greater overall compensation from the CEO. And because the CEO is herself an agent of outside investors, this extra compensation may take the form not of cash wages, but rather of preferential capital budgeting allocations. One interesting feature of our model is that it implies a kind of \u201csocialism\u201d in internal capital allocation, whereby weaker divisions get subsidized by stronger ones.\n\n==\n\nZhang 2007:\n\nTitle: Incremental Reform and Distortions in China\u2019s Product and Factor Markets\n\nAbstract: The purpose of economic reform is to reduce distortions and enhance efficiency. However, when reforms are partial and incremental, individuals and local governments are often able to capture the rent inherent in the gradual transition process. Young (2000) warned that such rent-seeking behavior might lead to increasing market fragmentation. Empirical studies have shown the opposite in the product market. This article argues that as the rent from China\u2019s product market has been squeezed out due to deepening reforms, rent-seeking behavior may have shifted to the capital market. Further reforms are needed in the capital market to squeeze out these rent-seeking opportunities, just as those from the product and labor markets were squeezed out earlier.\n\n==\n\nAuty 2000:\n\nTitle: Political economy of resource abundant states\n\nAbstract: Natural resource-abundance tends to undermine investment efficiency whereas a resource-poor endowment places a premium on efficiency. The type of political state provides the link between the natural resource endowment and the economic outcome. Developmental political states tend to be associated with resource-poor countries, albeit not exclusively so. Their governments pursue a coherent economic policy that maximizes social welfare. Resource-poor countries with developmental states industrialize early and follow a competitive industrialization model that sustains rapid and egalitarian growth. In resource abundant countries, contests for rents generate factional and predatory states that distribute the rents through indirect means that distort the economy. The most likely result is a growth collapse, recovery from which may take decades.",
        "final_summary": "The papers suggest that rent-seeking behavior and government intervention contribute to resource allocation distortions, hindering productivity and economic growth. Deacon 2015 highlights that rent-seeking diverts resources away from productive employment, potentially attracting entrepreneurs to rent-seeking rather than wealth creation. Murphy 1993 emphasizes that rent-seeking exhibits increasing returns, making it more attractive than productive activity and leading to multiple equilibria with high rent-seeking and low output. Esteban 2006 argues that wealth inequality can distort public resource allocation as vested interests lobby for preferential treatment. Kurer 1993 adds that corruption and rent-seeking result in sub-optimal policies and misallocation of resources. Ngo 2008 explores how rent-seeking has become institutionalized in China's economic governance, creating a structural nexus with corruption. Scharfstein 1997 shows how rent-seeking by division managers can undermine internal capital markets and lead to inefficient investment. Zhang 2007 suggests that incremental reforms may shift rent-seeking behavior from product markets to capital markets. Auty 2000 concludes that resource-abundant countries with factional and predatory states experience growth collapses due to rent-seeking."
    },
    {
        "query": "access to primary health care for diabetes West Africa",
        "paper_list_string": "Nielsen 2017:\n\nTitle: Accessing diabetes care in rural Uganda: Economic and social resources\n\nAbstract: ABSTRACT Non-communicable diseases including type 2 diabetes (T2D) are increasing rapidly in most Sub-Saharan African (SSA) countries like Uganda. Little attention has been given to how patients with T2D try to achieve treatment when the availability of public health care for their disease is limited, as is the case in most SSA countries. In this paper we focus on the landscape of availability of care and the therapeutic journeys of patients within that landscape. Based on fieldwork in south-western Uganda including 10 case studies, we explore the diabetes treatment options in the area and what it takes to access the available treatment. We analyse the resources patients need to use the available treatment options, and demonstrate that the patients\u2019 journeys to access and maintain treatment are facilitated by the knowledge and support of their therapy management groups. Patients access treatment more effectively, if they and their family have money, useful social relations, and knowledge, together with the capacity to communicate with health staff. Patients coming from households with high socio-economic status (SES) are more likely to have all of these resources, while for patients with low or medium SES, lack of economic resources increases the importance of connections within the health system.\n\n==\n\nBeran 2018:\n\nTitle: Access to diabetes care and treatment in Africa\u00a0: challenges and opportunities.\n\nAbstract: Access to drugs is a key element in the treatment of diabetes. Access remains an issue because of their price, availability, and especially affordability. Two elements included in the sustainable development goals -\u00a0universal health coverage and partnerships\u00a0- offer an opportunity to improve access.\n\n==\n\nAssayed 2014:\n\nTitle: Primary health care approach to Diabetes mellitus in Malawi\n\nAbstract: Diabetes Mellitus (DM) is one of non-communicable diseases that have public health significance. In Malawi, it is estimated that diabetes prevalence rate is 5.6% among the age group 25-64 in the year 2009. In Malawi, diabetes has not gained adequate attention in terms of all resources such as personnel, medicinal and laboratory equipment and reagents. Primary Health Care approach to the management of diabetes is not only low labour intensive but improves patient retention as well especially in rural areas. Health Education, for instance, improves self- care and life style. Adopting this strategy will help poor Malawians affected with diabetes access the much needed care affordably. Nurse-led diabetes clinics strategy hits two birds with one stone in that remote areas \u2013where nurses already working- could be easily provided with the services and nurses are the most available cadre in the hands of the Ministry of Health. So, Primary Health Care (PHC) approach, if carefully used, will help Malawians affected with diabetes to have access to the much needed care thereby improving their own quality of life.\n\n==\n\nFeleke 2006:\n\nTitle: An assessment of the health care system for diabetes in Addis Ababa, Ethiopia\n\nAbstract: Background : During the past decade, diabetes mellitus has emerged as an important clinical and public health problem through out the world. WHO estimated the number of cases of diabetics in Ethiopia to be about 800,000 in 2000, and projected that it would increase to about 1.8 million by the year 2030. Care for diabetic patients may require close and sustained support from a health care team, adequate financial resources, and advanced patient knowledge and motivation. In this respect, there is lack of information in the country. Objective : This study was conducted with the aim of assessing the characteristics of the health care system for diabetic patients, how diabetic patients are cared for including medical care and adequacy of facilities for the prevention of complications and outcomes at primary health care and secondary health care levels. Methods : An assessment of the characteristics of the health care system (structure) for diabetic patients was conducted in 21 health centres (primary health care level) and 5 regional hospitals (secondary health care level) in Addis Ababa. Furthermore, a total of 106 diabetic patients drawn from six out of the 21 health centres and another 123 diabetic patients drawn from three of the six hospitals were consecutively interviewed. Results : All the Regional hospitals and one of the health centres were running established diabetic referral clinics. Similarly, a lack of professionals was observed in all the health institutions in general and the health centres, in particular. Only 21% of patients had access for blood glucose monitoring at the same health institutions. The emphasis given for diabetic education (24%) was less than expected. Only 11 (5%) of diabetic patients were able to do self blood glucose monitoring at home. Fifty one percents of patients didn't have urine analysis, BUN, creatinine and lipid profile in the previous 1-2 years. None of diabetic patients had haemoglobin Alc (HbA1c) determination. Nearly 75% of the patients required admissions directly or indirectly due to uncontrolled diabetes. About 87% of the diabetics had regular follow ups at their respective health centres and hospitals. Hypertension (34%), diabetes related eye disease (33%) and renal disease (21%) were the major associated illnesses observed among the diabetics. Sixty-six patients (23%) had a total of about 131 admissions. Conclusion : Although there is a well-established health infrastructure for diabetics care in Addis Ababa, the diabetic care is below the acceptable standard. The finding of this study may, thus, help to clarify issues related to potential changes in the health care system dealing with diabetes and for strengthening the referral system for diabetes health care. Ethiopian Journal of Health Development Vol. 19(3) 2005: 203-210\n\n==\n\nWhiting 2003:\n\nTitle: Challenges to Health Care for Diabetes in Africa\n\nAbstract: The aim of this review is to summarize the contextual, clinical and health system challenges to the delivery of health care for diabetes in Africa. Planners need to allocate sufficient resources in a context where resources for health in general are insufficient. Choices need to be made between different options for health care within this context and mechanisms are required to facilitate the implementation of the selected options and ensure that quality of care is maintained.\n\n==\n\nWhiting 2003:\n\nTitle: Diabetes in Africa. Challenges to health care for diabetes in Africa.\n\nAbstract: : The aim of this review is to summarize the contextual, clinical and health system challenges to the delivery of health care for diabetes in Africa. Planners need to allocate sufficient resources in a context where resources for health in general are insufficient. Choices need to be made between different options for health care within this context and mechanisms are required to facilitate the implementation of the selected options and ensure that quality of care is maintained.\n\n==\n\nHall 2011:\n\nTitle: Diabetes in Sub Saharan Africa 1999-2011: Epidemiology and public health implications. a systematic review\n\nAbstract: BackgroundDiabetes prevalence is increasing globally, and Sub-Saharan Africa is no exception. With diverse health challenges, health authorities in Sub-Saharan Africa and international donors need robust data on the epidemiology and impact of diabetes in order to plan and prioritise their health programmes. This paper aims to provide a comprehensive and up-to-date review of the epidemiological trends and public health implications of diabetes in Sub-Saharan Africa.MethodsWe conducted a systematic literature review of papers published on diabetes in Sub-Saharan Africa 1999-March 2011, providing data on diabetes prevalence, outcomes (chronic complications, infections, and mortality), access to diagnosis and care and economic impact.ResultsType 2 diabetes accounts for well over 90% of diabetes in Sub-Saharan Africa, and population prevalence proportions ranged from 1% in rural Uganda to 12% in urban Kenya. Reported type 1 diabetes prevalence was low and ranged from 4 per 100,000 in Mozambique to 12 per 100,000 in Zambia. Gestational diabetes prevalence varied from 0% in Tanzania to 9% in Ethiopia. Proportions of patients with diabetic complications ranged from 7-63% for retinopathy, 27-66% for neuropathy, and 10-83% for microalbuminuria. Diabetes is likely to increase the risk of several important infections in the region, including tuberculosis, pneumonia and sepsis. Meanwhile, antiviral treatment for HIV increases the risk of obesity and insulin resistance. Five-year mortality proportions of patients with diabetes varied from 4-57%. Screening studies identified high proportions (> 40%) with previously undiagnosed diabetes, and low levels of adequate glucose control among previously diagnosed diabetics. Barriers to accessing diagnosis and treatment included a lack of diagnostic tools and glucose monitoring equipment and high cost of diabetes treatment. The total annual cost of diabetes in the region was estimated at US$67.03 billion, or US$8836 per diabetic patient.ConclusionDiabetes exerts a significant burden in the region, and this is expected to increase. Many diabetic patients face significant challenges accessing diagnosis and treatment, which contributes to the high mortality and prevalence of complications observed. The significant interactions between diabetes and important infectious diseases highlight the need and opportunity for health planners to develop integrated responses to communicable and non-communicable diseases.\n\n==\n\nLevitt 2008:\n\nTitle: Diabetes in Africa: epidemiology, management and healthcare challenges\n\nAbstract: Diabetes is an increasing problem in sub-Saharan Africa. Type 2 diabetes, the most common form, is becoming more prevalent owing to rising rates of obesity, physical inactivity and urbanisation. Type 1 diabetes exists in two major forms in the region: type 1A or autoimmune and type 1B or ketosis-prone type 2 diabetes. At present there are scanty epidemiological data on either. The current morbidity of diabetes is primarily due to the high rates of microvascular complications, while macrovascular complications, once rare, are becoming more common, particularly in the urban setting. Further, despite the HIV epidemic, the total number of people with diabetes in the region is expected to grow because of changing demography. A concerted multisectoral effort will be critical to ensuring improvement in healthcare delivery for people with diabetes in the region.",
        "final_summary": "Access to primary health care for diabetes in West Africa is influenced by economic and social resources, with patients from high socio-economic status having better access (Nielsen 2017). The affordability and availability of diabetes drugs pose significant challenges (Beran 2018). In Malawi, a primary health care approach, particularly nurse-led diabetes clinics, is suggested as a strategy to improve access to care (Assayed 2014). However, in Addis Ababa, Ethiopia, despite a well-established health infrastructure, the quality of diabetic care is below the acceptable standard (Feleke 2006). The need for sufficient resources and mechanisms to facilitate the delivery of diabetes care in Africa is emphasized (Whiting 2003). High prevalence of diabetes, barriers to diagnosis and treatment, and the economic impact of diabetes are highlighted in Sub-Saharan Africa (Hall 2011). The increasing prevalence of diabetes in the region necessitates a multisectoral effort to improve healthcare delivery (Levitt 2008)."
    },
    {
        "query": "what is the relation between cervical disc pathology and occupation as a  a crane operator?",
        "paper_list_string": "Piccinni 1992:\n\nTitle: [The prevalence of spondylopathies among the crane operators in the port of Venice].\n\nAbstract: A group of 78 crane operators were examined using the \"EPM Research Unit\" method for assessing spine function. Crane operators work in a fixed posture characterized by a flexed position of the cervical spine, isometric load of the extensor cervical muscles and increased dorsal kyphosis. The prevalence of spinal disorders was 39.7% for the cervical tract, 37.2% for the dorsal tract and 38.5% for the lumbar region. The results were compared with those concerning a control group. A statistically significant difference was found in the number of cervical and dorsal complaints (O.R. = 3.33 and 2.69), whereas the difference was nearly significant for the lumbar tract (O.R. = 1.65). The results suggest that this category of workers may be subject to an increased risk for the spine.\n\n==\n\nHagberg 1987:\n\nTitle: Prevalence rates and odds ratios of shoulder-neck diseases in different occupational groups.\n\nAbstract: The aim of the present study was to evaluate the association and impact of occupational exposure and diseases of the shoulder and neck. Prevalence rates, odds ratios, aetiological fractions, and their confidence intervals were computed for pooled and non-pooled data of previous published reports. By comparison with office workers and farmers, dentists had an increased odds ratio for cervical spondylosis (two studies) and for shoulder joint osteoarthrosis. Meat carriers, miners, and \"heavy workers\" also had significantly higher rates of cervical spondylosis compared with referents. Compared with iron foundry workers, civil servants had a significant odds ratio (4.8) of cervical disc disease and a 0.79 aetiological fraction. Whether this was due to exposure or healthy worker effect was not clear. In four occupational groups with high shoulder-neck load an odds ratio of 4.0 was found for thoracic outlet syndrome with an aetiological fraction of 0.75. Rotator cuff tendinitis in occupational groups with work at shoulder level (two studies) showed an odds ratio of 11 and an aetiological fraction of 0.91. Keyboard operators had an odds ratio of 3.0 for tension neck syndrome (five studies). Unfortunately, owing to the scanty description of the work task, the exposure could be analysed only by job title. Examination of published reports shows clearly that certain job titles are associated with shoulder-neck disorders. High rates and aetiological fractions for rotator cuff tendinitis and tension neck syndrome suggest that preventive measures could be effective. Although job descriptions are brief, the associations noted suggest that highly repetitive shoulder muscle contractions, static contractions, and work at shoulder level are hazardous exposure factors. In reports of cross sectional studies of occupational shoulder-neck disorders presentation of age, exposure, and effect distribution may help for future meta-analysis.\n\n==\n\n\u00c5lund 1992:\n\nTitle: Work-related chronic neck impairment. Neck motion analysis in female traverse crane operators.\n\nAbstract: Twenty-one female steel industry traverse crane operators with long-term sick-leave (3 (1-8) years) due to chronic neck disability underwent careful analysis of case history, physical status and electrogoniometric three-dimensional recordings of active neck motion. Results were compared with those from working female crane operators having identical work posture and tasks and, further, with a group of working female clerks. The sick-listed crane operators had previous frequent contacts with the primary health care because of complaints from the neck and back. In comparison with the reference groups, the sick-listed crane operators showed tenderness of the trapezius and levator scapulae muscles and a short neck stature in combination with impaired active neck motion range with reduced motion speed. The motion pattern was however unchanged. The findings are consistent with the clinical picture of chronic neck myalgia that persisted despite long-term absence from the previous exposure to high static work load upon the neck-shoulders.\n\n==\n\nNourollahi-Darabad 2017:\n\nTitle: Full shift assessment of back and head postures in overhead crane operators with and without symptoms\n\nAbstract: Prolonged sitting with a flexed back and neck is recognized as being associated with an increased risk of neck and back pain disorders among overhead crane operators. The aim of this study was to compare back and head postures over a full shift of work between operators who experience back and neck pain, and healthy operators.\n\n==\n\nHagen 1997:\n\nTitle: Relationship Between Subjective Neck Disorders and Cervical Spine Mobility and Motion-Related Pain in Male Machine Operators\n\nAbstract: Study Design. In a single blinded procedure, quantified cervical clinical tests were performed on machine operators with and without subjective reports of neck pain. Objectives. To evaluate whether subjective reports of neck pain are associated with limited and painful cervical spine mobility. Summary of Background Data. Spinal motion evaluation and pain rating are commonly used for assessing impairment in patients with spinal disorders. However, it is still unclear how cervical spinal motion and corresponding pain rating are affected by subjective reports of neck pain. Methods. Forty-nine male forest machine operators completed the Standardized Nordic questionnaire on musculoskeletal symptoms. Maximal voluntary cervical spinal flexion-extension, bilateral axial rotation, and lateral flexion were measured with a goniometer ad modum Myrin, while corresponding pain was rated on Borg's scale. Results. Nineteen men (38.8%) reported no neck pain within the past 12 months, nine men (18.4%) reported pain during the past 12 months (but not during the past 7 days), and 21 men (42.8%) had experienced neck pain during the past 7 days. The results of two of the range-of-motion tests (flexion and left axial rotation) and three of the pain ratings (during flexion, extension, and left axial rotation) differed significantly between men who reported pain the past 7 days and those with no pain, and they correlated significantly with the severity of pain as reported in the questionnaire. Conclusions. Many forest machine operators reporting neck pain in the Nordic questionnaire did have limited and painful cervical spine mobility. Range-of-motion tests may be useful tools to describe impairment and constitute a basis for assessment for therapeutic interventions.\n\n==\n\nLuoma 1998:\n\nTitle: Lumbar disc degeneration in relation to occupation.\n\nAbstract: OBJECTIVES\nThe aim was to study risk factors of lumbar disc degeneration demonstrable with magnetic resonance imaging (MRI) with special emphasis on occupational load and back accidents.\n\n\nMETHODS\nThe subjects in this cross-sectional study were 53 machine drivers, 51 construction carpenters, and 60 municipal office workers aged 40-45 years. Data on possible risk factors were available from current structured questionnaires and for 4 and 7 years in retrospect. The prevalence of lumbar disc degeneration L2/L3-L5/S1 was determined with MRI.\n\n\nRESULTS\nAn increased risk was found for posterior disc bulges among the carpenters and for anterior disc bulges among the machine drivers, but decreased signal intensity was not related to occupation. Car driving was also associated with anterior disc bulges. All signs of disc degeneration were related to a history of back accidents. Disc degeneration was not related to body height, overweight, smoking, or the frequency of physical exercise.\n\n\nCONCLUSIONS\nOccupational load affects the risk of disc degeneration of the lumbar spine. Accidental back injuries and motor vehicle driving are associated with an increased risk of disc degeneration. Anterior and posterior disc bulges seem to be related to different types of physical loads.\n\n==\n\nMaimaiti 2019:\n\nTitle: Cervical musculoskeletal disorders and their relationships with personal and work-related factors among electronic assembly workers.\n\nAbstract: INTRODUCTION\nElectronics assembly workers are reported to have a high prevalence of musculoskeletal disorders (MSDs). This study investigated the prevalence of cervical MSDs and the complex relationships between cervical MSDs and individual, physical, psychosocial factors among electronics assembly workers.\n\n\nMETHODS\nIn this cross-sectional survey, self-administered questionnaires from 700 workers in electronics manufacturing workshops were analysed. Information concerning musculoskeletal symptoms, personal and work-related factors was collected. Finally, the prevalence of cervical MSDs was computed for different subgroups, and the relationships with different factors were analyzed using logistic regression and structural equation modeling (SEM).\n\n\nRESULTS\nThe total 12\u202fmonth prevalence of cervical MSDs among the survey population was 29.4%. Variables of gender, job tenure, twisting head frequently, neck flexion/extension for long time and work required to be done quickly showed significant associations with MSDs in a multivariate logistic regression (P\u202f<\u202f0.05). The SEM analysis showed moderate and significant correlations between postural load (\u03b3\u202f=\u202f0.279), gender (\u03b3\u202f=\u202f0.233) and cervical MSDs, while there were weak but significant correlations between vibration (\u03b3\u202f=\u202f0.024), work stress (\u03b3\u202f=\u202f0.126), job tenure (\u03b3\u202f=\u202f0.024) and cervical MSDs. Both work stress and vibration affected the MSDs indirectly through postural load.\n\n\nCONCLUSIONS\nThe logistic regression results support previous general epidemiological MSD studies, and indicates that individual, physical, and psychosocial factors are related to cervical MSDs. The SEM provides a better approximation of the complexity of the relationship between risk factors and cervical MSDs. Improving awkward postures may be effective ways to control the influence of occupational stressors or vibration on MSDs. Practical Applications: The study is to improve prevention of MSDs among electronics assembly workers and promote their occupational health.\n\n==\n\nBurdorf 1990:\n\nTitle: An epidemiological study of low-back pain in crane operators.\n\nAbstract: A survey among workers in a steel factory was conducted to determine the risk for low-back pain (LBP) among male crane operators. Crane operators (n = 33) were matched for age with male control workers (n = 30) and examined for frequency and nature of LBP by the on-site medical service. Comparison between crane operators and controls showed a statistically significant odds ratio for LBP of 3.6. Although crane operators had been exposed more often to backstraining factors in previous occupations, in the employed logistic analysis only their current job explained the elevated occurrence of LBP. It is suggested that workers in sedentary position with exposure to whole-body vibration are at special risk for LBP. The results of this study provide evidence to strongly recommend persons with a history of back complaints not to seek employment as crane operators.",
        "final_summary": "The papers suggest a potential relationship between occupation as a crane operator and cervical disc pathology. Piccinni 1992 found a higher prevalence of spinal disorders, specifically in the cervical tract, among crane operators compared to a control group. Hagberg 1987 reported that certain occupational groups, including crane operators, had higher rates of cervical spondylosis. \u00c5lund 1992 found that sick-listed crane operators had impaired active neck motion range and tenderness in specific neck muscles. However, further research is needed to establish a definitive link between occupation as a crane operator and cervical disc pathology."
    },
    {
        "query": "Automated analysis of electrochemical impedance spectroscopy data",
        "paper_list_string": "Boukamp 2004:\n\nTitle: Electrochemical impedance spectroscopy in solid state ionics: recent advances\n\nAbstract: Electrochemical Impedance Spectroscopy (EIS) has become an important research tool in Solid State Ionics. Some new developments are highlighted: new methods of automatic parameter extraction from impedance measurements are briefly discussed. The Kramers\u2013Kronig data validation test presents another powerful tool in the impedance data analysis. The partial-fit and subsequent subtraction procedure, in combination with the K\u2013K test, can lead to a more appropriate equivalent circuit model for the Complex Nonlinear Least Squares (CNLS)-fit routine. \n \nVery low frequency measurements (<1 mHz) are of significant interest for characterization of intercalation electrodes (anodes and cathodes) for novel high energy density batteries. The low frequency impedances can be obtained by simple means through a carefully conducted Fourier transform of the time domain relaxation data. Impedance analysis of thin film LixCoO2 cathodes shows that combination of a Finite Length Warburg (FLW) with a Finite Space Warburg (FSW) is viable model for the Li+ diffusion process.\n\n==\n\nMurbach 2018:\n\nTitle: Analysis of Li-Ion Battery Electrochemical Impedance Spectroscopy Data: An Easy-to-Implement Approach for Physics-Based Parameter Estimation Using an Open-Source Tool\n\nAbstract: The quantitative analysis of electrochemical impedance spectroscopy (EIS) data is important for both characterization and prognostic applications in many electrochemical systems. Here we describe an open-source platform, the ImpedanceAnalyzer, for easy-to-use physics-based analysis of experimental EIS spectra. To demonstrate the use of the platform, we explore the basic capabilities of the pseudo two-dimensional (P2D) battery model to predict publicly available experimental EIS data from a 1500 mAh commercial lithium-ion (LiCoO 2 /graphite) cell. An a priori computed dataset of 38,800 P2D-based impedance spectra simulations, covering a wide range of frequencies (1 mHz to 100 kHz) and model parameters, enables a straightforward least squares matching approach for analyzing experimental spectra. We \ufb01nd an average error of 1.73% between the best-matching computed spectrum from the 38,800 member library and the experimental spectrum being analyzed. Our analysis shows there is signi\ufb01cant opportunity to improve the \ufb01t between experimental data and physics-based impedance simulations by a combination of a larger computed dataset, local optimization, and further additions to the model physics. The approach and open source tools developed here can be easily extended to other electrochemical systems.\n\n==\n\nHuang 2016:\n\nTitle: Graphical analysis of electrochemical impedance spectroscopy data in Bode and Nyquist representations\n\nAbstract: Abstract Though it becomes a routine to fit impedance data to an equivalent electric circuit model (EECM) using complex nonlinear least square (CNLS) to extract physical parameters from impedance data, two formidable challenges still remain: to build a physically meaningful EECM and to find good initial estimates for model parameters. In this study, combining graphical analysis of impedance data in both Bode and Nyquist plots, a two-step procedure is proposed to address the challenges: (1) a frequency derivative phase angle method is developed in Bode plot to identify the number of time constants (or electrochemical processes); (2) graphical analysis of impedance data in Nyquist plot is used sequentially for initial parameter determination. Major graphical analysis methods are compared in terms of frequency resolution, accuracy and complexity using synthetic data. The superiority of the proposed procedure is illustrated using the experimental data of a three-electrode lithium-ion cell.\n\n==\n\nMacdonald 1990:\n\nTitle: Review of mechanistic analysis by electrochemical impedance spectroscopy\n\nAbstract: The use of electrochemical impedance spectroscopy (EIS) for mechanistic analysis is briefly reviewed with particular emphasis on the mathematical techniques that may be used to derive theoretical impedance functions and to extract kinetic data. The advantages of EIS for kinetic and mechanism analysis include the fact that it is a steady state technique, that it employs small signal analysis, and that it is capable of probing relaxations over a very wide frequency range ( 1 MHz) using readily available instrumentation. EIS also has an enormous advantage over classical transient techniques in that the validity of the data is readily checked using the Kramers-Kronig transforms. The principal disadvantage of the method is that many workers are discouraged by the level of mathematics required to properly analyze impedance data. This has led many researchers to analyze their data in terms of simple equivalent electrical circuits, and hence to ignore the great power of EIS for deriving mechanistic and kinetic information for processes that occur at electrochemical interfaces.\n\n==\n\nPopkirov 1993:\n\nTitle: Validation of experimental data in electrochemical impedance spectroscopy\n\nAbstract: Abstract Fe, Ag and polymer coated Pt electrodes were investigated using a FFT impedance spectrometer which operates with a sum of properly defined sine waves as a perturbation. Based on an analysis of the discrete response spectra, it is possible to discuss the validation of the measured impedance data. New frequencies are observed in a number of cases, and their distribution gives information about possible causes for impedance spectrum distortion, eg about a poor linearity and/or instability of the electrode interphase during the measurement. In some cases it is possible to correct the measured data in order to obtain valid impedance spectra.\n\n==\n\nWit 1996:\n\nTitle: Electrochemical impedance spectroscopy as a tool to obtain mechanistic information on the passive behaviour of aluminium\n\nAbstract: Electrochemical Impedance Spectroscopy can be used very effectively for studies regarding the passivity of metals. In general there are two different methods to interpret measured impedance data. The first method uses electrical equivalent circuits. Alternatively a transfer function can be calculated, ie a theoretical impedance expression based on an assumed physical model. The latter method is a more fundamental approach but often corroding systems including passive alloys are too complicated to arrive at useful transfer functions. In that case the impedance data are in general analysed according to the first method. An equivalent circuit is fitted to the measurement data. For some cases the transfer function can be derived and analysed, when accepting some approximations. As an example the passive behavior of aluminium will be discussed.\n\n==\n\nMcintyre 1996:\n\nTitle: Electrochemical impedance spectroscopy; a tool for organic coatings optimizations\n\nAbstract: Abstract Electrochemical impedance spectroscopy (EIS) was applied to the optimization of automotive electrodeposited coatings, container interior coatings and industrial maintenance coatings. The electrochemical impedance data were used to predict corrosion protection, film porosity, solution absorption into the coatings and film delamination properties. Variables such as resin contents, crosslink densities, cure temperatures, and solvent types and contents were evaluated for these various types of coatings. In general the electrochemical impedance data correlated well with conventional exposure tests results such as salt fog, cyclic scab corrosion and delamination tests. The impedance spectra permits a rather rapid (15\u201375 min per sample) assessment of the film's characteristics even when no visually observable changes have occurred. Electrochemical impedance spectroscopy provides a technique to optimize coatings while reducing the time of coating evaluations and gives insight into the chemical and physical properties of the coatings.\n\n==\n\nKuipers 2020:\n\nTitle: An Algorithm for an Online Electrochemical Impedance Spectroscopy and Battery Parameter Estimation: Development, Verification and Validation\n\nAbstract: Abstract More advanced battery diagnostic approaches are required for a safer and more reliable operation of today's and future battery technologies. For the purpose of evaluating a battery's internal conditions, Electrochemical impedance spectroscopy has proven to be a powerful tool, but nowadays it is only used in laboratory setups. It could provide valuable information about the battery's internal states, if instead it is applied online in an actual battery application. Therefore, we have developed an efficient algorithm, which is designed to run on a battery management system continuously carrying out measurements of the electrochemical impedance by iteratively evaluating measurements of battery current and voltage. Furthermore, the algorithm adapts the parameters of an equivalent circuit model to best match the battery's impedance, hence providing characteristic measures of a battery's internal conditions. The algorithm is implemented in a generic form, which severs as a baseline that can be adjusted to more specific requirements and circumstances in corresponding battery applications. The scope of this work focuses on an introduction of the generic algorithm as well as a proof-of-concept. The later was carried out in two steps. First, the algorithm was run against a battery model. Subsequently, the algorithm was validated utilizing measurements from a real battery cell. In this case, laboratory electrochemical impedance spectroscopy measurements served as reference. The algorithm was able to estimate impedance with a high accuracy in both tests. A high accuracy was also achieved for the parameter estimation; however, its accuracy decreases with large superposed DC current rates.",
        "final_summary": "Boukamp 2004 discusses new methods of automatic parameter extraction from impedance measurements and the Kramers-Kronig data validation test for impedance data analysis. Murbach 2018 presents the ImpedanceAnalyzer, an open-source platform designed for the analysis of EIS spectra, demonstrating its use with a pseudo two-dimensional battery model. Huang 2016 proposes a two-step graphical analysis procedure in Bode and Nyquist plots for impedance data, aiding in the identification of time constants and initial parameter determination. Macdonald 1990 reviews the advantages and challenges of using EIS for mechanistic analysis, emphasizing the mathematical complexity involved in data interpretation. Popkirov 1993 focuses on the validation of impedance data, identifying causes for spectrum distortion and suggesting corrective measures. Wit 1996 explores the use of EIS for studying metal passivity, comparing data interpretation methods using electrical equivalent circuits and physical model-based transfer functions. McIntyre 1996 applies EIS for optimizing organic coatings, correlating impedance data with various coating properties. Lastly, Kuipers 2020 introduces an algorithm for continuous online EIS and battery parameter estimation, adapting equivalent circuit model parameters to match the battery's impedance."
    },
    {
        "query": "what is the most optimal risk threshold to set when assessing the danger of an activity?",
        "paper_list_string": "DeKay 2002:\n\nTitle: Risk-based decision analysis in support of precautionary policies\n\nAbstract: A decision-analytic model for avoiding a risky activity is presented. The model considers the benefit and cost of avoiding the activity, the probability that the activity is unsafe, and scientific tests or studies that could be conducted to revise the probability that the activity is unsafe. For a single decision maker, thresholds are identified for his or her current subjective probability that the activity is unsafe. These thresholds indicate whether the preferred course of action is avoiding the activity without further study, engaging in the activity without further study, or conducting a test or research programme to obtain additional information and following the result. When these thresholds are low, precautionary action is more likely to be warranted. When there are multiple stakeholders, differences in their perceptions of the benefit and cost of avoidance and differences in their perceptions of the accuracy of the additional information provided by the test or research programme combine to create differences in their decision thresholds. Thus, the model allows for the rational expression of differences among parties in a way that highlights disagreements and possible paths to conflict resolution. The model is illustrated with an application to phytosanitary standards in international trade and examined in terms of recent empirical research on lay perceptions of risks, benefits, and trust. Further research is suggested to improve the elicitation of model components, as a way of fostering the legitimate application of risk-based decision analysis in precautionary policy making.\n\n==\n\nStam 1998:\n\nTitle: Environmental risk: towards an integrated assessment of industrial activities\n\nAbstract: In Dutch environmental pollution policy, great importance is attached to the assessment of hazards due to failures and accidents in industrial installations. In particular, for the aquatic environment a systematic approach was developed to be used in environmental licensing procedures. Two software models for environmental risk assessment of industrial activities are available, VERIS and RISAM. At present, these models are being integrated in one comprehensive model. The results of the new model can be used as a starting-point for preventive, repressive or mitigating measures, as well as for a basis of a risk management system. The question of which standards, such as limit values for risk thresholds, should be set is still a matter of discussion. It is to be expected that in spring 1998 a first discussion paper on the topic will be published.\n\n==\n\nGrassi 2009:\n\nTitle: A fuzzy multi-attribute model for risk evaluation in workplaces.\n\nAbstract: Risk assessment constitutes a critical phase of the safety management process. It basically consists of evaluating the risks involved in the execution of working activities, so as to provide the managers with information suitable to address intervention measures. The most important activity in risk assessment is the risk evaluation task but, despite its importance, national and international regulations have never formalized a standard methodology for addressing it, leaving companies complete freedom to adopt the approach they consider appropriate. As a consequence, companies generally settle on the use of simple methodologies which frequently are not able to emphasize all the main aspects affecting risk in the workplace and to produce a precise ranking of hazardous activities. In this paper, a new methodology for risk evaluation is then proposed with the aim to overcome limitations of the classical approaches. In particular, new factors are introduced to take into account effects of human behavior and environment on risk level, other than the classical injury magnitude and occurrence probability of an accident. The proposed methodology also integrates an estimative approach based on the fuzzy logic theory, which permits more coherence in the evaluation process, producing a very suitable final rank of hazardous activities. Finally, an actual case study is reported to show the capabilities of the new risk evaluation model.\n\n==\n\nPhillips 2012:\n\nTitle: Predicting the risk of future dangerousness.\n\nAbstract: The purpose of assessing dangerousness is to determine whether an individual poses a risk of endangering self or others now or in the near future and to identify what interventions are necessary to minimize that risk.\n\n==\n\nTrbojevic 2005:\n\nTitle: Risk criteria in EU\n\nAbstract: This paper focuses on risk criteria used in the EU for population living in vicinity of hazardous facilities. The criteria vary from fully risk-based and goal setting to the prescriptive consequence based criteria. In spite of the philosophical differences in the formulation of risk criteria, there is a single EU Directive (Seveso 2) on the control of major accident hazards involving chemicals which applies to every member state and clearly the level of safety across the EU should be very similar if not the same. Therefore the first aim of the paper was to minimise the differences between the variety of safety approaches across the EU in order to develop risk acceptance criteria with the potential to converge to a unified set. The second aim was to develop societal risk criteria completely consistent with the legally applied individual risk criteria. public is As Low As Reasonably Practicable (ALARP). The Health and Safety Executive (HSE) publishes from time to time the risk levels it considers as intolerable or tolerable under certain circumstances and while these risk levels cover all industrial activities in the UK, the primary instrument for risk control is ALARP dynamics. Table 1 Comparison of Individual Risk Criteria IRPA UK The Netherlands Hungary Czech Republic 10 Intolerable limit for members of the public 10 Risk has to be reduced to the level As Low As Reasonably Practicable (ALARP) Limit for existing installations. ALARA principle applies Upper limit Limit for existing installations. Risk reduc tion must be carried out 3 x 10 LUP limit of acceptability (converted from risk of dangerous dose of 3 x 10) 10 Broadly acceptable level of risk Limit for the new installations and general limit after 2010. ALARA applies Lower limit Limit for the new installations 10 Negligible level of risk 10 Negligible level of risk The situation in other countries is not very clear; for example in the Netherlands, two regions Rijmond and Schiphol are excluded from the new criteria (in force after 2010), while if there is a lower risk situation than the norms require, the Authorities can allow building up to the norm; the enforcement of ALARA is questionable. It can be seen from Table 1 that individual risk of 10 per year represents the upper limit in Europe for existing installations, while in the UK the intolerable limit is 10 but ALARP is strictly imposed, meaning that in reality the risk is well below the limit. The upper limit for individual risk for new installations in Czech Republic and in the Netherlands after 2010 is 10 per year. It should also be noted that the individual risk in the LUP guidelines in the UK, HSE 2004a, in terms of a dangerous dose of 3 x 10 per year can be converted to individual risk of death of 3 x 10 per year. The quoted value for the Netherlands (10 and 10) represent so called location risk (risk contour), or the individual risk to a person who is permanently at the particular location. In addition, in the case of the Netherlands, the risk value corresponds to one establishment (facility), and the cumulative risks from several establishments are not taken into account. The negligible risk levels specified in the UK as 10 per year and in the Netherlands as 10 per year are not questionable and it will be assumed that 10 value can be accepted across the EU for the time being. 2.2 Impact Criteria The example of the consequence (impact) based criteria used in France, Salvi & Gaston 2004, is presented in Table 2. These criteria apply to the list of reference or predefined scenarios such as boiling liquid expanding vapour explosion (for liquefied combustible gases), unconfined vapour cloud explosion (for liquefied combustible gases), total instantaneous loss of containment (for liquefied, nonliquefied and toxic gases), instantaneous rupture of the largest pipeline leading to the highest mass flow (toxic gas installations), fire in the largest tank (for flammable liquids), etc. Table 2 Impact Thresholds\n\n==\n\nWoodruff 2005:\n\nTitle: Consequence and likelihood in risk estimation: A matter of balance in UK health and safety risk assessment practice\n\nAbstract: This paper argues that existing semi-quantitative risk estimation methodologies commonly applied within UK health and safety risk assessment practice are biased towards considerations of possible consequence rather than overall risk. This is seen to be leading to a slow shift towards risk aversion within UK health and safety decision making. A reworking of existing risk estimation methodology is proposed. Instead of seeking an explicit value for the level of risk the paper suggests that in lower risk industrial and commercial sectors it is sufficient to establish whether the risk is likely to fall within an intolerable, tolerable or acceptable risk zone. Once this evaluation has been completed it is further argued that risks judged to fall within the tolerable zone, which equates to a legal duty to reduce risk so far as reasonably practicable in UK, can be prioritised using values of exposure to the hazard. The method is seen as having significant advantages over other semi-quantitative risk estimation approaches presently used in UK.\n\n==\n\nBuchanan 1997:\n\nTitle: Assessing risk: limits to the measurement of the target behaviours\n\nAbstract: A serious problem in the assessment of the risk of violence or other dangerous or antisocial behaviour is that the target acts do not lend themselves to measurement as readily as supposed. This problem imposes limits to the validity of research in this field. The limits become particularly serious when the sole data on the behaviour are from official, national statistics on crime. Risk factors derived solely from such an association must be regarded as suspect. Criminal data alone thus pose a double jeopardy on clinical practice-calling into question on the one hand some of the so-called information based principles behind assessments and on the other offering relatively little to the direct assessment process in the individual case. Some of the reasons behind this call for caution are discussed.\n\n==\n\nKaltoft 2018:\n\nTitle: Risk Thresholds and Risk Classifications Pose Problems for Person-Centred Care\n\nAbstract: Classification of a continuous risk score into risk levels is common. However, while the absolute risk score is essential, it is arguably unethical to label anyone at 'high, moderate or low risk' of a serious event, simply because management based on a single criterion (e.g. avoiding the target condition) has been determined to be effective or cost-effective at a population level. Legally, mono-criterial risk labeling can inhibit the obtaining of a fully-informed, preference-based consent, since multiple considerations (various benefits and harms) matter to most individuals, not only the single criterion that is the basis of the provided risk category. These ethical and legal challenges can be met by preference-sensitive multi-criteria decision support tools. In this future vision paper, we demonstrate, at a conceptual proof-of-method level, how such decision support can and should be developed without reference to risk-level classifications. The statin decision is used as illustration, without any empirical claims.",
        "final_summary": "The optimal risk threshold for assessing the danger of an activity is not definitively addressed across the papers. DeKay (2002) introduces a decision-analytic model that uses subjective probabilities and thresholds to decide between avoiding an activity, engaging in it, or conducting further research. Stam (1998) discusses the integration of environmental risk assessment models, but does not provide a clear risk threshold. Grassi (2009) suggests a new risk evaluation methodology for workplaces, incorporating factors beyond traditional injury magnitude and occurrence probability. Phillips (2012) focuses on assessing dangerousness to identify necessary interventions, without specifying a risk threshold. Trbojevic (2005) examines risk criteria in the EU for populations near hazardous facilities, but does not explicitly aim to develop unified risk acceptance criteria. Woodruff (2005) proposes a reworking of risk estimation methodology, prioritizing risks within tolerable zones. Buchanan (1997) discusses the challenges of assessing risk of violence or dangerous behavior, particularly when relying solely on crime statistics. Lastly, Kaltoft (2018) argues against mono-criterial risk labeling and advocates for preference-sensitive multi-criteria decision support tools."
    },
    {
        "query": "\u00ab Comment les personnels soignants travaillant dans les structures sanitaires peuvent-ils mettre en \u0153uvre des strat\u00e9gies pour minimiser la stigmatisation et faciliter l'acc\u00e8s et l'utilisation du kit de prophylaxie post-exposition (KIT-PEP) par les survivantes de violences sexuelles, dans l'Est de la R\u00e9publique D\u00e9mocratique du Congo ? ",
        "paper_list_string": "Duroch 2010:\n\nTitle: Violences sexuelles en R\u00e9publique D\u00e9mocratique du Congo : r\u00e9sistances et appropriations institutionnelles par les ONG\n\nAbstract: Ce travail se propose d\u2019analyser les processus d\u2019apprentissage de l\u2019organisation humanitaire Medecins Sans Frontieres (MSF) autour de la notion de victimes de violences sexuelles. Une premiere partie est consacree a un essai conceptuel et critique des concepts de victimes de viols, notamment dans le champ des sciences sociales, ainsi qu\u2019a une introduction a l\u2019histoire de la Republique Democratique du Congo (RDC). La deuxieme partie porte sur l\u2019etude qualitative d\u2019une des plus importantes interventions de l\u2019organisation MSF a l\u2019Est de la RDC en faveur des victimes de violences sexuelles. 2695 anamneses medicales de patients recus par l\u2019organisation ont ete etudiees afin de comprendre le phenomene des viols de masse dans cette region, des entretiens semi-directifs ont egalement ete menes avec les volontaires et cadres de l\u2019organisation participant a la mise en \u0153uvre de ce type d\u2019operation. Un dernier volet decrit les elements qui ont conduit MSF a considerer le phenomene des violences sexuelles sur ses terrains d\u2019interventions ainsi que les processus d\u2019appropriation et de resistance de l\u2019organisation vis-a-vis de ces problematiques.\n\n==\n\nLongombe 2008:\n\nTitle: Fistula and Traumatic Genital Injury from Sexual Violence in a Conflict Setting in Eastern Congo: Case Studies\n\nAbstract: The Eastern region of the Democratic Republic of Congo (DRC) is currently undergoing a brutal war. Armed groups from the DRC and neighbouring countries are committing atrocities and systematically using sexual violence as a weapon of war to humiliate, intimidate and dominate women, girls, their men and communities. Armed combatants take advantage with impunity, knowing they will not be held to account or pursued by police or judicial authorities. A particularly inhumane public health problem has emerged: traumatic gynaecological fistula and genital injury from brutal sexual violence and gang-rape, along with enormous psychosocial and emotional burdens. Many of the women who survive find themselves pregnant or infected with STIs/HIV with no access to treatment. This report was compiled at the Doctors on Call for Service/Heal Africa Hospital in Goma, Eastern Congo, from the cases of 4,715 women and girls who suffered sexual violence between April 2003 and June 2006, of whom 702 had genital fistula. It presents the personal experiences of seven survivors whose injuries were severe and long-term, with life-changing effects. The paper recommends a coordinated effort amongst key stakeholders to secure peace and stability, an increase in humanitarian assistance and the rebuilding of the infrastructure, human and physical resources, and medical, educational and judicial systems. R\u00e9sum\u00e9 La r\u00e9gion orientale de la R\u00e9publique d\u00e9mocratique du Congo (RDC) conna\u00eet actuellement une guerre particuli\u00e8rement brutale. Des groupes arm\u00e9s de la RDC et des pays voisins commettent des atrocit\u00e9s et utilisent syst\u00e9matiquement la violence sexuelle comme arme de guerre pour humilier, intimider et dominer les femmes, les jeunes filles, leurs partenaires masculins et les communaut\u00e9s. Les combattants arm\u00e9s jouissent de l\u2019impunit\u00e9, sachant qu\u2019ils ne devront pas r\u00e9pondre de leurs actes et ne seront pas poursuivis par la police ou les autorit\u00e9s judiciaires. Un probl\u00e8me de sant\u00e9 publique particuli\u00e8rement atroce est apparu : les fistules gyn\u00e9cologiques traumatiques et les l\u00e9sions g\u00e9nitales caus\u00e9es par les violences sexuelles et les viols collectifs, avec des cons\u00e9quences psychosociales et psychologiques tragiques. Beaucoup de victimes se retrouvent enceintes ou infect\u00e9es par des IST ou le VIH, sans acc\u00e8s au traitement. Ce rapport a \u00e9t\u00e9 pr\u00e9par\u00e9 \u00e0 l\u2019h\u00f4pital Heal Africa/Doctors on Call for Service \u00e0 Goma, en RDC orientale, \u00e0 partir de 4715 cas de femmes et de jeunes filles ayant subi des violences sexuelles entre avril 2003 et juin 2006, dont 702 pr\u00e9sentaient une fistule g\u00e9nitale. Il d\u00e9crit les exp\u00e9riences personnelles de sept victimes dont les blessures \u00e9taient si graves qu\u2019elles ont chang\u00e9 leur vie. L\u2019article recommande aux principales parties prenantes de coordonner leurs efforts pour garantir la paix et la stabilit\u00e9, accro\u00eetre l\u2019assistance humanitaire et reconstruire l\u2019infrastructure, les ressources humaines et mat\u00e9rielles ainsi que les syst\u00e8mes m\u00e9dicaux, \u00e9ducatifs et judiciaires. Resumen La regi\u00f3n oriental de la Rep\u00fablica Democr\u00e1tica del Congo (RDC) actualmente se encuentra asolada por una guerra brutal. Grupos armados de RDC y pa\u00edses vecinos est\u00e1n cometiendo atrocidades y utilizando la violencia sexual sistem\u00e1ticamente como un arma de guerra para humillar, intimidar y dominar a las mujeres, ni\u00f1as, sus hombres y comunidades. Los combatientes armados sacan provecho con impunidad, sabiendo que no se les har\u00e1 responsable ni ser\u00e1n perseguidos por la polic\u00eda o autoridades judiciales. Ha surgido un problema de salud particularmente inhumano: f\u00edstula ginecol\u00f3gica traum\u00e1tica y lesi\u00f3n genital a consecuencia de actos brutales de violencia sexual y violaci\u00f3n en grupo, as\u00ed como enormes secuelas psicosociales y psicol\u00f3gicas. Muchas de las mujeres que sobreviven se encuentran embarazadas o infectadas con ITS/VIH, sin acceso a tratamiento. Este informe fue compilado en el Hospital Doctors on Call for Service/Heal Africa Hospital, en Goma, Congo oriental, de los casos de 4,715 mujeres y ni\u00f1as que sufrieron violencia sexual entre abril de 2003 y junio de 2006, de las cuales 702 ten\u00edan f\u00edstula genital. Se exponen las experiencias personales de siete sobrevivientes cuyas lesiones eran graves y de largo plazo, con efectos que les cambiaron la vida. Se recomienda un esfuerzo coordinado entre las partes interesadas clave a fin de lograr paz y estabilidad, un aumento en ayuda humanitaria y reconstrucci\u00f3n de la infraestructura, recursos humanos y f\u00edsicos, y sistemas m\u00e9dicos, educativos y judiciales.\n\n==\n\nBondo 2020:\n\nTitle: Etude des facteurs favorisant l\u2019impunit\u00e9 a la l\u00e9gislation en mati\u00e8re de violences sexuelles en R\u00e9publique D\u00e9mocratique du Congo\n\nAbstract: Les violences sexuelles constituent un v\u00e9ritable probl\u00e8me de sant\u00e9 publique et une cause de violation massive des droits de l\u2019Homme, principalement des femmes et des jeunes enfants. Cette \u00e9tude s\u2019emploie \u00e0 rep\u00e9rer les facteurs ou obstacles qui influent n\u00e9gativement sur la punissabilit\u00e9 des auteurs des infractions \u00e0 la loi n\u00b006-018 du 20 juillet 2006 relative aux violences sexuelles en R\u00e9publique D\u00e9mocratique du Congo. Ces facteurs sont de plusieurs ordres, notamment sociologique, culturel, \u00e9conomique et juridique. Il importe de les d\u00e9celer ou mieux de les d\u00e9nicher pour permettre aux rouages juridictionnels du pays, qui se veut un \u00ab Etat de droit \u00bb, de bien rendre la justice par l\u2019application rigoureuse des lois relatives \u00e0 cette mati\u00e8re de violences sexuelles afin de restaurer les victimes de ces affreux actes dans leur droit et dignit\u00e9.\n\n==\n\nD'Errico 2013:\n\nTitle: \u2018You say rape, I say hospitals. But whose voice is louder?\u2019 Health, aid and decision-making in the Democratic Republic of Congo\n\nAbstract: In the last decade, scholars and humanitarians have rightly drawn attention to the high rates of gender-based violence in the eastern Democratic Republic of Congo (DRC), which are associated with the high levels of conflict in the country since 1996. However, this focus detracts from the general health problems that stem from the deterioration of the health sector, which began long before the outbreak of war. This article analyses local perceptions of the determinants of maternal health and illness in eastern DRC, and identifies ways in which women cope with barriers to health care that derive from an inadequate and/or absent health-care system. The article demonstrates that in both urban and rural locations in all four provinces of eastern DRC, women have organised to address their own vulnerabilities, which, according to them, amount to more than exposure to gender-based violence. The existence of these informal systems demonstrates the need to reassess the image of Congolese women as primarily passive victims and/or targets of violence. The article suggests that these culturally rooted indigenous solutions be evaluated as worthy recipients of development funding, which is often exclusively offered to international organisations. [\u00ab Vous dites viol, je dis h\u00f4pitaux. Mais qui parle le plus fort ? \u00bb : La sant\u00e9, l'aide et la prise de d\u00e9cision dans la R\u00e9publique D\u00e9mocratique du Congo]. Durant la derni\u00e8re d\u00e9cennie, les \u00e9rudits et les humanitaires ont correctement attir\u00e9 l'attention sur les taux \u00e9lev\u00e9s de violence sur la seule base du genre dans l'est de la R\u00e9publique D\u00e9mocratique du Congo (RDC), qui sont en lien avec les hauts niveaux de conflit dans le pays depuis 1996. Pourtant, cette attention sur les violences sexuelles d\u00e9tourne l'attention qui devrait \u00eatre port\u00e9e aux probl\u00e8mes sanitaires g\u00e9n\u00e9raux qui sont dus \u00e0 la d\u00e9t\u00e9rioration du secteur de la sant\u00e9, qui a commenc\u00e9 longtemps avant l'\u00e9mergence de la guerre. Cet article examine les perceptions locales des d\u00e9terminants de la sant\u00e9 maternelle et de la maladie dans l'est de la RDC et identifie les mani\u00e8res avec lesquelles les femmes s'adaptent aux difficult\u00e9s d'acc\u00e8s aux soins de sant\u00e9 qui r\u00e9sultent d'un syst\u00e8me de sant\u00e9 publique insuffisant et/ou absent. L'article montre que dans les zones tant urbaines que rurales et dans les quatre provinces de l'est de la RDC, les femmes se sont organis\u00e9es pour faire face \u00e0 leurs propres vuln\u00e9rabilit\u00e9s, qui, selon elles, repr\u00e9sentent un probl\u00e8me plus important que l'exposition \u00e0 la violence fond\u00e9e sur le genre. L'existence de ces syst\u00e8mes informels d\u00e9montre le besoin de reconsid\u00e9rer l'image des femmes congolaises comme \u00e9tant en premier lieu des victimes passives et/ou des objets de violences. L'article sugg\u00e8re que ces solutions culturellement enracin\u00e9es dans le mod\u00e8le local pourraient \u00eatre \u00e9valu\u00e9es comme des b\u00e9n\u00e9ficiaires louables de l'aide au d\u00e9veloppement, laquelle est souvent exclusivement d\u00e9livr\u00e9e \u00e0 des organisations internationales. Mots-cl\u00e9s : Congo ; viol ; sant\u00e9 ; violence li\u00e9e au genre ; conflit\n\n==\n\nMoufflet 2008:\n\nTitle: Le paradigme du viol comme arme de guerre \u00e0 l'Est de la R\u00e9publique d\u00e9mocratique du Congo\n\nAbstract: Cet article revisite l\u2019interpretation majoritaire des violences sexuelles de l\u2019est du Congo comme relevant exclusivement du paradigme des \u00ab viols comme arme de guerre \u00bb. Apres une description de ce phenomene et du dispositif d\u2019assistance pour les victimes a Goma, cet article questionne les effets de cette interpretation sur la qualite et l\u2019efficacite de l\u2019aide accordee aux victimes de violences sexuelles dans les Kivus.\n\n==\n\nBaaz 2011:\n\nTitle: La complexit\u00e9 de la violence : Analyse critique des violences sexuelles en R\u00e9publique D\u00e9mocratique du Congo (RDC)\n\nAbstract: Ce rapport est base sur une etude de cas originale, incluant des entretiens extensifs avec les forces armees en Republique Democratique du Congo (RDC). En explorant de maniere critique et remettant ...\n\n==\n\nTshite 2021:\n\nTitle: Analyse des probl\u00e8mes li\u00e9s au parcours juridique des victimes des violences sexuelles dans la province de la Tshopo en R\u00e9publique D\u00e9mocratique du Congo\n\nAbstract: Cette \u00e9tude vise deux objectifs, \u00e0 savoir : d\u00e9gager les probl\u00e8mes li\u00e9s au parcours juridique des victimes de violences sexuelles dans la province de la Tshopo et, proposer ce qui doit \u00eatre fait pour \u00e9laguer ces probl\u00e8mes et permettre une prise en charge efficace et \u00e9quitable. Pour y arriver, l\u2019\u00e9tude qui s\u2019inscrit dans le contexte de la th\u00e9orisation ancr\u00e9e a mobilis\u00e9 comme sources les dossiers des victimes des violences sexuelles en justice, les comptes rendus des r\u00e9unions, les groupes de discussion, les r\u00e9cits de vie des victimes et la litt\u00e9rature grise. Partant, il se d\u00e9gage que les probl\u00e8mes identifi\u00e9s sont multiples et de diverses natures. On r\u00e9pertorie notamment : le dysfonctionnement des services judiciaires ; l\u2019absence des preuves par des parties concern\u00e9es (m\u00e9decins, police, agents p\u00e9nitentiaires, etc.) ; une pauvret\u00e9 extr\u00eame des victimes ; les man\u0153uvres dilatoires tendant \u00e0 repousser perp\u00e9tuellement les audiences pour freiner le proc\u00e8s ou bloquer l\u2019action par des proc\u00e9dures irr\u00e9guli\u00e8res ; l\u2019arrangement \u00e0 l\u2019amiable ; l\u2019insuffisance et l\u2019\u00e9loignement g\u00e9ographique des juridictions comp\u00e9tentes en mati\u00e8re de violence sexuelle ; l\u2019environnement social des victimes ; et l\u2019ins\u00e9curit\u00e9 et la crainte de repr\u00e9sailles. Que faire ? Le probl\u00e8me des violences sexuelles n\u00e9cessite l\u2019implication de toutes les communaut\u00e9s. Aussi, les campagnes de sensibilisation, les sessions de formations ainsi que des actions concr\u00e8tes qui pourraient apporter des solutions aux probl\u00e8mes r\u00e9pertori\u00e9s. \nThis study has a dual objective which firstly aims to identify the problems associated with the legal process of victims of sexual violence in the province of Tshopo, and followingly to share recommendations to eliminate the identified problems to ensure that victims\u2019 are dealt with effectively and fairly along the criminal proceedings. To achieve this, the study, using a Grounded Theory methodology, used the files of victims of sexual violence in the courts, the minutes of meetings, discussion groups, the life stories of victims and grey literature as sources. The problems identified are multiple and varied and include: dysfunctional judicial services; lack of evidence from the parties concerned (doctors, police, prison officers, etc.); extreme poverty of the victims; delaying tactics that tend to perpetually postpone hearings in order to slow down the trial or block the action through irregular procedures; out-of-court settlements; the inadequacy and geographical remoteness of the jurisdictions competent in matters of sexual violence; the social environment of the victims; and insecurity and fear of reprisals. What can be done? The problem of sexual violence requires the involvement of the whole community. Given that the problem of sexual violence requires the involvement of the whole community, this research proposes awareness-raising campaigns, training sessions and concrete actions as solutions to the problems identified.\n\n==\n\nFassin 1994:\n\nTitle: Le Domaine Priv\u00e9 de la Sant\u00e9 Publique Pouvoir, politique et sida au Congo\n\nAbstract: L'histoire du gouvernement des conduites humaines en mati\u00e8re d'hygi\u00e8ne et de reproduction, et plus g\u00e9n\u00e9ralement l'\u00e9tude des relations entre sant\u00e9, sexualit\u00e9 et politique sont au centre des derniers travaux de Michel Foucault. Analysant parall\u00e8lement la mise en pratique des biopolitiques et la mise en discours du sexe dans les soci\u00e9t\u00e9s occidentales, il montre d'une part, comment l'autorit\u00e9 publique d\u00e9ploie une activit\u00e9 croissante dans la gestion des affaires priv\u00e9es des personnes alors m\u00eame que se d\u00e9veloppe l'id\u00e9ologie lib\u00e9rale, et d'autre part, comment la m\u00e9decine, se r\u00e9clamant de la v\u00e9rit\u00e9 scientifique, entreprend de rationaliser et de normaliser les comportements, tant dans le domaine de la sant\u00e9 que dans celui de la sexualit\u00e9.",
        "final_summary": "Healthcare workers in the Eastern Democratic Republic of Congo (DRC) can implement strategies to minimize stigma and facilitate access to post-exposure prophylaxis (PEP) kits for survivors of sexual violence. Duroch 2010 discusses the learning processes of M\u00e9decins Sans Fronti\u00e8res (MSF) regarding victims of sexual violence. Longombe 2008 highlights the prevalence of traumatic genital injuries from sexual violence in the region. Bondo 2020 identifies sociological, cultural, economic, and legal factors that impede the punishment of sexual violence perpetrators. D'Errico 2013 emphasizes the need to address general health problems and barriers to healthcare access. Moufflet 2008 questions the focus on sexual violence as a weapon of war and its impact on aid effectiveness. Tshite 2021 analyzes the legal challenges faced by sexual violence victims. Fassin 1994 examines the intersection of public health, power, politics, and sexuality in the Congo. These papers collectively provide insights into the context, challenges, and potential strategies for healthcare workers to address stigma and facilitate access to PEP kits for survivors of sexual violence in the Eastern DRC."
    },
    {
        "query": "performance evaluation as component of flexible work policy",
        "paper_list_string": "Yang 2002:\n\nTitle: An Evaluation of Flexible Workday Policies in Job Shops\n\nAbstract: Job shops have long faced pressures for improvement in a challenging and volatile environment. Today's trends of global competition and shortening of product life cycles suggest that both the challenges and the intensity of market volatility will only increase. Consequently, the study of tactics for maximizing the flexibility and responsiveness of a job shop is important. Indeed, there is a significant body of literature that has produced guidelines on when and how to deploy tactics such as alternate routings for jobs and transfers of cross-trained workers between machines. \n \n \n \nIn this paper we consider a different tactic by adjusting the length of workdays. Hours in excess of a 40-hour week are exchanged for compensatory time off at time and a half, and the total amount of accrued compensatory time is limited to no more than 160 hours in accordance with pending legislation. We propose several simple flexible workday policies that are based on an input/output control approach and investigate their performance in a simulated job shop. We find significant gains in performance over a fixed schedule of eight hours per day. Our results also provide insights into the selection of policy parameters.\n\n==\n\nYang 2003:\n\nTitle: Exploratory analysis of flexible workday policies in a job shop\n\nAbstract: The degree to which simple policies for varying the length of each workday can affect job shop performance was investigated. The performance of a job shop operating on a fixed 8-hour day schedule was compared with a job shop that had some flexibility to vary an employee's workday by compensating hours in excess of an 8-hour day with an equivalent amount of time off. Thus, each flexible workday policy maintains an average workday of 8 hours/day for each employee. We simulated a hypothetical job shop to assess the effects of various environmental factors and workday policies on measures of interest to management and labour. We found significant improvement in job flow time with overtime (and time off) as little as an average of 24 min for every 8 hours of work.\n\n==\n\nMenezes 2011:\n\nTitle: Flexible Working and Performance: A Systematic Review of the Evidence for a Business Case\n\nAbstract: Interest in the outcomes of flexible working arrangements (FWAs) dates from the mid-1970s, when researchers attempted to assess the impact of flexitime on worker performance. This paper reviews the literature on the link between FWAs and performance-related outcomes. Taken together, the evidence fails to demonstrate a business case for the use of FWAs. This paper attempts to explain the findings by analysing the theoretical and methodological perspectives adopted, as well as the measurements and designs used. In doing so, gaps in this vast and disparate literature are identified, and a research agenda is developed.\n\n==\n\nReddy 2021:\n\nTitle: Performance Evaluation: A Glimpse of Its Prominence in This Modern Globalised Economy\n\nAbstract: Nowadays, performance evaluation has become an inevitable factor in the field of all sectors. It has become a formal procedure in measuring the progress of employee\u2019s work scales. It is an important procedure in measuring the growth of work progress and it promotes the promotion of the employees. Performance evaluation helps in grading the employees annually and aids in salary raises. Further, it also enhances the quality of work progress, provides useful feedback regarding the work performed. This paper tries to investigate the importance of performance evaluation and its importance in the organisatinal sector. Further, this paper tries to underline the need for a performance evaluation system in the field of work in this present globalized technological world.\n\n==\n\nEt.al 2021:\n\nTitle: Supervisor Support as a Moderator between Flexible Working Arrangement and Job Performance \u2013 Malaysia Evidence\n\nAbstract: Over the past years, organizations are forced to redesign their workplace to fulfil global labour market needs and to remain competitive. Due to the ever-changing working environment and competitive markets, organizations opted to choose more flexible work options so they can respond to the volatile environment. This study explores the moderation effect of supervisor support (SS) between flexible working arrangements (FWAs) and employee performance (EP). The population for this study primarily focused on the organizations that adopt the flexible working arrangement such as flexible working time, working from home, and compressed work-week practices in Malaysia. The companies selected are based on the listing of global organizations practices FWAs mentioned in TalentCorp Malaysia Flexible Working Arrangement collaboration with Malaysian Federation Employer report, 2015. The questionnaires were distributed through the Human Resource Department of each organization. The result demonstrated that there is a relationship between FWAs and EP but there was no moderation effect of SS between FWAs and EP relationship. This study supports the idea that the employees seem to benefit from the adoption of FWAs at the workplace. It allows both gender and single or married employees to manage their time better particularly allowing the employees to attend to personal needs. Since FWAs itself is the support given by employers to employees, therefore, the employees were unable to associate supervisor support role as a contributor to help them manage their work and home responsibilities. This study showed that employee motivation to perform are affected by the policies adopted by organizations. Adopting efficient work practices such as FWAs could stimulate employee engagement to improve their work performance. Therefore, organizations that are currently searching for a strategy that can improve productivity and employee performance should consider FWAs as an option. However, the implementation ofFWAs needs to be customized according to the employee's needs. Offering FWAs in the organization will enhance organizational and individual outcomes.\n\n==\n\nMenezes 2017:\n\nTitle: Flexible Working, Individual Performance, and Employee Attitudes: Comparing Formal and Informal Arrangements\n\nAbstract: In the context of a wider trend to individualize HRM, this paper examines the relationship between flexible working arrangements and individual performance. Drawing on a range of theories, it examines potential indirect effects on employee performance via job satisfaction and organizational commitment and analyses whether these relationships vary according to whether the arrangement was set up through a formal process, or negotiated informally between the employee and their line manager. Extant research has tended to focus on formal arrangements, however, informal arrangements are widespread and may better accommodate work-life preferences, thereby potentially fostering more positive attitudes from employees. Survey data from 2617 employees in four large organizations with well-established flexible working policies are analysed. Results from structural equation models show average positive indirect effects from informal, but also negative direct effects, from formal flexible working. When two forms of flexible working amenable to being set up by both formal and informal means are examined separately: formal arrangements for flexibility over working hours are found to be negatively associated with performance, but also a source of greater job satisfaction; informal remote working arrangements have positive indirect effects via organizational commitment and job satisfaction on worker performance.\n\n==\n\nStavrou 2005:\n\nTitle: Flexible work bundles and organizational competitiveness: a cross\u2010national study of the European work context\n\nAbstract: The present study explores the categorization of flexible work arrangements (FWAs) into bundles and their connection to organizational competitiveness in the European Union. The measures of competitiveness were performance, turnover, and absenteeism. Four moderators were used in the study, organization sector, industry sector, organization size, and organizational women-supportiveness. The analyses revealed four FWA Bundles, namely Non-Standard Work Patterns, Work Away from the Office, Non-Standard Work Hours and Work Outsourced. Non-Standard Work Patterns were found to be related to decreased turnover (in the private sector), while Work Away from the Office was related to improved performance and reduced absenteeism. Non-Standard Work Hours and Work Outsourced (within the public sector) were positively related to turnover, suggesting that these types are possibly not being used as true flexibility arrangements. Finally, post-hoc analysis revealed that Non-Standard Work Hours was related to increased performance only among Swedish organizations. Implications for management and future research are discussed. Copyright \u00a9 2005 John Wiley & Sons, Ltd.\n\n==\n\nObisi 2017:\n\nTitle: IMPACT OF FLEXIBLE WORK ARRANGEMENT ON EMPLOYEES PERFORMANCE IN PUBLIC SCHOOLS IN LAGOS STATE, NIGERIA\n\nAbstract: Flexible work arrangement has been developed to minimize work pressure, health hazards and improve productivity. The study examined the relationship between flexible work arrangement and employees performance with emphasis on public schools in Lagos State. Daily commuting by workers in public schools in Lagos State could expose them to high risk of health hazards due to fixed term employment. Furthermore, increase in commuting raises the level of road congestions, high cost of transport, maintenance of vehicles, traffic and road facilities with exhaustion and stress as negative impacts on commuters. Questionnaire were designed to assess the views of respondents and potency of flexible work arrangement in the state. The study adopted a survey research and CMS Grammar School as a case study. Based on random sampling method, a total of 160 employees were selected as sample size drawn from a population size of 400 workers. Analysis was conducted with Statistical Package for the Social Sciences (SPSS) and Pearson correlation coefficient as test statistics. The hypothesis that there is no significant relationship between flexible work arrangement and employees\u2019 performance was formulated to determine the statistical significance of the study. The study found out that there was a significant relationship between flexible work arrangement and employees\u2019 performance thus the null hypothesis was rejected. The study is used to explain the negative impact of road congestion, hazardous emissions and delays on workers\u2019 health and employees\u2019 productivity in organisations in Lagos State.",
        "final_summary": "The papers suggest that flexible work policies can have an impact on performance evaluation. Yang 2002 and Yang 2003 found that adjusting the length of workdays in job shops can lead to significant improvements in performance. However, Menezes 2011 argues that the evidence fails to demonstrate a business case for the use of flexible working arrangements (FWAs) in general. On the other hand, Et.al 2021 found a positive relationship between FWAs and employee performance, although supervisor support did not moderate this relationship. Menezes 2017 found that informal flexible working arrangements may have positive indirect effects on performance through job satisfaction and organizational commitment. Stavrou 2005 identified different bundles of FWAs, with some bundles related to improved performance and reduced turnover. Finally, Obisi 2017 highlighted the potential benefits of flexible work arrangements in minimizing work pressure and improving productivity."
    },
    {
        "query": "Are emotions constructed?",
        "paper_list_string": "Kurth 2019:\n\nTitle: Are Emotions Psychological Constructions?\n\nAbstract: According to psychological constructivism, emotions result from projecting folk emotion concepts onto felt affective episodes. While constructivists acknowledge there is a biological dimension to emotion, they deny that emotions are (or involve) affect programs. So they also deny emotions are natural kinds. However, the essential role that constructivism gives to felt experience and folk concepts leads to an account that is extensionally inadequate and functionally inaccurate. Moreover, biologically oriented proposals that reject these commitments are not similarly encumbered. Recognizing this has two implications: biological mechanisms are more central to emotion than constructivism allows, and the conclusion that emotions are not natural kinds is premature.\n\n==\n\nLindquist 2008:\n\nTitle: Constructing Emotion\n\nAbstract: This study examined the hypothesis that emotion is a psychological event constructed from the more basic elements of core affect and conceptual knowledge. Participants were primed with conceptual knowledge of fear, conceptual knowledge of anger, or a neutral prime and then proceeded through an affect-induction procedure designed to induce unpleasant, high-arousal affect or a neutral affective state. As predicted, only those individuals for whom conceptual knowledge of fear had been primed experienced unpleasant core affect as evidence that the world was threatening. This study provides the first experimental support for the hypothesis that people experience world-focused emotion when they conceptualize their core affective state using accessible knowledge about emotion.\n\n==\n\nMesquita 2016:\n\nTitle: The cultural construction of emotions.\n\nAbstract: A large body of anthropological and psychological research on emotions has yielded significant evidence that emotional experience is culturally constructed: people more commonly experience those emotions that help them to be a good and typical person in their culture. Moreover, experiencing these culturally normative emotions is associated with greater well-being. In this review, we summarize recent research showing how emotions are actively constructed to meet the demands of the respective cultural environment; we discuss collective as well as individual processes of construction. By focusing on cultural construction of emotion, we shift the focus toward how people from different cultures 'do' emotions and away from which emotions they 'have'.\n\n==\n\nAverill 1980:\n\nTitle: A CONSTRUCTIVIST VIEW OF EMOTION\n\nAbstract: ABSTRACT Traditionally, the emotions have been viewed from a biological perspective; that is, the emotions have been seen as genetically determined and relatively invariable responses. The present chapter, by contrast, views the emotions as social constructions. More precisely, the emotions are here defined as socially constituted syndromes or transitory social roles. A role-conception does not deny the contribution of biological systems to emotional syndromes; it does, however, imply that the functional significance of emotional responses is to be found largely within the sociocultural system. With regard to subjective experience, a person interprets his own behavior as emotional in much the same way that an actor interprets a role\u201cwith feeling.\u201d This involves not only the monitoring of behavior (including feedback from physiological arousal, facial expressions, etc.), but also an understanding of how the emotional role fits into a larger\u201cdrama\u201d written by society. Some of the biological, personal, and situational factors that influence emotional behavior are also discussed.\n\n==\n\nKippax 1988:\n\nTitle: Constructing emotions: Weaving meaning from memories\n\nAbstract: This paper is in three sections\u2014theory, method and data. We consider that emotions are socially constructed, and begin with an interpretation of social constructionism as it applies to emotion. The conclusion drawn from the analysis of theories from a variety of disciplines is that it is important to incorporate and extend the notion of appraisal into a study of emotion. This notion is prominent in the cognitive approach and the social constructionist approach allows the cognitive and social views of emotion to be integrated. \n \n \n \nThis approach enabled us to examine the process whereby individuals engage with each other, and hence with the social structures, in their construction of emotions. We employed a method \u2018memory-work\u2019 which uses written memories as source material. This is discussed at some length since it is integrated within a theory of socialization which is closely related to social constructionism. \n \n \n \nFinally, we present an analysis of some data which illustrate and demonstrate the processes involved in constructing emotion. Our work confirms and extends the social constructionist view. Paradoxically, in highlighting the notion of agency, we stress the essentially social nature of the individual and overcome some of the problems which arise in methods which examine only the individual or the society.\n\n==\n\nFranco 2020:\n\nTitle: Constructed emotions and superinformation: a constructor-theoretic approach\n\nAbstract: In this paper we apply the constructor-theoretic approach to the theory of constructed emotions, showing that core affect valence and knowledge can be considered as two different observables, leading to information or superinformation conditions: this depends on subject's strategy, coherently with the affect infusion model. In the second part of the article we show that additional hypotheses on the structure of information allows to study emotions in terms of the contructor-theoretic version of phase task. Quantum algorithms are presented as an example of the connection between emotions and memory tasks.\n\n==\n\nSibia 2013:\n\nTitle: Social Construction of Emotions\n\nAbstract: The traditional view that emotions are primarily biological, involuntary bodily changes or evolved adaptations has been challenged by social constructivists such as psychologists, anthropologist and philosopher Claire. The proponents of the social constructivist approach argue that emotions have meaning and are experienced and expressed in a particular socio-cultural context. According to \u201cemotions are not only remnants of our phylogenetic past, nor can they be explained in strictly physiological terms. Rather, they are social constructions, and they can be fully understood only on a social level of analysis.\u201d \n \n \nKeywords: \n \nemotion; \nemotional development\n\n==\n\nPober 2018:\n\nTitle: What Emotions Really Are (In the Theory of Constructed Emotions)\n\nAbstract: Recently, Lisa Feldman Barrett and colleagues have introduced the Theory of Constructed Emotions (TCE), in which emotions are constituted by a process of categorizing the self as being in an emotional state. The view, however, has several counterintuitive implications: for instance, a person can have multiple distinct emotions at once. Further, the TCE concludes that emotions are constitutively social phenomena. In this article, I explicate the TCE*, which, while substantially similar to the TCE, makes several distinct claims aimed at avoiding the counterintuitive implications plaguing the TCE. Further, because of the changes that comprise the TCE*, emotions are not constitutively social phenomena.",
        "final_summary": "The papers suggest that emotions are constructed through a complex interplay of biological, cognitive, cultural, and social factors. Kurth (2019) critiques the constructivist view, arguing that it underplays the role of biological mechanisms in emotions. Lindquist (2008) provides experimental support for the constructivist hypothesis, demonstrating that individuals primed with fear conceptualization interpreted their unpleasant core affect as evidence of a threatening world. Mesquita (2016) emphasizes the cultural construction of emotions, arguing that emotional experiences are shaped by cultural norms. Averill (1980) proposes that emotions are socially constituted syndromes or transitory social roles, with the sociocultural system playing a significant role in emotional responses. Kippax (1988) integrates cognitive and social views of emotion, highlighting the importance of appraisal and social interaction in the construction of emotions. Franco (2020) applies a constructor-theoretic approach to constructed emotions, suggesting that core affect valence and knowledge can be influenced by the subject's strategy. Sibia (2013) challenges the traditional view of emotions as primarily biological, arguing for a socio-cultural context in the experience and expression of emotions. Lastly, Pober (2018) introduces the Theory of Constructed Emotions, which posits that emotions are constituted by self-categorization in an emotional state, but suggests modifications to avoid counterintuitive implications. (Kurth 2019; Lindquist 2008; Mesquita 2016; Averill 1980; Kippax 1988; Franco 2020; Sibia 2013; Pober 2018)"
    },
    {
        "query": "4Vs of big data and big data forms at Uber",
        "paper_list_string": "Fu 2021:\n\nTitle: Real-time Data Infrastructure at Uber\n\nAbstract: Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.\n\n==\n\nVossen 2013:\n\nTitle: Big data as the new enabler in business and other intelligence\n\nAbstract: The term \u201cbig data\u201d will always be remembered as the big buzzword of 2013. According to the Wikipedia, big data \u201cis a collection of data sets so large and complex that it becomes difficult to process using on-hand database management tools\u201d. In other perceptions, the \u201c3 Vs\u201d that characterize it (i.e., volume, velocity, and variety) or the \u201c4 Vs\u201d (adding veracity to the previous three) are responsible for the fact that it exceeds an organization\u2019s own data as well as its storage or compute capacity for accurate and timely decision-making. In essence, big data refers to the situation that more and more aspects and artifacts of everyday life, be it personal or professional, are available in digital form, e.g., personal or company profiles, social network and blog postings, buying histories, health records, to name just a few, that increasingly more data gets dynamically produced especially on the Internet and on the Web, and that nowadays the tools and techniques are available for evaluating and analyzing all that data in various combinations. Numerous companies already foresee the enormous business effects that analytical scenarios based on big data can have, and the impacts that it will hence have on advertising, commerce, and business intelligence (BI). This paper reviews the issues, techniques, and applications of big data, with an emphasis on future BI architectures.\n\n==\n\nIshwarappa 2015:\n\nTitle: A Brief Introduction on Big Data 5Vs Characteristics and Hadoop Technology\n\nAbstract: Abstract Big data is a collection of massive and complex data sets and data volume that include the huge quantities of data, data management capabilities, social media analytics and real-time data. Big data analytics is the process of examining large amounts of data. There exist large amounts of heterogeneous digital data. Big data is about data volume and large data set's measured in terms of terabytes or petabytes. This phenomenon is called Bigdata. After examining of Bigdata, the data has been launched as Big Data analytics. In this paper, presenting the 5Vs characteristics of big data and the technique and technology used to handle big data. The challenges include capturing, analysis, storage, searching, sharing, visualization, transferring and privacy violations. It can neither be worked upon by using traditional SQL queries nor can the relational database management system (RDBMS) be used for storage. Though, a wide variety of scalable database tools and techniques has evolved. Hadoop is an open source distributed data processing is one of the prominent and well known solutions. The NoSQL has a non-relational database with the likes of MongoDB from Apache.\n\n==\n\nWang 2018:\n\nTitle: A Big Data-as-a-Service Framework: State-of-the-Art and Perspectives\n\nAbstract: Due to the rapid advances of information technologies, Big Data, recognized with 4Vs characteristics (volume, variety, veracity, and velocity), bring significant benefits as well as many challenges. A major benefit of Big Data is to provide timely information and proactive services for humans. The primary purpose of this paper is to review the current state-of-the-art of Big Data from the aspects of organization and representation, cleaning and reduction, integration and processing, security and privacy, analytics and applications, then present a novel framework to provide high-quality so called Big Data-as-a-Service. The framework consists of three planes, namely sensing plane, cloud plane and application plane, to systemically address all challenges of the above aspects. Also, to clearly demonstrate the working process of the proposed framework, a tensor-based multiple clustering on bicycle renting and returning data is illustrated, which can provide several suggestions for rebalancing of the bicycle-sharing system. Finally, some challenges about the proposed framework are discussed.\n\n==\n\nCasado 2015:\n\nTitle: Emerging trends and technologies in big data processing\n\nAbstract: Big Data encompasses large volume of complex structured, semi\u2010structured, and unstructured data, which is beyond the processing capabilities of conventional databases. The processing and analysis of Big Data now play a central role in decision making, forecasting, business analysis, product development, customer experience, and loyalty, to name but a few. In this paper, we examine the distinguishing characteristics of Big Data along the lines of the 3Vs: variety, volume, and velocity. Accordingly, the paper provides an insight into the main processing paradigms in relation to the 3Vs. It defines a lifecycle for Big Data processing and classifies various available tools and technologies in terms of the lifecycle phases of Big Data, which include data acquisition, data storage, data analysis, and data exploitation of the results. This paper is first of its kind that reviews and analyzes current trends and technologies in relation to the characteristics, evolution, and processing of Big Data. Copyright \u00a9 2014 John Wiley & Sons, Ltd.\n\n==\n\nZhang 2017:\n\nTitle: A Survey on Emerging Computing Paradigms for Big Data\n\nAbstract: The explosive growth of data volume and the ever-increasing demands of data value extraction have driven us into the era of big data. The \u201c5V\u201d (Variety, Velocity, Volume, Value, and Veracity) characteristics of big data pose great challenges to traditional computing paradigms and motivate the emergence of new solutions. Cloud computing is one of the representative technologies that can perform massive-scale and complex data computing by taking advantages of virtualized resources, parallel processing and data service integration with scalable data storage. However, as we are also experiencing the revolution of Internet-of-things (IoT), the limitations of cloud computing on supporting lightweight end devices significantly impede the flourish of cloud computing at the intersection of big data and IoT era. It also promotes the urgency of proposing new computing paradigms. We provide an overview on the topic of big data, and a comprehensive survey on how cloud computing as well as its related technologies can address the challenges arisen by big data. Then, we analyze the disadvantages of cloud computing when big data encounters IoT, and introduce two promising computing paradigms, including fog computing and transparent computing, to support the big data services of IoT. Finally, some open challenges and future directions are summarized to foster continued research efforts into this evolving field of study.\n\n==\n\nWillis 2020:\n\nTitle: Using \u2018Big Data\u2019 to understand the impacts of Uber on taxis in New York City\n\nAbstract: Uber has coupled their ability to dodge regulation with their ride-haling app, togrow from a Silicon Valley start-up, to a global powerhouse challenging establishedtaxis in over 700 Metropolitan areas. Through the lens of Schumpter\u2019s \u2018CreativeDestruction\u2019 and Roger\u2019s \u2018Diffusion of Innovation\u2019, this paper aims to investigatehow Uber challenges the established taxis, focusing on yellow taxis in New YorkCity. To capture Uber\u2019s popularity we employ innovative new data from theNew York Times API. We couple these data with data about taxi usage andquality of service from the New York City Taxi & Limousine Commission. Wethen employ well established time-series modelling techniques in order to test ifand how Uber affected the well established taxi market in New York City. Ourresults illustrate the disruptive nature of Uber in terms of leading to a decreasein traditional taxi rides, but also to a behavioural change of customers raisingmore complaints about taxi services.\n\n==\n\nBangare 2021:\n\nTitle: The Architecture, Classification, and Unsolved Research Issues of Big Data extraction as well as decomposing the Internet of Vehicles (IoV)\n\nAbstract: with the faster advancement of the Internet of Vehicles and the increasing growth of units of information demonstrate a critical role of big data in Internet of vehicles. The big data has gained a lot of traction in academics and industry. It helps in making factual decision making and a lot in expanding business and industry. Additionally, the data gathered from linked automobiles was observed, also crowd sourcing, urban development may be boosted oversight. The objective provides some sort of comprehensive survey of all relevant self scrutiny publications published in early year. We conducted a thorough review of the study articles in order to identify possible. So the survey demonstrates how big data may enact a critical part in delivering accurate as well as meaningful forecasts, as well as a detailed review of numerous approaches, devices, and styles for utilizing information in Internet of Vehicles. Adjacent to an overview of the most recent research on the use of big data in Internet of vehicles, additionally, the stated has been proposed. Also the paper extract and decompose a lot of information on traffic of vehicle interneting. It includes suggestions and directions for open working in internet of vehicles.",
        "final_summary": "The papers collectively provide insights into the 4Vs of big data and its forms at Uber. Fu (2021) discusses Uber's real-time data infrastructure, highlighting the challenges of processing large volumes of data for various use cases. Vossen (2013) defines big data as data sets that are difficult to process using traditional tools and emphasizes the 3Vs (volume, velocity, and variety) or 4Vs (adding veracity) as characteristics of big data. Ishwarappa (2015) explores the 5Vs of big data (volume, variety, velocity, value, and veracity) and mentions Hadoop as a technology for handling big data. Wang (2018) presents a framework for Big Data-as-a-Service, addressing challenges related to organization, processing, security, and analytics. Zhang (2017) surveys emerging computing paradigms for big data, including cloud computing, fog computing, and transparent computing, to support IoT-based big data services. Casado (2015) examines the characteristics of big data (variety, volume, and velocity) and provides an overview of processing paradigms and tools. Willis (2020) investigates the impact of Uber on traditional taxis in New York City using big data analysis, showing a decrease in traditional taxi rides and changes in customer behavior. Bangare (2021) discusses the role of big data in the Internet of Vehicles (IoV) and presents a survey of approaches and techniques for utilizing big data in the IoV context."
    },
    {
        "query": "community structure in online dating markets",
        "paper_list_string": "Bruch 2019:\n\nTitle: Structure of online dating markets in US cities\n\nAbstract: We study the structure of heterosexual dating markets in the United States through an analysis of the interactions of several million users of a large online dating website, applying recently developed network analysis methods to the pattern of messages exchanged among users. Our analysis shows that the strongest driver of romantic interaction at the national level is simple geographic proximity, but at the local level, other demographic factors come into play. We find that dating markets in each city are partitioned into submarkets along lines of age and ethnicity. Sex ratio varies widely between submarkets, with younger submarkets having more men and fewer women than older ones. There is also a noticeable tendency for minorities, especially women, to be younger than the average in older submarkets, and our analysis reveals how this kind of racial stratification arises through the messaging decisions of both men and women. Our study illustrates how network techniques applied to online interactions can reveal the aggregate effects of individual behavior on social structure.\n\n==\n\nFelmlee 2017:\n\nTitle: The Invisible Contours of Online Dating Communities: A Social Network Perspective\n\nAbstract: Abstract This study analyzed the e-mail exchange network of participants of a national dating website. The investigation examined whether aggregated partner preferences give rise to distinct, \u201cinvisible,\u201d clusters in online dating networks that structure dating opportunities and result in homophilous subgroups. The findings identified and visualized the ten largest network clusters of participants who interacted with each other and examined the dater characteristics most responsible for cluster membership. Rated attractiveness and age were the strongest cluster correlates, whereas education and race were relatively uncommon determinants. In sum, daters\u2019 interdependent actions created aggregate communities unseen by the users themselves, but no less influential for dating opportunities, that were based more on attractiveness and age than on race and education.\n\n==\n\nMasden 2015:\n\nTitle: Understanding the Role of Community in Online Dating\n\nAbstract: Online dating sites have become a common means of finding a romantic partner. And yet, these sites differ greatly from many other socially oriented websites: perhaps most notably, the pairwise style of interaction afforded by these sites prevents a robust online community from forming. Users, however, have taken matters into their own hands by creating thriving external forums for discussion of specific dating sites. We report on a multiple methods study of two online dating services, via observation and interviews with users of the forums associated with these sites. Our findings suggest that these forums play an essential role in creating an \"outsourced community\" for the dating sites, and also reveal practices around how some users \"game the system\" in online dating, the prevalence of harassment in online dating, and users' frustrations with current dating sites. We conclude with a number of recommendations for system design.\n\n==\n\nBruch 2018:\n\nTitle: Aspirational pursuit of mates in online dating markets\n\nAbstract: We present an empirical analysis of heterosexual dating markets in four U.S. cities using data from a popular online dating service. Romantic courtship is often described as taking place in a dating market where men and women compete for mates, but the detailed structure and dynamics of dating markets have historically been difficult to quantify for lack of suitable data. In recent years, however, the advent and vigorous growth of the online dating industry has provided a rich new source of information on mate pursuit. We present an empirical analysis of heterosexual dating markets in four large U.S. cities using data from a popular, free online dating service. We show that competition for mates creates a pronounced hierarchy of desirability that correlates strongly with user demographics and is remarkably consistent across cities. We find that both men and women pursue partners who are on average about 25% more desirable than themselves by our measures and that they use different messaging strategies with partners of different desirability. We also find that the probability of receiving a response to an advance drops markedly with increasing difference in desirability between the pursuer and the pursued. Strategic behaviors can improve one\u2019s chances of attracting a more desirable mate, although the effects are modest.\n\n==\n\nBurtch 2016:\n\nTitle: ASSESSING AND QUANTIFYING NETWORK EFFECTS IN AN ONLINE DATING MARKET\n\nAbstract: Motivat ion. Two-sided markets (or multi-sided markets more generally) are roughly defined as platforms that enable transaction and interaction amongst end-users of different types (Parker and Val Alstyne 2005; Rochet and Tirole 2006). Examples of these markets are now quite prevalent \u2013 e.g., Hulu matches video content providers with end-consumers and advertisers, eBay connects buyers and sellers, and AirBNB links renters and hosts. Perhaps the oldest example, however, is the market for dating, which we consider here.\n\n==\n\nHeino 2010:\n\nTitle: Relationshopping: Investigating the market metaphor in online dating\n\nAbstract: In this manuscript we explore the ways in which the marketplace metaphor resonates with online dating participants and how this conceptual framework influences how they assess themselves, assess others, and make decisions about whom to pursue. Taking a metaphor approach enables us to highlight the ways in which participants\u2019 language shapes their self-concept and interactions with potential partners. Qualitative analysis of in-depth interviews with 34 participants from a large online dating site revealed that the marketplace metaphor was salient for participants, who employed several strategies that reflected the assumptions underlying the marketplace perspective (including resisting the metaphor). We explore the implications of this metaphor for romantic relationship development, such as the objectification of potential partners.\n\n==\n\nBishop 2010:\n\nTitle: Understanding and Facilitating the Development of Social Networks in Online Dating Communities: A Case Study and Model\n\nAbstract: Online dating is a big business, allowing people from the comfort of their own home to view and read about potential mates all around the world. Different dating sites offer different services. However, it is not yet commonplace for Web sites dedicated to dating to use the social networking tools used by popular online communities, such as those that use the personal homepage and message board genres. The ecological cognition framework (ECF) provides a theoretical model regarding online dating communities\u2019 behavior and relationship development. A model based on the ECF is proposed and provides a basis for developing online dating services that effectively support relationship development. Two investigations are presented in this chapter, one that uses a case study approach to identify and describe online dating services from the perspective of a specific case and another that assess the effectiveness of existing online dating services based on the guidelines developed from the case study. The case study provides a useful insight into the nature of social networking from the perspective of a specific case, which led to guidelines for developing e-dating systems that when evaluated showed that the most popular social networking services also score well against the criteria proposed in those guidelines.\n\n==\n\nSchmitz 2014:\n\nTitle: The Online Dating Market: Theoretical and Methodological Considerations\n\nAbstract: The internet and the social web have cemented their position as an integral part of our everyday lives. While no longer unusual for many purposes, such as acquiring information, communicating, or shopping online, searching for a partner on the web still seems to constitute a particular challenge for the modern Western conception of mating and intimacy, which is grounded in ideas of romantic love, spontaneity, and \u201cdestiny.\u201d Nevertheless, finding a partner online is a significant phenomenon in quantitative terms today; online dating and matchmaking services are used by a great number of people around the world, as multiple international studies have shown (see Hogan et al. 2011; Ben-Zeev 2004; Schmitz et al. 2011). It comes, then, as no surprise that scientists invest considerable time and effort in analyzing this modern social and technological manifestation of intimacy, love, relationships and marriage, which are of \u201cperennial\u201d interest.",
        "final_summary": "The papers suggest that online dating markets exhibit community structure. Bruch 2019 found that dating markets in US cities are partitioned into submarkets based on age and ethnicity, with varying sex ratios. Felmlee 2017 identified distinct clusters in online dating networks based on attractiveness and age rather than race and education. Masden 2015 highlighted the role of external forums in creating an \"outsourced community\" for dating sites. Bruch 2018 revealed a hierarchy of desirability in dating markets, with individuals pursuing partners who are more desirable than themselves. Burtch 2016 discussed the network effects in online dating markets. Heino 2010 explored the marketplace metaphor in online dating and its impact on self-concept and partner assessment. Bishop 2010 emphasized the importance of social networking tools in facilitating relationship development in online dating communities. Schmitz 2014 acknowledged the significance of online dating as a modern manifestation of intimacy and relationships."
    },
    {
        "query": "recovery time from injection for meralgia parenthetica and Iliotibial band syndrome",
        "paper_list_string": "Ataizi 2018:\n\nTitle: Surgical options for meralgia paresthetica: long-term outcomes in 13 cases\n\nAbstract: Abstract Background: Meralgia paresthetica is an entrapment neuropathy of the lateral femoral cutaneous nerve that may cause paresthesias, pain, and sensory loss of the anterior and lateral thigh. Treatment is primarily medical. Surgery is an option for patients who do not respond to medical treatments. Methods: Patients whose symptoms did not improve after medical and conservative treatment for at least 3 months were included in this study. These patients underwent neurolysis and decompression surgery and had a mean postoperative follow-up of 38 months. Their pain levels were assessed by the VAS scoring system. Results: In 8 (61.5%) patients, the symptoms completely resolved within the first 3 months. In 5 (38.5%) patients, the complaints persisted partially and the recovery was observed after 12 months. In patients having a metabolic etiology, the duration of recovery was up to 12 months. Conclusion: The long term results of surgery are good though only partial improvemnts in reported pain were seen in the early postoperative period, especially in patients with a metabolic etiology.\n\n==\n\nEdelson 1994:\n\nTitle: Meralgia paresthetica in children.\n\nAbstract: We reviewed the findings in twenty children and adolescents who had meralgia paresthetica, a common entity in adults that has only rarely been reported in children. Ten patients had bilateral involvement; thus, the study included thirty lesions. Twenty-four lesions were eventually treated with open decompression of the lateral femoral cutaneous nerve; the results of twenty-one of these operations were followed for at least two years. The presenting symptom was severe pain resulting in marked restriction of activities. The pain could be reproduced with palpation of the nerve, and a trial injection of Xylocaine (lidocaine) always produced transient relief of symptoms. The average age at the onset of the symptoms was ten years (range, one to seventeen years); the diagnosis was missed initially in ten patients, which resulted in multiple, unnecessary diagnostic tests being done. The average duration of the symptoms before the patient was first seen was twenty-four months (range, two to eighty-four months), and the average duration of follow-up after the twenty-one operations was thirty-eight months (range, twenty-five to sixty months). Fourteen of the twenty-one operations led to an excellent result, with complete relief of pain and no restriction of activities; five led to a good result, with occasional pain but no limitation of sports or other activities; and two led to a fair result, with pain that interfered with sports activities but not with walking. We believe that meralgia paresthetica is much more common in children than has been previously recognized.(ABSTRACT TRUNCATED AT 250 WORDS)\n\n==\n\nSiu 2005:\n\nTitle: Neurolysis for meralgia paresthetica: an operative series of 45 cases.\n\nAbstract: BACKGROUND\nFailure of symptom relief after neurolysis for meralgia paresthetica (MP) is reported frequently, yet systematic outcome analysis is limited in the modern literature. The present operative series of 45 cases aims to address this issue.\n\n\nMETHODS\nFrom 1996 to 2000, all patients who had neurolysis for MP by our senior author were enrolled in the study. Nerve entrapment was confirmed preoperatively by electrophysiological studies or a positive response to local anesthetic and steroid injection. Attention to the anatomical variability of the lateral femoral cutaneous nerve was made during nerve exploration. Decompression of the nerve was performed at the level of the iliac fascia, the inguinal ligament, and the fascia of the thigh distally. The outcome of surgery was assessed 6 weeks after the procedure followed by 3 monthly intervals if symptoms persisted. Telephone interviews were conducted to assess long-term results.\n\n\nRESULTS\nForty-five decompressive procedures were performed in 42 patients over the 5-year period. The average duration of symptoms was 31 (2.5-180) months. All patients were followed for an average of 4.1 years. Complete and partial symptom improvements were noted in 33 (73%) and 9 (20%) cases, respectively. No recurrence was reported. Analysis of clinical variables demonstrated that the duration of symptoms preoperatively did not affect the rate of complete symptom relief, but obese patients (body mass index > 30) was 6 times more likely to have (odds ratio 6.16, P = 0.04) incomplete relief after surgery at long-term follow-up.\n\n\nCONCLUSION\nHigh success rate was recorded in our series of 45 cases. Prolonged duration of symptoms did not preclude favorable outcome but obesity had a negative association with good outcome.\n\n==\n\nWeier 2010:\n\nTitle: Meralgia paresthetica of the contralateral leg after total hip arthroplasty.\n\nAbstract: In the United States, total hip arthroplasty (THA) is typically performed with the patient in the lateral decubitus position. Positioning of the morbidly obese patient can be technically challenging and may require special positioning equipment. Although the increased incidence of complications after THA in obese patients has been well documented, neurologic complications in the contralateral limb are rare. This article describes a case of a patient with impairment of the lateral femoral cutaneous nerve in the contralateral leg after THA.A 55-year-old woman with a body mass index of 34.24 kg/m(2) underwent THA in the right lateral decubitus position. Because of her body habitus, a bean-bag positioner was used. Total hip arthroplasty was performed using a direct lateral approach. Intraoperative surgical time was 2.5 hours, and total anesthesia time was 3.5 hours. A few days postoperatively, the patient began to experience \"burning and shooting\" pain in the contralateral hip, but she did not report this pain until 6 weeks postoperatively. She was treated initially with a single lidocaine injection. When this was ineffective, she was treated with topiramate (100 mg daily) and vitamin B6 (100 mg orally twice daily). The symptoms lessened markedly at 5 months and resolved completely at 9 months postoperatively.Meralgia paresthetica is an uncommon, but known, complication of THA. To our knowledge, it has been reported only in the operative limb. This report reinforces the need for careful positioning to avoid pressure over the anterior superior iliac spine intraoperatively.\n\n==\n\nMd 2011:\n\nTitle: Treatment of Meralgia Paresthetica with Ultrasound-Guided Pulsed Radiofrequency Ablation of the Lateral Femoral Cutaneous Nerve\n\nAbstract: Abstract:\u2002 A 23-year-old female with an 18-month history of left anterolateral thigh paresthesias and burning pain consistent with meralgia paresthetica was referred to our clinic after failing trials of physical therapy, nonsteroidal anti-inflammatories, gabapentin, and amitriptyline. We performed 3 lateral femoral cutaneous nerve blocks with corticosteroid over a 4-month period; however, each block provided only temporary relief. As this pain was limiting the patient\u2019s ability to perform her functions as an active duty service member, we elected to perform a pulsed radiofrequency treatment of the lateral femoral cutaneous nerve with ultrasound guidance and nerve stimulation. After locating the lateral femoral cutaneous nerve with ultrasound and reproducing the patient\u2019s dysthesia with stimulation, pulsed radiofrequency treatment was performed at 42\u00b0C for 120\u00a0seconds. The needle was then rotated 180\u00b0 and an additional cycle of pulsed radiofrequency treatment was performed followed by injection of 0.25% ropivacaine with 4\u00a0mg of dexamethasone. At 1.5 and 3\u00a0month follow-up visits, the patient reported excellent pain relief with activity and improved ability to perform her duties as an active duty service member. \u25aa\n\n==\n\nFowler 2012:\n\nTitle: Treatment of Meralgia Paresthetica with Ultrasound\u2010Guided Pulsed Radiofrequency Ablation of the Lateral Femoral Cutaneous Nerve\n\nAbstract: Abstract:\u2002 A 23\u2010year\u2010old female with an 18\u2010month history of left anterolateral thigh paresthesias and burning pain consistent with meralgia paresthetica was referred to our clinic after failing trials of physical therapy, nonsteroidal anti\u2010inflammatories, gabapentin, and amitriptyline. We performed 3 lateral femoral cutaneous nerve blocks with corticosteroid over a 4\u2010month period; however, each block provided only temporary relief. As this pain was limiting the patient\u2019s ability to perform her functions as an active duty service member, we elected to perform a pulsed radiofrequency treatment of the lateral femoral cutaneous nerve with ultrasound guidance and nerve stimulation. After locating the lateral femoral cutaneous nerve with ultrasound and reproducing the patient\u2019s dysthesia with stimulation, pulsed radiofrequency treatment was performed at 42\u00b0C for 120\u2003seconds. The needle was then rotated 180\u00b0 and an additional cycle of pulsed radiofrequency treatment was performed followed by injection of 0.25% ropivacaine with 4\u2003mg of dexamethasone. At 1.5 and 3\u2003month follow\u2010up visits, the patient reported excellent pain relief with activity and improved ability to perform her duties as an active duty service member. \u25aa\n\n==\n\nLu 2021:\n\nTitle: Meralgia paresthetica treated by injection, decompression, and neurectomy: a systematic review and meta-analysis of pain and operative outcomes.\n\nAbstract: OBJECTIVE\nMeralgia paresthetica is caused by entrapment of the lateral femoral cutaneous nerve (LFCN) and often presents with pain. Multiple treatment options targeting the LFCN can be pursued to treat the pain should conservative measures fail, with the most common options being injection, neurolysis, and neurectomy. However, their efficacy in causing pain relief and their clinical outcomes have yet to be directly compared. The aim of this study was to interrogate the contemporary literature and quantitatively define how these options compare.\n\n\nMETHODS\nThe electronic databases Ovid Embase, PubMed, SCOPUS, and the Cochrane Library were interrogated from inception to May 2020 following the PRISMA guidelines. Candidate articles were screened against prespecified criteria. Outcome data were abstracted and pooled by random-effects meta-analysis of proportions.\n\n\nRESULTS\nThere were 25 articles that satisfied all criteria, reporting outcomes for a total of 670 meralgia paresthetica patients, with 78 (12%) treated by injection, 496 (74%) by neurolysis, and 96 (14%) by neurectomy. The incidence of complete pain relief was 85% (95% CI 71%-96%) after neurectomy, 63% (95% CI 56%-71%) after neurolysis, and 22% (95% CI 13%-33%) after injection, which were all statistically different (p < 0.01). The incidence of revision procedures was 12% (95% CI 4%-22%) after neurolysis and 0% (95% CI 0%-2%) after neurectomy, which were significantly lower than 81% (95% CI 64%-94%) after injection (p < 0.01). The incidences of treatment complications were statistically comparable across all three treatments, ranging from 0% to 5% (p = 0.34).\n\n\nCONCLUSIONS\nThere are multiple treatment options to target pain in meralgia paresthetica. The incidence of complete pain relief appears to be the greatest among the 3 interventions after neurectomy, accompanied by the lowest incidence of revision procedures. These findings should help inform patient preference and expectations. Greater exploration of the anatomical rationale for incomplete pain relief after surgical intervention will assist in optimizing further surgical treatment for meralgia paresthetica.\n\n==\n\nPhilip 2009:\n\nTitle: Successful treatment of meralgia paresthetica with pulsed radiofrequency of the lateral femoral cutaneous nerve.\n\nAbstract: BACKGROUND\nMeralgia paresthetica is a rarely encountered sensory mononeuropathy characterized by paresthesia, pain or sensory impairment along the distribution of the lateral femoral cutaneous nerve caused by entrapment or compression of the nerve as it crosses the anterior superior iliac spine and runs beneath the inguinal ligament.\n\n\nOBJECTIVE\nWe describe the first reported use of pulsed radiofrequency neuromodulation to relieve the intractable pain associated with meralgia paresthetica.\n\n\nCASE REPORT\nA 33-year-old morbidly obese female with a history of lower back pain and previous spinal fusion presented with sensory dysesthesias and paresthesias in the right anterolateral thigh, consistent with meralgia paresthetica. Temporary relief occurred with multiple lateral femoral cutaneous nerve and fascia lata blocks at 2 different institutions. The patient expressed dissatisfaction with her previous treatments and requested \"any\" therapeutic intervention that might lead to long-lasting pain relief. At this time, we located the anterior superior iliac spine and reproduced concordant dysesthesia. Pulsed radiofrequency was then undertaken at 42 degrees C for 120 seconds followed by dexamethasone and bupivicaine. The patient reported exceptional and prolonged pain relief at 6-month follow-up.\n\n\nLIMITATIONS\nSince this case report is not a prospective, randomized, controlled or blinded study, no conclusions may be drawn from the results attained on behalf of this single individual. Additional, larger group analyses studying this technique while eliminating bias from patient variables would be essential prior to assuming any validity to using pulsed radiofrequency techniques of neuromodulation for managing peripheral neuropathic pain processes.\n\n\nCONCLUSION\nThe patient had experienced long-standing pain that was recalcitrant to conservative/pharmacologic therapy and multiple nerve blocks with local steroid instillations. A single treatment with pulsed radiofrequency resulted in complete and sustained cessation of pain. No side effects were evident. Pulsed radiofrequency of the LFCN may offer an effective, low risk treatment in patients with meralgia paresthetica who are refractory to conservative medical management or are unwilling or unfit to undergo surgery.",
        "final_summary": "The recovery time from injection for meralgia paresthetica varies depending on the treatment approach and individual factors. Ataizi 2018 found that surgical neurolysis and decompression resulted in complete resolution of symptoms within 3 months for 61.5% of patients. Edelson 1994 reported successful outcomes in children after open decompression surgery. Siu 2005 demonstrated a high success rate for neurolysis, with complete and partial symptom improvements in 73% and 20% of cases, respectively. Weier 2010 described a rare case of meralgia paresthetica in the contralateral leg after total hip arthroplasty. Fowler 2012 highlighted the use of ultrasound-guided pulsed radiofrequency ablation for pain relief in meralgia paresthetica. Lu 2021 conducted a meta-analysis and found that neurectomy had the highest incidence of complete pain relief and the lowest rate of revision procedures. Philip 2009 reported successful treatment of meralgia paresthetica with pulsed radiofrequency neuromodulation."
    },
    {
        "query": "magnesium malate",
        "paper_list_string": "Turck 2018:\n\nTitle: Magnesium citrate malate as a source of magnesium added for nutritional purposes to food supplements\n\nAbstract: Abstract The present scientific opinion deals with the assessment of the bioavailability of magnesium, from the proposed nutrient source, magnesium citrate malate (MgCM), when added for nutritional purposes to food supplements. MgCM is a mixed salt consisting of magnesium cations and citrate and malate anions, and with a magnesium content of 12\u201315%. MgCM is proposed to be used in food supplements that are intended to provide up to 300\u2013540 mg/day magnesium. The data provided demonstrate that the production process results in batches of MgCM that comply with the product specifications and that the product is stable throughout its proposed shelf life. The human studies provided demonstrate that magnesium from MgCM is bioavailable. However, the extent of its bioavailability per se or compared to other magnesium sources cannot be established due to the lack of an appropriate magnesium source as a comparator in the studies provided or relevant kinetic data for magnesium. One publication provided in the dossier reported that supplementation with MgCM decreases calcium absorption, but this finding was not supported by publications on different magnesium salts and therefore the Panel could not draw conclusions from this finding. The Panel concludes that MgCM is a source from which magnesium is bioavailable, but the extent of its bioavailability cannot be established. The Panel notes that at the proposed maximum use levels of MgCM, the existing tolerable upper intake level for magnesium in nutritional supplements, water, or added to food and beverages (250 mg/day) is exceeded.\n\n==\n\nYounes 2018:\n\nTitle: Evaluation of di\u2010magnesium malate, used as a novel food ingredient and as a source of magnesium in foods for the general population, food supplements, total diet replacement for weight control and food for special medical purposes\n\nAbstract: Abstract The present scientific opinion deals with the evaluation of the safety of di\u2010magnesium malate (DMM) proposed as a novel food ingredient and as a source of magnesium for use in foods for the general population, food supplements, total diet replacement for weight control and food for special medical purposes (FSMP), and with the bioavailability of magnesium from this source. Additional information was sought from the applicant during the assessment process. However, despite several requests, the applicant did not provide the additional data. Consequently, the Panel performed this assessment on the basis of the available data and concluded that there was insufficient scientific evidence of a difference between the proposed novel food ingredient named as DMM and magnesium malate already authorised as a source of magnesium included in Annex II to Directive 2002/46/EC. Accordingly, the Panel was unable to assess the safety of DMM as a novel food ingredient. The Panel concluded that based on the data provided it was not possible to assess the dissociation of DMM into magnesium and malic acid. The Panel further concluded that if DMM dissociates, magnesium would be available following ingestion of DMM and the availability would appear similar to values reported for other sources of magnesium already permitted. Finally, the Panel noted that the proposed use levels could result in exposures to magnesium greater than its upper level (UL) (250 mg/day) for food supplements and for food for special medical purposes.\n\n==\n\nBlaquiere 1987:\n\nTitle: Speciation studies in relation to magnesium bioavailability. Formation of Mg(II) complexes with glutamate, aspartate, glycinate, lactate, pyroglutamate, pyridoxine and citrate, and appraisal of their potential significance towards magnesium gastrointestinal absorption\n\nAbstract: Abstract Associations of Mg2+ ions with organic ligands are frequently advertised as likely to enhance the bioavailability of magnesium from orally administered commercial preparations. However, no systematic study of the relevant equilibria has been produced so far to substantiate these assertions, and no superiority has yet been demonstrated for any magnesium salt on clinical grounds. After a review concerning different aspects of magnesium gastrointestinal absorption, in particular with respect to calcium interactions, this paper deals with the determination of formation constants for magnesium complexes with glutamate, aspartate, glycinate, lactate, pyroglutamate, pyridoxine and citrate, under physiological conditions of ionic strength (0.15 mol dm\u22123) and temperature (37 \u00b0C). Corresponding results are then used to assess the potential capacity of each of these ligands to mobilise Mg2+ ions into membrane diffusible complexes. At usual therapeutic concentrations, pyroglutamate and pyridoxine do not coordinate magnesium in appreciable amounts, and glycinate and lactate do not form any neutral complex. In contrast, glutamate, aspartate and citrate do form neutral magnesium species; the extent of their expected effects with respect to magnesium uptake by enterocyte membranes is discussed on the basis of relevant computer simulations. Potential calcium interactions are also examined, which required the determination of formation constants for calcium complexes with these three ligands.\n\n==\n\nMaguire 2004:\n\nTitle: Magnesium chemistry and biochemistry\n\nAbstract: Magnesium apparently derives its name from Magnesia, a district in the Volos area of Thessaly in northeastern Greece. It was used to refer to any of several minerals ranging from magnesite (magnesium carbonate or \u2018magnesia alba\u2019) to magnetite (manganese dioxide or \u2018magnesia nigra\u2019) and also magnesia stone (talc or soapstone), a magnesium silicate. However, there are also two ancient towns in Asia Minor named Magnesia from the Greek word magnes used to refer to the magnetic iron ore discovered in this area. Magnesia could have been derived from magnes but used for another mineral in a different place, just as magnesia in Greece referred to several minerals. Magnesium metal was first prepared by Sir Humphrey Davy around 1810 by making the amalgam of magnesium with mercury and then distilling off the mercury electrolytically (Treneer 1963). Davy named the new metal magnium and the word magnesium was initially used for manganese, derived from the mineral \u2018magnesia nigra\u2019. The pure metal is highly reactive and all magnesium in the biosphere is either the free cation Mg2+ in aqueous solutions or some salt or mineral form. There are several common mineral forms of magnesium in the environment. Dolomite [MgCa(CO3)2] derives its name from the Dolomite range in the Italian Alps. The story of the recognition of dolomite and various controversies that have ranged and still range around this mineral makes interesting reading for the historian of science (Hacking 1999; McKenzie 1991). Epsomite (MgSO4 \u00b7 7H2O) derives its name from the Epsom district in England, originally south of London but now incorporated into greater London. Epsom water originally came from a spring that arose on the common of Epsom village. Epsomite proved to be a major constituent of the waters. Application proved efficacious in healing of external ulcers, and the water became a destination for the sick. Until the first decade of the 18th century, Epsom was a well-known spa with many visitors partaking of the waters internally, where it acts as a purgative (Sakula 1984). Other minerals containing large amounts of magnesium are olivine (Mg2SiO4), magnesium calcite (MgSO4) and chrysolite [asbestos, Mg3Si2O5(OH)4], as well as garnet and spinel, which also contain aluminum with the magnesium.\n\n==\n\nSerefko 2013:\n\nTitle: Magnesium in depression.\n\nAbstract: Magnesium is one of the most essential mineral in the human body, connected with brain biochemistry and the fluidity of neuronal membrane. A variety of neuromuscular and psychiatric symptoms, including different types of depression, was observed in magnesium deficiency. Plasma/serum magnesium levels do not seem to be the appropriate indicators of depressive disorders, since ambiguous outcomes, depending on the study, were obtained. The emergence of a new approach to magnesium compounds in medical practice has been seen. Apart from being administered as components of dietary supplements, they are also perceived as the effective agents in treatment of migraine, alcoholism, asthma, heart diseases, arrhythmias, renal calcium stones, premenstrual tension syndrome etc. Magnesium preparations have an essential place in homeopathy as a remedy for a range of mental health problems. Mechanisms of antidepressant action of magnesium are not fully understood yet. Most probably, magnesium influences several systems associated with development of depression. The first information on the beneficial effect of magnesium sulfate given hypodermically to patients with agitated depression was published almost 100 years ago. Numerous pre-clinical and clinical studies confirmed the initial observations as well as demonstrated the beneficial safety profile of magnesium supplementation. Thus, magnesium preparations seem to be a valuable addition to the pharmacological armamentarium for management of depression.\n\n==\n\nSeitz 2014:\n\nTitle: Magnesium degradation products: effects on tissue and human metabolism.\n\nAbstract: Owing to their mechanical properties, metallic materials present a promising solution in the field of resorbable implants. The magnesium metabolism in humans differs depending on its introduction. The natural, oral administration of magnesium via, for example, food, essentially leads to an intracellular enrichment of Mg(2+) . In contrast, introducing magnesium-rich substances or implants into the tissue results in a different decomposition behavior. Here, exposing magnesium to artificial body electrolytes resulted in the formation of the following products: magnesium hydroxide, magnesium oxide, and magnesium chloride, as well as calcium and magnesium apatites. Moreover, it can be assumed that Mg(2+) , OH(-) ions, and gaseous hydrogen are also present and result from the reaction for magnesium in an aqueous environment. With the aid of physiological metabolic processes, the organism succeeds in either excreting the above mentioned products or integrating them into the natural metabolic process. Only a burst release of these products is to be considered a problem. A multitude of general tissue effects and responses from the Mg's degradation products is considered within this review, which is not targeting specific implant classes. Furthermore, common alloying elements of magnesium and their hazardous potential in vivo are taken into account.\n\n==\n\nMacIntyre 1963:\n\nTitle: An outline of magnesium metabolism in health and disease\u2014A review\n\nAbstract: Magnesium is plentiful and makes up 2.1 per cent of the earth\u2019s crust [I]. It occurs in the insoluble form in large masses as magnesite, MgC03, and as dolomite, MgC03.CaC03. It is also present in less plentiful minerals such as asbestos (a calcium magnesium silicate), and in meerschaum, serpentine and talc, which are also silicates. In the soluble form it is present as carnallite, kainite, schijnite and kieserite. Sea water and some spring waters contain magnesium in solution. Magnesium is usually classified with the alkaline earth metals (Group II A of the Mendeleef classification), although in many ways it has closer resemblance to zinc and cadmium (Group II B). It is a light, white metal of atomic number 12 and atomic weight 24.32. It is extremely reactive and will burn in air, carbon dioxide or the halogens. Like the other metals of the alkaline earths, magnesium readily forms divalent ions, losing its two valency electrons, leaving the ion with the electronic shell of the inert gas (neon, in the case of magnesium) two units lower in atomic number. Ions of higher valency are therefore not known. The general chemical reactions of magnesium are too well known to need description.\n\n==\n\nSerefko 2016:\n\nTitle: Magnesium and depression.\n\nAbstract: Magnesium is one of the most important elements in the human body and is involved in a number of biochemical processes crucial for the proper functioning of the cardiovascular, alimentary, endocrine, and osteoarticular systems. It also plays a vital modulatory role in brain biochemistry, influencing several neurotransmission pathways associated with the development of depression. Personality changes, including apathy, depression, agitation, confusion, anxiety, and delirium are observed when there is a deficiency of this element. Rodents receiving a diet deficient in magnesium displayed depressive behaviour that was reversed by antidepressant drugs. Poor nutrition, gastrointestinal and renal diseases, insulin resistance and/or type 2 diabetes, alcoholism, stress, and certain medications may lead to magnesium deficiency. Since the extracellular concentration of magnesium ions may not reflect their intracellular level, none of the current methods of evaluating magnesium status is regarded as satisfactory. The mood-improving potential of magnesium compounds have been confirmed by the results of numerous pre-clinical and clinical studies. It seems that magnesium supplementation is well-tolerated and enhances the efficacy of conventional antidepressant treatments, and as such could be a valuable addition to the standard treatments for depression, although differences in bioavailability between inorganic and organic compounds should be taken into consideration.",
        "final_summary": "Magnesium malate is a potential source of bioavailable magnesium, as suggested by Turck 2018's study on magnesium citrate malate. However, the bioavailability of di-magnesium malate, another form of magnesium malate, could not be definitively established due to insufficient data, although it was suggested that if it dissociates, magnesium would be available (Younes 2018). Blaquiere 1987 discussed the potential of magnesium complexes, including those with citrate, to enhance magnesium absorption. The importance of magnesium in brain biochemistry and its potential role in depression treatment was highlighted by Serefko 2013 and Serefko 2016. However, caution regarding excessive magnesium intake was noted (Younes 2018; Seitz 2014). Further research is needed to fully understand the effects and optimal use of magnesium malate."
    },
    {
        "query": "Firm growth types",
        "paper_list_string": "Wu 2012:\n\nTitle: Firm Growth Type and Capital Structure Persistence\n\nAbstract: We find that growth type (identified by a two-way sort on firm initial market-to-book ratio and asset tangibility) can parsimoniously predict significantly dispersed and persistently distinct future leverage ratios. Growth type is persistent; growth-type-sorted cross-sections of corporate fundamental variables (such as tangible versus intangible investment style) are also meaningfully persistent. As economic and market conditions improve, low growth type firms are keener to issue new debt than equity, whereas high growth type firms are least likely to issue debt and keenest to issue equity. These findings demonstrate that firms rationally invest and seek financing in a manner compatible with their growth types. Consistent with a generalized Myers\u2013Majluf framework, growth type compatibility enables distinct growth types and hence specifications of market imperfection or informational environments to persist. Growth type is apparently a fundamental factor for capital structure persistence.\n\n==\n\nWu 2012:\n\nTitle: Firm Growth Type and Capital Structure Persistence\n\nAbstract: We find that growth type (identified by a two-way sort on firm initial market-to-book ratio and asset tangibility) can parsimoniously predict significantly dispersed and persistently distinct future leverage ratios. Growth type is persistent; growth-type-sorted cross-sections of corporate fundamental variables such as tangible versus intangible investment style also meaningfully persist. With an increase in market-to-book ratios, low growth type firms are keener to issue new debt than equity, whereas high growth type firms are least likely to issue debt and keenest to issue new equity. These findings demonstrate that firms rationally invest and seek financing in a manner compatible with their growth type. Consistent with a generalized Myers-Majluf framework, growth type compatibility enables distinct growth types and hence specifications of market imperfection or informational environments to persist. Growth type is apparently a fundamental factor for capital structure persistence.\n\n==\n\nBon\u010da 2018:\n\nTitle: Firm growth types and key macroeconomic aggregates through the economic cycle\n\nAbstract: The paper investigates the role and impact of different groups of firms according to their growth type on macroeconomic aggregates at various stages of the economic cycle based on the entire population of firms in Slovenia. The applied classification of growing and fast-growing firms is based on microeconomic theory. Results exhibit that despite larger year-to-year fluctuations, firms with growth towards their long-term equilibrium contributed most to macroeconomic aggregates, i.e. employment, capital and sales, especially in times of economic prosperity. Firms with growth that shifts them closer to their short-term equilibrium proved to be more important primarily for assuring employment stability. Furthermore, we show that using single growth measures prevents us from identifying all growing firms and capturing the true contribution of particular growth groups of firms to studied macroeconomic aggregates. The paper provides both theoretical and empirical information for managers for designing different types of firm growth and enables policy makers to adopt adequate industrial policy measures.\n\n==\n\nBirley 1990:\n\nTitle: Growth and performance contrasts between \u2018types\u2019 of small firms\n\nAbstract: This paper studies the interrelationships in the strategic profile of a sample of small firms, and, by using cross\u2010sectional analysis, attempts to identify any evidence to support the \u2018stages of growth\u2019 theories. Three surrogates for comparative growth were used in the analysis: number of employees, sales turnover, and profitability. A cluster analysis identified eight different \u2018types\u2019 of small firms characterized by \u2018internal\u2019 variables of ownership, management, and product structure; and by \u2018external\u2019 variables of product/market positioning. Analysis of variance tests found no significant differences between the clusters with regard to size. The results suggest that firms do change, but not necessarily in any prescribed sequence. Indeed, the evidence presented in this paper suggests that future research should be focused on developing theories which better describe the heterogeneity of the sector by analyzing the development within clusters of firms rather than seeking generalized overarching theories.\n\n==\n\nNichter 2009:\n\nTitle: Small Firm Growth in Developing Countries\n\nAbstract: Summary Although the vast majority of small firms in developing countries never expand beyond a few employees, some experience rapid and substantial growth. This study explores factors associated with small firm growth. We discuss key findings for four types of factors: (1) individual entrepreneur characteristics; (2) firm characteristics; (3) relational factors (such as social networks or value chains); and (4) contextual factors (such as the business environment). We conclude by suggesting implications for development practitioners.\n\n==\n\nDelmar 2003:\n\nTitle: Arriving at the high growth firm\n\nAbstract: This study explores heterogeneity in how firms have achieved high growth. Using the population of all firms in Sweden with more than 20 employees in existence in 1996 (n= 11,748), we analyzed their development for each year of the previous 10 years (1987 to 1996). From this population of all firms in Sweden, multiple criteria were used to define a sample of high growth firms (n = 1,501). Using nineteen different measures of firm growth (such as relative and absolute sales growth, relative and absolute employee growth, organic growth versus acquisition growth, and the regularity and volatility of growth rates over the ten year period) we identified seven different types of firm growth patterns. These patterns were related to firm age and size as well as industry affiliation. Implications for research and practice are offered.\n\n==\n\nBarringer 2005:\n\nTitle: A Quantitative Content Analysis of the Characteristics of Rapid-Growth Firms and Their Founders\n\nAbstract: A group of firms that is attracting attention are rapid-growth firms. These firms are of particular interest because rapid growth is an indication of market acceptance and firm success. By studying rapidgrowth firms, researchers can help all firms better understand the attributes associated with firm growth. Despite these positive outcomes, however, rapid firm growth is difficult to achieve and maintain. This study adds to the literature through a quantitative content analysis of the narrative descriptions of 50 rapid-growth firms and a comparison group of 50 slow-growth companies. The purpose of the study was to draw from the narratives a list of empirically grounded growth-related attributes that are associated with rapid-growth firms. The findings of the study resulted in the advancement of a conceptual model of the attributes of rapid-growth firms in four areas: founder characteristics, firm attributes, business practices, and human resource management (HRM) practices.\n\n==\n\nPark 2010:\n\nTitle: Firm growth patterns: examining the associations with firm size and internationalization.\n\nAbstract: Abstract Understanding the growth patterns of an industry is essential for establishing sustainable growth strategies. However, until recently little had been known about restaurant firm growth patterns. Thus, this study examined the growth patterns of restaurant firms in association with firm size class and internationalization, after controlling for total and long-term debt leverage, retained earnings, and growth opportunity. Overall, the results of this study showed that small restaurant firms grow faster than large restaurant firms but the growth rate decreases as firm size increases. Furthermore, the growth rate of large firms decreased more slowly than small firms. In terms of internationalization, this study found that as firm size increases, the growth rate of small international firms decreases more rapidly than that of small domestic firms. However, the growth rate of large international firms decreases more slowly than that of large domestic firms. These findings indicate the appropriateness of internationalization strategies for large restaurant firms but the inappropriateness of these strategies for small firms. More detailed results and discussion are also provided.",
        "final_summary": "The papers collectively provide insights into firm growth types. Wu 2012 identifies growth types based on initial market-to-book ratio and asset tangibility, finding that these types predict future leverage ratios. Low growth type firms are more likely to issue debt as market conditions improve, while high growth type firms prefer equity issuance. Bon\u010da 2018 examines the impact of different growth types on macroeconomic aggregates, showing that firms closer to long-term equilibrium contribute more to employment, capital, and sales during economic prosperity. Birley 1990 challenges the notion of prescribed growth sequences, suggesting that future research should focus on analyzing development within clusters of firms. Nichter 2009 explores factors associated with small firm growth in developing countries, including individual entrepreneur characteristics, firm characteristics, relational factors, and contextual factors. Delmar 2003 identifies seven different types of firm growth patterns in Sweden, related to firm age, size, and industry affiliation. Barringer 2005 conducts a quantitative content analysis of rapid-growth firms, identifying growth-related attributes in founder characteristics, firm attributes, business practices, and human resource management practices. Park 2010 examines growth patterns of restaurant firms, finding that small firms grow faster than large firms, and the growth rate decreases as firm size increases. Internationalization strategies are more appropriate for large firms."
    },
    {
        "query": "Deep Learning for Predictive Control in Industrial Plants",
        "paper_list_string": "Bellini 2021:\n\nTitle: A Deep Learning Approach for Short Term Prediction of Industrial Plant Working Status\n\nAbstract: Predictive Maintenance has gained more and more research and commercial interests, being a pivotal topic for improving the efficiency of many production industrial plants to minimize downtimes, as well as to reduce operational costs for interventions. Solutions reviewed in literature are increasingly based on machine learning and deep learning methods for prediction of fault proneness with respect to normal working conditions. Many state-of-the art solutions are not actually applied in real scenarios, and have restrictions to be executed in real-time in the production environment. In this paper, a framework for predictive maintenance is presented. It has been built upon a deep learning model based on Long-Short Term Memory Neural Networks, LSTM and Convolutional LSTM. The proposed model provides a one-hour prediction of the plant status and indications on the areas in which the intervention should be performed by using explainable LSTM technique. The solution has been validated against real data of ALTAIR chemical plant, demonstrating an high accuracy with the capability of being executed in real-time in a production operative scenario. The paper also introduced business intelligence tools on maintenance data and the architectural infrastructure for the integration of predictive maintenance approach.\n\n==\n\nKoprinkova-Hristova 2013:\n\nTitle: Reinforcement Learning for Predictive Maintenance of Industrial Plants\n\nAbstract: Abstract The reinforcement learning is a well-known approach for solving optimization problems having limited information about plant dynamics. Its key element, named \u201ccritic\u201d is aimed at prediction of future \u201cpunish/reward\u201d signals received as a result of undertaken control actions. The main idea in the present work is to use such a \u201ccritic\u201d element for prediction of approaching alarm situations based on limited measurement information from the industrial plant. In order to train the critic network in real time it is proposed to use a special kind of a fast trainable recurrent neural network, called Echo State Network (ESN). The approach proposed is demonstrated on an example for predictive maintenance of a mill fan in Maritsa East 2 Thermal Power Plant\n\n==\n\nLu 2007:\n\nTitle: Generalized predictive control using recurrent fuzzy neural networks for industrial processes\n\nAbstract: Abstract This paper presents a design methodology for predictive control of industrial processes via recurrent fuzzy neural networks (RFNNs). A discrete-time mathematical model using RFNN is constructed and a learning algorithm adopting a recursive least squares (RLS) approach is employed to identify the unknown parameters in the model. A generalized predictive control (GPC) law with integral action is derived based on the minimization of a modified predictive performance criterion. The stability and steady-state performance of the resulting control system are studied as well. Two examples including the control of a nonlinear process and the control of a physical variable-frequency oil-cooling machine are used to demonstrate the effectiveness of the proposed method. Both results from numerical simulations and experiments show that the proposed method is capable of controlling industrial processes with satisfactory performance under setpoint and load changes.\n\n==\n\nKarimanzira 2020:\n\nTitle: Deep Learning Based Model Predictive Control for a Reverse Osmosis Desalination Plant\n\nAbstract: Reverse Osmosis (RO) desalination plants are highly nonlinear multi-input-multioutput systems that are affected by uncertainties, constraints and some physical phenomena such as membrane fouling that are mathematically difficult to describe. Such systems require effective control strategies that take these effects into account. Such a control strategy is the nonlinear model predictive (NMPC) controller. However, an NMPC depends very much on the accuracy of the internal model used for prediction in order to maintain feasible operating conditions of the RO desalination plant. Recurrent Neural Networks (RNNs), especially the Long-Short-Term Memory (LSTM) can capture complex nonlinear dynamic behavior and provide long-range predictions even in the presence of disturbances. Therefore, in this paper an NMPC for a RO desalination plant that utilizes an LSTM as the predictive model will be presented. It will be tested to maintain a given permeate flow rate and keep the permeate concentration under a certain limit by manipulating the feed pressure. Results show a good performance of the system.\n\n==\n\nWang 2020:\n\nTitle: Deep Learning-Based Model Predictive Control for Continuous Stirred-Tank Reactor System\n\nAbstract: A continuous stirred-tank reactor (CSTR) system is widely applied in wastewater treatment processes. Its control is a challenging industrial-process-control problem due to great difficulty to achieve accurate system identification. This work proposes a deep learning-based model predictive control (DeepMPC) to model and control the CSTR system. The proposed DeepMPC consists of a growing deep belief network (GDBN) and an optimal controller. First, GDBN can automatically determine its size with transfer learning to achieve high performance in system identification, and it serves just as a predictive model of a controlled system. The model can accurately approximate the dynamics of the controlled system with a uniformly ultimately bounded error. Second, quadratic optimization is conducted to obtain an optimal controller. This work analyzes the convergence and stability of DeepMPC. Finally, the DeepMPC is used to model and control a second-order CSTR system. In the experiments, DeepMPC shows a better performance in modeling, tracking, and antidisturbance than the other state-of-the-art methods.\n\n==\n\nBieker 2020:\n\nTitle: Deep model predictive flow control with limited sensor data and online learning\n\nAbstract: The control of complex systems is of critical importance in many branches of science, engineering, and industry, many of which are governed by nonlinear partial differential equations. Controlling an unsteady fluid flow is particularly important, as flow control is a key enabler for technologies in energy (e.g., wind, tidal, and combustion), transportation (e.g., planes, trains, and automobiles), security (e.g., tracking airborne contamination), and health (e.g., artificial hearts and artificial respiration). However, the high-dimensional, nonlinear, and multi-scale dynamics make real-time feedback control infeasible. Fortunately, these high-dimensional systems exhibit dominant, low-dimensional patterns of activity that can be exploited for effective control in the sense that knowledge of the entire state of a system is not required. Advances in machine learning have the potential to revolutionize flow control given its ability to extract principled, low-rank feature spaces characterizing such complex systems. We present a novel deep learning model predictive control framework that exploits low-rank features of the flow in order to achieve considerable improvements to control performance. Instead of predicting the entire fluid state, we use a recurrent neural network (RNN) to accurately predict the control relevant quantities of the system, which are then embedded into an MPC framework to construct a feedback loop. In order to lower the data requirements and to improve the prediction accuracy and thus the control performance, incoming sensor data are used to update the RNN online. The results are validated using varying fluid flow examples of increasing complexity.\n\n==\n\nSantander 2022:\n\nTitle: Deep learning economic model predictive control for refinery operation: A fluid catalytic cracker - fractionator case study\n\nAbstract: An integrated deep learning - economic model predictive control (EMPC) framework for large scale processes is presented. The framework is successfully implemented to a realistic fluid catalytic cracker (FCC) - fractionator process. Scenarios under the effect of no disturbances (nominal) and with disturbances are simulated demonstrating fast computation (potentially allowing industrial implementation) and improved performance (taking into account process nonlinear behavior, enhancing the process operating profit).\n\n==\n\nSun 2019:\n\nTitle: Probabilistic Sequential Network for Deep Learning of Complex Process Data and Soft Sensor Application\n\nAbstract: Soft sensing of quality/key variables is critical to the control and optimization of industrial processes. One of the main drawbacks of data-driven soft sensors is to deal with the dynamic and nonlinear characteristics of process data. This paper proposes a deep learning structure and corresponding training algorithm for the purpose of soft sensor, which is called probabilistic sequential network. The proposed model merges unsupervised feature extraction and supervised dynamic modeling approaches to improve the prediction performance. It is mainly based on the Gaussian-Bernoulli restricted Boltzmann machine and the recurrent neural network structure. To avoid the overfitting problem in the training procedure of deep learning algorithms, the L2 regularization and dropout technique are adopted. The new method can not only deeply extract the nonlinear feature but also widely capture dynamic characteristic of process data. Effectiveness and superiority of the new method are validated through an actual CO2 absorption column, compared to traditional methods.",
        "final_summary": "The papers suggest that deep learning techniques, such as recurrent neural networks (RNNs) and long-short term memory (LSTM) networks, can be effective for predictive control in industrial plants.\n\nBellini 2021 presents a deep learning model based on LSTM and Convolutional LSTM for short-term prediction of industrial plant working status, demonstrating high accuracy and real-time execution capability.\n\nKoprinkova-Hristova 2013 proposes using reinforcement learning with Echo State Networks (ESNs) for predictive maintenance in industrial plants, specifically for predicting approaching alarm situations based on limited measurement information.\n\nLu 2007 introduces a design methodology for predictive control using recurrent fuzzy neural networks (RFNNs) in industrial processes, showing satisfactory performance in controlling nonlinear processes and physical variable-frequency oil-cooling machines.\n\nKarimanzira 2020 applies deep learning-based model predictive control (NMPC) using LSTM as the predictive model for a reverse osmosis desalination plant, achieving good performance in maintaining desired permeate flow rate and concentration.\n\nWang 2020 proposes a deep learning-based model predictive control (DeepMPC) using a growing deep belief network (GDBN) for modeling and controlling a continuous stirred-tank reactor (CSTR) system, demonstrating better performance compared to other methods in modeling, tracking, and antidisturbance.\n\nBieker 2020 presents a deep learning model predictive control framework using recurrent neural networks (RNNs) to accurately predict control-relevant quantities of complex fluid flow systems, achieving considerable improvements in control performance.\n\nSantander 2022 implements an integrated deep learning - economic model predictive control (EMPC) framework for a fluid catalytic cracker (FCC) - fractionator process, demonstrating fast computation and improved performance in process operating profit.\n\nSun 2019 proposes a probabilistic sequential network for soft sensor applications in industrial processes, combining unsupervised feature extraction and supervised dynamic modeling approaches, showing effectiveness and superiority compared to traditional methods.\n\nOverall, these papers highlight the potential of deep learning techniques for predictive control in industrial plants, offering improved accuracy, real-time execution, and enhanced performance in various process scenarios."
    },
    {
        "query": "usage of 2d and 3d reflective markers/targets in civil and industrial lidar applications",
        "paper_list_string": "Zhao 2019:\n\nTitle: Mapping with Reflection - Detection and Utilization of Reflection in 3D Lidar Scans\n\nAbstract: This paper presents a method to detect reflection of 3D light detection and ranging (Lidar) scans and uses it to classify the points and also map objects outside the line of sight. Our software uses several approaches to analyze the point cloud, including intensity peak detection, dual return detection, plane fitting, and finding the boundaries. These approaches can classify the point cloud and detect the reflection in it. By mirroring the reflection points on the detected window pane and adding classification labels on the points, we can improve the map quality in a Simultaneous Localization and Mapping (SLAM) framework. Experiments using real scan data and ground truth data showcase the effectiveness of our method.\n\n==\n\nZhao 2020:\n\nTitle: Mapping with Reflection - Detection and Utilization of Reflection in 3D Lidar Scans\n\nAbstract: This paper presents a method to detect reflection of 3D light detection and ranging (Lidar) scans and uses it to classify the points and also map objects outside the line of sight. Our software uses several approaches to analyze the point cloud, including intensity peak detection, dual return detection, plane fitting, and finding the boundaries. These approaches can classify the point cloud and detect the reflection in it. By mirroring the reflection points on the detected window pane and adding classification labels on the points, we can improve the map quality in a Simultaneous Localization and Mapping (SLAM) framework. Experiments using real scan data and ground truth data showcase the effectiveness of our method.\n\n==\n\nFidera 2004:\n\nTitle: TERRESTRIAL LIDAR FOR INDUSTRIAL METROLOGY APPLICATIONS: MODELLING, ENHANCEMENT AND RECONSTRUCTION\n\nAbstract: ABSRACT: Time-of-flight laser scanners represent an emerging technology for capturing 3D data. They use a pulsed laser with a high data acquisition rate. The reflectance of surface varies by properties of materials and is an issue when scanning. Laboratory experiments were conducted using different types of piping materials. This paper examines the effects of scanning with various types of materials, which are used in industrial piping system, in terms of measurement accuracy. Maximum coverage angles of point clouds and diameters for each pipe are measured and compared. Application of a masking tape and dulling spray was found to be effective in improving the accuracy of the data.\n\n==\n\nKavaya 1983:\n\nTitle: Target reflectance measurements for calibration of lidar atmospheric backscatter data.\n\nAbstract: Wavelength and angular dependence of reflectances and depolarization in the 9\u201311-\u03bcm region are reported for four standard targets: flowers of sulfur, flame-sprayed aluminum, 20-grit sandblasted aluminum, and 400-grit silicon carbide sandpaper. Measurements are presented and compared using a cw CO2 grating-tunable laser in a laboratory backscatter apparatus, an integrating sphere, and a coherent pulsed TEA-CO2 lidar system operating in the 9\u201311-\u03bcm region. Reflectance theory related to the use of hard targets to calibrate lidar atmospheric backscatter data is discussed.\n\n==\n\nHuang 2021:\n\nTitle: LiDARTag: A Real-Time Fiducial Tag System for Point Clouds\n\nAbstract: Image-based fiducial markers are useful in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and vision-based simultaneous localization and mapping (SLAM). The state-of-the-art fiducial marker detection algorithms rely on the consistency of the ambient lighting. This article introduces LiDARTag, a novel fiducial tag design and detection algorithm suitable for light detection and ranging (LiDAR) point clouds. The proposed method runs in real-time and can process data at 100\u00a0Hz, which is faster than the currently available LiDAR sensor frequencies. Because of the LiDAR sensors\u2019 nature, rapidly changing ambient lighting will not affect the detection of a LiDARTag; hence, the proposed fiducial marker can operate in a completely dark environment. In addition, the LiDARTag nicely complements and is compatible with existing visual fiducial markers, such as AprilTags, allowing for efficient multi-sensor fusion and calibration tasks. We further propose a concept of minimizing a fitting error between a point cloud and the marker's template to estimate the marker's pose. The proposed method achieves millimeter error in translation and a few degrees in rotation. Due to LiDAR returns\u2019 sparsity, the point cloud is lifted to a continuous function in a reproducing kernel Hilbert space where the inner product can be used to determine a marker's ID. The experimental results, verified by a motion capture system, confirm that the proposed method can reliably provide a tag's pose and unique ID code. The rejection of false positives is validated on the Google Cartographer indoor dataset and the Honda H3D outdoor dataset. All implementations are coded in C++ and are available at https://github.com/UMich-BipedLab/LiDARTag.\n\n==\n\nLee 2012:\n\nTitle: Advanced compact 3D lidar using a high speed fiber coupled pulsed laser diode and a high accuracy timing discrimination readout circuit\n\nAbstract: At last year, we have been developing 3D scanning LIDAR designated as KIDAR-B25, which features the 3D scanning structure based on an optically and mechanically coupled instrument. In contrast with previous scanning LIDARs, vertical scanning is realized using two stepping motors synchronized with movement and moves in a spiral. From the results of outdoor experiments conducted last year to evaluate and measure the LIDAR performance and stability, we identified some limitations and problems that should be resolved. In the first instance, the samples per second are inefficient for use in detection, object clustering, and classification. In addition, the accuracy and precision of distance at every point is seriously affected by the reflectance and distance of the target. Therefore, we have focused on improving the 3D LIDAR range finding performance, speed of measurement, and stability regardless of environmental variation. Toward the realization of these goals, in this paper, we deal with two improvements compared with previous 3D LIDAR.\n\n==\n\nDegnan 2014:\n\nTitle: Moderate to high altitude, single photon sensitive, 3D imaging lidars\n\nAbstract: We describe several scanning, single photon sensitive, 3D imaging lidars that operate at aircraft AGLs between 2 and 9 km and speeds in excess of 200 knots. With 100 beamlets and laser fire rates up to 32 kHz, we have interrogated up to 3.2 million ground pixels per second, some of which record multiple returns from volumetric scatterers such as tree canopies. High range resolution has been achieved through the use of laser pulsewidths between 100 and 700 picoseconds. The systems are presently being deployed on a variety of aircraft to demonstrate their utility in multiple applications including large scale surveying and surveillance, bathymetry, forestry, etc.\n\n==\n\nWeibring 2003:\n\nTitle: Versatile mobile lidar system for environmental monitoring.\n\nAbstract: A mobile lidar (light detection and ranging) system for environmental monitoring is described. The optical and electronic systems are housed in a truck with a retractable rooftop transmission and receiving mirror, connected to a 40-cm-diameter vertically looking telescope. Two injection-seeded Nd:YAG lasers are employed in connection with an optical parametric oscillator-optical parametric amplification transmitter, allowing deep-UV to mid-IR wavelengths to be generated. Fast switching that employs piezoelectric drivers allows multiwavelength differential absorption lidar for simultaneous measurements of several spectrally overlapping atmospheric species. The system can also be used in an imaging multispectral laser-induced fluorescence mode on solid targets. Advanced LabVIEW computer control and multivariate data processing render the system versatile for a multitude of measuring tasks. We illustrate the monitoring of industrial atmospheric mercury and hydrocarbon emissions, volcanic sulfur dioxide plume mapping, fluorescence lidar probing of seawater, and multispectral fluorescence imaging of the facades of a historical monument.",
        "final_summary": "The papers suggest that the usage of 2D and 3D reflective markers/targets in civil and industrial LiDAR applications can provide benefits in terms of mapping, detection, classification, and measurement accuracy.\n\nZhao 2019 and Zhao 2020 propose a method to detect and utilize reflections in 3D LiDAR scans, improving point cloud classification and mapping quality in Simultaneous Localization and Mapping (SLAM) frameworks.\n\nFidera 2004 examines the effects of scanning different types of materials used in industrial piping systems, finding that the application of masking tape and dulling spray can improve measurement accuracy.\n\nKavaya 1983 discusses the reflectance measurements of standard targets for calibrating lidar atmospheric backscatter data, providing insights into the use of hard targets for calibration purposes.\n\nHuang 2021 introduces LiDARTag, a fiducial tag design and detection algorithm suitable for LiDAR point clouds. It operates in real-time, is unaffected by changing ambient lighting, and can complement existing visual fiducial markers.\n\nLee 2012 focuses on improving the range finding performance, measurement speed, and stability of 3D scanning LiDAR systems, addressing limitations related to sample rate efficiency, distance accuracy, and precision.\n\nDegnan 2014 describes single photon sensitive 3D imaging LiDARs that operate at moderate to high altitudes, enabling large-scale surveying, surveillance, and other applications.\n\nWeibring 2003 presents a versatile mobile LiDAR system for environmental monitoring, capable of multiwavelength differential absorption LiDAR, multispectral fluorescence imaging, and other measuring tasks.\n\nIn summary, the papers highlight the potential of using 2D and 3D reflective markers/targets in civil and industrial LiDAR applications for improved mapping, detection, classification, and measurement accuracy."
    },
    {
        "query": "narrative suspense theory",
        "paper_list_string": "Schneider 2003:\n\nTitle: Storylining Suspense - An Authoring Environment for Structuring Non-linear Interactive Narratives\n\nAbstract: $%675$&7 We describe an approach to a new authoring method for interactive storytelling. After positioning digital storytelling in a theoretical context to literature, we consider the author\u2019s view of the tools to be used and introduce a coherent environment that does not restrict the creative process and lets the author feel comfortable, leading him to create well-narrated, interactive non-linear stories. We describe the implementation of the story engine authoring module, which is followed by a project description.\n\n==\n\nLi-che 2013:\n\nTitle: Reading of Narrative Suspense from the Cross-disciplinary Perspectives\n\nAbstract: Narratology started in literature and then moved in the cross-disciplinary way towards broader areas.Sports games are given much attention to in media and with narration they become more attractive.The uncertainty of sports games gives birth to suspense and hence suspense is an important narrative element.Based on sports,communication and literary methods,this paper studies the plots,structures and mental suspense in the coverage of sports games in an effort to offer a brand-new reading of suspense from the cross-disciplinary perspectives.\n\n==\n\nCheong 2006:\n\nTitle: A Computational Model of Narrative Generation for Suspense\n\nAbstract: The generation of stories by computers, with applications ranging from computer games to education and training, has been the focus of research by computational linguists and AI researchers since the early 1970s. Although several approaches have shown promise in their ability to generate narrative, there has been little research on the generation of stories that evoke specific cognitive and affective responses in their readers. The goal of this research is to develop a system that produces a narrative designed specifically to evoke a targeted degree of suspense, a significant contributor to the level of engagement experienced by users of interactive narrative systems. The system that I present takes as input a plan data structure representing the goals of a storyworld's characters and the actions they perform in pursuit of them. Adapting theories developed by cognitive psychologists, my system uses a plan-based model of narrative comprehension to determine the final content of the story in order to manipulate a reader's level of suspense in specific ways. In this thesis, I outline the various components of the system and describe an empirical evaluation that I used to determine the efficacy of my techniques. The evaluation provides strong support for the claim that the system is effective in generating suspenseful stories.\n\n==\n\nDoust 2015:\n\nTitle: A domain-independent model of suspense in narrative\n\nAbstract: Many computational models of narrative have focussed on the structure of the narrative world. Such models have been implemented in a wide variety of systems, often linked to characters\u2019 goals and plans, where the goal of creating suspenseful stories is baked into the structure of each system. There is no portable, independently motivated idea of what makes a suspenseful story. \n \nOur approach is instead to take the phenomenon of suspense as the starting point. We extend an existing psychological model of narrative by Brewer and Lichtenstein (1982) which postulates suspense, curiosity and surprise as the fundamental elements of entertaining stories. We build a formal model of these phenomena using structures we call narrative threads. \n \nNarrative threads are a formal description of a reader\u2019s expectations about what might happen next in a given story. Our model uses a measure for the imminence of the predicted conflict between narrative threads to create a suspense profile for a given story. We also identify two types of suspense: conflict-based and revelatory suspense. \n \nWe tested the validity of our model by asking participants to give step- by-step self-reported suspense levels on reading online story variants. The results show that the normalised average scores of participants (N = 46) are in agreement with the values predicted by our model to a high level of statistical significance. \n \nOur model\u2019s interface with storyworld knowledge is compatible with recent developments in automatic harvesting of world knowledge in the form of event chains such as Chambers and Jurafsky (2008). This means that it is in principle scalable. By disentangling suspense from specific narrative content and planning strategies, we arrive at a domain-independent model that can be reused within different narrative generation systems. We see our work as a signpost to encourage the further development of narrative models based on what we see as its fundamental ingredients.\n\n==\n\nWulff 2013:\n\nTitle: Conceptual Blending in Narrative Suspense: Making Pain of Anxiety Sweet\n\nAbstract: Suspense, \"a curious mixture of pain and pleasure\" (Barnet et al. 1971: 107), is viewed as a two-fold phenomenon having extratextual (experiential) and intratextual (narrative) dimensions (Vorderer et al. 1996: vii). Traditionally, narrative suspense has been regarded as resulting from such unfolding of the plot when the progress of events acquires a conflicting and alternative pattern. The latter triggers the readers\u2019 effort to navigate a challenging problem space for an important target outcome or to restore what is behind instances of narrative ellipsis. The emergent pattern makes the possibility of various outcomes salient while leaving it delayed (Gerrig 1993: 77-90). Accounting for the effect suspense entails has remained so far within the range of psychological and narratological quests. Probing into intricacies of literary narrative and verbal texture alone does not provide a sufficient ground for revealing the underlying mechanisms of suspense as a textual challenge. Given the basic assumptions of cognitive poetics (see Freeman 2000) and possible worlds theory in its narrative version (Pavel 1986; Ryan 1991; Dolezel 1998; Werth 1999), literary suspense can be viewed as an emotively charged emergent conceptual structure that results from (i) the conflict of mental spaces (cf. Fauconnier 1994) differing in their modality and/or (ii) the clash of alternative narrative subworlds involving the protagonist as a focalizer. The reconstructed conceptual integration network does not only account for the textual concept inferred but gives access to the mechanism of empathetic distress (Zillman 1996: 217), \"a sweet pain of anxiety\" (Brewer 1996: 108) experienced by the reader.\n\n==\n\nIndrusiak 2018:\n\nTitle: Narrative Suspense in Edgar Allan Poe and Alfred Hitchcock\n\nAbstract: This article explores the construction of narrative suspense in Edgar Allan Poe\u2019s and Alfred Hitchcock\u2019s works. Central to their creations is a dual narrative structure that builds tension by articulating two stories in one. Narratological analyses of Poe\u2019s tales and of Hitchcock\u2019s thrillers indicate that their suspense does not stem from gothic topoi, but from plots skilfully wrought to manipulate the readers/viewers\u2019 attention. The first part of the article introduces the concept of narrative suspense. Next, structural principles defended by Poe and Hitchcock are presented. The last two parts discuss narrative techniques in the works by both authors. Given the authors\u2019 influence within their respective media, the analysis has implications for studies of the short story and of the suspense thriller as a narrative genre.\n\n==\n\nKhrypko 2011:\n\nTitle: Towards the problem of maintaining suspense in interactive narrative\n\nAbstract: The paper introduces the problem of creating and maintaining suspense in interactive narrative. We argue that by manipulating choice options offered to the audience in suspenseful scenes in the interactive story both when it is read the first time and after it is reread later, is possible to create stronger suspense.\n\n==\n\nUidhir 2011:\n\nTitle: An Eliminativist Theory of Suspense\n\nAbstract: Suspense realism claims suspense is itself a genuine, distinct emotion. While current theories of suspense are suspense realist theories, I argue for a suspense eliminativist theory according to which suspense is not itself a genuine, distinct emotion but a subspecies of an emotion, or the class comprising all and only those subspecies, chiefly demarcated by the necessity of uncertainty and primarily the province of (primarily fictional) narrative encounters. There are several prima facie compelling reasons to think suspense eliminativism is capable of grounding a theory equally if not better able to resolve the principal concerns suspense theory must address.",
        "final_summary": "Li-che 2013 explores narrative suspense in sports games coverage from cross-disciplinary perspectives. Cheong 2006 develops a computational model aimed at generating narratives designed to evoke a targeted degree of suspense. Doust 2015 proposes a domain-independent model of suspense in narrative, focusing on conflict-based and revelatory suspense. Wulff 2013 examines the role of conceptual blending in creating suspense in literary narratives. Indrusiak 2018 analyzes the construction of narrative suspense in the works of Edgar Allan Poe and Alfred Hitchcock. Khrypko 2011 discusses maintaining suspense in interactive narratives through manipulation of choice options. Lastly, Uidhir 2011 presents an eliminativist theory of suspense, challenging the notion of suspense as a distinct emotion."
    },
    {
        "query": "halal executive",
        "paper_list_string": "Rosli 2022:\n\nTitle: Conceptual Core Competency Model for Halal Executives in Food Manufacturing Companies\n\nAbstract: A competent halal executive is essential and an integral part of the Halal Management System in Halal-certified companies. Core competencies in general are linked with job productivity, job performance, and organisational performance. In Malaysia, initiatives are currently underway to produce competent and professional Halal executives. As for now, the competencies of halal executives are yet to be defined. Defining and identifying the core competencies is pivotal as the Malaysian Halal industry needs to appoint more competent halal executives to serve the halal-certified companies. Thus, this article aims to identify the set of competencies for halal executives via a literature review. This would allow for the further development of the Halal executive core competencies conceptual model. In addition, the background of Halal executives in the Malaysian Halal industry is also discussed to provide context for this review article. The conceptual model proposed is useful in discussing and developing the idea of core competencies, especially in the halal manufacturing industry context. A comprehensive core competencies model is useful for preparing future halal executives, evaluating halal executives' practises in the workplace, and for their lifelong professional development. Other countries could use the same core competencies framework to inform the development of the human resource related to Halal.\n\n==\n\nAhmad 2019:\n\nTitle: Disequilibrium and Divinity Salience as Invariant Structures in The Halal Executives\u2019 Experience of Eudaimonia\n\nAbstract: The global Halal industry is large and continues to grow as the global Muslim population increases in size and dispersion. There are 1.84 billion Muslims today spread over 200 countries and is expected to increase to 2.2 billion by 2030. The industry will be worth USD6.4 trillion by the end of 2018 with more non-traditional players and emergent markets. The stakes are high with pressures to generate novel and sustainable practices. This goes beyond systems and hard skills as it needs to cut into the self \u2013 the person of virtues in virtuous acts, not because they \u201chave to\u201d but because it is the purpose of humankind or his telos - to be \u201cliving well\u201d and \u201cacting well\u201d or eudaimonia. This study seek to explore Halal executives\u2019 lived experience of \u201ceudaimonia.\u201d. Using Giorgi\u2019s descriptive psychological phenomenological method for data analysis, the study elicits two distinct invariant structures \u2013 \u2018disequilibrium in status quo\u2019 and \u2018divinity salience\u2019.\n\n==\n\nWilson 2014:\n\nTitle: The Halal phenomenon \u2013 an extension or a new paradigm?\n\nAbstract: \u2018Halal\u2019, an Arabic word from the Muslim holy book of the Qur\u2019an, can be translated simply as meaning permissible, according to the teachings of Islam. However, the call to certify products and practices formally, and subsequently then label them has given rise to a new cultural phenomenon. Demands to carry overt labels and messages; and expectations as to what Halal is and should be are on the increase inside and outside of Muslim geographies across the globe. For economic and technological reasons, this is also transferring the domain of halal more and more to a vanguard of businesses; as opposed to being just a religious imperative driven by the clergy. Furthermore, the current landscape of the halal industry has arrived at a crossroads: where debates consider whether halal offers opportunities for product and brand extensions, rendering it as a niche marketing approach; or if in fact it presents the opportunity to create a new business paradigm. This article offers practitioners and academics an overview and insight into an industry that has been assessed as having a worldwide value of USD $2.3 trillion.\n\n==\n\nWilson 2012:\n\nTitle: Charting the rise of the halal market \u2013 tales from the field and looking forward\n\nAbstract: Purpose \u2013 To present a real-world industry perspective on the Halal market, which sits within several industry sectors, across the globe \u2013 with the aim of capturing expert knowledge and stimulating further research areas. \n \nMethodology/approach \u2013 Iterative face-to-face and email interviews with two international key experts and conference organisers in the halal sector, over a period of eight weeks. These were supported by naturalistic observation methods at the 2012 World Halal Forum, Kuala Lumpur, Malaysia. A phenomenological approach was taken, which encouraged subjects to reflect longitudinally on critical moments and turning points. \n \nFindings \u2013 It is clear that the stakeholders in the Halal constitute a new kind of industry in their own right: a diverse, complex, yet integrated industry - with a global reach that crosses geographic, cultural and even religious boundaries. The Halal industry intersects and shares common values with many other industry sectors and yet it has a unique set of issues and concerns that are specific to the Halal market. In the face of these factors, the field is still relatively in its infancy \u2013 meaning that currently there are gaps in knowledge, differences in opinion and key concerns; which looking to the future need to be addressed through continuous structured debate and dialogue. \n \nImplications \u2013 Derived from the findings, it appears that the Halal sector is in need of further detailed research: across the Halal supply chain; which tackles contentious issues of legislation and Islamic jurisprudence in the modern global market; and faithfully captures the consumer experience. \n \nOriginality/Value \u2013 This paper provides insight into the Halal sector across territories and sectors, which is of value to practitioners and researchers.\n\n==\n\nHanzaee 2011:\n\nTitle: Intention To Halal Products In The World Markets\n\nAbstract: Emphasis on the importance of halal products is now growing. It is fast becoming a new market force and brand identifier and is now moving into the mainstream market, affecting and changing perception on how business should be conducted, including from a marketing point of view. Halal is no longer just purely a religious issue. It is in the realm of business and trade, and it is becoming a global symbol for quality assurance and lifestyle choice. Halal products are those that are Shariah compliant, i.e. do not involve the use of haram (prohibited) ingredients, exploitation of labor or environment, and are not harmful or intended for harmful use. The realm of halal may extend to all consumables such as toiletries, pharmaceuticals, cosmetics and services including finance. Halal is a typical credence process attribute, i.e. a quality characteristic that can hardly be evaluated or ascertained by the individual consumer, even upon or after consuming the food. Nowadays, Muslims are making their presence felt socially and politically and are requesting halal-certified food products. Currently, a certain degree of assurance is provided by halal certification of halal products. There are two types of halal certificates, and their duration depends on the type of food or beverage. Any individual Muslim, Islamic organization or agency can issue a halal certificate, but the acceptability of the certificate depends upon the country of import or the Muslim community served through such certification. With the complexity of manufacturing systems and the utilization of all animal by products, any product consumed by Muslims may be certified, whether the product is consumed internally or applied to the body externally. The halal certification process starts with choosing an organization that meets the needs for the markets to be serviced. The halal logo is an authoritative, independent and reliable testimony to support halal food claims. This conceptually-based paper critically reviews and amalgamates a diverse range of literature concerning Islamic rules. The way that organizations issue halal certificates for halal producers and use of the halal logo on products. In addition, the paper utilizes necessary religion sources and legal documents. The halal concept (especially for foods) is truly from the farm to the table, and requires nutritious items prepared from permissible ingredients in a clean and hygienic manner. Muslim consumers are very similar to any other consumer segments, demanding healthy and quality products, which must also conform to Shariah requirements. Halal certificate can play an important role to assure consumers that the product has got the necessary conditions of halal product.\n\n==\n\nMarzuki 2012:\n\nTitle: Restaurant Manager and Halal Certification in Malaysia\n\nAbstract: Halal certification at restaurants ensures that foods are served according to Islamic dietary laws. Halal means permitted or lawful or fit for consumption. The purpose of this study is to explore the attributes of halal certification among Malaysian restaurant managers. A national mail survey was conducted on 2,080 respondents along with 33 interviews in 4 selected states and in the Federal Territory of Kuala Lumpur. The results indicate that restaurant managers have high expectations toward halal certification as it signifies attributes such as trust, safety, hygiene, and also perceived by participants as an important aspect in the food service industry.\n\n==\n\nBohari 2013:\n\nTitle: An analysis on the competitiveness of halal food industry inMalaysia: an approach of SWOT and ICT strategy\n\nAbstract: The halal food industry is vital important to the world wide Muslims to ensure hygiene, cleanliness and not detrimental to their health and well-being in whatever they consume, use and purchase. Theoretically, halal is possible to be the benchmark for quality with specifically refers whatever is permitted by Syariah Islamiah and applies to every activity carried out by mankind. Practically, halal when used in relation to the business sector, it refers to business operation and conducted in a manner deemed permissible in Islam. Meanwhile, current development shows that the halal food industry has become one of important sectors that contributes to the societal development as well as it contributes to the national growth. The objective of the present paper is to analyze the competitiveness of business environment of halal food industry in Malaysia. The main method that used are PEST and SWOT analysis where each factors that are related to \nthe halal food industry will be screened and evaluated of it importance. In addition, ICT-based strategy will be recommended on the halal food industry to enhance their current competitive forces as well as preparation for future challenges of the industry.\n\n==\n\nFischer 2016:\n\nTitle: Manufacturing halal in Malaysia\n\nAbstract: In Arabic, halal literally means \u2018permissible\u2019 or \u2018lawful\u2019. Halal is no longer an expression of an esoteric form of production, trade and consumption, but part of a huge and expanding globalized market in which certification, standards and technoscience play important roles. Over the past three decades, Malaysia has become a world leader in the global expansion of halal markets. This has come about in large part because the state and government of Malaysia have taken on the role of halal-certifying authority within the country. In effect, it has certified, standardized and bureaucratized halal production, trade and consumption. Based on fieldwork in Malaysia, this article explores how manufacturing companies understand and practise halal certification, standards and technoscience. I argue that while existing studies of halal overwhelmingly explore micro-social aspects such as the everyday consumption among Muslim groups, \u2018the bigger institutional picture\u2019 that frames such consumption production and regulation are not well understood. By studying halal production, I provide an ethnography of the overlapping technologies and techniques of production and regulation that together warrant a product as \u2018halal\u2019 and thereby help to format the market.",
        "final_summary": "The papers collectively delve into the multifaceted role of a \"halal executive.\" Rosli (2022) underscores the need to identify core competencies for halal executives in food manufacturing companies. Ahmad (2019) provides insights into the lived experience of \"eudaimonia\" among halal executives, emphasizing the significance of disequilibrium and divinity salience. Wilson (2014) discusses the halal phenomenon as a new cultural and business paradigm. Hanzaee (2011) highlights the growing importance of halal products in global markets and the crucial role of halal certification. Marzuki (2012) explores the perceptions and expectations of halal certification among Malaysian restaurant managers. Bohari (2013) presents an analysis of the competitiveness of the halal food industry in Malaysia, suggesting the use of SWOT and ICT strategies. Lastly, Fischer (2016) provides an in-depth look into the manufacturing of halal in Malaysia, emphasizing the importance of certification, standards, and technoscience."
    },
    {
        "query": "What can existing businesses learn from the business approaches of the dot-com organizations?",
        "paper_list_string": "Serrano 2010:\n\nTitle: Implementing an e-business model for a dot-com SME: Lessons Learned\n\nAbstract: One of the key steps to develop an e-business solution is the definition of a Business Model (BM), which requires the expertise from different areas such as finance, technology, marketing, and project management. It is known that Small and Medium Enterprises (SMEs) count with limited resources to undertake this type of e-business initiatives, and thus prompt to failure. This paper tells the story of the process of selection of a BM (and related documents) needed for the design and implementation of an undergoing dot-com initiative leaded by an SME in Mexico. The results of such study are summarized in a series of recommendations that SME may found useful when embarking in similar projects. Some key findings from these recommendations are: partnership with academic/research institutions, the key role of project management, communication, and identification of links between strategy and technology.\n\n==\n\nMichael 2001:\n\nTitle: Strategy and the Internet.\n\nAbstract: Many of the pioneers of Internet business, both dot-coms and established companies, have competed in ways that violate nearly every precept of good strategy. Rather than focus on profits, they have chased customers indiscriminately through discounting, channel incentives, and advertising. Rather than concentrate on delivering value that earns an attractive price from customers, they have pursued indirect revenues such as advertising and click-through fees. Rather than make trade-offs, they have rushed to offer every conceivable product or service. It did not have to be this way--and it does not have to be in the future. When it comes to reinforcing a distinctive strategy, Michael Porter argues, the Internet provides a better technological platform than previous generations of IT. Gaining competitive advantage does not require a radically new approach to business; it requires building on the proven principles of effective strategy. Porter argues that, contrary to recent thought, the Internet is not disruptive to most existing industries and established companies. It rarely nullifies important sources of competitive advantage in an industry; it often makes them even more valuable. And as all companies embrace Internet technology, the Internet itself will be neutralized as a source of advantage. Robust competitive advantages will arise instead from traditional strengths such as unique products, proprietary content, and distinctive physical activities. Internet technology may be able to fortify those advantages, but it is unlikely to supplant them. Porter debunks such Internet myths as first-mover advantage, the power of virtual companies, and the multiplying rewards of network effects. He disentangles the distorted signals from the marketplace, explains why the Internet complements rather than cannibalizes existing ways of doing business, and outlines strategic imperatives for dot-coms and traditional companies.\n\n==\n\nCinca 2003:\n\nTitle: An approach to the measurement of intangible assets in dot com\n\nAbstract: A sample of 40 firms that operate on the Internet is studied to explore ways of\nidentifying and measuring intangible assets in this area of business. The firms meet three\nconditions: operate on the Internet, have available accounting information, and are quoted on\nthe stock exchange. Data was obtained for four web metrics indicators, 30 ratios that combine\naccounting and web traffic information, and a measure of efficiency based on Data Envelopment\nAnalysis. Modelling relied on multivariate statistical approaches. Two intangible assets were\nidentified: one was related to internal structure and was associated with managerial efficiency\nin achieving an impact in the Internet; and another one was associated with external image\nand customer loyalty.\n\n==\n\nTse 2003:\n\nTitle: Business strategies for small firms in the new economy\n\nAbstract: The development of the new economy through the advent of the Internet and the World Wide Web has created many threats and opportunities for firms in general and small businesses in particular. There appears to be an inextricable linkage among the new economy, new enterprise, and the new technology, which may have a potential effect on the way small businesses formulate their business strategies. This paper provides a theoretical approach that looks at the advantages of the digitisation of the economy and strategy formulation of small businesses taking three specific themes into account \u2013 virtualisation, molecularisation, and disintermediation. The strengths and weaknesses of various e\u2010commerce strategies in the context of these dimensions are discussed. Some strategies that are suitable for small companies are recommended and four principles are stated in order to assist these firms to formulate strategy in the new economy.\n\n==\n\nYip 2004:\n\nTitle: Using Strategy to Change Your Business Model\n\nAbstract: Business models rose to prominence in the dot-com boom. Their usefulness may not have ended with the dot-com bust. George S. Yip believes that business models may shed important light on how we understand and practise strategy. Business academics and consultants have been writing about strategy for over 40 years. Yet there is still great confusion as to what strategy is. The Internet boom and bust introduced a new term, \"business model\", that may go a long way to clearing up the confusion.\n\n==\n\nWirtz 2010:\n\nTitle: Strategic development of business models: Implications of the web 2.0 for creating value on the internet\n\nAbstract: There is virtually a consensus that, to remain competitive, firms must continuously develop and adapt their business models. However, relatively little is known about how managers can go about achieving this transformation, and how, and to what extent, different types of business models should be adapted. To illustrate the differential effect of environmental changes on different business model types, this article draws from the \u20184C\u2019 Internet business model typology to elaborate on how a recent wave of changes on the Internet - the emergent Web 2.0 phenomenon - is affecting each of its four business model types. We argue that Web 2.0 trends and characteristics are changing the rules of the \u2018create and capture value\u2019 game, and thus significantly disrupt the effectiveness of established Internet business models. Since systematic empirical knowledge about Web 2.0 factors is very limited, a comprehensive Web 2.0 framework is developed, which is illustrated with two cases and verified through in-depth interviews with Internet business managers. Strategic recommendations on how to what extent different Web 2.0 aspects affect each business model type are developed. Executives can use the ideas and frameworks presented in the article to benchmark their firm\u2019s efforts towards embracing the changes associated with the Web 2.0 into their business model. Full text PDF available online.\n\n==\n\nKor 2007:\n\nTitle: Teaching Information System Students to be Entrepreneurs: A Dot.Com Case Study\n\nAbstract: This paper describes a unique entrepreneurial venture initiation class conducted in the spring 2007 semester at The Wharton School, University of Pennsylvania. The students of this class started a business together and shared in the value created. The business was a social networking Web site, www.wishfood.com, forming a community of individuals with an interest in food and wishing to share recipes. The course was designed to expose students to the process of starting a business, to teach the technical skills required, and to imbue an entrepreneurial spirit. This paper covers the structure of the course, the contributions of the students, the business timeline, and the outcomes of the course. This paper will allow a professor to replicate and learn from the successes and failures of the course.\n\n==\n\nTeece 2017:\n\nTitle: Business models, value capture, and the digital enterprise\n\nAbstract: Firms across all industries are embracing internet-based digitization strategies to expand or improve their business. In many cases, though, internet-based businesses pursue customer growth ahead of profits. The path to profitability, which is a core element of a business model, should not be an afterthought. A well-designed business model balances the provision of value to customers with the capture of value by the provider. The elements of a business model and the dynamic capabilities that help design, implement, and refine a model for an organization and its business ecosystem are reviewed. The article then translates these concepts with respect to key organizational design decisions such as that of licensing versus practicing an innovation, insourcing versus outsourcing, and building a business ecosystem.",
        "final_summary": "The papers suggest that existing businesses can learn valuable lessons from the business approaches of dot-com organizations. Serrano 2010 emphasizes the importance of defining a solid business model, highlighting the role of project management, communication, and the alignment of strategy and technology. Michael 2001 argues that dot-coms often deviated from sound strategy principles, focusing on customer acquisition rather than profitability. However, Yip 2004 introduces the concept of business models as a means to clarify strategic thinking. Cinca 2003 explores the measurement of intangible assets in dot-coms, identifying internal structure and external image as key assets. Tse 2003 discusses the impact of the new economy on small businesses, recommending strategies such as virtualization, molecularization, and disintermediation. Wirtz 2010 examines the influence of Web 2.0 on different business model types, emphasizing the need for adaptation. Kor 2007 presents a case study on teaching entrepreneurship through a dot-com venture, providing insights into the process and outcomes. Teece 2017 underscores the importance of designing business models that balance value provision to customers with value capture by the provider. Overall, these papers highlight the significance of strategic thinking, adaptation, and the alignment of business models with changing technological landscapes."
    },
    {
        "query": "military law in medieval Europe",
        "paper_list_string": "Niewinski 2020:\n\nTitle: Inter arma enim silent leges (In times of war, the laws fall silent). Violence in Medieval warfare\n\nAbstract: The present paper addresses the subject of military violence in the Middle Ages. The medieval chroniclers and writers provide numerous instances of wartime atrocities and massacres of non-combatants and prisoners of war. The attempts to place restraints on the conduct of war were undertaken by the Church as early as in the 11th century. It has been examined how these efforts together with the code of chivalry, royal ordinances and the writings of ius in bello theorists influenced soldiers\u2019 behaviour and whether they were instrumental in reducing civilians suffering. The conclusion has been reached that despite all the endeavours to limit arms violence it basically remained unmitigated.\n\n==\n\nDowning 1992:\n\nTitle: The Military Revolution and Political Change: Origins of Democracy and Autocracy in Early Modern Europe\n\nAbstract: To examine the long-run origins of democracy and dictatorship, Brian Downing focuses on the importance of medieval political configurations and of military modernization in the early modern period. He maintains that in late medieval times an array of constitutional arrangements distinguished Western Europe from other parts of the world and predisposed it toward liberal democracy. He then looks at how medieval constitutionalism was affected by the \"military revolution\" of the early modern era--the shift from small, decentralized feudal levies to large standing armies. Downing won the American Political Science Association's Gabriel Almond Award for the dissertation on which this book was based.\n\n==\n\nAyton 1995:\n\nTitle: The Medieval military revolution : state, society and military change in medieval and early modern Europe\n\nAbstract: Introduction: the military revolution from a medieval perspective, Andrew Ayton, J.L. Price. The conqueror's footprints in Domesday Book, J.J.N. Palmer towns, mottes and ring-works of the conquest, Barbara English alms for the Holy land - the English Templars and their patrons, John Walker knights, esquires and military service - the evidence of the Armorial cases before the Court of Chivalry, Andrew Ayton town defences in medieval England and Wales, D.M. Palliser war and peace in the Works of Erasmus - a medieval perspective, Peter Heath Josse Clichtove and the Just War, Howell A. Lloyd \"wise and experimented\" - Sir William Pelham, Elizabethan soldier and landlord, c.1560-87, R.W. Ambler a state dedicated to war? the Dutch Republic in the 17th century, J.L. Price.\n\n==\n\nPowers 1970:\n\nTitle: The Origins and Development of Municipal Military Service in the Leonese and Castilian Reconquest, 800\u20131250\n\nAbstract: In modern society, enmeshed with confrontations involving the individual, military service and the state, historians are often inclined to make comparisons with the distant past which offer relief from the pressures of contemporary history. Regarding military service, the Middle Ages are occasionally suggested as an age when combat was sporadic, when only the small feudal aristocracy encountered a martial obligation, and when the remainder of society could concentrate on the other burdens of life, free of the paraphernalia of war, hot or cold. As with many romantic generalizations concerning the period, the comparative bliss of the medieval non-combatant is open to question. Many would note, however, that the feudal classes did possess a monopoly on warfare for several centuries in parts of Continental Europe, and would tend to place all discussion of military institutions within a feudal context.\n\n==\n\nNicholson 2004:\n\nTitle: Medieval Warfare: Theory and Practice of War in Europe, 300-1500\n\nAbstract: Warfare in medieval times was never static or predictable - although there were ideals and conventions to follow, in the field commanders had to use their initiative and adapt to the needs of the moment. In this concise, wide-ranging study, Helen Nicholson provides the essential introductory guide to a fascinating subject. \n \nMedieval Warfare \n- surveys and summarises current debates and modern research into warfare throughout the whole of the medieval period across Europe \n- sets medieval warfare theory and practice firmly into context as a continuation and adaptation of practice under the Roman Empire, tracing its change and development across more than a millennium \n- considers military personnel, buildings and equipment, as well as the practice of warfare by land and sea\n\n==\n\nContamine 1984:\n\nTitle: War in the Middle Ages\n\nAbstract: Acknowledgements. List of Maps and Figures. Translatora s Note. Preface to the English Edition. List of Abbreviations. Part I: The State of Knowledge: General Characteristics of Medieval Military History:. 1. The Barbarians from the Fifth to the Ninth Century. 2. The Feudal Age from the Beginning of the Tenth to the Middle of the Ninth Century. 3. Medieval Society in its Prime from the Mid--Twelfth to the Early Fourteenth Century. 4. Free Companions, Gunpowder and Permanent Armies from the Beginning of the Fourteenth to the end of the Fifteenth Century. Part II: Themes and Perspectives:. 5. Arms and Armour. 6. Artillery. 7. The Art of War. 8. War, Government and Society. 9. Towards a History of Courage. 10. Juridical, Ethical and Religious Aspects of War. Conclusion. Part III: Bibliography: . 11. General. 12. The Art of War: Conflicts, Campaigns, Sieges and Battles. 13. Institutions, Societies and Military Attitudes. 14. Arms, Armour and War Engines. 15. Cannons and Gunpowder Artillery. 16. Castles and Fortifications. 17. War Economy and Taxation. 18. War and Peace: Laws, Ethics and Christianity. Bibliography. Index.\n\n==\n\nVerbruggen 1978:\n\nTitle: The art of warfare in Western Europe during the Middle Ages: From the eighth century to 1340\n\nAbstract: Part 1 Historiographical problems: weaknesses of modern military historians in discussing medieval warfare the strength of medieval armies the combat limitations of the clerical sources secular and other reliable sources the importance of the vernacular changes in tactics revealed by lesser-known sources generalizations from detailed accounts accounts of battles by eye-witnesses. Part 2 The knights: the knight and his equipment the \"scara\" the Marchfield and the Mayfield the knight's training collective training - private wars collective training - tournaments the psychology of knights on the battlefield mass flight and panic fear in knights facing foot-soldiers self-interest and profits of war the able commander sense of honour and duty rough manners and knightly customs faith and religious conviction the arming of knights and its influence solidarity in the knightly families and clans, and a lord's retinue tactical units in knightly warfare discipline in camp and on the march battle discipline orders by trumpets and banners commands and evolutions - a comparison with Byzantium the tactical significance of the standard the re-grouping of units and the manoeuvre of feigned flight the importance of order within the formation tactical aims protection of the flanks the place of the commanders in battle the knights fighting on foot the course of a battle. Part 3 The foot-soldiers the foot-soldiers and their remarkable development in the 14th century historical survey the numerical strength of the foot-soldiers the equipment of the foot-soldiers solidarity or cohesion of the foot-soldiers the battle psychology of the foot-soldiers the tactics of the foot-soldiers the Battle of Courtai, llth July 1302 the Battle of Arques, 4th April 1303 the Battle of Mons-en-Pevele, 18th August 1304. Part 4 General tactics: choice and exploitation of terrain the march into battle battle order co-operation of cavalry and foot-soldiers in battle the position of the supreme commander and the reserve corps retreat and flight from the field of battle the Battle of the Lake of Antioch, 9th February 1098 the sortie and the Battle of Antioch, 28th June 1098 the Battle of Thielt, 21st June 1128 the Battle of Arsuf, 7th September 1191 the Battle of Bouvines, 27th July 1214 the Battle of Worringen, 5th June 1288. Part 5 Strategy: the knowledge of strategy plans for wars offensive strategy defensive strategy the influence of the church on methods of warfare.\n\n==\n\nMeron 1995:\n\nTitle: Henry's Wars and Shakespeare's Laws: Perspectives on the Law of War in the Later Middle Ages\n\nAbstract: The legal environment the war of rights - just war, jus ad bellum and jus in bello declarations of war and truce agreements responsibility of princes the siege of Harfleur and treatment of occupied territory - the limits of protection - at the walls of Harfleur, mercy, hostages, Harfleur's surrender, deporting the French and settling the English, sack and massacre, privileged categories of inhabitants and Henry's ordinances, lost of privileged status - women and clergy, treatment of cities conquered after siege, treatment of commanders who surrendered to adversaries, denials of quarter, rape of women, pillage and Church property, compelling obedience by destruction, Henry's proscriptions and the Medieval army Henry's challenge to the Dauphin - the duel that never was and games of chivalry Medieval and Renaissance ordinances of war - codifying discipline and humanity, Agincourt - prisoners of war, reprisals and necessity heralds, ambassadors and the Treaty of Troyes rebels.",
        "final_summary": "Military law in medieval Europe was multifaceted and complex. Niewinski (2020) underscores the prevalence of wartime atrocities and the Church's limited success in restraining such violence. Downing (1992) posits that medieval political configurations and military modernization played a significant role in the emergence of democracy and autocracy. Ayton (1995) provides a medieval perspective on the military revolution, touching on aspects such as town defenses and the societal impact of war. Powers (1970) disputes the idea that only the feudal aristocracy had martial obligations, emphasizing the wider societal involvement in military service. Nicholson (2004) surveys current debates and research into warfare throughout the medieval period, considering military personnel, buildings, and equipment. Verbruggen (1978) offers an in-depth examination of knights and their tactics. Lastly, Meron (1995) delves into the legal aspects of war, including just war theory and the treatment of occupied territories."
    },
    {
        "query": "electrochemical apatsensor for Hg(II) detection",
        "paper_list_string": "Gao 2022:\n\nTitle: Dual signal-based electrochemical aptasensor for simultaneous detection of Lead(II) and Mercury(II) in environmental water samples.\n\nAbstract: The Pb2+-binding aptamer (PBA) and Hg2+-binding aptamer (HBA) have been widely adopted as biosensing elements for lead(II) (Pb2+) and mercury (II) (Hg2+), due to the formation of stable and specific Pb2+-G-quadruplex and T-Hg2+-T structures. However, the currently developed electrochemical sensors are only applicable for single-element analysis of Pb2+ or Hg2+. Herein, a dual signal interface was realized by in-situ grafting the signal tags (Melamine-Cu2+ complex and Nile blue) bearing different redox potentials on the terminal of PBA and HBA that were co-immobilized on gold electrode. The results show that the interface has two pairs of independent redox peaks, ascribing to the melamine-Cu2+ complex and Nile blue, respectively. The sensor is not only applicable for single-element detection of Pb2+ or Hg2+, but also for simultaneous detection of them. The surface density of PBA and HBA on the gold electrode and the grafting efficiency of the electroactive tags on the aptamers have been determined. The detection limits for Pb2+ and Hg2+ were estimated to be 0.98 pM and 19 pM, respectively. The dual signal sensor is successfully applied for Pb2+ and Hg2+ determination in real water samples, showing its potential in the environmental monitoring of Pb2+ and Hg2+.\n\n==\n\nLiao 2015:\n\nTitle: Development of a new electrochemical sensor for determination of Hg(II) based on Bis(indolyl)methane/Mesoporous carbon nanofiber/Nafion/glassy carbon electrode\n\nAbstract: Abstract A new BIM/MCNF/Nafion-modified glassy carbon electrode, combining the specific Hg 2+ -complexing ability of bis(indolyl)methane (BIM) with the unique properties of mesoporous carbon nanofiber (MCNF), was fabricated for anodic stripping analysis of Hg 2+ . BIM was synthesized and served as an ionophore for highly selective adsorption heavy metal Hg 2+ from the sample solution. MCNF displayed extraordinary electrical conductivity and great adsorption ability to enhance the sensitivity of the modified electrode. Nafion, a sulfonated cation-exchange polymer, supplied the good stability of the modifying layer. The as-prepared electrochemical sensor significantly improved selectivity and sensitivity towards Hg 2+ determination. The stripping current increased linearly with Hg 2+ concentration at the range of 5\u2013500\u00a0nM under 15\u00a0min accumulation at open-circuit potential, with a detection limit of 0.3\u00a0nM. Meanwhile, the proposed electrode could avoid some interference of other heavy metal ions coexisting with Hg 2+ , such as Cd 2+ , Pb 2+ and Cu 2+ . This new method has been successfully applied for the detection of Hg 2+ in Yellow River with satisfactory results.\n\n==\n\nLu 2016:\n\nTitle: Novel electrochemical sensing platform for quantitative monitoring of Hg(II) on DNA-assembled graphene oxide with target recycling.\n\nAbstract: This work designs a new electrochemical sensing platform for the quantitative monitoring of mercury ion (Hg(2+)) on poly-T(15) oligonucleotide-functionalized graphene oxide by coupling with DNase I-assisted target recycling amplification. The assay was carried out on the basis of T-Hg(2+)-T coordination chemistry by using target-induced dissociation of indicator-labeled poly-T(15) oligonucleotide from graphene oxide nanosheets. The electronic signal was amplified through DNase I-triggered target recycling. Experimental results indicated that the amperometric response of DNA-based sensing platform deceased with the increasing Hg(2+) concentration in the sample, and has a detection limit of 0.12nM with a dynamic working range of up to 50nM. Our strategy afforded exquisite selectivity for Hg(2+) against other environmentally related metal ions. More significantly, this methodology displayed high reproducibility and acceptable accuracy, thus representing an optional sensing scheme for the screening of Hg(2+) in environmental water samples.\n\n==\n\nKatseli 2020:\n\nTitle: Miniature 3D-printed integrated electrochemical cell for trace voltammetric Hg(II) determination\n\nAbstract: Abstract This work describes an integrated miniature all-3D-printed device for the determination of trace Hg(II) by anodic stripping voltammetry (ASV). The device is fabricated through a single-step printing process using a dual-extruder 3D printer and is composed of a mini vessel (printed from a polylactic acid (PLA) filament) and of 3 thermoplastic electrodes printed from a carbon-loaded PLA conductive filament and integrated at the bottom of the vessel. The working electrode surface is modified with a thin gold film electroplated in situ. The formation of the gold deposit was studied by linear sweep voltammetry and optical microscopy and the electrode materials as well as the chemical and instrumental conditions for Hg(II) determination were optimized. The limit of detection for Hg(II) is 0.52\u202f\u03bcg\u202fL\u22121, the within-sensor repeatability (expressed as the coefficient of variation of 8 measurements of 20\u202f\u03bcg\u202fL\u22121 Hg(II) at the same device) is 3.9 % and the between-cell reproducibility (expressed as the coefficient of variation 5 measurements of 20\u202f\u03bcg\u202fL\u22121 Hg(II) conducted at different cells) is 8.9 %. The 3D-printed device was successfully applied to the determination of Hg(II) in bottled water and fish oil samples.\n\n==\n\nWang 2021:\n\nTitle: Electrochemical aptasensor based on gold modified thiol graphene as sensing platform and gold-palladium modified zirconium metal-organic frameworks nanozyme as signal enhancer for ultrasensitive detection of mercury ions.\n\nAbstract: Gold modified thiol graphene (Au@HS-rGO) was prepared and applied as sensing platform for constructing the electrochemical aptasensor. While gold-palladium modified zirconium metal-organic frameworks (AuPd@UiO-67) nanozyme was employed as signal enhancer for detecting mercury ions (Hg2+) sensitively. Herein, gold nanoparticles (Au NPs) were modified on HS-rGO to form the thin Au@HS-rGO layer. Then the substrate strand (Apt1) was modified on the platform through Au-S bond. The signal strand (Apt2) was further decorated on the platform in the presence of Hg2+. Herein, the Apt2 was labeled with AuPd@UiO-67 nanozyme, which exhibited catalase-like properties to catalyze H2O2, thereby generating the electrical signal. With the concentration of Hg2+ increased, the amount of modified Apt2-AuPd@UiO-67 increased, leading to the rise of current response. Since the current responses were linear with concentration of Hg2+, the detection of Hg2+ can be achieved. Under the optimum conditions, the prepared electrochemical aptasensor exhibited wide linear range from 1.0\u00a0nmol/L to 1.0\u00a0mmol/L, along with a low detection limit of 0.16\u00a0nmol/L. Moreover, the electrochemical aptasensor showed excellent selectivity, reproducibility and stability, together with superior performance in actual water sample analysis. Therefore, this proposed electrochemical aptasensor may have promising applications and provide references for environmental monitoring and management.\n\n==\n\nYu 2004:\n\nTitle: Surface plasmon resonance sensor for Hg(II) detection by binding interactions with polypyrrole and 2-mercaptobenzothiazole\n\nAbstract: Abstract A SPReeta sensor was developed for Hg(II) detection by monitoring the binding interactions with polypyrrole (PPy) and 2-mercaptobenzothiazole (2-MBT). Chemical binding of Hg2+ ions in water with a PPy thin film immobilized on the gold surface could be monitored by surface plasmon resonance (SPR). The SPR angle was found to be a useful analytical signal for monitoring the binding interaction. An increase in SPR angle of 780\u00b110\u00a0RU (1 RU=1\u00d710\u22124\u00b0) was obtained for 10\u00a0ppm Hg2+ biding with PPy film, and a linear dynamic range was observed between 0.1 and 10\u00a0ppm. After Hg2+ absorption into the polymer, enhancement of the SPR detection sensitivity was achieved by adding 2-MBT to the injected solution. This produced further binding interaction with the Hg2+ bound on PPy. The detection limit was improved down to 0.01\u00a0ppm with a SPR angle increase of 20\u00b110\u00a0RU. It should be recognized that both 2-MBT and PPy worked together in the present SPReeta method to enhance the specificity of Hg2+ determination.\n\n==\n\nZeng 2017:\n\nTitle: Practical and regenerable electrochemical aptasensor based on nanoporous gold and thymine-Hg2+-thymine base pairs for Hg2+ detection.\n\nAbstract: A simple, practical and reusable electrochemical aptasensor, based on thymine-Hg2+-thymine (T-Hg2+-T) coordination chemistry and nanoporous gold (NPG) for signal amplification, was designed for sensitive and selective detection of mercury ions (Hg2+). The thiol modified T-rich hairpin capture probe was self-assembled onto the surface of the NPG modified electrode for hybridizing with ferrocene-labeled T-rich probe in the presence of Hg2+ via T-Hg2+-T coordination chemistry. As a result, the hairpin capture probe was opened, and the ferrocene tags were close to the NPG modified electrode. Taking advantage of the amplification effect of NPG electrode for increasing the reaction sites of thiol modified capture probe, the proposed electrochemical aptasensor could detect Hg2+ quantitatively in the range of 0.01-5000nM, with a detection limit as low as 0.0036nM which is much lower than the maximum contamination level for Hg2+ in drinking water defined by the U.S. Environmental Protection Agency. Moreover, the proposed electrochemical aptasensor can be regenerated by adding cysteine and Mg2+. The aptasensor was also used to detect Hg2+ from real water samples, and the results showed excellent agreement with the values determined by atomic fluorescence spectrometer. This aptasensor showed a promising potential for on-site detecting Hg2+ in drinking water.\n\n==\n\nChu-mong 2016:\n\nTitle: A FRET based aptasensor coupled with non-enzymatic signal amplification for mercury (II) ion detection.\n\nAbstract: In this work, the idea of incorporating a non-enzymatic signal amplification with a regular aptasensor was tested. In this proof of principle, the sensor was designed for the detection of mercury (II) ions (Hg(2+)) based on the F\u00f6rster Resonance Energy Transfer (FRET), and the catalyzed hairpin assembly (CHA) technique that was used as the signal amplification method. This sensor comprised a mercury aptamer-catalyst complex (Apt-C) and two types of hairpin DNA: H1 labeled with fluorescein and H2 labeled with tetramethylrhodamine. In the presence of Hg(2+), two facing thymine bases in the mercury aptamer strand were coordinated with one mercury ion. This caused the release of the catalyst for the catalyzed hairpin assembly (CHA) reaction that turned H1 and H2 hairpins into H1-H2 hybrids. FRET was then used to report the hairpin-duplex transformation. The sensor showed excellent specificity towards Hg(2+) over other possible interfering cations present at even a 100 fold greater concentrations. It had a linear range of 10.0-200.0nM, and a good detection limit of 7.03\u00b10.18nM, which is lower than the regulatory mercury limit for drinking water (10nM or 2ppb). The sensor was used to detect spiked Hg(2+) in nine real surface water samples collected from three different areas. Acceptable recoveries and small standard deviations indicated that the sensor was practically applicable, and the proposed idea to incorporate a CHA amplification in a regular aptasensor was not only feasible but beneficial. The same principles can be applied to develop sensors for various different targets.",
        "final_summary": "The papers collectively provide insights into the development of electrochemical aptasensors for Hg(II) detection. Gao 2022 presents a dual signal-based aptasensor capable of simultaneously detecting Pb(II) and Hg(II) in environmental water samples. Liao 2015 introduces a modified glassy carbon electrode for sensitive and selective determination of Hg(II) using bis(indolyl)methane and mesoporous carbon nanofiber. Lu 2016 proposes a sensing platform based on DNA-assembled graphene oxide for quantitative monitoring of Hg(II) with target recycling amplification. Katseli 2020 describes a miniature 3D-printed integrated electrochemical cell for trace voltammetric Hg(II) determination. Wang 2021 presents an electrochemical aptasensor utilizing gold-modified thiol graphene as a sensing platform and gold-palladium modified zirconium metal-organic frameworks nanozyme as a signal enhancer for ultrasensitive Hg(II) detection. Yu 2004 demonstrates a surface plasmon resonance sensor for Hg(II) detection using polypyrrole and 2-mercaptobenzothiazole. Zeng 2017 introduces a practical and regenerable electrochemical aptasensor based on nanoporous gold and thymine-Hg(II)-thymine base pairs for Hg(II) detection. Chu-mong 2016 presents a FRET-based aptasensor coupled with non-enzymatic signal amplification for Hg(II) ion detection. These papers collectively demonstrate various approaches and techniques for the development of electrochemical aptasensors for sensitive and selective detection of Hg(II) ions in different environmental samples."
    },
    {
        "query": "What is the common pH of glauconite in soils?",
        "paper_list_string": "Kisiel 2018:\n\nTitle: Weathering of glauconite in an alkaline environment \u2014 A case study from Krakow area, Poland\n\nAbstract: Abstract The purpose of the study was to investigate glauconite weathering in regolith profile developed on Santonian marls in Zabierzow (South Poland) most likely during Paleogene. Because of the high carbonates content the studied regolith appears to constitute a unique case where glauconite underwent weathering under alkaline conditions. Unweathered glauconite\u2013bearing marls and regolith profile were sampled. Samples pH was determined for 1:1 rock\u2013water solution. Mineral composition of bulk rock, clay fraction ( Samples were characterized by alkaline reaction (pH between 7.66 and 7.98). XRD analysis of unweathered marls showed presence of calcite, glauconite, quartz, smectite and traces of kaolinite, apatite and pyrite. The weathered material was depleted in glauconite and pyrite while enriched in goethite. Mossbauer spectroscopy showed that oxidation of part of octahedral Fe2+ located within the glauconite took place. FTIR spectroscopy showed that in regolith profile intensities of OH\u2013stretching band assigned to glauconite decreased upward while goethite bands intensities increased, relatively to unweathered marls. In the studied profile glauconite underwent dissolution. Si and Mg were leached out with weathering solution while due to alkaline pH Fe3+ crystallized in situ in form of goethite having likely substitutions of Al. Neoformation of smectite\u2013rich mixed layered mica\u2013smectite minerals from the solution was also likely to take place.\n\n==\n\nMcRae 1975:\n\nTitle: The presence of indigenous glauconite in soils and its effect on soil fertility:I. Soils developed on sandy drift (Barming series)\n\nAbstract: SUMMARY The fertility, especially potassium status, of the Barming series in East Kent was studied by laboratory and pot test procedures to determine the effect of indigenous glauconite. Half of 12 otherwise similar soils contained appreciable glauconite. Glauconite contributed positively to the potassium status of the soils, especially the B horizons (45\u201365 cm), and was associated with higher values of exchangeable potassium, cation exchange capacity, nitric acid soluble potassium and intensity of potassium supply as measured by \u0394\u1e20. More potassium was taken up and more released from initially non-exchangeable sources to ryegrass from the glauconitic soils. The available magnesium content of the soils was also positively correlated with glauconite content but available phosphate was not.\n\n==\n\nJim\u00e9nez-Mill\u00e1n 1998:\n\nTitle: Glauconite and phosphate peloids in mesozoic carbonate sediments (eastern subbetic zone, betic cordilleras, se spain)\n\nAbstract: Abstract Glauconite and Ca phosphate peloids occur in Jurassic and Cretaceous bioclastic carbonate rocks from pelagic swell sequences of the Algayat-Crevillente Unit (Subbetic Zone). The size and morphology of the peloids are controlled by the bioclasts. The glauconite in both stratigraphic positions is K rich (>0.69 atoms p.f.u.) and shows well-defined 10 \u00c5 lattice fringes. Poorly crystalline areas with a composition of Fe-smectite are found within the peloids, indicating the nature of the glauconitic precursor. This precursor would be formed in the shielded microenvironments of the bioclast and later transformed to glauconite by equilibration of peloids with sea water that culminated with the crystallization of a phosphatic phase. The greater presence of smectite areas in the Jurassic peloids and the lower K contents (0.69-0.81) of these glauconites, compared with the Cretaceous glauconites (0.81-0.89) can be explained by the calcitic early diagenetic cementation which stopped the process of glauconitization.\n\n==\n\nShekhar 2017:\n\nTitle: Physico-chemical treatment of glauconitic sandstone to recover potash and magnetite\n\nAbstract: Abstract Potassium is one of the important nutrients for healthy growth and development of plants. This article explores the possibility of extracting potassium from lean grade indigenous glauconitic sandstone, which is locked in the host matrix. Detail physico-chemical and mineralogical characterization of glauconitic sandstone collected from Guneri area of Kuchchh district, Gujarat (India) was carried out by X-ray diffraction (XRD), Scanning electron microscopy (SEM) followed by Energy-dispersive X-ray (EDX), X-ray fluorescence (XRF), Atomic absorption spectrometer (AAS) and conventional analytical tools. The major matrix of glauconite sample was found to contain 3.93% K 2 O, 10.75% Fe 2 O 3 , 4.41% Al 2 O 3 and 70.35% SiO 2 . The K 2 O content could be enriched from 3.93% to 5.52% by simple screening and rejection of\u00a0+425\u00a0\u03bcm particles fraction. Chemical leaching of enriched fraction with hydrochloric acid (HCl) yields very low recovery (\n\n==\n\nN 1996:\n\nTitle: Glauconite and phosphate peloids in Mesozoic carbonate sediments ( Eastern Subbetic Zone , Betic Cordilleras , SE Spain )\n\nAbstract: AB STRACT: Glauconite and Ca phosphate peloids occur in Jurassic and Cretaceous bioclastic carbonate rocks from pelagic swell sequences of the Algayat-Crevillente Unit (Subbetic Zone). The size and morphology of the peloids are controlled by the bioclasts. The glauconite in both stratigraphic positions is K rich (>0.69 atoms p.f.u.) and shows well-defined 10 A\u00ca lattice fringes. Poorly crystalline areas with a composition of Fe-smectite are found within the peloids, indicating the nature of the glauconitic precursor. This precursor would be formed in the shielded microenvironments of the bioclast and later transformed to glauconite by equilibration of peloids with sea water that culminated with the crystallization of a phosphatic phase. The greater presence of smectite areas in the Jurassic peloids and the lower K contents (0.69 \u00ad 0.81) of these glauconites, compared with the Cretaceous glauconites (0.81 \u00ad 0.89) can be explained by the calcitic early diagenetic cementation which stopped the process of glauconitization.\n\n==\n\nDiaz 2003:\n\nTitle: Effect of glauconite on the elastic properties, porosity, and permeability of reservoir rocks\n\nAbstract: Glauconite is an iron rich variety of clay that can be found as individual pellets, composite grains, and intergranular cement. Its density ranges between 2.4 g/cm3 and 2.95 g/cm3, averaging 2.67 g/cm3. It has a Moh's scale hardness of 2. Authigenic glauconite is formed under a limited range of geologic and geochemical conditions; it typically develops on the outer margins of continental shelves, in areas of low sediment input (Odin, 1980), and its presence is valuable as an indicator of transgressive sequences.\n\nIdentifying glauconite in the subsurface is important for depositional environment interpretation, stratigraphic correlation, dating, tracing of unconformities, and geochemical exploration for source and reservoir rocks (Srivastava, 1986). A number of commercial hydrocarbon reservoirs are glauconitic sandstones\u2014for example in Colombia, Ecuador, Peru, Venezuela, Australia, Eastern China, North Sea, United States, Canada, Saudi Arabia, and Ireland.\n\nAlthough glauconite tends to exist as grains and as such is part of the rock framework, under moderate overburden pressure, these grains are easily compacted (Figure 1) and may form a pseudomatrix that occludes the original primary porosity. This behavior is in contrast to that observed in clay minerals. This problem, and the fact that there are no published studies about the elastic properties of glauconite and glauconitic sandstones, motivated this research to understand their rock physics properties. We present analyses of data from five lithologies containing varying amounts of glauconite and identify the best seismic attributes to evaluate its presence and the reservoir quality. Figure 1. \nOptical image of a glauconitic sandstone (made at 20X magnification) showing formation of a pseudomatrix that occludes the original primary porosity. Glauconite=green, Quartz=white. \n\n\n\nThe samples in this study come from Caballos Formation in Putumayo and Upper Magdalena Basins, Colombia (Figure 2), which is described as a marine transgressive blanket sandstone deposited in a shallow \u2026\n\n==\n\nCarson 1993:\n\nTitle: The glauconite\u2212phosphate association in hardgrounds: examples from the Cenomanian of Devon, southwest England\n\nAbstract: Abstract Glauconite and phosphate from the Cenomanian of Devon, southwest England, may be classified into different groups on the basis of petrography. Chemical analysis of these different fabrics reveals a strong association between glauconite and phosphate occurring on a sub-microscopic scale. Despite this intimate association, certain deductions may be made about the mineralogy of the glauconite (notably the K concentration) and its relationship to its morphology. Glauconite and phosphate formation took place in the transitional zone between oxic and anoxic environments (suboxic zone). Iron oxyhydroxides were a potential source of phosphorus and iron, while 'sulphide fluxing' maintained pH levels appropriate for carbonate dissolution and subsequent mineralisation.\n\n==\n\nMcRae 1975:\n\nTitle: The presence of indigenous glauconite in soils and its effect on soil fertility:II. Soils developed on Gault Clay (Denchworth series)\n\nAbstract: SUMMARY Twelve stagnogley soils of the Denchworth series from East Kent were studied to determine the effect of indigenous glauconite on their fertility. The presence or absence of glauconite was not the major factor controlling the potassium or magnesium status which was largely determined by the nature of the < 2 \u03bcm clay. Glauconitic soils had much higher levels of available phosphate than corresponding non-glauconitic soils due to calcium phosphate associated with the glauconite in the parent material. Weathering has partially converted this to aluminium and iron phosphates which were the main forms of phosphate present, especially in the A (0\u201315 cm) horizons.",
        "final_summary": "Kisiel 2018 found that glauconite weathered under alkaline conditions with a pH between 7.66 and 7.98. McRae 1975 observed that glauconite contributed positively to the potassium status of soils, but did not provide specific pH values. Carson 1993 found an association between glauconite and phosphate occurring on a sub-microscopic scale, and mentioned that \"sulphide fluxing\" maintained pH levels appropriate for carbonate dissolution and subsequent mineralisation, although specific pH values were not provided."
    },
    {
        "query": "\"supply chain demand risk\" theory",
        "paper_list_string": "Trkman 2009:\n\nTitle: Supply Chain Risk in Turbulent Environments \u2013 A Conceptual Model for Managing Supply Chain Network Risk\n\nAbstract: Supply risk or the likelihood of supply disruptions is emerging as a key challenge to supply chain management. The ability to identify which supplier has greater potential of a disruption is a critical first step in managing the frequency and impact of these disruptions that often significantly impact on the supply chain. This paper presents preliminary research concepts regarding a new approach to the identification and prediction of supply risk. This approach to the assessment and classification of suppliers is based on supplier's attributes, performances and supply chain characteristics, while it is also modified by factors in the supplier's specific environment. The challenges posed to supply chains due to a turbulent environment (both from within the industry and external influences) are examined. A new method for the assessment and classification of suppliers based on their characteristics, performances and the environment of the industry in which they operate is presented. The findings are explained within the contingency theory.\n\n==\n\nMandal 2012:\n\nTitle: Conceptualizing a Model for Demand -Side Risk in Supply Chain\n\nAbstract: With growing competition and dynamicity in the tastes and preferences of customers, there is a recent growth in the number of supply chain disruptions as well; for e.g. the recent flood in Thailand affecting the global operations of hard-disk manufacturers like Seagate, Western Digital to car makers Honda and Toyota. Thus an increased need is felt by these firms to adopt effective risk mitigation strategies which have further increased the necessity of studying several types of risk inherent in the supply chains and corresponding significant variables affecting each of them. Till date, there is no study conceptualizing a framework for measuring exclusively each of the risk types inherent in the supply chain. The present study attempts to formulate a framework for managing demand side risk from a supply chain perspective for a focal firm through structured literature review and have contributed by adding important predictors like trust towards customer, trust towards suppliers, and relationship quality with suppliers, relationship quality with customers and information sharing with suppliers with the existing ones like supplier dependence, customer dependence, supplier concentration, single sourcing and global sourcing.\n\n==\n\nXiao-wo 2013:\n\nTitle: Transmission Analysis of Demand and Exchange Rate Risks in the Supply Chain\n\nAbstract: The main purpose of this study is to investigate how demand risk and exchange rate risk are transmitted in a supply chain consisting of a supplier,a manufacturer and a retailer.In the supply chain the manufacturer buys materials from the supplier and sells them to the retailer via wholesale price contracts.The demand risk roots in the uncertainty of final market demand while the exchange rate risk stems from the assumption that the manufacturer and the retailer operate in different countries.Under the assumption the manufacturer's revenue per product sold is susceptible to exchange rate fluctuation. To examine risk transmission in the supply chain,weconstruct a two-stage game model to assess the strategic interactions among those three supply chain members.In the model,the manufacturer offers wholesale prices to retailer and supplier in stage 1.The supplier and the retailer simultaneously respond to the manufacturer' offers in stage 2 via an order quality and a supply quantity,respectively.In our proposed model,the demand risk is defined as a process where based the observed final market demand.The retailer strategically orders from the manufacturer and then forces(via the equilibrium of the game model) the manufacturer and the supplier to adjust their profit according to the final market demand.The exchange rate risk refers to when faced with volatile fluctuation of exchange rate the manufacturer strategically offers wholesale prices to the retailer and the supplier respectively and then forces(via the equilibrium of the game model) the retailer and the supplier to vary their profits according to the exchange rate. With the model equilibrium,we measure the risk that the retailer,the manufacturer and the supplier bearwith the variance of their equilibrium profit.We further conduct a statistical analysis of the variance of the final market demand and the exchange rate.The results show that(1) the expected profit of the retailer,the manufacturer and the supplier increases along with the variance of the final demand and exchange rate;(2) the profit variance of the retailer,the manufacturer and the supplier increases along with the variance of the final demand and exchange rate;(3) the relative risks between the retailer and the manufacturer(measured by the ratio of the retailer's profit variance to the manufacturer's profit variance) increases in the slope of the market demand function(a higher slope indicates a lower degree of the flexibility of the retailer's response to price changes in the final market) while the relative risk between the supplier and the manufacturer(measured by the ratio of the supplier's profit variance to the manufacturer's profit variance) increases in the slope of the supplier's margin cost function(a higher slope indicates a lower degree of the flexibility of the supplier's response to the manufacturer's price changes). To summarize,the demand risk and the exchange rate risk can be transmitted in a supply chain viasupply chain partners' strategic interactions.Moreover,the risk transmission in a supply chain depends on both the bullwhip effect and supply chain partners' profit margins.\n\n==\n\nGurtu 2021:\n\nTitle: Supply Chain Risk Management: Literature Review\n\nAbstract: The risks associated with global supply chain management has created a discourse among practitioners and academics. This is evident by the business uncertainties growing in supply chain management, which pose threats to the entire network flow and economy. This paper aims to review the existing literature on risk factors in supply chain management in an uncertain and competitive business environment. Papers that contained the word \u201crisk\u201d in their titles, keywords, or abstracts were selected for conducting the theoretical analyses. Supply chain risk management is an integral function of the supply network. It faces unpredictable challenges due to nations\u2019 economic policies and globalization, which have raised uncertainty and challenges for supply chain organizations. These significantly affect the financial performance of the organizations and the economy of a nation. Debate on supply chain risk management may promote competitiveness in business. Risk mitigation strategies will reduce the impact caused due to natural and human-made disasters.\n\n==\n\nCavinato 2004:\n\nTitle: Supply chain logistics risks: From the back room to the board room\n\nAbstract: Risks and uncertainties are ever more noted and factored into decision making today, and those stemming from supply chains are prominent in the competitiveness and viability of companies and organizations. The idea that every supply chain is made up of five internal chain/network constructs is presented, and these are physical, financial, informational, relational, and innovational. Further, four categorizations of relevant product/supply costs are presented as are four types of supply risks.\n\n==\n\nNagurney 2005:\n\nTitle: Supply chain networks, electronic commerce, and supply side and demand side risk\n\nAbstract: In this paper, we develop a supply chain network model in which both physical and electronic transactions are allowed and in which supply side risk as well as demand side risk are included in the formulation. The model consists of three tiers of decision-makers: the manufacturers, the distributors, and the retailers, with the demands associated with the retail outlets being random. We model the optimizing behavior of the various decision-makers, with the manufacturers and the distributors being multicriteria decision-makers and concerned with both profit maximization and risk minimization. We derive the equilibrium conditions and establish the finite-dimensional variational inequality formulation. We provide qualitative properties of the equilibrium pattern in terms of existence and uniqueness results and also establish conditions under which the proposed computational procedure is guaranteed to converge. We illustrate the supply chain network model through several numerical examples for which the equilibrium prices and product shipments are computed. This is the first multitiered supply chain network equilibrium model with electronic commerce and with supply side and demand side risk for which modeling, qualitative analysis, and computational results have been obtained.\n\n==\n\nDu 2012:\n\nTitle: Supply chain wholesale price contract and risk hedge strategy under risk of demand and exchange rate\n\nAbstract: Jointly consider the demand risk and exchange rate risk of the global supply chain. With a Two-echelon Supply Chain under global environment for modeling background, this paper establishes a dynamic game model about manufacturer and retailer, gains the results of the equilibrium, studies how the uncertainty of the exchange rate and exogenous demand influence the manufacturer's original wholesale price , incentive degree, the retailer's order quantity and exchange rate risk hedge strategy. The results show that the manufacturer's original wholesale price and the incentive degree are unaffected by exchange rate and demand exogenous fluctuation. The retailer's order quantity is unaffected by exchange rate exogenous fluctuation, but decreases with an increase in the demand exogenous fluctuation. The retailer's exchange rate risk exposure proportion decreases with an increase in the exchange rate exogenous fluctuation, and increases with an increase in the demand exogenous fluctuation. The results indicate that the retailer's exchange rate risk hedge strategy can stabilize the supply chain operation strategy.",
        "final_summary": "Trkman (2009) presents a conceptual model for managing supply chain network risk, focusing on supplier attributes, performances, and supply chain characteristics. Mandal (2012) proposes a framework for managing demand-side risk, emphasizing the role of trust, relationship quality, and information sharing. Xiao-wo (2013) investigates how demand risk and exchange rate risk are transmitted in a supply chain, highlighting the strategic interactions among supply chain members. Gurtu (2021) reviews risk factors in supply chain management, underscoring the impact of uncertainties and the importance of risk mitigation strategies. Cavinato (2004) categorizes supply chain risks into physical, financial, informational, relational, and innovational risks. Nagurney (2005) develops a supply chain network model that includes both supply side and demand side risk. Du (2012) explores the effects of demand and exchange rate risks on a global supply chain, focusing on the manufacturer's wholesale price and the retailer's order quantity and exchange rate risk hedge strategy."
    },
    {
        "query": "Conflict and happiness or life satisfaction",
        "paper_list_string": "Chiu 1998:\n\nTitle: Relationships among role conflicts: Role satisfactions and life satisfaction: Evidence from Hong Kong\n\nAbstract: In this study the direct effects among work/family conflicts, job, marital and life satisfactions as reported by a Hong Kong sample were investigated. One thousand seven hundred questionnaires were sent to people in 3 different professions in Hong Kong and 497 successful responses were obtained. The findings indicated that work and family conflicts as well as interrole conflict affected job satisfaction and marital satisfaction. Likewise, life satisfaction reported by the respondents was affected by both their level of job satisfaction and marital satisfaction.\n\n==\n\nSulimah 2018:\n\nTitle: When Conflict be a Trigger of Depression: between Job and Life Satisfaction\n\nAbstract: The objective of this study is to investigate the effect of work-family conflict and family-work conflict on depression behavior mediated by job satisfaction and life satisfaction. The respondents in this research were 133 female nurses in two public hospitals in Indonesia. Methods of data collection were by using observation, interviews, and questionnaires. The sampling technique used the technique of proportional random sampling. Methods of data analysis were by using factor analysis, regression analysis and path analysis. Data analysis were by using SPSS software version 21. The results show that all hypothesis in the research were supported. Work-family conflict has an effect on depression behavior either directly or mediated by job satisfaction and life satisfaction. Similarly, the work-family conflict has an impact on the behavior of depression either direct or mediated by job satisfaction and life satisfaction. Our findings bear out the notion that job satisfaction and life satisfaction mediates the influence of dual role conflict on depression, highlighting the effect of individual satisfaction within this area. The suggestions for nurses, they should improve job satisfaction and life satisfaction so that it can overcome the dual role conflict they suffered.\n\n==\n\nArslan 2010:\n\nTitle: The relationship between conflict communication, self- esteem and life satisfaction in university students\n\nAbstract: The current study used a survey model to analyze 306 university students to investigate relationship between life satisfaction, self-esteem and conflict communication. Data were collected from the Rosenberg Self-esteem Scale,\u00a0the Satisfaction with Life Scale\u00a0and Conflict Communication Scale.\u00a0Pearson product-moment correlation coefficients were determined.\u00a0The results of the study show that self-esteem is positively correlated with confrontation, emotional expression, self-disclosure and life satisfaction. The results also show that life satisfaction is positively correlated with confrontation, emotional expression and self-disclosure. \n \n \u00a0 \n \n Key words:\u00a0Conflict communication, self-esteem, and life satisfaction\n\n==\n\nMcMillan 2012:\n\nTitle: Examining the Relationship Between Work/Life Conflict and Life Satisfaction in Executives\n\nAbstract: The Problem Determining \u201cwhat\u201d and \u201chow\u201d people are satisfied personally and with work has become an ongoing stream of research for both academics and practitioners. Yet research has contributed to confusing the issue and has not been able to provide any distinct answers to this problem. The Solution This study goes beyond current research by examining how problem-focused coping resources are used to ameliorate the negative relationship between work/life conflict and life satisfaction. Based on a sample of 491 executives, structural equation modeling (SEM) was used to test the mediational ability of problem-solving coping. Problem-solving coping was found to partially mediate the negative relationship between work/life conflict and life satisfaction. The Stakeholders Human resource development (HRD) scholars and practitioners interested in researching and reducing work/life conflict. A discussion of the importance of HRD interventions targeting problem-focused coping skills is included.\n\n==\n\nNg 2021:\n\nTitle: Happiness or Life Satisfaction?\n\nAbstract: Life satisfaction is likely to be more (than happiness) liable to be affected by shifts in the aspiration level, reducing the comparability of the resulting indices. Life satisfaction and/or preference may differ from happiness due to a positive valuation on the contribution to or a concern for the happiness of others. In the presence of such a divergence, levels of life satisfaction may be misleading.\n\n==\n\nPark 2007:\n\nTitle: A Study on Family Conflict and Life Satisfaction for Immigrant Women\n\nAbstract: The purpose of this study was to examine the differences among families in terms of conflict, life satisfaction according to socio-demographic variables, and self-esteem, and to analyze the effects of these variables influencing marital satisfaction for immigrant women. The subjects of this study were 127 immigrant women in Sunchon. Trained researchers interviewed the subjects with structured questionnaires. The data were analyzed using Cronbach's , frequency, percentage, mean, standard deviation, ANOVA, Duncan's test, Pearson's r, and stepwise multiple regression. The statistical package of SPSS is used to perform these analyses. The result of this study is summarized as follows: The family conflict scores of the immigrant women were lower than the median. The average score of life satisfaction was higher than the median. The family conflict of the immigrant women showed significant differences according of age.\n\n==\n\nChan 2015:\n\nTitle: Life Aspirations: Their Impact on Family and Work Conflict and Life Satisfaction in Hong Kong and Taiwan\n\nAbstract: Family and work conflict (F/WC) has gained prominence in developed economies in Asia as individuals are faced with the increased demands from work and families that are still considered very important. Hong Kong and Taiwan best illustrate the changing social and economic conditions that have given rise to this conflict. In these two societies, people have demonstrated a strong emphasis on work, even when it entails the sacrifice of family tasks and duties. An array of informal and formal services have been mobilized to allow people to fulfill their duties to work and family. Based on surveys conducted in Hong Kong and Taiwan in late 2009, this paper investigated F/WC and life satisfaction. Specifically, it examined changing life aspirations, both material and non-material, as predictors of F/WC and life satisfaction. The surveys found that respondents had high aspirations for both their family and their work, and for both material and non-material assets, and that they experienced high-level F/WC. Based on the statistical analysis, we argue that minimizing the interference of family life on work and emphasizing non-material life aspirations could promote better life satisfaction.\n\n==\n\nWelsch 2008:\n\nTitle: The Social Costs of Civil Conflict: Evidence from Surveys of Happiness\n\nAbstract: This paper uses data on stated subjective well-being to capture the intangible costs of civil conflict. By running cross-national regressions with happiness as the dependent variable, and the number of conflict victims and income as explanatory variables, it investigates if and in which way civil conflict affects happiness, and derives the implied monetary equivalent of the unhappiness caused. The paper finds that the number of conflict victims and their change over time significantly affect subjective well-being directly through health and psychic effects as well as indirectly through reduced income. The non-pecuniary effects are found to be larger than the income-related effect. A change over time in the number of victims has a stronger impact on well-being than the current number. There are sizeable monetary equivalents to these effects. Copyright 2008 The Authors.",
        "final_summary": "Conflict can impact happiness or life satisfaction. Chiu 1998 found that work and family conflicts, as well as interrole conflict, affected job satisfaction, marital satisfaction, and life satisfaction. Sulimah 2018 supported this, suggesting work-family conflict can lead to depression, mediated by job satisfaction and life satisfaction. Arslan 2010 found positive correlations between conflict communication, self-esteem, and life satisfaction. McMillan 2012 highlighted the negative relationship between work/life conflict and life satisfaction, which was partially mediated by problem-solving coping. Ng 2021 discussed the differences between happiness and life satisfaction, suggesting that life satisfaction may be more influenced by shifts in aspiration levels. Park 2007 examined family conflict in immigrant women, but did not provide specific findings on the relationship between conflict and life satisfaction. Chan 2015 explored the impact of life aspirations on family and work conflict and life satisfaction, emphasizing the importance of minimizing family interference and focusing on non-material aspirations. Welsch 2008 found that conflict victims and their change over time significantly affected subjective well-being, with non-pecuniary effects being larger than income-related effects."
    },
    {
        "query": "How does one consolidate the superposition principle\u2014where a mere electron is capable of a simultaneous multi-spatial existence\u2014with the linearity of motion associated with speed comprehended as a function of space over time?",
        "paper_list_string": "Brezhnev 2018:\n\nTitle: Linear Superposition as a Core Theorem of Quantum Empiricism\n\nAbstract: Clarifying the nature of the quantum state |\u03a8\u27e9 is at the root of the problems with insight into counter-intuitive quantum postulates. We provide a direct\u2014and math-axiom free\u2014empirical derivation of this object as an element of a vector space. Establishing the linearity of this structure\u2014quantum superposition\u2014is based on a set-theoretic creation of ensemble formations and invokes the following three principia: (I) quantum statics, (II) doctrine of the number in the physical theory, and (III) mathematization of matching the two observations with each other (quantum covariance). All of the constructs rest upon a formalization of the minimal experimental entity\u2014the registered micro-event, detector click. This is sufficient for producing the C-numbers, axioms of linear vector space (superposition principle), statistical mixtures of states, eigenstates and their spectra, and non-commutativity of observables. No use is required of the spatio-temporal concepts. As a result, the foundations of theory are liberated to a significant extent from the issues associated with physical interpretations, philosophical exegeses, and mathematical reconstruction of the entire quantum edifice.\n\n==\n\nOrd 2015:\n\nTitle: The superposition principle and images in spacetime\n\nAbstract: Feynman\u2019s path integral approach to the Young double-slit experiment makes it clear that phase and the superposition principle are necessary to \u00a0obtain interference fringes. However, the origins of phase and superposition are not considered in the PI approach, and the relation to classical physics remains elegant but formal. In this work we show that both phase and superposition emerge from \u00a0special relativity extended to account for binary periodic clocks in piecewise inertial frames. This pushes the peculiarities of quantum propagation back to the interaction of the two relativity postulates. The effects discussed are illustrated by considering \u00a0images of binary clocks in spacetime diagrams.\n\n==\n\nCorichi 2006:\n\nTitle: Quantum superposition principle and geometry\n\nAbstract: Within the Geometrical Formulation of Quantum Mechanics, quantum states are rays in the standard Hilbert space of the theory. The resulting formulation is very elegant from the geometrical viewpoint, since it allows to cast the main postulates of the theory in terms of two geometric structures, namely a symplectic structure and a Riemannian metric. However, the superposition principle of quantum mechanics is not naturally incorporated, since the quantum state space is non-linear. In this note we offer some steps to incorporate the superposition principle within the geometric description. In this respect, we argue that it is necessary to make the distinction between a projective superposition principle and a decomposition principle that should replace the standard superposition principle. We illustrate our proposal with two very well known examples, namely the spin 1/2 system and the two slit experiment, where the distinction is clear.\n\n==\n\nMobley 2005:\n\nTitle: A Reexamination of Time and Special Relativity Assuming a Constant Speed for Quantum Subcomponents.\n\nAbstract: Special relativity is reexamined assuming a new postulate \u2014 that the most fundamental, dimensionless subcomponents of matter travel with the same speed relative to all fixed points of reference \u2014 the speed of light. This postulate corresponds to an alternative definition of time, equating time with the spatial displacement of these subcomponents. Time is motion. This motion is found to correlate with the intrinsic spin and quantized angular momentum associated with elementary particles. The invariant properties of this motion are used to define our equations for time and space relativity in the same manner that the constancy of the speed of light was used by Einstein to generate the equations of special relativity. The predictions are nearly identical to special relativity. Interestingly, the transformation equations create the appearance of additional compactified spatial dimensions. Through modeling the action of interconnecting strings linking these subcomponents, the concepts of simultaneity and action at a distance are developed in relation to the proper time of a quantum particle. The force accelerating these subcomponents is used to derive the energy of formation, consistent with the mass-energy relations of special relativity and quantum theory. The uncertainty principle can be related to the time and space uncertainties in measuring the location of these subcomponents. Thus this alternative conceptual approach can provide a useful starting point for an alternative integration of time, relativity, and quantum theories.\n\n==\n\nGudder 1970:\n\nTitle: A Superposition Principle in Physics\n\nAbstract: A superposition principle is considered both in classical mechanics and in the quantum logic approach to quantum mechanics. It is shown, roughly speaking, that in classical mechanics the only type of superposition of states is a mixture and that no pure state is a nontrivial superposition of other pure states. In quantum mechanics it is shown that, if a superposition principle holds, then the quantum logic is a complete atomic lattice.\n\n==\n\nUrusovskii 2020:\n\nTitle: Electron Trembling as a Result of Its Stationary Motion in an Extra Space along Helical Line of the Compton Radius with the Speed of Light (\u201cZitterbewegung\u2019\u2019 in Multidimensional Space)\n\nAbstract: Because there is additional space in which the observed three-dimensional Universe expands, it is believed that elementary particles move at the speed of light in full space in a vicinity of a hyper-surface of three-dimensional sphere that is our Universe. Any interpretation of a spin and isotopic spin of electron requires at least three additional spatial dimensions. As applied to six-dimensional space, the simplest interpretation of the Heisenberg\u2019s uncertainties relation, de Broglie waves, Klein-Gordon equation, electron proper magnetic moment, CPT-symmetry, spin, and isotopic spin is consistent with the results of the theory of relativity and quantum mechanics. Taking into account the movement of elementary particle (at the speed of light) along a helical line of Compton radius, when the axis of the helix is placed on that hyper-surface, we find a trajectory of the particle.\n\n==\n\nZych 2018:\n\nTitle: Relativity of quantum superpositions\n\nAbstract: In modern physics only relative quantities are considered to have physical significance. For example, position assigned to a system depends on the choice of coordinates, and only relative distances between different systems have physical relevance. On the other hand, in quantum theory the scenario where one system, A, is localised around certain position while another system B is in a spatial superposition is considered to be physically different from the scenario where A is in a superposition, while B is localised. Different physical effects are anticipated in the two scenarios especially when the two systems have widely different masses. Here we show that for any superposed amplitudes that are related by a symmetry transformation, the above scenarios are physically equivalent: probabilities for any measurement are the same whether we assign the superposition state to system A or to system B. More generally, linearity of quantum theory guarantees that if a theory is invariant under some symmetry transformations it is necessarily also invariant under their arbitrary `superpositions'. Thus the notion of a superposition turns out to be relative to the choice of coordinates, once it is realised that relations between coordinates do not need to be classical.\n\n==\n\nAnand 2015:\n\nTitle: Comment on \u2018Limitations on the superposition principle: superselection rules in non-relativistic quantum mechanics\u2019\n\nAbstract: This is a comment to the paper, Limitations on the superposition principle: superselection rules in non-relativistic quantum mechanics by C Cisneros et al 1998 Eur. J. Phys. 19 237. doi:10.1088/0143-0807/19/3/005. The proof that the authors construct for the limitation on the superposition of state vectors corresponding to different sectors of the Hilbert space, partitioned by a superoperator has a flaw as outlined below.",
        "final_summary": "Brezhnev (2018) establishes the linearity of quantum superposition using set-theoretic ensemble formations and minimal experimental entities. Ord (2015) shows that phase and superposition emerge from special relativity when extended to binary periodic clocks. Corichi (2006) suggests the superposition principle can be incorporated within the geometric description of quantum mechanics. Mobley (2005) reexamines special relativity by assuming a constant speed for quantum subcomponents, linking time, spin, and quantized angular momentum. Gudder (1970) explores the superposition principle in both classical mechanics and quantum logic. Urusovskii (2020) proposes that electron motion in additional spatial dimensions can account for spin and isotopic spin. Zych (2018) argues that the notion of superposition is relative to the choice of coordinates in quantum theory. Lastly, Anand (2015) provides a critique on the limitations of the superposition principle in non-relativistic quantum mechanics."
    },
    {
        "query": "Follow-up frequency and duration for patients with gonadotroph pituitary adenomas",
        "paper_list_string": "Manieri 2000:\n\nTitle: Gonadotroph cell pituitary adenomas in males.\n\nAbstract: BACKGROUND\nConsidered exceptional in the past, gonadotroph cell pituitary adenomas account for 3.5-6.4% of total surgically excised pituitary adenomas when examined with immunospecific staining. The aim of this study was to describe the clinical, hormonal, radiological and immunohistochemical features, the management and the follow-up of our patients with gonadotroph adenoma.\n\n\nMETHODS\nIn this retrospective study we describe 14 male subjects aged 19-70 yrs affected by gonadotroph cell pituitary adenomas; the patients were studied by hormonal, radiological and immunohistochemical investigations and followed up for 3-13 yrs by ambulatory and/or hospitalized care.\n\n\nRESULTS\nVisual impairment and/or decreased libido and erectile dysfunction were the symptoms at presentation. Increased serum gonadotropin concentrations were shown in 3 patients. Reduced levels of testosterone were present in 9 patients, and normal in the remainder. At diagnosis all patients had pituitary macroadenomas, with wide extrasellar extension in 12. All patients underwent trans-sphenoidal surgery and immunohistochemical staining of surgically excised specimens showed the presence of gonadotroph and alpha-subunit cells in all pituitary adenomas. After surgery 3 patients had clear radiological evidence of normal pituitary; in the others a doubtful MRI picture or a residual adenomatous tissue were present. In the patients who did not undergo radiotherapy immediately after surgery, a regrowth of tumoral tissue was shown in 1-10 yrs.\n\n\nCONCLUSIONS\nWe stress the importance of a close follow-up of patients with gonadotroph adenomas after surgery, and we raise the question of whether radiotherapy may be useful for avoiding any further adenomatous regrowth.\n\n==\n\nYoung 1996:\n\nTitle: Gonadotroph adenoma of the pituitary gland: a clinicopathologic analysis of 100 cases.\n\nAbstract: OBJECTIVE\nTo determine the clinical and pathologic features in a large cohort of randomly selected patients with gonadotroph pituitary adenomas.\n\n\nDESIGN\nWe retrospectively reviewed clinical, surgical, and pathologic findings in 100 patients (79 men and 21 women, 30 to 82 years old) with this tumor.\n\n\nRESULTS\nDiagnosis of a pituitary tumor was prompted by visual loss (43%), symptoms of hypopituitarism (22%), headache (8%), or a combination of these findings (10%); 17% of the patients were asymptomatic. Visual field defects were present in 68% of the study group, and complete or partial anterior pituitary failure was present in 77%. Serum prolactin concentrations were increased (maximum, 110 ng/mL) in 33% of patients. Hypersecretion of follicle-stimulating hormone (FSH) and luteinizing hormone (LH) was documented in 11 and 5 patients, respectively. The serum level of alpha-subunit was increased in 1 of 29 patients in whom it was measured. All patients had pituitary macroadenomas, only 21% of which were grossly invasive. The surgical approach was transsphenoidal in all but two patients, who underwent transfrontal craniotomy; gross total tumor resection was achieved in 90%. By definition, all tumors had at least 10% immunoreactivity for LH, FSH, or both. All tumors were chromophobic. Ultrastructurally, the tumors were characterized as gonadotroph adenomas of \"male\" (45%) or \"female\" (9%) type as well as null-cell adenomas of the oncocytic (35%) or nononcocytic (11%) type. After a median follow-up of 4.3 years, 69% of the patients who had had visual field defects noted normalization or improvement. Persistent or clinically recurrent pituitary tumor tissue was present in 42%. A second pituitary surgical procedure was required in eight patients.\n\n\nCONCLUSION\nMost patients with clinically evident gonadotroph pituitary tumors have loss of vision and hypopituitarism. Hypersecretion of FSH or LH is unusual, and no distinct hormone-dependent clinical phenotype is present. Transsphenoidal surgical treatment generally yields normalization or improvement of visual field defects.\n\n==\n\nSnyder 1987:\n\nTitle: Gonadotroph cell pituitary adenomas.\n\nAbstract: The frequency of gonadotroph cell adenomas among all unselected pituitary adenomas is likely much higher than previously suspected. The prevalence in one series of 139 men with pituitary macroadenomas was 17 per cent (24 per cent if adenomas secreting only alpha subunit are included). The clinical characteristics of patients with gonadotroph cell adenomas are similar. Most are middle-aged men who have a history of normal pubertal development and a normal fertility history and by examination are normally virilized and have testes of normal size. They are brought to medical attention because of visual impairment, which is the result of the enormous size of the adenoma. The most common hormonal characteristics of gonadotroph cell adenomas in vivo is hypersecretion of FSH, which is often accompanied by hypersecretion of FSH-beta and alpha subunits and less often by hypersecretion of LH-beta or intact LH. Another common characteristic is secretion of FSH and/or LH-beta in response to TRH. A few patients with gonadotroph cell adenomas hypersecrete intact LH and therefore have supranormal serum testosterone concentrations. A larger number have secondary hypogonadism, because the adenomas are not secreting intact LH but are compressing the normal gonadotroph cells and impairing LH secretion. These patients have concentrations of intact LH that are not elevated, despite subnormal testosterone concentrations. The testosterone increases markedly in response to human chorionic gonadotropin. Both the clinical and hormonal characteristics of gonadotroph cell adenomas usually make them readily distinguishable from pituitary enlargement due to long-standing primary hypogonadism. Most gonadotroph cell adenomas are now managed first by transsphenoidal surgery to attempt to restore vision as quickly as possible, and then by supervoltage radiation to prevent regrowth of the remaining adenomatous tissue. Surgery usually does improve vision, as well as the pretreatment hormonal abnormalities, and radiation reduces FSH hypersecretion further. Dopamine agonist therapy is experimental but warrants further trial. The hormonal abnormalities detected prior to treatment, such as supranormal basal concentrations of FSH, alpha, and FSH-beta and the FSH and LH-beta responses to TRH, can be used to monitor the response to therapy.\n\n==\n\nSnyder 1985:\n\nTitle: Gonadotroph cell adenomas of the pituitary.\n\nAbstract: Although the frequency of gonadotroph cell adenomas among all unselected pituitary adenomas is not yet known, it is probably much higher than previously suspected. The true incidence is probably somewhere between the 3-4% found in surgical and autopsy series, which is probably an underestimate because of its reliance on tissue content, and the 17% (24% when alpha-secreting adenomas are included) of 139 patients from this institution, which may be an overestimate of the incidence among all adenomas, because it is heavily weighted to very large adenomas in men only. Most patients who have been reported to have gonadotroph cell adenomas have similar clinical characteristics. Most are middle-aged men who have a history of normal pubertal development and a normal fertility history, and by examination are normally virilized and have testes of normal size. They are brought to medical attention because of visual impairment, which is the result of the enormous size of the adenoma. The most common hormonal characteristic of gonadotroph cell adenomas in vivo is hypersecretion of FSH, which is often accompanied by hypersecretion of FSH beta and alpha-subunit and less often by hypersecretion of LH beta or intact LH. Another common characteristic is secretion of FSH and/or LH in response to TRH. A few patients with gonadotroph cell adenomas hypersecrete intact LH and, therefore, have supranormal serum testosterone concentrations. A larger number have secondary hypogonadism because the adenomas are not secreting intact LH, but are compressing the normal gonadotroph cells and impairing LH secretion. These patients have concentrations of intact LH that are not elevated in spite of subnormal testosterone concentrations. Testosterone levels increase markedly in response to hCG. The hormonal characteristics of gonadotroph adenomas in dispersed cell culture are similar to their characteristics in vivo, including hypersecretion of FSH and LH subunits and responsiveness to TRH. Both the clinical and hormonal characteristics of gonadotroph cell adenomas usually make them readily distinguishable from pituitary enlargement due to long-standing primary hypogonadism. Pituitary adenomas that hypersecrete only alpha-subunit in vivo may also be adenomas of gonadotroph cells, because some of them secrete large amounts of FSH as well as alpha-subunit in culture. Most gonadotroph cell adenomas are now treated first by transsphenoidal surgery, to attempt to restore vision as quickly as possible, and then by supervoltage radiation to prevent regrowth of the remaining adenomatous tissue.(ABSTRACT TRUNCATED AT 400 WORDS)\n\n==\n\nLanglois 2017:\n\nTitle: Clinical profile of silent growth hormone pituitary adenomas; higher recurrence rate compared to silent gonadotroph pituitary tumors, a large single center experience\n\nAbstract: PurposeStudy and comparison of characteristics of silent growth hormone adenomas (SGHA), silent corticotroph adenomas (SCA), and silent gonadotroph adenomas (SGA) in a single institution cohort of surgically treated pituitary adenomas.MethodsRetrospective analysis of SGHA surgically resected over 10 years: SGHA was defined as no clinical or biochemical evidence of acromegaly and positive GH immunostaining.ResultsOf 814 pituitary surgeries; 2.1% (n\u2009=\u200917) were SGHA, 4.5% (n\u2009=\u200937) SCA, and 18.9% (n\u2009=\u200970/371; 2011\u20132016) SGA. Mean age at SGHA diagnosis was 43 years, with a large female predominance (82%). Mean tumor size and cavernous/sphenoid sinus invasiveness for SGHA, SCA, and SGA were 1.5\u2009\u00b1\u20091.0\u2009cm and 25%, 2.5\u2009\u00b1\u20091.2\u2009cm and 43%, 2.9\u2009\u00b1\u20092.0\u2009cm and 41%, respectively (tumor size p\u2009=\u20090.009, SGHA vs. SGA, and invasion p; not-significant). During mean follow-up of 3.9 years, two patients (11%) developed elevated insulin-like growth factor-1 and five patients (29%) required a second surgery for tumor recurrence. Rate of surgical reintervention was similar to SCA (31%), but higher than SGA (10%) (p\u2009=\u20090.035, SGHA vs. SGA), and 18% underwent radiation therapy, similar to SCA (19%, p; not-significant) but higher than SGA (2.9%, p\u2009=\u20090.018).ConclusionThis is the largest single center study characterizing SGHA behavior with SGA and SCA control groups in a cohort of surgically resected pituitary adenomas. SGHA present mostly in young females, and should be closely followed due to their higher likelihood of recurrence and potential of progression to clinical acromegaly. We propose that a complete hormonal staining panel be routinely performed for all pituitary adenomas.\n\n==\n\nHo 1997:\n\nTitle: The clinicopathological characteristics of gonadotroph cell adenoma: a study of 118 cases.\n\nAbstract: Gonadotroph cell adenoma was the most common pituitary adenoma, constituting 35% of the pituitary adenomas in our series of 339 patients with surgically removed pituitary adenomas from June 1987 to December 1995. The average age of these patients was 53 years, with a male predominance (1.5:1). The most common neurological symptoms were visual symptoms and headache. Amenorrhea and galactorrhea were recorded in 41% and 14%, respectively, of the female patients of reproductive age. Oncocytic change of varying degrees was seen in 69% of the tumors, and the average age of these patients (56 years) was older than those who had no such change (47 years) (P < .005). Five types of gonadotroph cell adenomas were recognized; they were tumors that contained (1) betaFSH, betaLH, and alphaSU, (2) betaFSH and betaLH, (3) betaFSH and alphaSU, (4) betaFSH, and (5) alphaSU. The immunostaining of betaFSH-containing cells was usually diffuse, whereas staining of betaLH- or alphaSU-containing cells was usually focal. Double immunostaining showed the immunoreactive cells containing one or any combination of the gonadotropin subunits. Increases in serum gonadotropin levels were only seen in 35% of the patients with gonadotroph cell adenoma. There was no correlation between serum hormonal levels and immunostaining results. Of the 33 recurrent cases, significant decrease or total absence of immunoreactivity of one or more hormone subunits in subsequent biopsy specimens were seen in three cases.\n\n==\n\nGraillon 2019:\n\nTitle: Functioning gonadotroph adenoma with severe ovarian hyperstimulation syndrome: A new emergency in pituitary adenoma surgery? Surgical considerations and literature review.\n\nAbstract: The authors reported 2\u00a0cases of functioning gonadotroph pituitary adenoma (FGPA) revealed by an ovarian hyperstimulation syndrome (OHSS) in young women. In the first case, OHSS was observed after GnRH analog injection. Pelvic echography revealed multiple voluminous ovarian cysts. Dopamine agonist posology failed in estradiol hypersecretion control, which necessitated endoscopic endonasal transsphenoidal surgery. The patient experienced improvement in pelvic pain as estradiol hypersecretion decreased during the first few postoperative days. Outcome was favorable, and her menstrual cycle was normal after two months. The second case was a young girl with spontaneous pelvic pain and elevated plasma FSH and estradiol levels. FGPA was confirmed on cerebral MRI. Dopamine agonists were introduced, and surgical removal of the pituitary tumor was scheduled for 7\u00a0days later. In the meantime, the patient was admitted and underwent surgery for bilateral adnexal torsion related to OHSS. The pituitary tumor was removed one week later. Outcome was favorable, and estradiol and FSH plasma levels were normal after 3\u00a0months. The ovarian cysts were no longer visible on echography after 3\u00a0months. Given the lack of efficacy of the current standard medical therapy, surgical removal of pituitary adenomas is the reference treatment for FGPA. The authors suggest that severe OHSS related to FGPA should be considered as a relative surgical emergency and that surgery should not be unduly delayed, given the unpredictable risk of adnexal torsion, particularly in case of voluminous ovarian cysts. The authors performed a literature review on this topic.\n\n==\n\nMurakami 2004:\n\nTitle: CASE REPORT: Management of ovarian hyperstimulation due to follicle\u2010stimulating hormone\u2010secreting gonadotroph adenoma\n\nAbstract: Terguride therapy was administered and a brain MRI was planned due to mild hyperprolactinaemia and galactorrhoea. The MRI revealed a pituitary tumour. Visual field examination, which was performed after the MRI, revealed bitemporal hemianopsia. The levels of other pituitary hormones were within the normal ranges and were as follows: growth hormone (GH), 1.68 ng/mL; thyrotropinstimulating hormone (TSH), 1.05 AIU/mL; and adenocorticotropic hormone (ACTH), 36.7 pg/mL. Terguride therapy was cancelled in a week, and transsphenoidal removal of the pituitary adenoma was performed as the initial therapy. Her ovarian cysts were not treated because she did not have severe abdominal pain and there was a low possibility of malignancy. Immunohistochemical staining of the pituitary tumour for pituitary hormones revealed that the tumour cells expressed LH, FSH and PRL but not GH, TSH or ACTH. The patient was diagnosed with gonadotroph adenoma. Her post-operative course was uneventful. She had withdrawal bleeding for several days after the operation. The bilateral huge ovarian cysts dramatically decreased in size and her hormone levels of FSH, PRL, oestradiol and LH improved within one month after the operation. Other pituitary hormone levels were not influenced by the transsphenoidal surgery. Her menstrual cycle resumed, and her visual field deficits improved gradually. At 26 years of age, five years after excision of the pituitary tumour, she had no recurrent signs of gonadotroph adenoma. She conceived spontaneously during the following year. She delivered a female baby weighing 2488 g by spontaneous vaginal delivery at 37 weeks of gestation.",
        "final_summary": "Close follow-up after surgery is crucial for patients with gonadotroph pituitary adenomas, as suggested by Manieri 2000. This study also raises the question of whether radiotherapy may be useful in preventing adenomatous regrowth. Young 1996 found that transsphenoidal surgery generally improves visual field defects in patients with gonadotroph pituitary tumors. Snyder 1987 highlights the need for transsphenoidal surgery followed by radiation to reduce FSH hypersecretion. Langlois 2017 reports a higher recurrence rate in silent gonadotroph adenomas compared to silent growth hormone adenomas, suggesting the need for vigilant follow-up. Ho 1997 emphasizes the need for surgical removal of gonadotroph adenomas due to the lack of efficacy of medical therapy. Graillon 2019 suggests that severe ovarian hyperstimulation syndrome related to functioning gonadotroph adenomas should be considered a relative surgical emergency. Murakami 2004 presents a case report where transsphenoidal removal of a gonadotroph adenoma resulted in improved hormone levels and ovarian cyst regression. These papers collectively highlight the importance of regular follow-up and surgical intervention in managing gonadotroph pituitary adenomas."
    },
    {
        "query": "Policy Lessons and the Future of Unconventional Monetary Policy",
        "paper_list_string": "Duarte 2020:\n\nTitle: Unconventional Monetary Policies: Lessons from the Past and the Present to Future Monetary Policy Frameworks\n\nAbstract: This article intends to debate important aspects related to past and recent experiences of monetary policy accommodation, focusing on unconventional monetary policies. We intend to draw lessons from these experiences to discuss the design of future monetary policy frameworks.First, by reporting several historical experiences of major central banks, we highlight that policies which after 2008 crisis were considered \u201cunconventional\u201d were not new, with central banks intervening to avoid broader deterioration of macro-financial conditions.Moreover, analyzing the experience of the European Central Bank after 2008, we observe this institution has adapted its measures according to its former programs and to other central banks' experiences, to face numerous challenges and enhance its framework. Finally, we argue that central banks need to take advantage of past and recent experiences to improve the design of their future monetary policy frameworks under an evolutionary perspective. Based on this, measures previously implemented would have three possible destinations in new frameworks: i) Be discarded, due to their predominantly adverse effects; ii) Not be regularly implemented, but be kept as backstop mechanisms if needed; iii) Be incorporated as regular measures of monetary policy frameworks.\n\n==\n\nWilliams 2011:\n\nTitle: Unconventional monetary policy: lessons from the past three years\n\nAbstract: Presentation to the Swiss National Bank Research Conference, Zurich, Switzerland, September 23, 2011\n\n==\n\nWestelius 2020:\n\nTitle: Twenty Years of Unconventional Monetary Policies: Lessons and Way Forward for the Bank of Japan\n\nAbstract: The Bank of Japan has used unconventional monetary policies to fight deflation and stabilize the financial system since the late 1990s. While the Bank of Japan\u2019s reflation efforts have evolved over time, inflation and inflation expectations have remained stubbornly low. This paper examines the evolution of monetary policy in Japan over the past twenty years, in order to draw relevant lessons and propose ways to strengthen the Bank of Japan\u2019s policy framework. In doing so the analysis focuses on three aspects of monetary policy: objectives and goals; policy strategies; and the communication framework. Moreover, the paper discusses coordination between monetary, fiscal, and financial policies, and how the corresponding institutional design could be strengthened.\n\n==\n\nWestelius 2020:\n\nTitle: Twenty Years of Unconventional Monetary Policies: Lessons and Way Forward for the Bank of Japan\n\nAbstract: The Bank of Japan has used unconventional monetary policies to fight deflation and stabilize the financial system since the late 1990s. While the Bank of Japan\u2019s reflation efforts have evolved over time, inflation and inflation expectations have remained stubbornly low. This paper examines the evolution of monetary policy in Japan over the past twenty years, in order to draw relevant lessons and propose ways to strengthen the Bank of Japan\u2019s policy framework. In doing so the analysis focuses on three aspects of monetary policy: objectives and goals; policy strategies; and the communication framework. Moreover, the paper discusses coordination between monetary, fiscal, and financial policies, and how the corresponding institutional design could be strengthened.\n\n==\n\nKuttner 2018:\n\nTitle: Outside the Box: Unconventional Monetary Policy in the Great Recession and Beyond\n\nAbstract: In November 2008, the Federal Reserve faced a deteriorating economy and a financial crisis. The federal funds rate had already been reduced to virtually zero. Thus, the Federal Reserve turned to unconventional monetary policies. Through \u201cquantitative easing,\u201d the Fed announced plans to buy mortgage-backed securities and debt issued by government-sponsored enterprises. Subsequent purchases would eventually lead to a five-fold expansion in the Fed\u2019s balance sheet, from $900 billion to $4.5 trillion, and leave the Fed holding over 20 percent of all mortgage-backed securities and marketable Treasury debt. In addition, Fed policy statements in December 2008 began to include explicit references to the likely path of the federal funds interest rate, a policy that came to be known as \u201cforward guidance.\u201d The Fed ceased its direct asset purchases in late 2014. Starting in October 2017, it has allowed the balance sheet to shrink gradually as existing assets mature. From December 2015 through June 2018, the Fed has raised the federal funds interest rate seven times. Thus, the time is ripe to step back and ask whether the Fed\u2019s unconventional policies had the intended expansionary effects\u2014and by extension, whether the Fed should use them in the future.\n\n==\n\nRossi 2019:\n\nTitle: Identifying and Estimating the Effects of Unconventional Monetary Policy: How to Do it and What Have We Learned?\n\nAbstract: How should one identify monetary policy shocks in unconventional times? Are unconventional monetary policies as effective as conventional ones? And has the transmission mechanism of monetary policy changed in the zerolower bound era? The recent Onancial crisis led Central banks to lower their interest rates in order to stimulate the economy, and interest rates in many advanced economies hit the zero lower bound. As a consequence, the traditional approach to the identification and the estimation of monetary policy faces new econometric challenges in unconventional times. This article aims at providing a broad overview of the recent literature on the identification of unconventional monetary policy shocks and the estimation of their effects on both financial as well as macroeconomic variables. Given that the prospects of slow recoveries and long periods of very low interest rates are becoming the norm, many economists believe that we are likely to face unconventional monetary policy measures often in the future. Hence, these are potentially very important issues in practice.\n\n==\n\nDell'Ariccia 2018:\n\nTitle: Unconventional Monetary Policies in the Euro Area, Japan, and the United Kingdom\n\nAbstract: The global financial crisis hit hard in the euro area, the United Kingdom, and Japan. Real GDP from peak to trough contracted by about 6 percent in the euro area and the United Kingdom and by 9 percent in Japan. In all three cases, central banks cut interest rates aggressively and then, as policy rates approached zero, deployed a variety of untested and unconventional monetary policies. In doing so, they hoped to restore the functioning of financial markets, and also to provide further monetary policy accommodation once the policy rate reached the zero lower bound. In all three jurisdictions, the strategy entailed generous liquidity support for banks and other financial intermediaries and large-scale purchases of public (and in some cases private) assets. As a result, central banks\u2019 balance sheets expanded to unprecedented levels. This paper examines the experience with unconventional monetary policies in the euro zone, the United Kingdom, and Japan. The paper starts with a discussion of how quantitative easing, forward guidance, and negative interest rate policies work in theory, and some of their potential side effects. It then reviews the implementation of unconventional monetary policy by the European Central Bank, the Bank of England, and the Bank of Japan, including a narrative of how central banks responded to the crisis and the evidence on the effects of unconventional monetary policy actions.\n\n==\n\nCecioni 2011:\n\nTitle: Unconventional Monetary Policy in Theory and in Practice\n\nAbstract: In this chapter, after discussing the theoretical underpinnings of unconventional monetary policy measures, we review the existing empirical evidence on their effectiveness, focusing on those adopted by central banks, particularly the Federal Reserve. These measures operate in two ways \u2014 through the signaling channel and through the portfolio balance channel. In the former, the central bank uses communication to steer interest rates and to restore confidence in the financial markets; the latter hinges on the imperfect substitutability of assets and liabilities in the balance sheet of the private sector and postulates that the central bank\u2019s asset purchases and liquidity provision lower financial yields and improve funding conditions in some markets. Our review of the empirical literature suggests that the unconventional measures were effective and that their impact on the economy was sizeable. However, a large degree of uncertainty surrounds the precise quantification of these effects.",
        "final_summary": "Unconventional monetary policies have been used by central banks to adapt to changing economic conditions and challenges. Duarte (2020) suggests that these policies, once considered \"unconventional\", are not new and should be incorporated into future monetary policy frameworks based on their effectiveness. Westelius (2020) provides a detailed analysis of the Bank of Japan's unconventional policies over the past twenty years, emphasizing the difficulty in achieving inflation targets despite evolving strategies. Kuttner (2018) critically assesses the Federal Reserve's unconventional policies during the Great Recession, questioning their intended expansionary effects and suggesting a need for reassessment. Rossi (2019) discusses the challenges in identifying and estimating the effects of unconventional monetary policy, highlighting the need for new approaches in unconventional times. Dell'Ariccia (2018) examines the experiences of the euro area, the United Kingdom, and Japan, focusing on the use of quantitative easing and negative interest rates. Cecioni (2011) reviews the effectiveness of unconventional measures, acknowledging the uncertainty in quantifying their effects. These papers collectively suggest a need for continuous learning, adaptation, and careful evaluation in shaping future unconventional monetary policy frameworks."
    },
    {
        "query": "PROJECT TEAM BUILDING FRAMEWORK",
        "paper_list_string": "Gronau 2007:\n\nTitle: Approach for requirement oriented team building in industrial processes\n\nAbstract: This contribution presents an approach for requirement oriented team building in industrial processes like product development. This will be based on the knowledge modelling and description language (KMDL^(R)) that enables the modelling and analysis of knowledge intensive business processes. First the basic elements of the modelling technique are described, presenting the concept and the description language. Furthermore it is shown how the KMDL^(R) process models can be used as a basis for the team building component. Therefore, an algorithm was developed that is able to propose a team composition for a specific task by analyzing the knowledge and skills of the employees, which will be contrasted to the process requirements. This can be used as guidance for team building decisions.\n\n==\n\nBulgakova 2022:\n\nTitle: DEVELOPMENT OF A METHODOLOGY FOR BUILDING A PROJECT TEAM IN THE HR-MANAGEMENT SYSTEM OF AN ORGANIZATION\n\nAbstract: During the development and implementation of a project, such an area of activity as teamwork should not be ignored. The process of forming a project team is an important detail of prelaunch preparation and an essential area of work during the project implementation period. Within the framework of this process, the priority area is the optimization of the activities of HR-management when solving the problem of combining specialists with various skills, values, ideas and the distribution of their roles and responsibilities. The urgency of the problem of improving the decision-making process in the HR management system in the context of digital transformation through the development of new models of activity, combining the capabilities of technology and traditional management in the face of the emergence of new information products and processes with fundamentally different qualities determined the direction and structure of the study. The aim of the study is to analyze the functional capabilities of digital technologies used in personnel management, to substantiate the need for the use of HR-management automated systems and decision-making support methods, as well as to develop a methodological approach to the implementation of the initial stage of team formation procedure. In the course of the research, the method of analysis of expert data, procedures of multi-criteria selection, the analytic hierarchy process, optimization models were used. The proposed approach involves a complex combination of a formalized algorithm for expert evaluation and of an optimization problem, namely, a sequential solution of the problem of optimal selection of candidates and the problem of distribution of candidates who have passed the rating selection. It is concluded that the developed methodology for the formation of project teams significantly expands the capabilities of HR analytics and contributes to a more efficient distribution of candidates for project vacancies, but at the same time acts only as a tool that helps an expert.\n\n==\n\nSilveira 2021:\n\nTitle: Successful Practices in ICT Team Building in International Projects\n\nAbstract: The purpose of the chapter is to address the importance of building the project team in order to contribute to maximize the number of project success histories in an international context. The design science research methodology allowed, based on the literature review on the thematic domain, to elaborate a reference framework in order to obtain the scientific validation of the work. The main conclusions focus on presenting the added value of the team building in the implementation and adaptation of software products. The need to define strategies when setting up the teams is emphasized, so that specialists are incorporated in the various domains, taking into account the specificity of the country in which the project will take place. Thus, the definition of team formation policies is considered a differentiating factor, in order to include the knowledge in the domains of the culture of the country, organizational culture and current legal and fiscal frameworks as well as the interpersonal relations component.\n\n==\n\nGharaibeh 2019:\n\nTitle: Project Team Building: Case Study Investigation in the Construction Industry in Jordan\n\nAbstract: Project teams play an important role in project performance. In the Engineering and Construction industry, the successful delivery of a project requires the collaboration of people with different skills and expertise which makes a healthy work relation among project team members very essential for the success of the project in particular and the organization in general. In this paper, we look at \"Project team building\" from three main dimensions: firstly, communication among project team members. Effectiveness of team communication in projects is becoming increasingly important due to the growing technical and organizational complexity of construction projects. Secondly, trust among project team members. Because trust is a major factor leading to the success or failure of construction project and finally, the role of leadership in project management, particularly in the construction field is very crucial to the success of the team. To achieve the above objective, we selected a case study investigation through which a major construction project in Jordan was analyzed. After studying the project related documents. We conducted several interviews and distributed several questionnaires asking key project stakeholders about the three dimensions of project team building defined. The study involved the identification of barriers and enablers for each dimension in the project team building model. The study concluded with a framework for project team building based on the dimensions of: trust, communication and leadership that could be applied on projects with similar context in Jordan.\n\n==\n\nGharaibeh 2019:\n\nTitle: Project Team Building: Case Study Investigation in the Construction Industry in Jordan\n\nAbstract: Project teams play an important role in project performance. In the Engineering and Construction industry, the successful delivery of a project requires the collaboration of people with different skills and expertise which makes a healthy work relation among project team members very essential for the success of the project in particular and the organization in general. In this paper, we look at \"Project team building\" from three main dimensions: firstly, communication among project team members. Effectiveness of team communication in projects is becoming increasingly important due to the growing technical and organizational complexity of construction projects. Secondly, trust among project team members. Because trust is a major factor leading to the success or failure of construction project and finally, the role of leadership in project management, particularly in the construction field is very crucial to the success of the team. To achieve the above objective, we selected a case study investigation through which a major construction project in Jordan was analyzed. After studying the project related documents. We conducted several interviews and distributed several questionnaires asking key project stakeholders about the three dimensions of project team building defined. The study involved the identification of barriers and enablers for each dimension in the project team building model. The study concluded with a framework for project team building based on the dimensions of: trust, communication and leadership that could be applied on projects with similar context in Jordan.\n\n==\n\nFran\u00e7a 2008:\n\nTitle: A QUALITATIVE RESEARCH ON SOFTWARE PROJECTS TEAM BUILDING\n\nAbstract: Recent research has shown that team building is a factor\n that can contribute significantly for software projects success or\n failure. This article presents the results of a qualitative\n research performed with Project Managers and software development\n teams, on the reality of the team formation in the industry and the\n importance of considering personal aspects to establish the teams\n and on the technical preparation of these managers with respect to\n the team building. The main result of this work is a requirement\n set for team building, of simple understanding and easy\n application, which might contribute to reduce certain failures in\n software project related to team working.\n\n==\n\nHlaoittinun 2008:\n\nTitle: A multidisciplinary team building method based on competency modelling in design project management\n\nAbstract: Abstract This research aims to develop a new team building method based on competency modelling in the field of project management. This method is divided into three main stages. First, a competency matrix based on a task-actor compatibility indicator helps to characterise the competency levels. Second, we apply a clustering algorithm in order to reduce the problem complexity and favour the employees\u2019 expertise. The clustering algorithm will decompose large sets of tasks and actors into smaller task groups related to different actor groups. It facilitates the project leaders to organise the actors into teams. Finally, the proposed task assignment model incorporates a learning curve in order to take the competency dynamics into account. Our computational experiments suggest that incorporating a clustering algorithm as a step of the method results in preserving expertise and thus helps project managers to find better tradeoffs between project cost (short term goal) and competency dynamics (long term goal).\n\n==\n\nThamhain 1987:\n\nTitle: Building high performing engineering project teams\n\nAbstract: This article summarizes four years of research into the drivers and barriers of effective teambuilding in engineering work environments. A simple input-output model is presented for organizing and analyzing the various factors which influence team performance. The field survey results supported by correlation analysis indicate that team performance is primarily associated with six driving forces and six barriers which are related to: leadership, job content, personal needs, and general work environment. Specific recommendations are made.",
        "final_summary": "The papers collectively provide insights into the research question of \"project team building framework.\" Gronau (2007) proposes an approach for requirement-oriented team building based on knowledge modeling and analysis of knowledge-intensive business processes. Bulgakova (2022) focuses on the optimization of HR management activities in forming project teams, emphasizing the use of digital technologies and decision-making support methods. Silveira (2021) highlights the importance of team building in international projects, emphasizing the need for strategies that consider specialists' incorporation and the specificities of the project's country. Gharaibeh (2019) investigates project team building in the construction industry, emphasizing the dimensions of communication, trust, and leadership. Fran\u00e7a (2008) emphasizes the importance of considering personal aspects and technical preparation in team formation for software projects. Hlaoittinun (2008) proposes a multidisciplinary team building method based on competency modeling in design project management. Lastly, Thamhain (1987) presents factors influencing team performance in engineering work environments, including leadership, job content, personal needs, and the work environment."
    },
    {
        "query": "What are the important themes and ideas in the text \"Frankenstein\", that students pursuing M.A. in English Literature find it difficult to understand and imagine?",
        "paper_list_string": "Davies 2004:\n\nTitle: Can Mary Shelley\u2019s Frankenstein be read as an early research ethics text?\n\nAbstract: The current, popular view of the novel Frankenstein is that it describes the horrors consequent upon scientific experimentation; the pursuit of science leading inevitably to tragedy. In reality the importance of the book is far from this. Although the evil and tragedy resulting from one medical experiment are its theme, a critical and fair reading finds a more balanced view that includes science\u2019s potential to improve the human condition and reasons why such an experiment went awry. The author argues that Frankenstein is an early and balanced text on the ethics of research upon human subjects and that it provides insights that are as valid today as when the novel was written. As a narrative it provides a gripping story that merits careful analysis by those involved in medical research and its ethical review, and it is more enjoyable than many current textbooks! To support this thesis, the author will place the book in historical, scientific context, analyse it for lessons relevant to those involved in research ethics today, and then draw conclusions.\n\n==\n\nBissonette 2010:\n\nTitle: Teaching the Monster: Frankenstein and Critical Thinking\n\nAbstract: Student readers often switch, with a taste for the unambiguous, from presuming Mary Shelley's monster to be wholly bad, to exonerating him completely, and sometimes swinging back to \"evil.\" Both sympathy and condemnation\u2014the \"whose fault is it\" debate\u2014curtail the kind of critical thinking and application that is the goal of most of the courses in which Frankenstein is taught. The students' imagination of the heart and the professor's conceptual framework, sympathy and analysis, run parallel to and often counteract or resist each other. This essay proposes the importance of emphasizing the monster himself, returning to the monster as a monster, refusing to allow him to melt into a symbol in the classroom; it further suggests a practice of \"teaching the monster\" as a pedagogy of alienation that can be applied beyond Frankenstein.\n\n==\n\nOzherelyev 2020:\n\nTitle: Philosophical contexts in Mary Shelley\u2019s novel \u00abFrankenstein\u00bb\n\nAbstract: The paper analyzes the key philosophical contexts and subtexts of M. Shelley\u2019s most famous work \u201cFrankenstein\u201d. According to the author of the article, the philosophical layer of this Gothic novel consists of ideas and maxims that directly inherit the concepts of the worldview platforms of Plato, J.-J. Russo, G. W. F. Hegel, K. F. Volney, W. Godwin, M. Wollstonecraft, as well as the philosophy of the New Age and romanticism. An assumption is made, on the one hand, about the proximity of some worldview attitudes of these philosophers and the author of \u201cFrankenstein\u201d and, on the other hand, about the deliberate introduction of philosophical passages into the fabric of the novel, which play the role of retardation elements.\n\n==\n\nDebnath 2019:\n\nTitle: Mary Shelley\u2019s Frankenstein As A Text About Nature and Culture\n\nAbstract: In Mary Shelley\u2019s novel Frankenstein (1818), we find several dichotomies: culture/nature, self/other, ego/id, male/female et cetera. In the novel, Victor is a scientist who wants to inject life into inanimate objects and thereby become a creator, a god. As science is an element of culture, Victor is associated with culture. But he represents the darker side of culture: scientism misused as fantasy. On the other hand, the creature is associated with nature. Though Victor infuses life into the monster through a scientific experiment, the monster is still a nature\u2019s child as he is brought up in the midst of wild natural landscape. In the novel, we find that \u2018male\u2019 science (as a part of culture), in the person of Victor, penetrates \u201cinto the recesses of nature\u201d (Shelley, 1818).\n\n==\n\nGinn 2013:\n\nTitle: Mary Shelley's Frankenstein: exploring neuroscience, nature, and nurture in the novel and the films.\n\nAbstract: The story of Victor Frankenstein's quest to conquer death produced a legacy that has endured for almost 200 years. Powerful in its condemnation of the scientist's quest to achieve knowledge at any cost, Mary Shelley's Frankenstein is one of the most enduring novels of all time. It has never been out of print and has been translated to both stage and screen many times since its \"birth.\" Numerous novels, short stories, and scripts have drawn upon Shelley's primary theme: the creation of a living organism from the dead, dying, and decaying body parts of human beings. Although Mary does not provide details of the animation process, particularly in her first edition, the process has been explored with a great deal of imagination and originality in the various cinematic portrayals of the story. Equally important as the theme of the scientist's quest for knowledge is the role that a creator plays in the life of its creation. Mary Shelley's novel pondered on how rejection would affect the offspring of such \"unnatural\" origins. In keeping with the \"scientific\" basis of the Creature's birth, cinematic portrayals attempted to provide a scientific rationale for the Creature's descent into madness and its evil behavior. From Robert Florey's initial script for the 1931 film directed by James Whale to the more recent films and television series, an abnormal brain is considered to be the cause of the madness and malignity of the Creature.\n\n==\n\nHoeveler 2003:\n\nTitle: Frankenstein , feminism, and literary theory\n\nAbstract: Cave ab homine unius libri , as the Latin epigram warns us: \u201cbeware the author of one book.\u201d Frankenstein has so overshadowed Mary Shelley's other books in the popular imagination that many readers believe - erroneously - that she is a one-book author. While this is decidedly not the case, Frankenstein has figured more importantly in the development of feminist literary theory than perhaps any other novel, with the possible exception of Charlotte Bronte's Jane Eyre . This essay will discuss the major feminist literary interpretations of the novel, beginning with Ellen Moers's landmark reading in Literary Women and then move to the more recent approaches taken by critics engaged in post-colonial theory, cultural studies, queer theory, and disability studies. In the process we will explore the provocative claim made by Fred Botting, who noted, \u201c Frankenstein is a product of criticism, not a work of literature.\u201d Let us begin by describing briefly the three major strands in feminist literary criticism: American, French, and British. American feminist literary critics (represented best perhaps by Sandra Gilbert and Susan Gubar) understand \u201cwomen\u2019s experiences\u201d to be the basis of the differences in women\u2019s writings. American feminist critics of the 1970s and 1980s tended to discuss recurring patterns of themes (i.e., the valorization of the quotidian value of domestic life, human community and relationships) or imagery (i.e., houses, claustrophobia, food and eating disorders, insanity, fetishizing of clothing, body image, etc.) in works by women. Led by the pioneering work of Elaine Showalter, such critics also took pains to rediscover \u201clost\u201d women writers and to demonstrate the continuities of a women\u2019s literary tradition.\n\n==\n\nHatch 2008:\n\nTitle: Disruptive affects: shame, disgust, and sympathy in Frankenstein\n\nAbstract: Many critical approaches to Frankenstein have relied on an ethical reading of the Creature\u2019s predicament \u2013 that he is benevolent within and ugly without. The emotional responses to the Creature\u2019s condition that the novel evokes are illuminated when cast in psychoanalytical light. This article traces how shame and disgust, as theorized by Silvan Tomkins, operate in the novel, and how these responses disrupt or undermine the function of sympathy, as described by Adam Smith. In doing so, the article attempts to show that ethical readings of the novel \u2013 readings which participate in both Enlightenment ideas of sympathy and Romantic ideas of the \u201cOther\u201d \u2013 remain problematic because of the enduring presence of shame and disgust throughout the novel. The novel remains as powerful as it is partly because of the irreconcilability of the affects of shame and disgust with the ethical operation of sympathy.\n\n==\n\nRauch 1995:\n\nTitle: The Monstrous Body of Knowledge in Mary Shelley's \"Frankenstein\"\n\nAbstract: i. The editor of this preface is not specified; the text is a translation of Giovanni (John) Aldini's An Account of the Late Improvements of Galvanism with a series of Curious and Interesting Experiments Performed Before the Commissioners of the French National Institute and Repeated Lately in the Anatomical Theatres of London (London: Cuthell and Martin, J. Murray, 1803). 2. The literature on Frankenstein is extensive. Several studies are worth noting here to give a sense of the breadth of the work that has been done. Ann Mellor has contributed significandy to feminist readings of the novel; her study is strongly grounded in textual analysis as well as close attention to contemporary events in science. Ellen Moers's early essay is a cornerstone of feminist thinking with respect to the novel. See also Mary Poovey's The Proper Lady and the Woman Writer (Chicago: U of Chicago P, 1984) and Margaret Homans' Bearing the Word (Chicago: U of Chicago P, 1986) for analyses of Shelley and of the novel. For psychoanalytic readings of the text see William Veeder's Mary Shelley & Frankenstein:",
        "final_summary": "Davies (2004) interprets \"Frankenstein\" as an early text on the ethics of human subject research, a theme that may be challenging for students to grasp. Bissonette (2010) suggests that students often struggle with the concept of the monster as a pedagogy of alienation, which is crucial for critical thinking. The philosophical contexts and subtexts in the novel, as analyzed by Ozherelyev (2020), can be complex, drawing from various philosophers' concepts. Debnath (2019) discusses the dichotomies of culture/nature and self/other, which can be difficult to fully comprehend. Ginn (2013) delves into the enduring themes of neuroscience, nature, and nurture in \"Frankenstein,\" which may be challenging for students to imagine. Hoeveler (2003) highlights the significant role of feminist literary interpretations of the novel, a perspective that students may find difficult to understand. Hatch (2008) explores the disruptive affects of shame, disgust, and sympathy in the narrative, which can be complex to grasp."
    },
    {
        "query": "What is the relationship between music and task performance+",
        "paper_list_string": "Chen 2022:\n\nTitle: The Effect of Background Music on Young People (10-24)\u2019 Task Performance\n\nAbstract: Current studies on the effects of background music are controversial and lack consensus. This paper analyzes a large number of previous studies on the relationship between background music and task performance. The purpose of this paper is to explore the best ways for young people to play background music and maximize the positive impact of background music on their learning/cognitive performance. The conclusion was that older students were better suited to playing background music while studying. Appropriate fast-major music with no Lyrics can have the best positive impact on task performance.\n\n==\n\nChaparro 2008:\n\nTitle: Does Background Music Impact Computer Task Performance\n\nAbstract: Summary: The effects of music on performance on a computer-mediated problem-solving task were examined. Participants completed the task in anonymous dyads as they were exposed to either Classical music, Punk music, or No Music. Results indicate that those in the Classical music condition performed better on the problem solving-task than those in the Punk music or No Music conditions. However, those listening to the Classical music offered more off-task comments during the task than those listening to No Music. Implications for website designers are discussed.\n\n==\n\nArboleda 2021:\n\nTitle: A stressful task is when to turn off the music: Effect of music on task performance mediated by cognitive effort\n\nAbstract: Studies on the effect of music on task performance are contradictory about this relationship\u2019s direction and valence. Task characteristics may be accounting for these inconclusive findings. Thus, this study employs effort to mediate music\u2019s effect on task performance (objective and perceived) under a stressful decision-making process. This is a between-group experiment with three conditions: slow-tempo music, fast-tempo music, or no music. We designed a computer web interface, where participants did a stressful task. Results demonstrated that participants made a strong effort under the conditions with music. Hence, turning the music off under stressful activities is favorable in terms of performance. The article contributes to understanding the interaction between music and task performance, expanding the discussion within a stressful task.\n\n==\n\nRauscher 1994:\n\nTitle: Music and Spatial Task Performance: A Causal Relationship.\n\nAbstract: MFOI/PCO2 Plus Postage. Cognitive Ability; Early Childhood Education; *Early Experience; Fine Arts; Humanities; *Motion; *Music; *PerceptuAl Development; Psychomotor Skills; *Spatial Ability; Young Children This research paper reports on testing the hypothesis that music and spatial task performance are causally related. Two complementary studies are presented that replicate and explore previous findings. One study of college students showed that listening to a Mozart sonata induces subsequent short-term spatial reasoning facilitation and tested the effects of highly repetitive music on spatial reasoning. The second study extends the findings of a preliminary pilot study of 1993 which suggested that music training of three-year-olds provides long-term enhancements of nonverbal cognitive abilities already present at significant leveli in infants. The paper concludes with a discussion of the scientific and educational implications, further controls, and future research objectives. Contains 10 references. (EH) Reproductions supplied by EDRS are the best that can be made from the original document. V i Date and Time of Presentation: Saturday, August 13, 1994, Ilam Westin Bonaventure Los Angeles, Lobby Level, Santa Barbara Room B Music and Spatial Task Performance: A Causal Relationship Frances H. Rauscher, Gordon L. Shaw, Linda J. Levine, Katherine N. Ky University of California, Irvine Eric L Wright Irvine Conservatory of Music Presented at the American Psychological Association 102nd Annual Convention in Los Angeles, CA August 12-16, 1994 BEST COPY AVAILABLE 4,0 7 f./.. I. . /, To THE DuC,ATI:!0.4AL RESOuHcES Nf ORMATIoN (.;1NTE4\n\n==\n\nGeethanjali 2016:\n\nTitle: Enhancement of Task Performance Aided by Music\n\nAbstract: Our study demonstrates the enhancement of task performance aided by music. Task performance and associated physiological changes were studied in participants who listened to music (Indian classical instrument or Indo jazz). Their mood and their perceived emotions were evaluated using positive and negative affect schedule and self assessment manikin scales, and their pulse rate was measured. Visual Go and No-go trials were given as task stimuli and task performance was measured. Changes in the mean pulse rate were significantly low (P < 0.05) during Indo jazz. We conclude that Indian classical instrument and Indo jazz induced positive effects and enhanced task performance.\n\n==\n\nHallam 2002:\n\nTitle: The Effects of Background Music on Primary School Pupils' Task Performance\n\nAbstract: Research on the effects of background music has a long history. Early work was not embedded within a theoretical framework, was often poorly conceptualised and produced equivocal findings. This paper reports two studies exploring the effects of music, perceived to be calming and relaxing, on performance in arithmetic and on a memory task in children aged 10-12. The calming music led to better performance on both tasks when compared with a no-music condition. Music perceived as arousing, aggressive and unpleasant disrupted performance on the memory task and led to a lower level of reported altruistic behaviour by the children. This suggests that the effects of music on task performance are mediated by arousal and mood rather than affecting cognition directly. The findings are discussed in relation to possible practical applications in the primary school and the home.\n\n==\n\nWolfe 1983:\n\nTitle: Effects of Music Loudness on Task Performance and Self-Report of College-Aged Students\n\nAbstract: This study examined the effects of the loudness of background music on task performance of college students. In addition, students' perceptions of the music and the effect these perceptions had on performance were investigated. Two hundred undergraduate nonmusic majors were randomly assigned to one of four experimental groups: (1) task only (no music); (2) task plus background music presented at 60\u201370 dB (all decibel measurements here are on the C scale); (3) task plus background music presented at 70\u201380 dB; and (4) task plus background music presented at 80\u201390 dB. The performance task consisted of computing mathematic problems, and a questionnaire was subsequently administered to the task plus background music groups to assess perceptions of the background environment. Results indicated that (1) the experimental conditions had no significant effect on task performance; (2) the majority of subjects did recognize the music and were able to identify some of the selections; and (3) subjects in the 80\u201390 dB group perceived the music as most distracting, with the majority of these same subjects reporting that the musical element of loudness seemed to be the contributing factor. Findings are discussed in relation to current music and noise research, and implications for educational and therapeutic applications are given.\n\n==\n\nDalton 2008:\n\nTitle: Effects of noise and music on human and task performance: A systematic review\n\nAbstract: The purpose of the present paper was to review the literature to develop an understanding of the effects of noise and music on human performance. The second purpose was to study the effects of music on a commonly performed task that is frequently accompanied by background music: driving. Background noise not only affects public health, but it also negatively affects human performance in such tasks as comprehension, attention, and vigilance. However, some studies have indicated that noise exposure may not affect simple vigilance. Despite music's distinct difference from noise it too affects human performance negatively and positively. The results are inconclusive on the effects of music and task performance. More specifically, the effects of music on driving performance are quite similar to that of noise on task performance. Music seems to alleviate driver stress and mild aggression while at times facilitating performance. However, during other conditions of music, driving performance is impaired. Different aspects of sound (i.e. volume, type, tempo) impact human performance differently. It is still unknown which aspect (music or noise) affects task performance to a greater degree.",
        "final_summary": "The relationship between music and task performance is multifaceted, influenced by factors such as music type and task nature. Chen 2022 found that background music, specifically fast-major music without lyrics, positively impacts older students' task performance. Chaparro 2008 discovered that classical music improved performance on a computer-mediated problem-solving task, though it led to more off-task comments. Arboleda 2021 suggests that music's effect on task performance may be mediated by the cognitive effort required, with music potentially hindering performance during stressful tasks. Geethanjali 2016 demonstrated that Indian classical instrument or Indo jazz music can enhance task performance. Hallam 2002 reported that calming music improved children's performance on arithmetic and memory tasks, while arousing, unpleasant music disrupted performance. Wolfe 1983 found no significant impact of music loudness on task performance, but music was perceived as most distracting at 80-90 dB. Dalton 2008 concluded that the effects of music on task performance are inconclusive, with different aspects of music, such as volume, type, and tempo, potentially having varying impacts (Chen 2022; Chaparro 2008; Arboleda 2021; Geethanjali 2016; Hallam 2002; Wolfe 1983; Dalton 2008)."
    },
    {
        "query": "there is a pressing need for increased Nigerian government investment in the development and enhancement of physical and technological infrastructure for Primary Health Centers (PHCs). This includes ensuring a stable supply of electricity, robust internet infrastructure, access to computers, and cutting-edge software.",
        "paper_list_string": "Abdulraheem 2012:\n\nTitle: Primary health care services in Nigeria: Critical issues and strategies for enhancing the use by the rural communities\n\nAbstract: Quality health is a fundamental right of all Nigerian citizens. While primary health care (PHC) centres are relatively uniformly distributed throughout\u00a0local government areas (LGAs) in Nigeria, the rural people tend to underuse the basic health services. This article examines some cross cutting issues in PHC and outlines strategies to enhance the utilization of health services by rural people. The responsibility for perpetuating the existing low use of PHC services should be held by PHC policy makers and LGA. Responsible health personnel can build a new social order, based on greater equity and human dignity, in which health for all by the year 2015, including that of rural populations, will no more be a dream but a reality. Capacity building and empowerment of communities through orientation, mobilization and community organization as regards training, information sharing and continuous dialogue, could further enhance the utilization of PHC services by rural populations. \n \n \u00a0 \n \n Key words: Healthcare, services, strategies, use, rural.\n\n==\n\nBenson 2011:\n\nTitle: Hospital Information Systems in Nigeria: A Review of Literature\n\nAbstract: This literature review was developed to examine empirically the factors hindering adoption of hospital information systems in Nigeria. The study was focused on the perceived paucity of health information technology policy in Nigeria and the causes of poor implementation of hospital information systems in the country. The findings of the literature review highlighted hindrances to the adoption of hospital information systems to include; the high cost of full implementation of a hospital information system, inadequate human capital, corruption, and problems associated with poor infrastructure in Nigeria. The recommendations were that, the Nigerian government needs to provide stable electricity, basic communication infrastructures, and Internet access to boost private initiatives in the adoption of health information technology across the country.\n\n==\n\nIbrahim 2006:\n\nTitle: Public-Private Partnership and sustainable primary healthcare facilities in Nigeria\n\nAbstract: The Nigerian healthcare system is divided into the primary, secondary and tertiary levels of care, which are under the three tiers of government (local, state and federal) with some overlapping of responsibilities. The Federal Ministry of Health (FMOH) develops policies and physical structures, and supervises the operations of the healthcare system. The Primary Health Care (PHC) level has been identified as the appropriate setting to tackle 90 - 95% of major causes of morbidity and mortality because of its proximity to over 80% of the populace. At the PHC level, the services provided include integrated preventive, promotive and community development activities. However, the implementation of PHC in Nigeria has lacked impact and sustainability, thus leading to the research discussed in this paper which is aimed at proposing an innovative procurement system for sustaining PHC facilities in Nigeria. The proposed system is focused at developing and encouraging a new market for investment in primary care and community-based facilities and services. It is a form of Public\u00ad-Private Partnership (PPP) that enjoys national support but with local control. One of the central objectives is the idea of bringing together all the various local stakeholders, interests and users that comprise the local health economy. The arrangement is expected to offer considerable social and economic benefits while satisfying the fundamental philosophy of PHC.\n\n==\n\nIbeneme 2020:\n\nTitle: Strengthening capacities among digital health leaders for the development and implementation of national digital health programs in Nigeria\n\nAbstract: Discussions on the use of digital health to advance health have continued to gain traction over the past decades. This is important considering the rising penetration of mobile phones and other digital technologies and the opportunity to leverage those digital and electronic health methods and innovations to accelerate Universal Health Coverage (UHC) and the health Sustainable Development Goals (SDGs). In Nigeria, however, the full benefits of digital technologies to strengthen the health systems are yet to be fully harnessed due to critical challenges in the sector. These challenges include but not limited to weak health systems governance, weak infrastructural investments, inadequate resources, weak human resource capacity, high cost of scaling-up and coordination issues among others. Lack of systems thinking, and design have significant impact on coordination of efforts and has resulted in the fragmentation and non-interoperability among various applications. To discuss these challenges and propose the way forward for rapid sustainable, scalable and cost-effective deployment of digital health in Nigeria, a digital health capacity development workshop was held in Abuja and across the six geo-political zones of Nigeria from 20th \u2013 30th November 2019. This paper documents key conclusions and achievements at the workshop. The workshop was organized around eleven modules and seven thematic areas which explored the Nigerian digital health governance and coordinating mechanisms in view of its status, leadership, financing and deployment for effective service delivery. It was attended by over 100 participants from varied background including Ministries of Health, Ministries of Communications and Digital Economy, International Organizations, Operators, Civil Society, Academia and Private Sector Experts. The workshop resolved that while digital health technologies offer profound opportunities to strengthen Nigerian health systems for UHC and the health SDGs, there should be a move from donor-driven pilot projects to robust, sustainable, cost-effective and nationally owned projects. This will involve a people-centered approach that should be demand-driven and not supply-driven to avoid wasting time on ineffective interventions, duplication of efforts and wastage of scarce health resources. Government ownership and leadership was identified as critical for sustainable financing and effective scale up of Digital Health projects in Nigeria. The DH capacity development workshop was a good forum to deliberate important issues regarding sustainable and cost-effective DH solutions that could be scaled to strengthen health service delivery in Nigeria. Insightful ideas for scaling DH in Nigeria and other related settings emanated from the workshop, necessitating the need for a focused government commitment and leadership in institutionalizing digital health in Nigeria.\n\n==\n\nFronczak 2015:\n\nTitle: Dual indices for prioritizing investment in decentralized HIV services at Nigerian primary health care facilities\n\nAbstract: Decentralizing health services, including those for HIV prevention and treatment, is one strategy for maximizing the use of limited resources and expanding treatment options; yet few methods exist for systematically identifying where investments for service expansion might be most effective, in terms of meeting needs and rapid availability of improved services. The Nigerian Government, the United States Government under the President's Emergency Plan for AIDS Relief (PEPFAR) program and other donors are expanding services for prevention of mother-to-child transmission (PMTCT) of HIV to primary health care facilities in Nigeria. Nigerian primary care facilities vary greatly in their readiness to deliver HIV/AIDS services. In 2012, MEASURE Evaluation assessed 268 PEPFAR-supported primary health care facilities in Nigeria and developed a systematic method for prioritizing these facilities for expansion of PMTCT services. Each assessed facility was scored based on two indices with multiple, weighted variables: one measured facility readiness to provide PMTCT services, the other measured local need for the services and feasibility of expansion. These two scores were compiled and the summary score used as the basis for prioritizing facilities for PMTCT service expansion. The rationale was that using need and readiness to identify where to expand PMTCT services would result in more efficient allocation of resources. A review of the results showed that the indices achieved the desired effect\u2014that is prioritizing facilities with high need even when readiness was problematic and also prioritizing facilities where rapid scale-up was feasible. This article describes the development of the two-part index and discusses advantages of using this approach when planning service expansion. The authors' objective is to contribute to development of methodologies for prioritizing investments in HIV, as well as other public health arenas, that should improve cost-effectiveness and strengthen services and systems in resource-limited countries.\n\n==\n\nAbosede 2014:\n\nTitle: Strengthening the Foundation for Sustainable Primary Health Care Services in Nigeria\n\nAbstract: Since 1978\u2019s Alma-Ata Declaration on Primary Health Care (PHC) with the popular slogan \u201cHealth for All by the Year 2000\u201d and subsequently the Millennium Development Goals (MDGs) of the Year 2000, Nigeria has made 3 major attempts to establish a sustainable PHC system and some of the reasons for failure of the first two have been identified. Even though there is some degree of general improvement, compared to other countries in the world, Nigeria\u2019s indices in spite of international aids have remained poor {Neonatal Mortality Rate (40/000), Under-five Mortality Rate (157/000) and Maternal Mortality Ratio (545/100000). The aim of this descriptive, interventional study is to highlight positive steps taken by governments to re-establish and revitalise PHC, especially by empowering communities, the challenges, and lessons learned as potentials for improvement of a universal, qualitative PHC system in Nigeria. The Health Reform Laws and Strategic Development Plans emphasize commitment to the promotion and protection of the health of the people as an essential service for sustained socio-economic development and better quality of life. Community structures (Local Government Health Authorities\u2013LGHAs, Ward Health Committees\u2013 WHCs and Health Facility Management Teams) backed by the law were either newly established or reconstituted and empowered. The study describes measures taken to strengthen the weak foundation of the PHC system in Lagos State (the study\u2019s index state), the outputs, outcomes and potential impacts. The challenges and lessons learned from dealing with them can be projected for improved national coverage\n\n==\n\nEgoeze 2014:\n\nTitle: An Evaluation of ICT Infrastructure and Application in Nigeria Universities\n\nAbstract: The need for adequate ICT infrastructure/facility in higher education institutions cannot be overemphasized, even as availability and utilization of these facilities are at times the indices for rating universities. In this descriptive survey study, ICT infrastructure and the extent of usage in Nigeria universities were investigated. Questionnaire was the instrument used for gathering information and based on related literature. A total of 452 respondents comprised of students, lecturers and administrators randomly selected from a total of 15 universities from different regions of Nigeria participated. Data collected were analyzed using mean statistic analysis and analysis of variance (ANOVA).\n\n==\n\nMusa 2001:\n\nTitle: Indigenous hospital technology in Nigeria: problems and recommendations.\n\nAbstract: In Nigeria, after gaining independence there arose a national desire for industrialization in order to reduce the high cost of sustaining national development. This desire has its accompanying problems which are examined with reference to indigenous hospital technology. Solutions to these problems are also suggested.",
        "final_summary": "The papers collectively highlight the pressing need for increased Nigerian government investment in the development and enhancement of physical and technological infrastructure for Primary Health Centers (PHCs). Abdulraheem (2012) emphasizes the underutilization of PHC services by rural communities and suggests strategies such as capacity building and community empowerment to enhance utilization. Benson (2011) identifies hindrances to the adoption of hospital information systems, including inadequate infrastructure, and recommends that the government provide stable electricity, basic communication infrastructures, and internet access. Ibrahim (2006) proposes a Public-Private Partnership (PPP) model to sustain PHC facilities, emphasizing the need for local stakeholder involvement and government support. Ibeneme (2020) discusses the challenges in harnessing the full benefits of digital health technologies in Nigeria, including weak infrastructure and human resource capacity, and emphasizes the importance of government ownership and leadership in scaling up digital health projects. Fronczak (2015) presents a systematic method for prioritizing investment in decentralized HIV services at primary health care facilities, considering both facility readiness and local need, highlighting the need for efficient resource allocation. Abosede (2014) highlights the attempts to establish a sustainable PHC system in Nigeria and the challenges faced, emphasizing the importance of empowering communities and strengthening the foundation of the PHC system. Egoeze (2014) evaluates the ICT infrastructure and usage in Nigerian universities, emphasizing the need for adequate ICT facilities in higher education institutions. Musa (2001) discusses the problems and recommendations related to indigenous hospital technology in Nigeria. Overall, the papers underscore the need for increased government investment in physical and technological infrastructure for PHCs, including stable electricity, robust internet infrastructure, access to computers, and cutting-edge software, to enhance healthcare services in Nigeria."
    },
    {
        "query": "skill model project manager",
        "paper_list_string": "Bhatia 2019:\n\nTitle: Project Portfolio Management (PPM) in Education Domain Using Skill Matcher Model\n\nAbstract: Project Portfolio Management has spurred significant research due to its importance in systematic management of corporate, government and academic projects. In this paper, we propose a skill matcher model for PPM which optimally matches students with various skills to a set of projects with specific skills requirements. We extract the skills required to execute a project from its description. We then generate a concept lattice of projects-skills by applying Formal Concept Analysis. The concepts so generated are assigned to students in an optimal manner by invoking the Stable Marriages Algorithm, keeping in view certain matching criteria. The experimental results show that the project teams so formed consists of all the skills required for completion of the project.\n\n==\n\nDuncan 1992:\n\nTitle: Competency model for the project managers of technical projects\n\nAbstract: Traditional job description techniques were developed to support compensation decisions for hourly wage earners in a manufacturing environment. Their resultant focus on activities performed on the job works well in this environment where the ability to perform the activity adequately is objectively verifiable by testing and observation. Although many organizations have adapted these techniques for salaried employees and service environments, the focus on activities performed has never been satisfactory. For example, stating that a project manager `prepares regular project status reports' tells us little about what to look for in a potential project manager or how to determine if a practicing project manager is ready for additional responsibilities. The concept of a `competency model' has been developed within the last decade to address this shortcoming. Competency models focus on what skills are needed to perform the tasks defined by the job description. For example, a project manager must be able to communicate well both orally and in writing in order to `prepare regular project status reports.'\n\n==\n\nBerg 2007:\n\nTitle: Mental Models in Project Management Coaching\n\nAbstract: Abstract: Project managers have always been given the highest priority to customer needs and demands; however, tougher market competition has created a stronger focus on knowledge, skills, and capability to lead a project team toward success. Traditionally, managers have emphasized knowledge and skills within technical disciplines as a key factor managing projects. Today we see a shift toward more emphasis on organizational and human aspects of project work. An effective tool for meeting this challenge is project management coaching. This article focuses on what coaching is about, when to use coaching, different types of coaching, the coaching process, and coaching roles. A case study is presented in the article, and analysis of data shows that coaching can help the project manager acquire necessary skills, achieve desired results, and contribute to career development.\n\n==\n\nSunindijo 2015:\n\nTitle: Project manager skills for improving project performance\n\nAbstract: Project managers have multifaceted responsibilities that significantly affect project success. Previous research has identified four skills of effective project managers, namely, conceptual, human, political, and technical skills, along with their 16 skill components. The aim of this paper is to determine skill components that influence time, cost, and quality performance in construction projects. Data were collected from 107 project managers using a questionnaire survey method. The analysis results show that interpersonal influence has positive relationship with project time performance. Project cost performance is influenced by four skill components, namely, emotional intelligence, interpersonal skill, apparent sincerity, and budgeting. Lastly, project quality performance is affected by eight skill components, which include visioning, emotional intelligence, interpersonal skill, transformational leadership, interpersonal influence, apparent sincerity, quality management, and document and contract administration. Construction organisations may use the finding as a guideline to appoint project managers with the 'right' skill profile or to focus their human resource development on skills that are important for project success.\n\n==\n\nHanna 2018:\n\nTitle: A data-driven approach for identifying project manager competency weights\n\nAbstract: Competent project managers (PMs) are the backbone of any construction project. It is extremely important to constantly develop and enhance their competencies. However, to establish effective training and development plans for PMs, the relative importance of the key competencies that define a PM\u2019s performance should be first understood. Instead of subjectively weighting the relative importance of differing competencies, this paper aims at developing an automated model that uses real-life data to compute the PM competency weights. The rationale behind the model is to maximize the distance in a higher dimensional space between average and exceptional PM performances. The model solves an eigenvalue problem, and identifies a single data-based weight for each competency. The model is generic and can be applied to various research settings to alleviate the problems associated with opinion-based assessment and reduce individuals\u2019 subjectivity. Findings within this paper reveal the most critical competencies that ...\n\n==\n\nChildre 1998:\n\nTitle: Project Management Skills\n\nAbstract: 1. Project management skills are important to develop because occupational and environmental health nurses are increasingly asked to implement and manage health related projects and programs. 2. Project management is the process of planning and managing project tasks and resources, and communicating the progress and results. This requires the coordination of time, tasks, equipment, people, and budget. 3. Three main critical skill areas are needed to be an effective project manager: behavioral skills such as negotiation, conflict resolution, and interpersonal problem solving; use of project management tools to manage project tasks and resources; and effective communication skills.\n\n==\n\nNapier 2009:\n\nTitle: IT project managers' construction of successful project management practice: a repertory grid investigation\n\nAbstract: Although effective project management is critical to the success of information technology (IT) projects, little empirical research has investigated skill requirements for IT project managers (PMs). This study addressed this gap by asking 19 practicing IT PMs to describe the skills that successful IT PMs exhibit. A semi\u2010structured interview method known as the repertory grid (RepGrid) technique was used to elicit these skills. Nine skill categories emerged: client management, communication, general management, leadership, personal integrity, planning and control, problem solving, systems development and team development. Our study complements existing research by providing a richer understanding of several skills that were narrowly defined (client management, planning and control, and problem solving) and by introducing two new skill categories that had not been previously discussed (personal integrity and team development). Analysis of the individual RepGrids revealed four distinct ways in which study participants combined skill categories to form archetypes of effective IT PMs. We describe these four IT PM archetypes \u2013 General Manager, Problem Solver, Client Representative and Balanced Manager \u2013 and discuss how this knowledge can be useful for practitioners, researchers and educators. The paper concludes with suggestions for future research.\n\n==\n\nPerce 1998:\n\nTitle: Project management skills.\n\nAbstract: 1. Project management skills are important to develop because occupational and environmental health nurses are increasingly asked to implement and manage health related projects and programs. 2. Project management is the process of planning and managing project tasks and resources, and communicating the progress and results. This requires the coordination of time, tasks, equipment, people, and budget. 3. Three main critical skill areas are needed to be an effective project manager: behavioral skills such as negotiation, conflict resolution, and interpersonal problem solving; use of project management tools to manage project tasks and resources; and effective communication skills.",
        "final_summary": "The papers collectively provide insights into the skills required for a project manager. Bhatia 2019 proposes a skill matcher model for project portfolio management, which optimally matches students' skills to project requirements. Duncan 1992 discusses the importance of competency models, focusing on the skills needed to perform project management tasks effectively. Berg 2007 emphasizes the shift towards organizational and human aspects of project work and highlights project management coaching as a tool for skill development. Sunindijo 2015 identifies various skill components that influence project performance, including interpersonal influence, emotional intelligence, and quality management. Hanna 2018 develops a data-driven model to compute project manager competency weights, maximizing the distance between average and exceptional performances. Childre 1998 emphasizes the critical skill areas of behavioral skills, project management tools, and effective communication for effective project management. Napier 2009 explores skill requirements for IT project managers, identifying categories such as client management, problem-solving, and team development. Overall, these papers highlight the importance of a diverse range of skills, including communication, leadership, problem-solving, and technical expertise, for successful project management."
    },
    {
        "query": "Higher Education and Economic Development in united arab emirates",
        "paper_list_string": "Nicks-McCaleb 2005:\n\nTitle: The Impact of State Funded Higher Education on Neighbourhood and Community in the United Arab Emirates.\n\nAbstract: This paper focuses on the provision of higher education in the United Arab Emirates (UAE) and the issues surrounding strategies employed by institutions to prepare tertiary level students for careers in the global economy. Dramatic growth and development in the Arabian Gulf region over the past two decades has made fundamental changes in the education system necessary. There has been a shift in focus from more traditional education and delivery methods, to contemporary approaches to support student learning with the emphasis on preparing students for careers in the knowledge economy. These changes in higher education, and its widespread provision, have impacted on the UAE and its economy in particular. This paper reviews the provision of higher education, its role in the rapidly developing society and economy of the UAE, and how the community is contributing to and benefiting from emerging partnerships.\n\n==\n\nHaddad 2020:\n\nTitle: The Impacts of Education on the Sustainable Development in United Arab Emirates: A Simultaneous Equations Approach\n\nAbstract: The United Arab Emirates (UAE) is an Asian country that consists of seven emirates. UAE is the eighth largest oil producer in the world which is the main source used for development. UAE is a member of the Gulf Cooperation Countries (GCC) council. UAE is one of the world\u2019s most open and successful economies. UAE maintains a free-market economy and is one of the most politically stable and secure in the region. All together ensures that UAE has a robust competitive edge since it is the region\u2019s premier commercial hub and the second largest economy. In addition, this economic environment encourages the establishment of private universities and branches of well-known foreign universities in UAE. In this study, the impact of education on the sustainable economic development is measured by human development index (HDI) using simultaneous equations approach of two equations. The results show that there are more than one million students enrolled in primary and secondary schools and more than 60 thousand enrolled in territory education in more than 561 programs distributed over 70 institutions, around 40% of them are noncitizens. The regression results show that there are impacts of education on the sustainable economic development at UAE. These results emphasize the role of education in creating sustainable development in UAE, in particular, as well as the rational policy of the UAE and confirm the results of other studies.\n\n==\n\nAshour 2016:\n\nTitle: Factors favouring or impeding building a stronger higher education system in the United Arab Emirates\n\nAbstract: ABSTRACT The paper examines the inherent strengths and weaknesses of the higher education system in the United Arab Emirates (UAE). Using a literature review, three elements were examined: its development and structure, quality, and research capacity. The findings reveal that the UAE\u2019s higher education sector has witnessed remarkable growth since 1997, guided by the state\u2019s policy commitment, and few restrictions on foreign universities opening branches in the UAE. This situation has led to substantial growth in the supply of higher education opportunities for the Emirati and expatriate population. Despite the UAE having achieved a high ranking for \u2018quality indicators\u2019 on the global level in the World Economic Forum\u2019s Competitiveness Reports, the quality of graduates, education, and research engagement is still debatable. Significant efforts are needed to ensure that these critical issues, as well as the understudied expansion of higher education institutions, are being addressed to erase the misalignment with the UAE\u2019s strategic goals.\n\n==\n\nSpranza 2016:\n\nTitle: Globalization of Higher Education in United Arab Emirates\n\nAbstract: This article explores the educational history of the United Arab Emirates, a young and vastly wealthy nation located in the Middle East. A nation for just over 40 years, its leaders have struggled with educating and supporting its native population and managing the massive infusion of immigrants created by the progressive job markets established within its borders. With a mix of mandated free education for natives and a lucrative educational investment market from various other nations, the educational structure in the UAE has become a unique system that attracts students and faculty from around the world. The opulent landscapes of the primary cities of Dubai and Abu Dhabi create a truly global educational environment poised to help lead the region into the future. As a country, the UAE population is highly concentrated in its two major cities of Abu Dhabi and Dubai, with 70 percent of the total population living in these locations. The oil industry boom in the late 1970\u2019s drew a large number of immigrant workers (initially primarily from India and Pakistan) and the trend of attracting droves of migrant workers has continued throughout the last 40 years. Today, approximately 88 percent of the total UAE population are expatriates drawn to the country for the labor market, leaving only 12 percent of the population as native Arabs (NQA, 2013). Currently, almost 65 percent of the national population is under the age of 24 so the country\u2019s growth in its native peoples is on the upswing. This presents the opportunity to cultivate a strong educated national workforce that could help balance the UAE\u2019s reliance on expatriates in the labor market. Education has been a priority for the UAE government since the country\u2019s inception in 1971 and the leaders have invested highly in the development and promotion of all levels of education, from nursery schools to doctoral level programs. However, the UAE government had a steep sand dune to climb since the national literacy rates in 1975 were 54 percent for men and only 31 percent for women. The UAE constitution and Federal Law No. 11 of 1972 established compulsory primary education and guaranteed free education at all levels for all UAE nationals. However, secondary education through grade 12 or age 18 was not made compulsory until 2012. The end result of the concentration on education for its citizens was shown in a 2010 report that indicated the national literacy rates were 89 percent for men and 91 percent for women (UNESCO, 2012). The government-sponsored higher education system was started in 1975 when the young nation opened the UAE University in the city of Al Ain near the border with Oman. UAEU now has six campuses across the country. The largest national institution, the Higher Colleges of Technology, was founded in 1988 and now has 17 campuses across the country. The third and newest national institution, Zayed University, was founded in 1998 and has two campus locations. The Ministry of Higher Education and Scientific Research (MHESR) governs and sets policy for all institutions of higher education across the country, but several Emirates also have governing bodies that regulate the education sector in their own territory. When the MHESR was created, it was established on four policy decisions that would guide its efforts. These decisions included: 1. The UAE would build and operate its own universities; 2. Qualified faculty that 17 Spranza: Globalization of Higher Education in United Arab Emirates Published by Digital Commons@Georgia Southern, 2016 meet international standards would be employed; 3. Instruction would be predominantly in English; and 4. Education was to be free for all qualified Emiratis, and would include women (MHESR, 2007). The Ministry also includes the Commission of Academic Accreditation (CAA) which certifies all institutions of higher education much like the SACS organization does here in the southern part of the United States. According to the CAA, there are currently 116 higher education institutions in the UAE with a cumulative enrollment of 116,912 across 644 accredited academic programs (NQA, 2013). Since there are only three federally-sponsored institutions out of the 116 qualified by the CAA, all of the other higher education institutions are international organizations or institutions that have formed partnerships or joint ventures with either the federal or local governments to establish campuses in the UAE. Due to the population concentration in Dubai and Abu Dhabi, most of the foreign institutions can be found in these two cities, with 19 institutions in Abu Dhabi and a mass of 62 institutions in Dubai (MHESR, 2015). Most of the private institutions can be found in one of the 15 economic \u201cfree zones\u201d established by the UAE federal government or local governments of Dubai and Abu Dhabi. The Free Zones are special areas in the city that offer tax exemptions and other incentives to foreign companies to encourage capital investments that benefit both parties, creating a true global market in these cities within the city. The Dubai Knowledge Village and the Dubai International Academic City are the primary locations of nearly all of the private institutions of higher education, and include institutions from the United States, the United Kingdom, France, Canada, Australia, and India. The institutions found in the free zones are governed by the Dubai Knowledge and Human Development Authority (KHDA) and further managed by the University Quality Assurance International Board (UQAIB) to ensure all institutions meet the benchmarks of quality and accreditation established by the UAE government. Given the mandate that all levels of education would be free to its citizens, the government of UAE spends over a quarter of the federal budget on education. In 2009, the education budget was approximately $10 billion, or 28 percent of the federal budget for that year. In contrast, the participation rate for eligible citizens in the higher education system is still considerably lower than that of the U.S. or European countries, with only 25 percent of eligible Emiratis attending as compared to 71 percent of eligible students in North America and Western Europe (UNESCO, 2009). However, the pending \u201cboom\u201d of youth coming-of-age (65 percent of national population is under the age of 24) in the next decade has the government of UAE looking for ways to keep up the funding levels for education to maintain the quality and meet the demand from its citizens. Costs per student has already impacted the funding model given the inflation rate and increased demand for matriculation, and the government has had to freeze acceptance rates in order to limit the volume of new students in an effort to maintain the quality of instruction and services at the government-funded institutions. Projections by the MHESR indicate that over 50,000 students will be knocking at the universities\u2019 door by the beginning of the next decade (MHESR, 2007). Additionally, if the native population of UAE follows the trend of other countries in regards to adult learners returning to college, the MHESR predict that could potentially add another 13,000 students to the 50,000 youths expected to flood the higher education system if only 10 percent of the 25-29 year olds decide to enroll (MHESR, 2007). Some of the demand for education has been met by the private institutions that have set up shop in the various free zones primarily in Dubai, but these are options typically only available to the more affluent upper-middle and upper class of the socioeconomic scale. Faculty teaching in both the federal system and throughout the private institutions are heavily international. Within the UAE federal system, only approximately 10 percent of the 18 Georgia Journal of College Student Affairs, Vol. 32 [2016], Iss. 1, Art. 4 https://digitalcommons.georgiasouthern.edu/gcpa/vol32/iss1/4 DOI: 10.20429/gcpa.2016.010104 2,568 full-time faculty in 2013-2014 were UAE nationals. Similarly, less than 2 percent of the 3,949 faculty teaching in the private institutions within the free zones during 2013-2014 are UAE nationals (MHESR, 2015). The mandate set by the MHESR regarding the international qualifications of its faculty had assisted the recruitment of foreign scholars in the past, however, due to a static funding level the ability for federal institutions to attract and retain top faculty has declined considerably as the salary and benefits packages offered are no longer considered to be competitive (MHESR, 2007). Additionally, studies have shown that the most popular degree offerings and largest faculty expenses are no longer congruent with the needs of the employer market. Business, engineering, and health sciences dominate the course offerings and faculty appointments across all levels of higher education in the UAE, with 50 percent of students at federal institutions pursuing these degrees; 60 percent of students at private institutions enrolled in these majors; and 60 percent of UAE nationals studying abroad studying these same majors (NQA, 2013). Areas of study with huge industry demands that are not currently being met within the UAE educational system include information technology, education, architecture and construction, natural and physical sciences, and tourism and hospitality (Kazim, 2013).\n\n==\n\nWilkins 2010:\n\nTitle: Higher education in the United Arab Emirates: an analysis of the outcomes of significant increases in supply and competition\n\nAbstract: During the last decade, several countries across the Middle and Far East have established higher education hubs, some of which have grown rapidly by attracting foreign universities to set up international branch campuses. The United Arab Emirates (UAE) is by far the largest host of international branch campuses globally, having over 40 providers at the end of 2009. The UAE higher education market has become highly competitive and, in the private sector, supply currently exceeds demand. This paper explores and analyses the outcomes and impacts of this market situation on student recruitment, the student experience, quality and institutional strategies. The long-term viability of international branch campuses in the UAE is also considered, in the context of local political and social issues.\n\n==\n\nBurden\u2010Leahy 2009:\n\nTitle: Globalisation and education in the postcolonial world: the conundrum of the higher education system of the United Arab Emirates\n\nAbstract: Occupying a crucial economic role in supporting capitalism through the supply of oil, the United Arab Emirates (UAE) is a very\u2010high income, early\u2010development stage nation with high annual economic growth levels but low levels of labour market participation by its citizens. The national higher education system was established in 1977 and offers a different context through which to examine the relevance of existing accounts of globalisation and education in a postcolonial nation. The paper concludes by suggesting ways in which the UAE context challenges and extends Tikly\u2019s proposed conceptual framework, described in his article, \u2018Globalisation and education in the postcolonial world: towards a conceptual framework\u2019, when considering the impact of globalisation on the higher education system of a very high\u2010income developing country.\n\n==\n\nMadichie 2013:\n\nTitle: An Exploratory Enquiry into the Internationalisation of Higher Education in the United Arab Emirates\n\nAbstract: In its bid to be at the forefront of higher education in the Arabian Gulf region, the United Arab Emirates (UAE) has embarked upon major internationalisation initiatives. This paper documents the extent of these efforts in the light of observed challenges. Our paper provides some practical/social implications for foreign universities - past, present and future, contemplating 'breaking in' or 'out' as the case may be.\n\n==\n\nBhayani 2014:\n\nTitle: The market route to higher education in UAE: its rationales and implications\n\nAbstract: Externalities with regards to higher education have increasingly being debated across the world, with many countries shifting part of the higher education responsibility to the individuals on the premise that it generates private benefits. The transformation of higher education from elite to mass system and inability of the government to shoulder the ever increasing burden of higher education; is cited as one of the reasons for adopting the market route to higher education and treating students as consumers. This consumerist turn to higher education has raised concerns about quality, critical pedagogy and the perception amongst employers that graduates lack skills. The market of higher education in UAE (United Arab Emirates) is relatively free and there are substantial numbers of higher education institutions giving consumers fair choice in deciding whether to pursue their education at public, private or University Branch Campuses (IBC). The purpose of this exploratory study is to evaluate the market of higher education in UAE with regards to quality and market acceptability of universities and colleges. As the higher education sector in UAE is predominantly teaching oriented, this exploratory study is intended to assess perceptions of employers about private higher education and thereby assess the effectiveness of market route. The study revealed that the employers prefer graduates from public universities or IBC\u2019s as compared to private ones citing the concerns about student\u2019s preparation for the world of employment. The policy implications for State and universities are discussed.",
        "final_summary": "Higher education has significantly influenced the economic development in the United Arab Emirates (UAE). Nicks-McCaleb (2005) underscores the shift in education towards preparing students for the knowledge economy, contributing to the UAE's rapid development. Haddad (2020) supports this, finding that education has a positive impact on sustainable economic development in the UAE. However, Ashour (2016) raises concerns about the quality of graduates and research engagement despite the growth in higher education opportunities. Spranza (2016) highlights the unique educational system in the UAE that attracts global students and faculty, but also points out the need to align education with strategic goals. Wilkins (2010) discusses the impact of the highly competitive higher education market in the UAE on student recruitment, quality, and institutional strategies. Lastly, Bhayani (2014) raises concerns about the market route to higher education in the UAE, noting that employers prefer graduates from public universities or university branch campuses. (Nicks-McCaleb 2005; Haddad 2020; Ashour 2016; Spranza 2016; Wilkins 2010; Bhayani 2014)"
    },
    {
        "query": "network analysis and class structure",
        "paper_list_string": "Stuckenschmidt 2005:\n\nTitle: Network Analysis as a Basis for Partitioning Class Hierarchies\n\nAbstract: We discuss the use of network analysis methods to support the automatic partitioning of large concept hierarchies. Different from other work in the area, we directly apply these methods on the structure of the hierarchy. We show that this way of using network analysis techniques can provide significant results with respect to identifying key concepts and using them to determine subsets of class hierarchies that are related content-wise. We discuss the methods used and evaluate the result on the ACM classification of computer science topis.\n\n==\n\nAitkin 2014:\n\nTitle: Statistical modelling of the group structure of social networks\n\nAbstract: This research evaluates the identification of group structure in social networks through the latent class model and a new Bayesian model comparison method for the number of latent classes. The approach is applied to a well-known network of women in Natchez Mississippi. The latent class analysis reproduces the group structure of the women identified by the original sociologists.\n\n==\n\nMarsden 1985:\n\nTitle: Social structure and network analysis\n\nAbstract: Network analysis is being increasingly looked to as a means of understanding social structure. It can shed light on how individual actions create social structure, how social structure constrains the individual, and how attitudes and behaviour are determined by social structure. Articles by leading proponents of network analysis and structuralism examine how these methodological techniques and this theoretical approach can be applied to a variety of social phenomena. Written by some of the leading proponents of network analysis, this book will be welcomed by professionals in sociology and their students.\n\n==\n\nHu 2014:\n\nTitle: Evolution formation and characteristic analysis of a class of information systems based on complex networks\n\nAbstract: A class of information systems composed of backbone networks and access networks is very popular. According to the complexity and organization forms of the class of information systems, two evolution formation mechanisms of the systems are constructed based on complex network theory. One of the mechanisms uses pre-planning and random access strategies, and the other uses pre-planning and preferential access strategies. Different planning schemes and access ways are modeled. Furthermore, an example of information systems is introduced, and the network construction laws of information systems are verified and analyzed by numerical simulation.\n\n==\n\nGuimer\u00e0 2007:\n\nTitle: Classes of complex networks defined by role-to-role connectivity profiles.\n\nAbstract: In physical, biological, technological and social systems, interactions between units give rise to intricate networks. These-typically non-trivial-structures, in turn, critically affect the dynamics and properties of the system. The focus of most current research on complex networks is, still, on global network properties. A caveat of this approach is that the relevance of global properties hinges on the premise that networks are homogeneous, whereas most real-world networks have a markedly modular structure. Here, we report that networks with different functions, including the Internet, metabolic, air transportation and protein interaction networks, have distinct patterns of connections among nodes with different roles, and that, as a consequence, complex networks can be classified into two distinct functional classes on the basis of their link type frequency. Importantly, we demonstrate that these structural features cannot be captured by means of often studied global properties.\n\n==\n\nMartin 2020:\n\nTitle: Networks, Status, and Inequality\n\nAbstract: The notion that there is a single class of objects, \u201cnetworks,\u201d has been a great inspiration to new forms of structural thinking. Networks are considered to be a set of largely voluntary ties that often span organizational boundaries. Despite being divorced from formal hierarchies, they make possible other forms of differentiation, such as status. It is common for network data to be used to produce measures of the status of the nodes (individuals, organizations, cultural products, etc.) and the distribution of these statuses to describe a backdrop of inequality that may condition action or other processes. However, it is also important that network researchers understand the backdrop of various forms of potential inequality that may condition the collection of network data.\n\n==\n\nZaidi 2011:\n\nTitle: Analysis, Structure and Organization of Complex Networks\n\nAbstract: Network science has emerged as a fundamental eld of study to model many physical and real world systems around us. The discovery of small world and scale free properties of these real world networks has revolutionized the way we study, analyze, model and process these networks. In this thesis, we are interested in the study of networks having these properties often termed as complex networks. In our opinion, research conducted in this eld can be grouped into four categories, Analysis, Structure, Processes-Organization and Visualization. We address problems pertaining to each of these categories throughout this thesis. The initial chapters present an introduction and the necessary background knowledge required for readers. Chapters (3, 4, 5, 6, 7) all introduce a specic problem leading up to its solution. In Chapter 3, we present a visual analytics method to analyze complex networks. Based on this method, we also introduce a new metric to calculate the presence of densely connected vertices in networks. Chapter 4 deals with models to generate arti- cial networks having small world and scale free properties. We propose a new model to generate networks with these properties along with the presence of community structures. Extending from the results of our analysis in Chapter 3, we introduce a fast agglomerative clustering algorithm in Chapter 5. In Chapter 6, we address the issue of visualizing these complex networks through a system which combines simplication, clustering and dedicated layout algorithms. Finally we address the issue of evaluating the quality of clusters for complex networks that do not have densely connected vertices in Chapter 7. Each chapter is followed by a mini-conclusion and further research prospects. In the end, we summarize our results and conclude the thesis by presenting some research directions based on our ndings.\n\n==\n\nAmaral 2000:\n\nTitle: Classes of small-world networks.\n\nAbstract: We study the statistical properties of a variety of diverse real-world networks. We present evidence of the occurrence of three classes of small-world networks: (a) scale-free networks, characterized by a vertex connectivity distribution that decays as a power law; (b) broad-scale networks, characterized by a connectivity distribution that has a power law regime followed by a sharp cutoff; and (c) single-scale networks, characterized by a connectivity distribution with a fast decaying tail. Moreover, we note for the classes of broad-scale and single-scale networks that there are constraints limiting the addition of new links. Our results suggest that the nature of such constraints may be the controlling factor for the emergence of different classes of networks.",
        "final_summary": "The papers suggest that network analysis is a valuable tool for understanding class structure. Stuckenschmidt 2005 demonstrates that network analysis methods can effectively partition large concept hierarchies, identifying key concepts and related subsets of class hierarchies. Aitkin 2014 applies latent class analysis to social networks, successfully reproducing the group structure identified by sociologists. Guimer\u00e0 2007 finds that complex networks can be classified into distinct functional classes based on their role-to-role connectivity profiles. Amaral 2000 identifies three classes of small-world networks based on their statistical properties: scale-free networks, broad-scale networks, and single-scale networks. These findings highlight the importance of network analysis in uncovering and understanding the class structure within various domains."
    },
    {
        "query": "How does time-restricted eating contribute to reducing metabolic disease?",
        "paper_list_string": "Wilkinson 2019:\n\nTitle: Ten-Hour Time-Restricted Eating Reduces Weight, Blood Pressure, and Atherogenic Lipids in Patients with Metabolic Syndrome.\n\nAbstract: In animal models, time-restricted feeding (TRF) can prevent and reverse aspects of metabolic diseases. Time-restricted eating (TRE) in human pilot studies reduces the risks of metabolic diseases in otherwise healthy individuals. However, patients with diagnosed metabolic syndrome often undergo pharmacotherapy, and it has never been tested whether TRE can act synergistically with pharmacotherapy in animal models or humans. In a single-arm, paired-sample trial, 19 participants with metabolic syndrome and a baseline mean daily eating window of \u226514 h, the majority of whom were on a statin and/or antihypertensive therapy, underwent 10\u00a0h of TRE (all dietary intake within a consistent self-selected 10\u00a0h window) for 12\u00a0weeks. We found this TRE intervention improves cardiometabolic health for patients with metabolic syndrome receiving standard medical care including high rates of statin and anti-hypertensive use. TRE is a potentially powerful lifestyle intervention that can be added to standard medical practice to treat metabolic syndrome. VIDEO ABSTRACT.\n\n==\n\nMoon 2020:\n\nTitle: Beneficial Effects of Time-Restricted Eating on Metabolic Diseases: A Systemic Review and Meta-Analysis\n\nAbstract: Various behavioral and physiological pathways follow a pre-determined, 24 hour cycle known as the circadian rhythm. Metabolic homeostasis is regulated by the circadian rhythm. Time-restricted eating (TRE) is a type of intermittent fasting based on the circadian rhythm. In this study, we aim to analyze systemically the effects of TRE on body weight, body composition, and other metabolic parameters. We reviewed articles from PubMed, EMBASE, and the Cochrane Library to identify clinical trials that compared TRE to a regular diet. We included 19 studies for meta-analysis. Participants following TRE showed significantly reduced body weight (mean difference (MD), \u22120.90; 95% confidence interval (CI): \u22121.71 to \u22120.10) and fat mass (MD: \u22121.58, 95% CI: \u22122.64 to \u22120.51), while preserving fat-free mass (MD, \u22120.24; 95% CI: \u22121.15 to 0.67). TRE also showed beneficial effects on cardiometabolic parameters such as blood pressure (systolic BP, MD, \u22123.07; 95% CI: \u22125.76 to \u22120.37), fasting glucose concentration (MD, \u22122.96; 95% CI, \u22125.60 to \u22120.33), and cholesterol profiles (triglycerides, MD: \u221211.60, 95% CI: \u221223.30 to \u22120.27). In conclusion, TRE is a promising therapeutic strategy for controlling weight and improving metabolic dysfunctions in those who are overweight or obese. Further large-scale clinical trials are needed to confirm these findings and the usefulness of TRE.\n\n==\n\nManoogian 2021:\n\nTitle: A Metabolomic Analysis to Assess How Time-Restricted Eating Improves Cardiometabolic Health\n\nAbstract: Abstract Time-Restricted Eating (TRE) is a consistent 6-12-hour daily eating window without any overt caloric reduction. TRE has been shown in pre-clinical and clinical studies to have widespread benefits including improved cardiometabolic health. Most clinical trials have studied healthy or overweight participants, but the effect of TRE on patients undergoing medical treatment for cardiometabolic disease is unclear. In this single-arm paired-sample pilot study, 19 participants with metabolic syndrome and who had an eating window of 14 hours or more at baseline were put on a 10-hour TRE intervention for 3 months. Despite most participants already taking antihypertensives and statins at baseline, there were significant decreases in blood pressure and LDL cholesterol. There were also significant decreases in HbA1c, waist circumference, and body weight. To better understand the mechanism behind these improvements, here we report the changes in plasma metabolite changes following the 3-month TRE intervention. These findings are important to understand the physiological effects of TRE, especially for individuals to use as a co-treatment.\n\n==\n\nRothschild 2014:\n\nTitle: Time-restricted feeding and risk of metabolic disease: a review of human and animal studies.\n\nAbstract: Time-restricted feeding (TRF), a key component of intermittent fasting regimens, has gained considerable attention in recent years. TRF allows ad libitum energy intake within controlled time frames, generally a 3-12 hour range each day. The impact of various TRF regimens on indicators of metabolic disease risk has yet to be investigated. Accordingly, the objective of this review was to summarize the current literature on the effects of TRF on body weight and markers of metabolic disease risk (i.e., lipid, glucoregulatory, and inflammatory factors) in animals and humans. Results from animal studies show TRF to be associated with reductions in body weight, total cholesterol, and concentrations of triglycerides, glucose, insulin, interleukin 6, and tumor necrosis factor-\u03b1 as well as with improvements in insulin sensitivity. Human data support the findings of animal studies and demonstrate decreased body weight (though not consistently), lower concentrations of triglycerides, glucose, and low-density lipoprotein cholesterol, and increased concentrations of high-density lipoprotein cholesterol. These preliminary findings show promise for the use of TRF in modulating a variety of metabolic disease risk factors.\n\n==\n\nHatori 2012:\n\nTitle: Time-restricted feeding without reducing caloric intake prevents metabolic diseases in mice fed a high-fat diet.\n\nAbstract: While diet-induced obesity has been exclusively attributed to increased caloric intake from fat, animals fed a high-fat diet (HFD) ad libitum (ad lib) eat frequently throughout day and night, disrupting the normal feeding cycle. To test whether obesity and metabolic diseases result from HFD or disruption of metabolic cycles, we subjected mice to either ad lib or time-restricted feeding (tRF) of a HFD for 8 hr per day. Mice under tRF consume equivalent calories from HFD as those with ad lib access yet are protected against obesity, hyperinsulinemia, hepatic steatosis, and inflammation and have improved motor coordination. The tRF regimen improved CREB, mTOR, and AMPK pathway function and oscillations of the circadian clock and their target genes' expression. These changes in catabolic and anabolic pathways altered liver metabolome and improved nutrient utilization and energy expenditure. We demonstrate in mice that tRF regimen is a nonpharmacological strategy against obesity and associated diseases.\n\n==\n\nManoogian 2021:\n\nTitle: Time-restricted eating for the prevention and management of metabolic diseases.\n\nAbstract: Time-restricted feeding (TRF, animal-based studies) and time-restricted eating (TRE, humans) are an emerging behavioral intervention approach based on the understanding of the role of circadian rhythms in physiology and metabolism. In this approach, all calorie intake is restricted within a consistent interval of less than 12 hours without overtly attempting to reduce calories. This article will summarize the origin of TRF/TRE starting with concept of circadian rhythms and the role of chronic circadian rhythm disruption in increasing the risk for chronic metabolic diseases. Circadian rhythms are usually perceived as the sleep-wake cycle and dependent rhythms arising from the central nervous system. However, the recent discovery of circadian rhythms in peripheral organs and the plasticity of these rhythms in response to changes in nutrition availability raised the possibility that adopting a consistent daily short window of feeding can sustain robust circadian rhythm. Preclinical animal studies have demonstrated proof of concept and identified potential mechanisms driving TRF-related benefits. Pilot human intervention studies have reported promising results in reducing the risk for obesity, diabetes, and cardiovascular diseases. Epidemiological studies have indicated that maintaining a consistent long overnight fast, which is similar to TRE, can significantly reduce risks for chronic diseases. Despite these early successes, more\n\n==\n\nTippairote 2020:\n\nTitle: Restoration of metabolic tempo through time-restricted eating (TRE) as the preventive measure for metabolic diseases\n\nAbstract: Abstract The characteristics of healthy bioenergetics are the overall balance of energy intake and expenditure, the alternate switching for different metabolic fuels, and the temporal rhythm of eating and fasting. These three bioenergetic attributes, herein, are termed as the metabolic tempo. Cumulative studies revealed the beneficial health effects of fasting. Most of the fasting regimens harness their innate mechanisms of enhancing metabolic fuel switching, thus improving metabolic flexibility. The emerging time-restricted eating (TRE) regimen includes the restoration of diurnal eating and fasting rhythms, improve the metabolic flexibility, while spontaneously reduces the food intake despite the ad-libitum eating. TRE thus simultaneously improves all three bioenergetic-tempo attributes when compared to the energy balance control in general obesity control. We reviewed fifteen human studies of TRE and TRE-liked interventions from 2007 to 2019. These studies reported promising beneficial metabolic effects on body weight, glycemic, and lipid controls while demonstrating most of the fasting-related metabolic and epigenetic responses in overweight and obese individuals. TRE is practically possible for long-termed implementation. Despite its potentials to restore the underlying dysregulated bioenergetics., there is no study confirming that TRE could prevent the development of common metabolic diseases in healthy subjects after long-term implementation. This gap of knowledge warrants future investigation.\n\n==\n\nCharlot 2021:\n\nTitle: Beneficial Effects of Early Time-Restricted Feeding on Metabolic Diseases: Importance of Aligning Food Habits with the Circadian Clock\n\nAbstract: The importance of metabolic health is a major societal concern due to the increasing prevalence of metabolic diseases such as obesity, diabetes, and various cardiovascular diseases. The circadian clock is clearly implicated in the development of these metabolic diseases. Indeed, it regulates physiological processes by hormone modulation, thus helping the body to perform them at the ideal time of day. Since the industrial revolution, the actions and rhythms of everyday life have been modified and are characterized by changes in sleep pattern, work schedules, and eating habits. These modifications have in turn lead to night shift, social jetlag, late-night eating, and meal skipping, a group of customs that causes circadian rhythm disruption and leads to an increase in metabolic risks. Intermittent fasting, especially the time-restricted eating, proposes a solution: restraining the feeding window from 6 to 10 h per day to match it with the circadian clock. This approach seems to improve metabolic health markers and could be a therapeutic solution to fight against metabolic diseases. This review summarizes the importance of matching life habits with circadian rhythms for metabolic health and assesses the advantages and limits of the application of time-restricted fasting with the objective of treating and preventing metabolic diseases.",
        "final_summary": "Time-restricted eating (TRE) appears to be a promising strategy in mitigating metabolic disease. Wilkinson 2019 found that patients with metabolic syndrome experienced improvements in weight, blood pressure, and atherogenic lipids following a 10-hour TRE intervention. Moon 2020's meta-analysis reported significant reductions in body weight, fat mass, and enhancements in cardiometabolic parameters such as blood pressure and cholesterol profiles due to TRE. In a pilot study, Manoogian 2021 observed that participants with metabolic syndrome and a baseline eating window of 14 hours or more experienced significant decreases in blood pressure, LDL cholesterol, HbA1c, waist circumference, and body weight after a 10-hour TRE intervention for 3 months. Rothschild 2014's review of animal and human studies associated time-restricted feeding (TRF), a component of intermittent fasting, with reductions in body weight and markers of metabolic disease risk. Hatori 2012's study on mice suggested that time-restricted feeding protected against obesity, hyperinsulinemia, hepatic steatosis, and inflammation. Tippairote 2020's review of human studies highlighted the potential metabolic benefits of TRE, including weight, glycemic, and lipid control. Lastly, Charlot 2021 emphasized the importance of aligning food habits with the circadian clock and suggested that time-restricted eating could improve metabolic health. (Wilkinson 2019; Moon 2020; Manoogian 2021; Rothschild 2014; Hatori 2012; Tippairote 2020; Charlot 2021)"
    },
    {
        "query": "write an essay on how general knowledge and awareness relate perceived behavioral control towards food products especially unfamiliar low trophic level aquaculture products.",
        "paper_list_string": "Zander 2018:\n\nTitle: Sustainable Aquaculture Products: Implications of Consumer Awareness and of Consumer Preferences for Promising Market Communication in Germany\n\nAbstract: ABSTRACT Sustainability issues are becoming increasingly important in Western food consumption. This research aims at identifying ways to increase the market share of sustainable fish from aquaculture by analyzing consumer awareness and preferences for sustainable aquaculture products and by drawing conclusions on improving communication in the German market. An integrative research approach was used: it began with an inventory of sustainable seafood products on the German market, followed by qualitative and quantitative consumer research, and ending with a stakeholder workshop, which reflected on the research outcome with practitioners. Sustainability and country of origin proved to be of minor relevance to the vast majority of the consumers interviewed. Indeed, attributes such as freshness, taste, and price frequently ranked higher. Like most of the other consumers, the small consumer segment that is generally interested in sustainability issues of food also often lacks knowledge of fish farming and its products. Consequently, communication of sustainability in aquaculture is challenging, and the development of this market requires concerted actions along the whole value chain, with the retail sector as the main actor.\n\n==\n\nLeng 2020:\n\nTitle: Consumer Awareness And Acceptance Of Aquaculture Practices And Products In Penang, Malaysia\n\nAbstract: High demand of seafood products combined with over-harvesting of wild catch aquatic products have elevated aquaculture as the alternative way to strike a balance between seafood product supply and demand. However, uncontrolled aquaculture activities are found to affect the environment negatively as well as the health of consumers. Better awareness and understanding can regulate aquaculture practices and bring about environmental health through consumers\u2019 pressure on the aquaculture production system. This study aimed to identify consumers\u2019 perception on aquaculture industry, to study consumers\u2019 awareness on sustainability of aquaculture practice and also to find ways to increase consumers\u2019 awareness and acceptance of sustainable aquaculture practice and its products. A mixed research method was used in this study combining quantitative questionnaire survey of respondents who bought aquaculture products at wet markets in Penang, and qualitative in-depth focus group discussions (FGDs) with a number of selected respondents. Research findings show that most of the respondents are aware that aquaculture might give lots of negative effects to human health and the environment but, they still chose to buy aquaculture products due to cheaper price than wild catch seafood products. Hence, from the findings, it is clear that even though people have awareness, they have little choice but to accept the quality of aquaculture products. However, the results also show that enhancing consumer awareness and understanding empowers consumers to put pressures on producers paving the way towards sustainable aquaculture and greater acceptance of aquaculture practices and products. 2357-1330 \u00a9 2020 Published by European Publisher.\n\n==\n\nFern\u00e1ndez-Polanco 2008:\n\nTitle: Factors Affecting Consumer's Attitudes Towards Aquaculture\n\nAbstract: Attitudes towards production processes increased their importance over food purchase decissions in recent years. This paper aims to test which factors mainly affects attitudes towards aquaculture. Factors are classified into three groups: demographic, situational, and personal and marketing infuences. Using consumer survey data, factor analysis techniques were used to obtain an attitude meassure from a set of scales. Diferent linear models and fit methods were performed with variables of the three groups of factors to test for significant effects over attitude scores. Significant effects were found with education, ocupation and income. Purchase and consumption place were also found to be significant causes of diferencies across attitudes. Finally Consumer involvement with food safety and sustainability and generic advertising credibility have shown significant effects over attitudes towards aquaculture.\n\n==\n\nHoque 2022:\n\nTitle: Personal and Socioeconomic Factors Affecting Perceived Knowledge of Farmed Fish\n\nAbstract: The global decline in ocean fisheries stocks, including those of wild fish, has given impetus to the rapid growth of seafood produced by aquaculture, as well as of farmed fish (FF). However, continuous asymmetric information leads to consumer ambiguity and confusion regarding their knowledge of farmed fish. The personal, psychological, and socioeconomic factors affect individual\u2019s behaviour; therefore, this study considers ambiguity tolerance (AT) and confusion avoidance (CA) as personal factors and the relevant socioeconomic factors and examines if they affect consumers\u2019 perceived FF knowledge. A total sample of 1041 households from the major two urban areas of Dhaka and Chittagong, Bangladesh, were randomly selected and interviewed using a structured questionnaire. The data were analysed employing exploratory factor analysis, descriptive statistics, structural equation modelling, and the multinomial logistic regression model. The findings reveal that both AT and CA affect FF knowledge significantly and positively. Furthermore, AT reduces CA, while CA plays a critical mediating role in explaining the effect of AT on FF knowledge. The socioeconomic variables such as a high rate of fish consumption, the habit of buying fish personally, and the tendency to buy fish from wet markets also significantly influence consumers\u2019 perceived FF knowledge. The findings provide a significant managerial implication for improving a way to develop substantial factors to increase FF knowledge and benefit consumers and society.\n\n==\n\nBanovi\u0107 2019:\n\nTitle: \u201cOne Fish, Two Fish, Red Fish, Blue Fish\u201d: How ethical beliefs influence consumer perceptions of \u201cblue\u201d aquaculture products?\n\nAbstract: Respecting ethical beliefs of consumers is an important precondition for food manufacturers in their attempt to improve their positioning in the European food market. Based on a cross-cultural survey of 2511 European participants, this research demonstrates how ethical beliefs affect consumer perceptions of \u201cblue\u201d (i.e. environmentally friendly) aquaculture products. The study further emphasises that the positive effect of ethical beliefs on purchase intention operates via an indirect route mediated by consumers\u2019 trust in a product category. Consumer involvement has limited moderation effect on the above relationships. To expand its \u201cblue\u201d business, a key policy recommendation to aquaculture product manufacturers and policy makers is to urge stable and reliable standards of control in environmentally responsible aquaculture production so that consumers can rely on the information source and increase their trust in aquaculture products.\n\n==\n\nPieniak 2013:\n\nTitle: Consumer knowledge and use of information about fish and aquaculture\n\nAbstract: This paper explores consumers\u2019 knowledge about fish and aquaculture and assesses the use and importance of different information cues about fish. Cross-sectional data were collected in 2008 through a consumer survey (n=3213) in the Czech Republic, Germany, Greece, Italy, Portugal, Romania, Sweden and the UK. Consumers\u2019 knowledge about fish generally, and about aquaculture in particular, was relatively low and differed significantly between countries. Consumers from all countries reported an indication of quality and/or food safety as an information cue when buying fish. The information sources most frequently used by Europeans were labelling and sellers in retail or supermarkets. The Internet was identified by consumers in all of the countries as one of the most important sources of information about sea and freshwater fish products. Policy makers and food marketers are encouraged to develop a simple and easily recognisable mark (relating to quality, food safety and nutrition) to assist consumer decision-making. Information campaigns focusing on issues such as the nutritional benefits of eating fish are also recommended.\n\n==\n\nAitken 2020:\n\nTitle: The positive role of labelling on consumers\u2019 perceived behavioural control and intention to purchase organic food\n\nAbstract: Abstract The consumption of organic food has increased dramatically in recent years in response to consumers\u2019 concerns with issues related to health, well-being and the environment. However, further increases are restricted by a number of barriers, one of the most important of which is information. Using the Reasoned Action Approach (RAA), this study is the first to examine the role of product specific information (labelling) to understand the gap between consumer attitude and behavioural intention to purchase organic food products. Based on responses from 1,052 New Zealand consumers, analysis using structural equation modelling demonstrates that labelling plays an important role within the perceived behavioural control dimension of the RAA, as well as directly on attitudes, in influencing behavioural intention. The more that respondents agree that labelling is actionable, the more positive their attitude and sense of control, and hence intention and (self-reported) behaviour. These findings suggest that by improving labelling systems to include more actionable information, such as the health, environmental and societal benefits of products, consumers perceived behavioural control can be increased to strengthen intentions to purchase organic products.\n\n==\n\nClaret 2014:\n\nTitle: Consumer beliefs regarding farmed versus wild fish\n\nAbstract: Aquaculture is a food-producing activity, alternative to traditional extractive fishing, which still acts as a reference for most consumers. The main objective of the present paper was to study which consumer beliefs, regarding farmed versus wild fish, hinder the potential development of the aquaculture sector. To achieve this purpose the study was organized into two complementary steps: a qualitative approach (focus groups) aimed at assessing consumer perception about wild and farmed fish and to identify the salient beliefs that differentiate them; and a quantitative approach (survey by means of a questionnaire) to validate the results obtained in the focus group discussions over a representative sample of participants (n = 919). Results showed that participants perceive clear differences between farmed and wild fish. Although no significant differences between both kinds of fish were detected on safety, in general farmed fish was perceived to be less affected by marine pollution, heavy metals and parasites. In the contrary, wild fish was considered to have healthier feeding, to contain fewer antibiotics and to be fresher, healthier, less handled and more natural. Beliefs related to quality were in favour of wild fish, while those related to availability and price were in favour of farmed fish. Significant differences were observed in the perception of both kinds of fish depending on the consumers' objective knowledge about fish, on the level of education, age and gender and on the three segments of consumers identified: \"Traditional/Conservative\", \"Connoisseur\", \"Open to aquaculture\". The results provided could play an important role when planning and designing efficient marketing strategies for promoting farmed fish by adapting the information provided to the perception of each segment of consumers identified by the present study.",
        "final_summary": "The papers suggest that consumer knowledge and awareness play a significant role in perceived behavioral control towards unfamiliar low trophic level aquaculture products. Zander 2018 found that consumer awareness of sustainability and country of origin was low, with attributes like freshness, taste, and price ranking higher. Leng 2020 highlighted that consumers were aware of the negative effects of aquaculture but still chose to buy due to cheaper prices. Fern\u00e1ndez-Polanco 2008 identified factors such as education, occupation, income, and purchase/consumption place as influencing attitudes towards aquaculture. Hoque 2022 found that personal factors like ambiguity tolerance and confusion avoidance, as well as socioeconomic factors like fish consumption habits, influenced perceived knowledge of farmed fish. Banovi\u0107 2019 emphasized the importance of ethical beliefs and trust in influencing consumer perceptions of environmentally friendly aquaculture products. Pieniak 2013 highlighted the role of information cues, such as labeling and the internet, in shaping consumer knowledge about fish and aquaculture. Aitken 2020 demonstrated that product-specific information, like labeling, influenced consumers' perceived behavioral control and intention to purchase organic food. Claret 2014 showed that consumer beliefs regarding farmed versus wild fish varied, with perceptions of safety, quality, availability, and price influencing preferences. Overall, consumer knowledge, awareness, beliefs, and information cues are crucial in shaping perceived behavioral control towards unfamiliar low trophic level aquaculture products (Zander 2018; Leng 2020; Fern\u00e1ndez-Polanco 2008; Hoque 2022; Banovi\u0107 2019; Pieniak 2013; Aitken 2020; Claret 2014)."
    },
    {
        "query": "what rpm is needed in centrifuge during nanofibers separation?",
        "paper_list_string": "Zhai 2020:\n\nTitle: Simple centrifugal fractionation to reduce the size distribution of cellulose nanofibers\n\nAbstract: Since cellulose nanofiber (CNF) has unique characteristics in terms of renewability, high specific elastic modulus and strength and transparency, it is attractive for a building block of future materials. CNF can be extracted from various natural resource by several means. However, the size of the extracted CNF is very broad and uniformity of the extracted CNF is very important for many applications. Thus, a fractionation process is necessary to obtain a uniformly sized CNF. In this paper, a simple centrifugal fractionation was carried out to reduce the size distribution of the extracted CNF suspension from hardwood pulp by the combination of TEMPO oxidation and aqueous counter collision methods. The original CNF suspension was diluted and centrifuged under low speed to remove cellulose microfibers then centrifuged under high speed to separate very small CNF. The centrifugation condition is 10 k rpm for 1 h followed by 45 k rpm for 4 h. The fractionated CNF was analyzed by an atomic force microscopy, and the length and width distribution histogram analysis was utilized. UV\u2013visible analysis, FT-IR and XRD crystallinity analysis were carried out to analyze all fractionated CNFs and the original CNF. After centrifugal fractionation, the width and length distribution range were reduced by 62% and 70%, respectively. It is shown that the centrifugal fractionation is an easy and efficient method to fractionate a uniform CNF suspension.\n\n==\n\nKonrath 2016:\n\nTitle: Application of magnetic bearing technology in high-speed centrifugation\n\nAbstract: Abstract Centrifuges represent a well-established tool in separation technology to handle high throughputs of suspensions with micron-sized particles. However, extraordinarily high centrifugal accelerations are required for even smaller particles. For this purpose, we make use of magnetic bearing and drive technology. We report the first application of this contactless technique in separation experiments. The presented prototype is based on a semi-continuous principle, where sediment is built up within the rotor while the liquid (which contains the fine fraction in case of classification) is discharged at the overflow weir. The new centrifuge allows rotational speeds of more than 64k\u00a0min \u22121 . A broad variation of parameters with centrifugal accelerations of up to C =100,000 and flow rates of up to 0.4\u00a0l/min provides a sound experimental basis for this study. The cut size and the product loss of three particle systems between 10\u00a0nm and 1\u00a0\u00b5m (silver nanoparticles, silica nanoparticles, and polystyrene) are precisely adjustable via the mentioned parameters. Furthermore, a good comparability with a commercially available device was found regarding the specific efficiency. We consider our approach to be a promising contribution to nanoscale separation technology where scalable high-throughput methods are rare.\n\n==\n\nKohsakowski 2019:\n\nTitle: Effective size separation of laser-generated, surfactant-free nanoparticles by continuous centrifugation\n\nAbstract: High-power, nanosecond, pulsed-laser ablation in liquids enables the continuous synthesis of highly pure colloidal nanoparticles (NPs) at an application-relevant scale. The gained mass-weighted particle size distribution is however often reported to be broad, requiring post treatment like centrifugation to remove undesired particle size fractions. To date, available centrifugation techniques are generally discontinuous, limiting the throughput and hindering economic upscaling. Hence, throughout this paper, a scalable, continuously operating centrifugation of laser-generated platinum NPs in a tubular bowl centrifuge is reported for the first time. To that end, using a 121 W ns-laser, the continuous production of a colloidal suspension of NPs, yet with broad particle size distribution has been employed, yielding productivities of 1\u20132 g h\u22121 for gold, silver, and platinum. The power-specific productivities (Au: 18 mg h\u22121 W\u22121, Pt: 13 mg h\u22121 W\u22121, Ag: 8 mg h\u22121 W\u22121, Ni: 6 mg h\u22121 W\u22121) are far higher than reported before. Subsequent downstream integration of a continuously operating tubular bowl centrifuge was successfully achieved for Pt NPs allowing the removal of undesired particle size with high throughput. By means of a systematic study of relevant centrifugation parameters involved, effective size optimization and respective size sharpness parameters for a maximum Pt NP diameter of 10 nm are reported. The results of the experimental centrifugation of laser-generated Pt NPs were in excellent agreement with the theoretically calculated cut-off diameter. After centrifugation with optimized parameters (residence time of 5 min; g-force of 38,454 g), the polydispersity indices of the Pt NPs size distributions were reduced by a factor of six, and high monodispersity was observed.\n\n==\n\nZhang 2014:\n\nTitle: Centrifugal Spinning: An Alternative Approach to Fabricate Nanofibers at High Speed and Low Cost\n\nAbstract: Nanofibers are an important class of material that is useful in a variety of applications, including filtration, tissue engineering, protective clothing, battery separators, energy storage, etc. So far, electrospinning is the most used method for producing nanofibers. However, the wide-spread commercial use of electrospinning is limited mainly due to its low production rate. Most other nanofiber production methods, such as melt-blowing, bicomponent fiber spinning, phase separation, template synthesis, and self-assembly, are complex and can only be used to make nanofibers from limited types of polymers. Centrifugal spinning is an alternative method for producing nanofibers from various materials at high speed and low cost. In centrifugal spinning, the spinning fluid is placed in a rotating spinning head. When the rotating speed reaches a critical value, the centrifugal force overcomes the surface tension of the spinning fluid to eject a liquid jet from the nozzle tip of the spinning head. The jet then undergoes a stretching process and is eventually deposited on the collector, forming solidified nanofibers. Centrifugal spinning is simple and enables the rapid fabrication of nanofibers for various applications. This article gives an overview on the centrifugal spinning process, and compares it with conventional nanofiber production methods.\n\n==\n\nKwon 2013:\n\nTitle: Separation of different sized nanoparticles with time using a rotational flow\n\nAbstract: In this paper, we describe the development of a microfluidic centrifuge with two inlets and two outlets potentially capable of rapidly separating nanoparticles and nanovesicles. Compared with the microfluidic centrifuge with a single inlet and outlet, the 2 \u00d72 microfluidic centrifuge gives improved centrifugation performance by increasing momentum flux transfer, angular velocity, and centrifugal acceleration. The center of flow rotation and the symmetry of the horizontal velocity in the microchamber were examined numerically. On the basis of the determined maximum velocity, the angular velocity and centrifugal acceleration were also evaluated. The centrifugation time of three different nanoparticles was examined by calculating the time when the nanoparticles left the microchamber for the first time. For visual observation and quantitative measurement of nanoparticle centrifugation, a 2 \u00d72 microfluidic centrifuge was fabricated and the experimental results demonstrate similar physical behavior to those of a mechanical centrifuge. On the basis of a comparison of the centrifugation time of two different nanoparticle populations of 300 and 700 nm in diameter, we propose that nanoparticles of different sizes can be physically separated by time under a range of inlet volume flow rates.\n\n==\n\nKwon 2013:\n\nTitle: Separation of Different Sized Nanoparticles with Time Using a Rotational Flow\n\nAbstract: In this paper, we describe the development of a microfluidic centrifuge with two inlets and two outlets potentially capable of rapidly separating nanoparticles and nanovesicles. Compared with the microfluidic centrifuge with a single inlet and outlet, the 2 \u00d72 microfluidic centrifuge gives improved centrifugation performance by increasing momentum flux transfer, angular velocity, and centrifugal acceleration. The center of flow rotation and the symmetry of the horizontal velocity in the microchamber were examined numerically. On the basis of the determined maximum velocity, the angular velocity and centrifugal acceleration were also evaluated. The centrifugation time of three different nanoparticles was examined by calculating the time when the nanoparticles left the microchamber for the first time. For visual observation and quantitative measurement of nanoparticle centrifugation, a 2 \u00d72 microfluidic centrifuge was fabricated and the experimental results demonstrate similar physical behavior to those of a mechanical centrifuge. On the basis of a comparison of the centrifugation time of two different nanoparticle populations of 300 and 700 nm in diameter, we propose that nanoparticles of different sizes can be physically separated by time under a range of inlet volume flow rates.\n\n==\n\nChen 2019:\n\nTitle: Centrifugal Spinning\u2014High Rate Production of Nanofibers\n\nAbstract: Abstract Nanofibers have attracted tremendous attention due to their flexibility, large surface area, and ease of modification, and they have been widely utilized in different applications such as filtration, tissue engineering, drug delivery, protective clothing, energy storage, etc. At this writing, the most commonly used method to produce nanofibers is electrospinning. However, the utilization of a high-voltage setup and the low production rate have become barriers to its use in large scale. Centrifugal spinning is an efficient approach to producing nanofibers from various materials. During centrifugal spinning, the polymer solution or polymer melt is ejected out of the rotating spinning head, and when the centrifugal force overcomes the surface tension of the polymer liquid material, the polymer jet undergoes a stretching process and is eventually deposited on the collector, forming solidified nanofibers. This chapter gives an overview of the history, working mechanism, influential parameters, and various applications of the centrifugal spinning method.\n\n==\n\nSatish 2022:\n\nTitle: A mini review on centrifugal spinning technique for production of nanofibers and its applications in drug delivery\n\nAbstract: Nanofibers possess distinctive physicochemical and biological properties and have been utilized in a wide range of research and commercial applications. Electrospinning is a widely used technology for generating nanofibers, but due to its low production rate, it lacks scalability. Centrifugal spinning is a low-cost process for producing nanofibers that is simple, quick, and scalable. It has emerged as a potential technology for fabricating nanofibers with a variety of applications due to its extremely high production rate and multifaceted features. The centrifugal spinning process involves injecting the polymer solution or melt in a rotating spinneret. When the spinning head reaches a critical speed, the centrifugal force overcomes the surface tension of the spinning liquid, and a liquid jet ejects from the nozzle tip. The jet is then stretched further, resulting in solidified nanofibers on the collector. The technique is capable of producing 3D nanofiber scaffolds from various polymers for biomedical applications. This paper describes in details the centrifugal spinning process, its classification, principle and applications of the centrifugal spun nano/micro fibers in drug delivery and wound dressing. Keywords: Nanofibers, Centrifugal spinning, Solubility enhancement, drug delivery.",
        "final_summary": "The RPM required for nanofiber separation in a centrifuge varies depending on the specific materials and desired outcomes. Zhai 2020 utilized a two-step centrifugation process at 10 k rpm and 45 k rpm to reduce the size distribution of cellulose nanofibers. Konrath 2016 achieved rotational speeds exceeding 64k min^\u22121, enabling precise control over nanoparticle cut size and product loss. Kohsakowski 2019 demonstrated continuous centrifugation of laser-generated nanoparticles, optimizing size separation at a g-force of 38,454 g. Kwon 2013's microfluidic centrifuge separated nanoparticles by size using rotational flow, suggesting the potential for time-based physical separation. Both Chen 2019 and Satish 2022 highlighted centrifugal spinning as an efficient, scalable method for nanofiber production."
    },
    {
        "query": "Misallocation of Resources",
        "paper_list_string": "Be\u0146kovskis 2015:\n\nTitle: Misallocation of Resources in Latvia: Did Anything Change During the Crisis?\n\nAbstract: This paper evaluates misallocation of resources in Latvia during 2007\u20132013 using firm-level data. I found that allocation of resources worsened before 2010 and improved afterwards. Initially, misallocation of intermediate inputs was the major source of aggregate TFP losses, while the importance of capital misallocation increased after the financial crisis. Determinants of changes in allocation efficiency may include growing competition in domestic markets, tighter credit supply and legal issues. However, I show that fragmentation of production induces bias to the estimates of firm-specific distortions. Thus, in the absence of inter-firm trade data, the conclusions on misallocation should be treated with some caution.\n\n==\n\nNicola 2020:\n\nTitle: Productivity Loss and Misallocation of Resources in Southeast Asia\n\nAbstract: This paper examines within-sector resource misallocation in three Southeast Asian countries -- Indonesia, Malaysia, and Vietnam. The methodology accounts for measurement error in revenues and costs. The firm-level evidence suggests that measurement error is substantial, resulting in an overestimation of misallocation by as much as 30 percent. Nevertheless, resource misallocation across firms within a sector remains large, albeit declining. The findings imply that there are considerable potential gains from efficient reallocation -- above 80 percent for Indonesia and around 20 to 30 percent for Malaysia and Vietnam. Private domestic firms and firms with higher productivity appear to face larger distortions that prevent them from expanding.\n\n==\n\nWei 2017:\n\nTitle: Resource misallocation in Chinese manufacturing enterprises: evidence from firm-level data\n\nAbstract: Abstract Despite the important policy and welfare implications of China's energy issues, to date there has been little investigation from a micro-level perspective of the misallocation of resources in manufacturing enterprises. To fill this gap, this paper deals with resource misallocations in China with special reference to energy and labor inputs. Using a large, novel dataset of manufacturing enterprises from Zhejiang province, we employ a simultaneous system of equations for (gross) industrial output value, wage bill, energy expenditure and carbon emissions. The explanatory variables include labor, educational attainment of employees, capital value, materials, and a list of energy-related variables. Our results show that, for the same energy content, non-coal energy inputs are more productive than coal and coal-related fuels. Non-coal energy is more environmentally friendly but more expensive than coal-related fuels. Using the model estimates, we conduct Wald tests on whether the marginal productivity of labor and energy significantly differ from their respective factor costs. The main findings are that there is substantial resource misallocation in manufacturing enterprises, with labor being underpaid and energy overpaid, on average, in comparison to their marginal productivity. For energy misallocation, however, the pattern varies considerably among the various subsectors. Specifically, energy is over-used in sectors such as Textile , Paper , Chemical , Plastics , Non-metallic , Ferrous metals and Non-ferrous metal . By contrast, energy is under-used in sectors such as Beverages , Textile-wearing apparel , Leather , Metal , Transport equipment , Electrical machinery and Communication equipment . These findings can assist decision-makers in targeting the sectors where there is the most misallocation, identifying which input factors are inappropriately used, improving economic efficiency and lowering overall production costs by reducing allocative inefficiency.\n\n==\n\nNguyen 2016:\n\nTitle: Resource Misallocation in Turkey\n\nAbstract: This paper examines resource misallocation within narrow industries in Turkey. It finds that resource misallocation in Turkey is substantial. The hypothetical gain from moving to\"U.S. efficiency\"is 24.5 percent of manufacturing total factor productivity in 2014. The evolution of resource misallocation over time and across disaggregated sectors is also examined. Improvement in allocative efficiency was sizable between 2003 and 2013, but significantly slower after 2007. However, the earlier trend reversed in 2014 and resource misallocation worsened in Turkey's manufacturing. The cross-sector analysis reveals that misallocation is most pronounced in textiles, transport, food, and leather.\n\n==\n\nLai-qu 2015:\n\nTitle: Effect of Administrative Monopoly on Resources Misallocation Caused by Ownership Differences\n\nAbstract: Based on the monopolistic competition model of heterogeneous enterprises,this paper puts forward the measuring model which is used to measure the degree of resources misallocation caused by the differences of ownership.Using Chinese industrial enterprises data,this paper measures the degree of resources misallocation.The results show that,although the degree of resources misallocation is on decline in the long time,it is still very serious.From 1998 to 2007,the ownership differences led to industrial total factor productivity(TFP) loss more than 200%every year.The degree of labor misallocation approached 100%,and the degree of capital misallocation was approximately 50%.This paper also shows us that the fundamental reason for resources misallocation caused by ownership differences is not just ownership differences itself,but administrative monopoly which is defined as the combination between administrative power of government and monopoly of state\u2014owned enterprises.Through governing financial system which is predominated by state-owned banks,the administrative departments bring the state-owned enterprises lower financing cost.Meanwhile through setting market entry barriers,controlling price,and providing lower financing cost,the administrative departments bring the state-owned enterprises higher profit and employee welfare,and then lead to capital and labor misallocation.\n\n==\n\nHe 2021:\n\nTitle: Resource Misallocation and Energy-Related Pollution\n\nAbstract: Developing countries face the conflict between economic development and environmental protection. Resource misallocation will not only affect the effectiveness of economic development, but also have environmental impacts. Based on two large-scale enterprise databases in China, this paper measured the level of enterprise resource allocation, and further used empirical research methods to investigate the environmental impact of enterprise resource misallocation and specific mechanisms. The results show that the low efficiency of resource allocation will harm the quality of China\u2019s environment. Further investigation, resource misallocation is accompanied by an increase in total energy input, a decrease in the labor-to-energy ratio and the capital-to-energy ratio, and a loss of energy efficiency, which in turn affects the environmental performance of enterprises. China is the largest developing country in the world, and research on China\u2019s environmental and economic issues is important. The conclusions of this paper can provide experience and suggestions for other developing countries to improve environmental quality and promote sustainable development from the perspective of resource misallocation.\n\n==\n\nMohommad 2021:\n\nTitle: Resource Misallocation in India\n\nAbstract: At the macro level, productivity is driven by technology and the efficiency of resource allocation, as outcomes of firms\u2019 decision making. The relatively high level of resource misallocation in India\u2019s formal manufacturing sector is well documented. We build on this research to further investigate the drivers of misallocation, exploiting micro-level variation across Indian states. We find that states with less rigid labor markets have lesser misallocation. We also examine the interaction of labor market rigidities with informality which is a key feature of India\u2019s labor markets. Our results suggest that reducing labor market rigidities in states with high informality has a net positive effect on aggregate productivity.\n\n==\n\nJia 2014:\n\nTitle: How Does Government Intervention Lead to Regional Resource Misallocation\u2014\u2014Based on Decomposition of Misallocation within and between Industries\n\nAbstract: This paper extends the Hsieh and Klenow model to measure the resource misallocation from the industry level.It compares the levels of misallocation in China's different areas and tests the government's policy factors by regression.The results indicate that the overall and within-industry misallocation index firsdy decreases,and then increases,however,the between-industry misallocation index increases all the time.The level of misallocation in the central and western regions is higher than that of the eastern region.The misallocation withinindustry and between-industry induces 30.25% and 4.72% gap between real output and potential output respectively.Fiscal subsidy,financial depression and administrative barrier have significant effects on the misallocation within-industry,and labor mobility control and financial depression influence the misallocation betweenindustry.It has important policy implications for China's economic reform to promote the process of marketization and regional TFP growth.",
        "final_summary": "The papers suggest that resource misallocation is a significant issue in various countries and industries. Be\u0146kovskis (2015) finds that resource misallocation in Latvia worsened before 2010 but improved afterwards, with misallocation of intermediate inputs initially causing TFP losses and capital misallocation becoming more important after the financial crisis. Nicola (2020) examines Southeast Asian countries and finds substantial resource misallocation within sectors, with potential gains from efficient reallocation ranging from above 80% for Indonesia to 20-30% for Malaysia and Vietnam. Wei (2017) focuses on China and reveals substantial resource misallocation in manufacturing enterprises, with labor being underpaid and energy overpaid compared to their marginal productivity. Nguyen (2016) examines resource misallocation in Turkey and finds it to be substantial, with a hypothetical gain of 24.5% of manufacturing total factor productivity. Lai-qu (2015) explores the effect of administrative monopoly on resource misallocation in China and identifies ownership differences and administrative monopoly as key factors contributing to misallocation. He (2021) investigates resource misallocation and its environmental impact in China, finding that low efficiency of resource allocation harms environmental quality. Mohommad (2021) focuses on India and finds that states with less rigid labor markets have lower misallocation, and reducing labor market rigidities in states with high informality positively affects aggregate productivity. Jia (2014) extends the analysis to China's different regions and identifies fiscal subsidy, financial depression, administrative barriers, and labor mobility control as factors influencing resource misallocation. Overall, these papers highlight the prevalence and detrimental effects of resource misallocation in various countries and industries, emphasizing the need for efficient resource allocation to enhance productivity and economic performance."
    },
    {
        "query": "sulforaphane cancer",
        "paper_list_string": "Zhang 2007:\n\nTitle: Discovery and development of sulforaphane as a cancer chemopreventive phytochemical\n\nAbstract: AbstractSulforaphane (SF) is a phytochemical that displays both anticarcinogenic and anticancer activity. SF modulates many cancer-related events, including susceptibility to carcinogens, cell death, cell cycle, angiogenesis, invasion and metastasis. We review its discovery and development as a cancer chemopreventive agent with the intention of encouraging further research on this important compound and facilitating the identification and development of new phytochemicals for cancer prevention.\n\n==\n\nRusso 2018:\n\nTitle: Nrf2 targeting by sulforaphane: A potential therapy for cancer treatment\n\nAbstract: ABSTRACT In the past decades, extensive studies have reported the potential chemopreventive activity of sulforaphane, an isothiocyanate derived from glucoraphanin, occurring in large amounts in Brassica genus plants. Sulforaphane was found to be active against several forms of cancer. A growing body of data shows that sulforaphane acts against cancer at different levels, from development to progression, through pleiotropic effects. In this review, we discuss the available experimental and clinical data on the potential therapeutic role of sulforaphane against cancer. Its effects range from the protection of cells from DNA damage to the modulation of the cell cycle via pro-apoptotic, anti-angiogenesis and anti-metastasis activities. At molecular level, sulforaphane modulates cellular homeostasis via the activation of the transcription factor Nrf2. Although data from clinical studies are limited, sulforaphane remains a good candidate in the adjuvant therapy based on natural molecules against several types of cancer.\n\n==\n\nGamet-Payrastre 2000:\n\nTitle: Sulforaphane, a naturally occurring isothiocyanate, induces cell cycle arrest and apoptosis in HT29 human colon cancer cells.\n\nAbstract: Sulforaphane is an isothiocyanate that is present naturally in widely consumed vegetables and has a particularly high concentration in broccoli. This compound has been shown to block the formation of tumors initiated by chemicals in the rat. Although sulforaphane has been proposed to modulate the metabolism of carcinogens, its mechanism of action remains poorly understood. We have previously demonstrated that sulforaphane inhibits the reinitiation of growth and decreases the cellular viability of quiescent human colon carcinoma cells (HT29). Moreover, the weak effect observed on differentiated CaCo2 cells suggests a specific anticancer activity for this compound. Here we investigated the effect of sulforaphane on the growth and viability of HT29 cells during their exponentially growing phase. We observed that sulforaphane induced a cell cycle arrest in a dose-dependent manner, followed by cell death. This sulforaphane-induced cell cycle arrest was correlated with an increased expression of cyclins A and B1. Moreover, we clearly demonstrated that sulforaphane induced cell death via an apoptotic process. Indeed, a large proportion of treated cells display the following: (a) translocation of phosphatidylserine from the inner layer to the outer layer of the plasma membrane; (b) typical chromatin condensation; and (c) ultrastructural modifications related to apoptotic cell death. We also showed that the expression of p53 was not changed in sulforaphane-treated cells. In contrast, whereas bcl-2 was not detected, we observed increased expression of the proapoptotic protein bax, the release of cytochrome c from the mitochondria to the cytosol, and the proteolytic cleavage of poly(ADP-ribose) polymerase. In conclusion, our results strongly suggest that in addition to the activation of detoxifying enzymes, induction of apoptosis is also involved in the sulforaphane-associated chemoprevention of cancer.\n\n==\n\nFahey 2002:\n\nTitle: Sulforaphane inhibits extracellular, intracellular, and antibiotic-resistant strains of Helicobacter pylori and prevents benzo[a]pyrene-induced stomach tumors\n\nAbstract: Gastric infection with Helicobacter pylori is a cosmopolitan problem, and is especially common in developing regions where there is also a high prevalence of gastric cancer. These infections are known to cause gastritis and peptic ulcers, and dramatically enhance the risk of gastric cancer. Eradication of this organism is an important medical goal that is complicated by the development of resistance to conventional antimicrobial agents and by the persistence of a low level reservoir of H. pylori within gastric epithelial cells. Moreover, economic and practical problems preclude widespread and intensive use of antibiotics in most developing regions. We have found that sulforaphane [(\u2212)-1-isothiocyanato-(4R)-(methylsulfinyl)butane], an isothiocyanate abundant as its glucosinolate precursor in certain varieties of broccoli and broccoli sprouts, is a potent bacteriostatic agent against 3 reference strains and 45 clinical isolates of H. pylori [minimal inhibitory concentration (MIC) for 90% of the strains is \u22644 \u03bcg/ml], irrespective of their resistance to conventional antibiotics. Further, brief exposure to sulforaphane was bactericidal, and eliminated intracellular H. pylori from a human epithelial cell line (HEp-2). In complementary experiments, sulforaphane blocked benzo[a]pyrene-evoked forestomach tumors in ICR mice. This protection resulted from induction of phase 2 detoxication and antioxidant enzymes, and was abrogated in mice lacking the nrf2 gene, which regulates phase 2 enzymes. Thus, the dual actions of sulforaphane in inhibiting Helicobacter infections and blocking gastric tumor formation offer hope that these mechanisms might function synergistically to provide diet-based protection against gastric cancer in humans.\n\n==\n\nDo 2010:\n\nTitle: Development of sulforaphane-encapsulated microspheres for cancer epigenetic therapy.\n\nAbstract: Even though conventional chemotherapeutic management of cancer has reduced morbidity and mortality to a great extent, virtually all chemotherapeutic agents cause damage to healthy cells, necessitating exploration of novel anticancer agents that exert their effects through an alternate mode of action. Objectives of our research were twofold. First, we explored the promising potential of histone deacetylase inhibitor sulforaphane for epigenetic therapy for cancer as this therapeutic approach aims to reverse aberrant epigenetic modifications that affect gene expression. In vitro cell culture studies performed using B16 and S91 melanoma cells showed that sulforaphane inhibited growth and proliferation of cancer cells by downregulating deacetylation enzymes. The second part of our research investigated polymeric drug delivery systems to increase therapeutic efficacy and to minimize potential side effects of R,S-sulforaphane. Albumin microspheres encapsulating sulforaphane were developed by spray drying. Microspheres were characterized for their morphology, size and zeta potential. Cell culture studies using melanoma cells and in vivo studies in melanoma tumor-bearing C57BL/6 mice demonstrated that albumin based polymeric delivery system was efficacious and has the potential to enhance the therapeutic effect and anticancer activity of sulforaphane.\n\n==\n\nShan 2006:\n\nTitle: Effect of sulforaphane on cell growth, G(0)/G(1) phase cell progression and apoptosis in human bladder cancer T24 cells.\n\nAbstract: Isothiocyanates (ITCs) from cruciferous vegetables have been shown to be effective in blocking initiation as well as progression of a range of chemically-induced tumors in animal models. In this study, sulforaphane, the most extensively studied ITC, was found to suppress the growth of T24 bladder cancer cells in vitro in a dose-dependent manner. Sulforaphane inhibited the proliferation of T24 cells with IC(5)0 values 26.9 and 15.9 microM following 24 and 48 h treatments. Sulforaphane (5-20 microM) induced early apoptosis and blocked cell cycle progression at G(0)/G(1) phase which was associated with upregulation of cyclin-dependent kinase inhibitor p27 expression. These results support a role for sulforaphane as an effective agent in the chemoprevention of bladder cancer.\n\n==\n\nHo 2009:\n\nTitle: Dietary sulforaphane, a histone deacetylase inhibitor for cancer prevention.\n\nAbstract: The reversible acetylation of histones is an important mechanism of gene regulation. During prostate cancer progression, specific modifications in acetylation patterns on histones are apparent. Targeting the epigenome, including the use of histone deacetylase (HDAC) inhibitors, is a novel strategy for cancer chemoprevention. Recently, drugs classified as HDAC inhibitors have shown promise in cancer clinical trials. We have previously found that sulforaphane (SFN), a compound found in cruciferous vegetables, inhibits HDAC activity in human colorectal and prostate cancer cells. Based on the similarity of SFN metabolites and other phytochemicals to known HDAC inhibitors, we previously demonstrated that sulforaphane acted as an HDAC inhibitor in the prostate, causing enhanced histone acetylation, derepression of P21 and Bax, and induction of cell cycle arrest/apoptosis, leading to cancer prevention. The ability of SFN to target aberrant acetylation patterns, in addition to effects on phase 2 enzymes, may make it an effective chemoprevention agent. These studies are important because of the potential to qualify or change recommendations for high-risk prostate cancer patients and thereby increase their survival through simple dietary choices incorporating easily accessible foods into their diets. These studies also will provide a strong scientific foundation for future large-scale human clinical intervention studies.\n\n==\n\nJiang 2018:\n\nTitle: Chemopreventive activity of sulforaphane\n\nAbstract: Cancer is one of the major causes of morbidity and mortality in the world. Carcinogenesis is a multistep process induced by genetic and epigenetic changes that disrupt pathways controlling cell proliferation, apoptosis, differentiation, and senescence. In this context, many bioactive dietary compounds from vegetables and fruits have been demonstrated to be effective in cancer prevention and intervention. Over the years, sulforaphane (SFN), found in cruciferous vegetables, has been shown to have chemopreventive activity in vitro and in vivo. SFN protects cells from environmental carcinogens and also induces growth arrest and/or apoptosis in various cancer cells. In this review, we will discuss several potential mechanisms of the chemopreventive activity of SFN, including regulation of Phase I and Phase II drug-metabolizing enzymes, cell cycle arrest, and induction of apoptosis, especially via regulation of signaling pathways such as Nrf2-Keap1 and NF-\u03baB. Recent studies suggest that SFN can also affect the epigenetic control of key genes and greatly influence the initiation and progression of cancer. This research may provide a basis for the clinical use of SFN for cancer chemoprevention and enable us to design preventive strategies for cancer management, reduce cancer development and recurrence, and thus improve patient survival.",
        "final_summary": "Sulforaphane, a phytochemical in cruciferous vegetables, exhibits anticarcinogenic and anticancer activity, affecting various cancer-related events (Zhang 2007). It demonstrates pleiotropic effects against cancer, including DNA protection, cell cycle modulation, and anti-angiogenesis and anti-metastasis activities (Russo 2018). Sulforaphane induces cell cycle arrest and apoptosis in colon cancer cells (Gamet-Payrastre 2000), and exhibits bacteriostatic and bactericidal effects against Helicobacter pylori, a bacterium linked to gastric cancer (Fahey 2002). It shows potential for epigenetic cancer therapy (Do 2010), inhibits bladder cancer cell growth and induces apoptosis (Shan 2006), acts as a histone deacetylase inhibitor for cancer prevention (Ho 2009), and demonstrates chemopreventive activity, including regulation of drug-metabolizing enzymes, cell cycle arrest, apoptosis, and epigenetic control of key genes (Jiang 2018)."
    },
    {
        "query": "polarization in the charge cycle of Li-S battery",
        "paper_list_string": "Peng 2020:\n\nTitle: Reducing polarization of lithium-sulfur batteries via ZnS/reduced graphene oxide accelerated lithium polysulfide conversion\n\nAbstract: Abstract Polarization of lithium-sulfur (Li-S) batteries consumes energy density upon cycling, which precludes their practical application in state-of-the-art devices. Herein a conformal zinc sulfide/reduced graphene oxide/elemental sulfur (ZnS/rGO/S) composite is synthesized and used as cathode materials in Li-S batteries. The synergetic regulation of ohmic, concentration, and electrochemical polarization decreases the overpotential and improves the cycling performance of the Li||ZnS/rGO/S cell. Combining theoretical calculations and experimental results, it is revealed that the ZnS spheres can absorb lithium polysulfide (LPS) intermediates, catalyze LPS conversion, and provide fast electronic/ionic diffusion kinetics. The ZnS/rGO/S cathode is further applied in pouch Li-S cells, demonstrating the practicability of sulfur cathodes with reduced polarization.\n\n==\n\nCheng 2018:\n\nTitle: Polar cross-linked polystyrene as polysulfides anchor enhanced cycle performance and coulombic efficiency for lithium sulfur batteries\n\nAbstract: Abstract Lithium-sulfur (Li-S) batteries have the theoretical energy density up to 2600\u202fWh/kg, but its commercialization is limited by poor cyclic performance, low coulombic efficiency and high material cost. In this work, the polar cross-linked polystyrene/sulfur (PCP/S) composite with a sulfur content of 58\u202fwt% is firstly synthesized as the cathodes for Li-S batteries, in which these polar cross-linked polystyrene agents effectively adsorb polysulfides with a large specific surface area and various functional groups, resulting in an enhancement of the cycle performance and coulombic efficiency for Li-S batteries. The first specific discharge capacity of the PCP/S cathode is 1309.7\u202fmAh\u202fg\u22121 at 0.2\u202fC, which can remain as high as 659.7\u202fmAh\u202fg\u22121 after 400\u202fcycles with a low capacity decay of 0.124% per cycle.\n\n==\n\nGao 2019:\n\nTitle: Strong charge polarization effect enabled by surface oxidized titanium nitride for lithium-sulfur batteries\n\nAbstract: The commercialization of high-energy-density and low-cost lithium-sulfur batteries has been severely impeded by capacity fading and electrochemical polarization. Here we report a strategy to entrap polysulfides and boost the cathodic redox kinetics by embedding the surface oxidized quantum-dot-size TiN (TiN-O) within the highly ordered mesoporous carbon matrix. While the carbon scaffold offers sufficient electrical contact to the insulate sulfur, benefiting the full usage of sulfur and physical confinement of polysulfides. The surface oxygen renders TiN-O with a strong charge polarization effect for polysulfides via S-O-Ti bond as verified experimentally and theoretically. The suppressed shuttle effect and high lithium ion diffusion coefficient (7.9\u2009\u00d7\u200910\u22128\u2009cm2\u2009s\u22121) lead to a high capacity of 1264\u2009mA\u2009h\u2009g\u22121 at 0.2\u2009C with a negligible capacity fading rate of 0.06% per cycle. Additionally, TiN-O based prototype soft-package cells also exhibit excellent cycling stability with flexibility, demonstrating their potential for practical applications.Capacity fading and electrochemical polarization pose barriers to the commercial use of lithium-sulfur batteries. Here the authors show improved charge polarization and electrochemical performance in a surface oxidized quantum-dot-size titanium nitride embedded within a highly ordered mesoporous carbon matrix.\n\n==\n\nHu 2016:\n\nTitle: Alleviating polarization by designing ultrasmall Li2S nanocrystals encapsulated in N-rich carbon as a cathode material for high-capacity, long-life Li\u2013S batteries\n\nAbstract: Lithium sulfide (Li2S), which has a high theoretical specific capacity of 1166 mA h g\u22121, has potential application in cathode materials because of its high safety and compatibility with lithium-free anodes for Li\u2013S batteries. However, its low electron conductivity and lithium transfer cause significant polarization in Li2S electrodes. Here, we demonstrate the use of ultrasmall Li2S nanocrystals encapsulated in N-rich carbon (NRC) as a cathode material for Li\u2013S batteries. By evaporating a mixture of polyacrylonitrile (PAN) and Li2S in dimethylformamide (DMF) solution and then subjecting the mixture to carbonization, a nano-Li2S@NRC composite with ultrasmall Li2S well dispersed in its carbon matrix was successfully synthesized. The obviously lower potential barriers and excellent cycling performance of nano-Li2S@NRC electrodes confirm their improved polarization because of the size effect of Li2S nanocrystals and the good electron transfer between Li2S and N-doped carbon. The nano-Li2S@NRC cathode delivers a high initial specific capacity of 1046 mA h g\u22121 of Li2S (\u223c1503 mA h g\u22121 of S) at 0.25C and 958 mA h g\u22121 of Li2S (\u223c1376 mA h g\u22121 of S) at 0.5C with a favorable cycling performance with an \u223c0.041% decay rate per cycle over 1000 cycles.\n\n==\n\nWang 2018:\n\nTitle: Advances in Polar Materials for Lithium\u2013Sulfur Batteries\n\nAbstract: Lithium\u2013sulfur batteries are regarded as promising candidates for energy storage devices due to their high theoretical energy density. Various approaches are proposed to break through the obstacles that are preventing Li\u2013S batteries from realizing practical application. Recently, the importance of the strong chemical interaction between polar materials and polysulfides is recognized by researchers to improve the performance of Li\u2013S batteries, especially with respect to the shuttle effect. Polar materials, unlike nonpolar materials, exhibit strong interactions with polysulfides without any modification or doping because of their intrinsic polarity, absorbing the polar polysulfides and thus suppressing the notorious shuttle effect. The recent advances on polar materials for Li\u2013S batteries are reviewed here, especially the chemical polar\u2013polar interaction effects toward immobilizing dissolved polysulfides, and the relationship between the intrinsic properties of the polar materials and the electrochemical performance of the Li\u2013S batteries are discussed. Polar materials, including polar inorganics in the cathode and polar organics as binder for the Li\u2013S batteries are respectively described. Finally, future directions and prospects for the polar materials used in Li\u2013S batteries are also proposed.\n\n==\n\nShen 2019:\n\nTitle: Stable cycling of lithium-sulfur batteries by optimizing the cycle condition\n\nAbstract: Abstract Lithium-sulfur (Li\u2013S) batteries suffer from poor utilization of active material and short cycle life due to the complicated multi-step reaction mechanisms. Herein, three conditional cycling methods, i.e. asymmetrical cycling, constant voltage (CV) discharge cycling, and partial cycling are designed in order to increase the cyclability of Li\u2013S batteries. It is found that the solid deposition process that takes place during the lower plateau of discharge is the major limiting step for achieving high discharge capacity and cycle retention, and the cathode surface coverage can be deferred by applying an optimal discharge/charge rate and CV discharge cycling. The asymmetrical cycling renders a specific capacity of ca. 700 mAh g\u22121 after 200 cycles, 30% higher than that under symmetrical cycling, while applying a CV discharge cycling enables a full retention of target specific capacity of ca. 800 mAh g\u22121 over 50 cycles. The partial cycling with a low number of phase transformation steps and reduced surface coverage at the end of discharge/charge also enhances cyclability. This work paves the way for understanding and improving the cycling performance of Li\u2013S batteries without increasing the cost of electrode design or changing the configuration of the cell.\n\n==\n\nZhang 2021:\n\nTitle: A Li-S battery with ultrahigh cycling stability and enhanced rate capability based on novel ZnO yolk-shell sulfur host\n\nAbstract: Abstract Currently, lithium-sulfur (Li-S) batteries still suffer from fast capacity decay, poor coulombic efficiency (CE) and short cycling lifespan, which result from the severe shuttle effect issue caused by high solubility and rapid diffusion of lithium polysulfides (LiPSs) in organic electrolytes. Here, yolk-shell zinc oxide (YS-ZnO) spheres are synthesized and for the first time, applied as a host for Li-S batteries to tackle this challenge. The polar ZnO exhibits high chemical anchoring ability toward LiPSs while the unique yolk-shell structure not only provides an additional physical barrier to LiPSs but also enables much more uniform sulfur distribution, thus significantly suppressing LiPSs shuttling effect meanwhile promoting sulfur conversion reactions. As a result, the YS-ZnO enables the Li-S battery to display an initial specific capacity of 1355\u00a0mAh\u00a0g\u22121 and an outstanding capacity retention capability (~89.44% retention rate) even after 500 cycles with the average CE of ~99.46% at the current of 0.5 C. By contrast, the capacity of conventional-ZnO-nanoparticles based battery severely decays to 472\u00a0mAh\u00a0g\u22121 after cycling for 500 times. More impressively, the S/YS-ZnO based Li-S battery can maintain a low decay rate of 0.040% every cycle and high average CE of 98.82% over 1000 cycles at 3 C.\n\n==\n\nPang 2016:\n\nTitle: Long-Life and High-Areal-Capacity Li-S Batteries Enabled by a Light-Weight Polar Host with Intrinsic Polysulfide Adsorption.\n\nAbstract: Lithium-sulfur batteries are attractive electrochemical energy storage systems due to their high theoretical energy density and very high natural abundance of sulfur. However, practically, Li-S batteries suffer from short cycling life and low sulfur utilization, particularly in the case of high-sulfur-loaded cathodes. Here, we report on a light-weight nanoporous graphitic carbon nitride (high-surface-area g-C3N4) that enables a sulfur electrode with an ultralow long-term capacity fade rate of 0.04% per cycle over 1500 cycles at a practical C/2 rate. More importantly, it exhibits good high-sulfur-loading areal capacity (up to 3.5 mAh cm(-2)) with stable cell performance. We demonstrate the strong chemical interaction of g-C3N4 with polysulfides using a combination of spectroscopic experimental studies and first-principles calculations. The 53.5% concentration of accessible pyridinic nitrogen polysulfide adsorption sites is shown to be key for the greatly improved cycling performance compared to that of N-doped carbons.",
        "final_summary": "The papers suggest that polarization in the charge cycle of Li-S batteries can be reduced through various approaches. Peng 2020 demonstrates that a ZnS/reduced graphene oxide composite decreases overpotential and improves cycling performance by regulating ohmic, concentration, and electrochemical polarization. Cheng 2018 shows that polar cross-linked polystyrene effectively adsorbs polysulfides, enhancing cycle performance and coulombic efficiency. Gao 2019 introduces surface oxidized titanium nitride, which entrap polysulfides and boost cathodic redox kinetics, resulting in high capacity and negligible capacity fading. Hu 2016 presents ultrasmall Li2S nanocrystals encapsulated in N-rich carbon, which improves polarization through good electron transfer and achieves high specific capacity with favorable cycling performance. Wang 2018 reviews the use of polar materials to immobilize dissolved polysulfides and suppress the shuttle effect in Li-S batteries. Shen 2019 explores conditional cycling methods, such as asymmetrical cycling and constant voltage discharge cycling, to increase cyclability by optimizing discharge/charge rates and reducing surface coverage. Zhang 2021 utilizes yolk-shell zinc oxide spheres as a host to suppress the shuttle effect, resulting in outstanding capacity retention and high coulombic efficiency. Pang 2016 introduces nanoporous graphitic carbon nitride, which exhibits strong chemical interaction with polysulfides and enables long-term capacity fade rates as low as 0.04% per cycle."
    },
    {
        "query": "Ridge Regression",
        "paper_list_string": "Newell 1981:\n\nTitle: Ridge Regression: An Alternative to Multiple Linear Regression for Highly Correlated Data\n\nAbstract: The use of ridge regression is proposed to overcome the problem of multicollinearity associated with the use of multiple linear regression in food technology. This technique is discussed and illustrated with an example from food technology.\n\n==\n\nKhalaf 2013:\n\nTitle: Modified Ridge Regression Estimators\n\nAbstract: Ridge regression is a variant of ordinary multiple linear regression whose goal is to circumvent the problem of predictors collinearity. It gives up the Ordinary Least Squares (OLS) estimator as a method for estimating the parameters of the multiple linear regression model . Different methods of specifying the ridge parameter k were proposed and evaluated in terms of Mean Square Error (MSE) by simulation techniques. Comparison is made with other ridge-type estimators evaluated elsewhere. The new estimators of the ridge parameters are shown to have very good MSE properties compared with the other estimators of the ridge parameter and the OLS estimator. Based on our results from the simulation study, we may recommend the new ridge parameters to practitioners.\n\n==\n\nHoerl 1970:\n\nTitle: Ridge Regression: Applications to Nonorthogonal Problems\n\nAbstract: This paper is an exposition of the use of ridge regression methods. Two examples from the literature are used as a base. Attention is focused on the RIDGE TRACE which is a two-dimensional graphical procedure for portraying the complex relationships in multifactor data. Recommendations are made for obtaining a better regression equation than that given by ordinary least squares estimation.\n\n==\n\nMarquardt 1975:\n\nTitle: Ridge Regression in Practice\n\nAbstract: Summary The use of biased estimation in data analysis and model building is discussed. A review of the theory of ridge regression and its relation to generalized inverse regression is presented along with the results of a simulation experiment and three examples of the use of ridge regression in practice. Comments on variable selection procedures, model validation, and ridge and generalized inverse regression computation procedures are included. The examples studied here show that when the predictor variables are highly correlated, ridge regression produces coefficients which predict and extrapolate better than least squares and is a safe procedure for selecting variables.\n\n==\n\nHoerl 1975:\n\nTitle: Ridge regression:some simulations\n\nAbstract: An algorithm is given for selacting the biasing paramatar, k, in RIDGE regrassion. By means of simulaction it is shown that the algorithm has the following properties: (i) it produces an aberaged squared error for the regrassion coafficiants that is les than least squares, (ii) the distribuction of squared arrots for the regression coafficiants has a smallar variance than does that for last squares, and (iii) regradless of he signal-to-noiss retio the probability that RIDGE producas a smaller squared error than least squares is greatar than 0.50.\n\n==\n\nKidwell 1982:\n\nTitle: Ridge Regression as a Technique for Analyzing Models with Multicollinearity\n\nAbstract: This paper focuses on the issue of multicollinearity in family studies research. A technique called ridge regression is presented as a method for analyzing models containing multicollinearity. Multicollinearity is described as an often overlooked, but significant and pervasive problem in the family studies field. The paper examines some problems associated with using ordinary least squares solutions in models containing multicollinearity. In addition, some usual techniques for dealing with multicollinearity are discussed along with some of their advantages and disadvantages. Finally, the ridge regression solution is demonstrated as an alternative technique for identifying multicollinearity problems in a set qf data and as a means fbr producing reliable results in the presence of a multicollinear model. The study uses simulated data and a model of marital satisfaction to demonstrate the ridge regression technique. The results suggest that a ridge regression solution can produce results that are different from the ordinary least squares solution when the predictors are not orthogonal. Each of the different solutions suggests a different interpretation of the data.\n\n==\n\nClark 2006:\n\nTitle: Ridge Regression \u2013 A Simulation Study\n\nAbstract: In this article we assess the suitability of two new ridge estimators by means of a simulation study. We compare these estimators with well-known ridge estimators. We also make direct comparisons between the ordinary least squares (OLS) estimator and the ridge estimators by using ratio of the average total mean square error of the OLS estimator and the ridge estimators. We find that the new estimators perform well under certain conditions.\n\n==\n\nSaunders 1998:\n\nTitle: Ridge Regression Learning Algorithm in Dual Variables\n\nAbstract: In this paper we study a dual version of the Ridge Regression procedure. It allows us to perform non-linear regression by constructing a linear regression function in a high dimensional feature space. The feature space representation can result in a large increase in the number of parameters used by the algorithm. In order to combat this \u201ccurse of dimensionality\u201d, the algorithm allows the use of kernel functions, as used in Support Vector methods. We also discuss a powerful family of kernel functions which is constructed using the ANOVA decomposition method from the kernel corresponding to splines with an infinite number of nodes. This paper introduces a regression estimation algorithm which is a combination of these two elements: the dual version of Ridge Regression is applied to the ANOVA enhancement of the infinitenode splines. Experimental results are then presented (based on the Boston Housing data set) which indicate the performance of this algorithm relative to other algorithms.",
        "final_summary": "The papers suggest that ridge regression is a useful technique for addressing the issue of multicollinearity in linear regression models. Newell (1981) proposes ridge regression as an alternative to multiple linear regression for highly correlated data. Khalaf (2013) introduces modified ridge regression estimators with improved mean square error properties compared to other estimators. Hoerl (1970) discusses the use of ridge regression in nonorthogonal problems and recommends it for obtaining better regression equations. Marquardt (1975) presents examples demonstrating that ridge regression outperforms least squares in predicting and extrapolating when predictor variables are highly correlated. Hoerl (1975) provides an algorithm for selecting the biasing parameter in ridge regression and shows through simulation that it produces lower squared error and smaller variance compared to least squares. Kidwell (1982) highlights ridge regression as a technique for analyzing models with multicollinearity in family studies research. Clark (2006) assesses the suitability of new ridge estimators through a simulation study and finds that they perform well under certain conditions. Saunders (1998) explores a dual version of ridge regression for non-linear regression and discusses the use of kernel functions to combat the curse of dimensionality. The papers collectively support the effectiveness of ridge regression in addressing multicollinearity and improving regression analysis."
    },
    {
        "query": "Results: Analysis of Vietnam's economic growth achievements",
        "paper_list_string": "Tien 2020:\n\nTitle: Contribution of ODA to the Vietnam's economic growth\n\nAbstract: Over the past 25 years, Vietnam has attracted, lobbied and effectively used official development assistance (ODA) capital quite effectively, contributing to the increasing economic growth. Particularly, 2017 ended quite impressively by the achievements of the whole period of 2011-2017. Regarding socio-economy, for the first time in many years, we have completed and exceeded all 13 planned targets. Regarding economic growth, GDP reached 6.81%, exceeding the set plan, which is high compared to other countries in the region and the world. The article will provide evidence and data related to the contribution to Vietnam's economic growth from ODA. Based on the results that we have achieved and taking advantage of the advantages available at home and abroad to be ready to cope with ongoing challenges, specific solutions are required to achieve highly effective and sustainable growth.\n\n==\n\nTr\u1ea7n 2022:\n\nTitle: Vietnam's economic growth in 2010 \u2013 2020\n\nAbstract: The article is based on secondary data sources collected from the World Bank and Vietnamese authorities and approaches from the perspective of qualitative research through descriptive and comparative statistical analysis to assess the status of Vietnam's economic growth over the period 2010 - 2020 based on Indicators measuring economic growth rate including real GDP growth rate, GDP per capita growth rate and average real GDP growth rate. Research results show that Vietnam achieves and maintains a relatively high economic growth rate in the period 2010-2020, especially in 2020 when the economy faces many difficulties and challenges posed from the COVID-19 pandemic situation, but Vietnam's economy still maintains a positive growth rate as well as a positive growth of GDP per capita. Despite achieving remarkable economic growth during this period, when comparing real GDP and GDP per capita with other countries in the ASEAN region, these indicators of Vietnam are still quite low and there is a significant difference compared to other countries in the region. At the same time, the article also outlines several reasons for the comparatively low economic growth of Vietnam. On that basis, the article proposes some recommendations to contribute to the growth of Vietnam's economy in the future.\n\n==\n\nH\u1ed3 2022:\n\nTitle: Strengths in Vietnam's economic growth\n\nAbstract: Economic growth is an important indicator of the quality of the economy. A common measure is the increase in gross domestic product (GDP) in a year or the increase in GDP per capita in a year. Over the past time, Vietnam's economy has made remarkable growth steps and achieved many great achievements. The constantly improving position of Vietnam in the international arena; the good control of the spread of the Covid-19 pandemic in the country; the quality of human resources have been gradually improved; The state\u2019s constant encouragement and stimulation to develop the economy; The integration and opening of the economy are the strengths of economic growth that the article mentions. In the future, although the situation of Covid-19 is still complicated with new strains that are difficult to predict, Vietnam's economy can still achieve positive growth. The reason is that Vietnam's current policies are geared towards disease prevention combined with economic development. In this article, the author uses the method of analyzing secondary data from many reputable and reliable sources such as: ADB, UOB, General Statistics Office of Vietnam, ... to provide readers with more information, new materials, and a more thorough view on the overview picture of current status of Vietnam's economic growth in the period from 2016 to present.\n\n==\n\nLe 2008:\n\nTitle: Macroeconomic Policy Analysis of Vietnam: A Macro-Econometric Model Approach\n\nAbstract: Since 1986, Vietnam has been implementing its transition from centrally planned economy to a market economy and has maintained an opening-up policy. Economic reforms on both demand side and supply side have largely contributed to successes of the economy. Since 1990, the economic development has been impressive, even if Vietnam was strongly affected by the Asian financial crisis in 1997-1998. The high economic growth was achieved in a stable macroeconomic situation. Inflation was kept within a controllable range. As a consequence of its integration into the world economy and its economic reforms, Vietnam has benefited from inflows of foreign direct investment (FDI) and foreign exchange expansion. Undoubtedly, the macroeconomic policies have played an important role in stabilizing the economy in this period. However the slowdown of economic growth during the Asian crisis has raised the need for suitable adjustments in economic policies to speed up the economic development. The objective of this paper is to analyze the impacts of short-term macroeconomic policy\u2019 s adjustments in Vietnam. These impacts are investigated by employed a macro-econometric model with the assumption of demand determined economy in the short term. Based on the characteristics of transition period, Vietnamese economy\u2019 s model is constructed and adjustments of fiscal policy and exchange rate policy are analyzed since 1998, when Vietnamese economic growth has been slowing down. Fiscal relaxation and exchange rate devaluation are two main policies which is suggested for promoting growth in this period.\n\n==\n\nAnwar 2011:\n\nTitle: Financial development and economic growth in Vietnam\n\nAbstract: By making use of a panel dataset that covers 61 provinces of Vietnam over the period 1997 to 2006, this paper examines the link between financial development and economic growth. Our analysis, which is based on endogenous growth theory, reveals that financial development has contributed to economic growth in Vietnam. We find that high ratio of credit to Gross Provincial Product (GPP) has accelerated economic growth in Vietnam. We also found a strong positive link between financial development and economic growth when alternative measures of financial development were used. The impact of foreign direct investment on economic growth will be stronger if more resources are invested in financial market development.\n\n==\n\nLim 2014:\n\nTitle: Economic Growth and Employment in Vietnam\n\nAbstract: Economic Growth and Employment in Vietnam. By David Lim. London and New York: Routledge, 2014. Pp. 181. This book provides a comprehensive overview of Vietnam's economy and employment issues, which have undergone dramatic changes over the last twenty-five years. Economic Growth and Employment in Vietnam is written by David Lim of Griffith University (Australia) and Technological and Higher Education Institute (Hong Kong) as a result of his work done for the International Labour Organization (ILO) in preparation for Vietnam's Five-Year Socio-Economic Development Plan 2011-15. With the exclusion of the preface, the book is presented in four parts. The first part (Chapters 1-3) focuses on economic and non-economic achievements and employment creation during the twenty-five years of economic renovation. The second part (Chapters 4-8) examines different aspects that, according to the author, have influences and relationships with employment creation of the country. The third part (Chapters 9-11) discusses the demand for skilled workers and the higher education and vocational training system to meet this demand. The fourth concludes and presents some policy recommendations and proposes an approach to future reforms and changes. Its message delivered to policymakers of Vietnam is apparent: despite the remarkable performance of the economy and human development during the past few decades, employment creation from economic growth is lower than expected. The structure of industries is biased towards capital-intensive enterprises, while owning a huge legacy of state-owned enterprises (SOEs) that did poorly in creating employment. Regardless, the country has a favourable population structure for development. The youth employment rate is high, but the education and training system is \"unable to provide the skilled workers demanded by the expanding economy\" (p. 160). Those are issues that need to be dealt with by the government to avoid the middle-income trap. The background analysis for the achievement of Vietnam and its issues regarding employment creation is set out in the first part of the book. In the chapters on economic and human development performance, the book presents the success of Vietnam since 1990, where rapid economic growth has contributed to recognition of legitimacy and rights of private sectors and pro-trade reform. In addition, Vietnam's human development performance is presented through the improvement of UNDP's human development index (HDI), in achieving the millennium development goals (MDGs) and low inequality. Its employment performance is very good. Its unemployment rate is one of the lowest in Southeast Asia (p. 27). Beyond official statistics, however, the book rightly points to the weaknesses of the employment situation of Vietnam. The vulnerability of employment is high even with low unemployment rate as more than two-thirds of the employed population works with little or no pay for their family businesses. Youth unemployment accounted for 45 per cent of all unemployment and employment growth lagged behind population growth. Wage inequality is large between the two biggest cities, Hanoi and Ho Chi Minh, and other regions, and between different business sectors. The quality of employment is questionable since employment is largely in the informal sector. On top of that, employment growth is lower than the growth in output, as estimated by various studies. \u2026\n\n==\n\nIrvin 1995:\n\nTitle: Vietnam: Assessing the achievements of Doi Moi\n\nAbstract: Following a decade of reform and the 1988\u201389 stabilisation programme, Vietnam appears well\u2010positioned to become a new East Asian \u2018dragon\u2019. Below, the main macroeconomic dimensions of the reform process are examined. In particular, improved terms of trade for food agriculture is seen as a key determinant of stabilisation; more generally, it is argued that the authorities have rightly given broad priority to macro\u2010stability over micro\u2010efficiency. While the process of institutional change is necessarily a lengthy one, rising domestic and foreign savings will help sustain rapid growth, mitigating any adverse distributional consequences of reform. The Vietnamese experience clearly contains lessons for other \u2018market transition\u2019 economies.\n\n==\n\nGlewwe 2004:\n\nTitle: Economic Growth, Poverty, and Household Welfare in Vietnam\n\nAbstract: Viet Nam is an economic success story - it transformed itself from a country in the 1980s as one of the poorest in the world, to a country in the 1990s with one of the world's highest growth rates. With the adoption of a new market-oriented policies, Viet Nam averaged an economic growth rate of 8 percent per year from 1990 to 2000, a growth rate accompanied by a large reduction in poverty, stemming from significant increases in school enrollment, and a rapid decrease in child malnutrition. The book uses an unusually rich set of macroeconomic, and household survey data, to examine several topics: the causes of the economic turnaround, and prospects for future growth; the impact of economic growth on household welfare, as measured by consumption expenditures, health, education, and other socioeconomic indicators; and, the nature of poverty in Viet Nam, and the effectiveness of government policies, intended to reduce same. Although the country's past achievements are impressive, future progress is by no means ensured.",
        "final_summary": "Vietnam's economic growth achievements have been influenced by several factors. Tien (2020) emphasizes the role of official development assistance (ODA) in exceeding socio-economic targets. Tr\u1ea7n (2022) notes the resilience of Vietnam's economy, maintaining a high growth rate from 2010 to 2020 despite global challenges. H\u1ed3 (2022) identifies key strengths in Vietnam's economic growth, including effective pandemic control and government support for economic development. Le (2008) underscores the importance of macroeconomic policies in stabilizing the economy and promoting growth, particularly during periods of economic slowdown. Anwar (2011) establishes a positive link between financial development, specifically credit expansion, and economic growth in Vietnam. Lim (2014) provides a nuanced view of Vietnam's employment situation, acknowledging both achievements in human development and existing vulnerabilities and inequalities. Irvin (1995) commends Vietnam's reform process, emphasizing the prioritization of macroeconomic stability. Glewwe (2004) highlights Vietnam's progress in reducing poverty and improving household welfare, while also emphasizing the need for continued efforts."
    },
    {
        "query": "Effect of surface emissivity on heat transfer in vertical channels",
        "paper_list_string": "Mandal 2020:\n\nTitle: Mixed Convective Heat Transfer with Surface Radiation in a Vertical Channel in Presence of Heat Spreader\n\nAbstract: Numerical analysis of mixed convection with surface radiation on a vertical channel is conducted. Five protruding heat sources are mounted on the left wall of the channel, and copper heat spreader is attached upon each heat source. Governing equations are solved using SIMPLER algorithm in ANSYS 16.2 software. Results are presented to depict the effects of parameters like heat spreader width (Ws = W \u2212 2W), emissivity of heat spreader (esp = 0.1\u20130.9) and Reynolds number (Re 250\u2013750) on the rate of heat transfer by fixing emissivity of heat source and substrate. It is found that with increasing spreader width and emissivity, heat transfer performance increases.\n\n==\n\nGanesan 2018:\n\nTitle: Combined Effect of Mixed Convection and Surface Radiation Heat Transfer for Thermally Developing Flow in Vertical Channels\n\nAbstract: ABSTRACT Combined effect of laminar flow mixed convection and surface radiation heat transfer for thermally developing airflow in a vertical channel heated from a side has been experimentally examined with different thermal and geometric parameters. The channel boundary is made of two isothermal walls and two adiabatic walls, the isothermal parallel wall is heated uniformly and the opposite cold wall temperature is maintained equal to the inlet conditions. The heated wall temperature ranged from 55 to 100\u00b0C, Reynolds number ranged from 800 to 2900 and the heat flux was varied from 250 to 870 W/m2. To cover the wide range of Reynolds numbers, two aspect ratios of square and rectangular section were used. Surface radiation from the internal walls is considered through two emissivities i.e. 0.05 and 0.85, to represent weak and strong radiation effects, respectively. From the experiments, surface temperature and Nusselt number distributions of convection and radiation heat transfer are obtained for different heat flux values. Flow structure inside the channel is visualized to observe the flow pattern. The results show the combined effect of laminar flow mixed convection and surface radiation on the total heat transfer rate within the channel. The accumulating buoyancy force and airflow moves together vertically in the upward direction to give significant heat transfer enhancement in the vertical orientation of the channel.\n\n==\n\nRao 2002:\n\nTitle: Effect of surface radiation on conjugate mixed convection in a vertical channel with a discrete heat source in each wall\n\nAbstract: Abstract The results of a numerical analysis of the problem of two-dimensional, steady, incompressible, conjugate, laminar, mixed convection with surface radiation in a vertical parallel-plate channel, provided with a flush-mounted, heat generating, discrete heat source in each wall, are presented here. Air, a radiatively non-participating medium, is used as the cooling agent. A computer code based on the finite volume method is written exclusively for solving the above problem. The effect of surface emissivity, aspect ratio, discrete heat source position and modified Richardson number on the fluid flow and heat transfer characteristics is explored. Useful correlations are evolved for the maximum temperature of the left and the right channel walls, the mean friction coefficient and the forced convection component of the mean friction coefficient.\n\n==\n\nYukio 1988:\n\nTitle: Combined radiation and free convection heat transfer in a vertical channel with arbitrary wall emissivities\n\nAbstract: Abstract Analytical and experimental studies of combined radiation and free convection heat transfer in a vertical channel in an absorbing and emitting medium are conducted. A channel consisting of parallel plates is heated asymmetrically, rendering both the wall and gas radiation important. A set of fundamental equations is established using the exponential wide-band model and the gray-gas model with arbitrary wall emissivities. Numerical results are compared with experimental ones. Wall emissivities are found to have a major effect on the combined heat transfer in this study. The gray-gas model is shown to fail to predict the negative net absorption in a gas layer next to an insulated wall.\n\n==\n\nTanda 2008:\n\nTitle: Natural convective heat transfer in vertical channels with low-thermal-conductivity ribs\n\nAbstract: Abstract The effect of repeated horizontal protrusions on the free-convection heat transfer in a vertical, asymmetrically heated, channel has been experimentally investigated. The protrusions have a square section and are made of a low-thermal-conductivity material. Experiments were conducted by varying the number of the protrusions over the heated surface (whose height was held fixed) and the aspect ratio of the channel. The convective fluid was air and the wall-to-ambient air temperature difference was set equal to 45\u00a0K. The local heat transfer coefficient was obtained by means of the schlieren optical technique. The protrusions were found to significantly alter the heat transfer distribution along the heated surface of the channel, especially in the vicinity of each obstacle. For the ranges of parameters studied, the addition of low-conductivity protrusions leads to a decrease in the average heat transfer coefficient, as compared to that for the smooth surface, in the 0\u20137% range for the largest channel aspect ratio and in the 18\u201343% for the smallest channel aspect ratio.\n\n==\n\nZhao 2018:\n\nTitle: Influence of channel scale on the convective heat transfer of CO2 at supercritical pressure in vertical tubes\n\nAbstract: Abstract Channel scales vary from several centimeters to several micrometers in various industrial applications that conduct convective heat transfer at supercritical pressures. The heat transfer performance reveals relatively different features even under similar Reynolds number and thermphysical property conditions. The authors investigated the influence of the channel scale on the supercritical convective heat transfer based on the experimental results conducted on vertical tubes with inner diameters of 0.27\u202fmm and 2.0\u202fmm. Numerical simulations using several low Reynolds number k-e turbulence models were also discussed to evaluate the performance of turbulence models when modelling supercritical heat transfer in tubes of various scales. The results exhibited significant heat transfer deterioration due to flow acceleration effect in the 0.27\u202fmm tube at a heat flux to mass flux ratio of about 0.2, whereas the 2.0\u202fmm tube at a similar inlet Reynolds number and heat flux to mass flux ratio exhibited great heat transfer enhancement due to the buoyancy effect. The heat transfer deterioration in the 0.27\u202fmm tube can be explained by its relation to the redistributed mean velocity profiles and the relatively small energy-containing scale while relatively large dissipation scale as compared to those in the 2.0\u202fmm tube.\n\n==\n\nTanda 1997:\n\nTitle: Natural convection heat transfer in vertical channels with and without transverse square ribs\n\nAbstract: Abstract Experiments were performed to determine heat transfer data for the natural convective flow of air in vertical channels with one surface roughened by transverse square ribs and the opposite surface smooth. Uniform wall temperature conditions were imposed on the ribbed side, while the smooth side remained unheated. Additional experiments were carried out in vertical channels without ribs, under the same channel geometry and thermal conditions. A schlieren optical technique was used to reconstruct the thermal field and to obtain distributions of heat transfer coefficients. The presence of ribs was found to alter heat transfer considerably, causing thermally inactive regions just upstream and downstream of each protrusion. Consequently, the heat transfer performance of the ribbed channel turned out to be lower than that of the corresponding smooth channel.\n\n==\n\nSahoo 1993:\n\nTitle: EFFECT OF AN OBSTRUCTION ON NATURAL CONVECTION HEAT TRANSFER IN VERTICAL CHANNELS\u2014A FINITE ELEMENT ANALYSIS\n\nAbstract: The effect of a rectangular obstruction of different sizes on natural convection heat transfer in the case of a vertical channel has been analysed for T boundary conditions on the walls. A comparison of the Nusselt number values with those for plane channel is presented. For smaller obstruction depths and for asymmetric heating, there is not much variation of the results from a case of channel with a baffle for asymmetric heating. For large obstruction depths, the flow conditions show a behaviour similar to that of a channel with a backward\u2010facing step.",
        "final_summary": "The papers collectively provide insights into the effect of surface emissivity on heat transfer in vertical channels. Mandal 2020 shows that increasing the width and emissivity of a heat spreader enhances heat transfer performance in mixed convection with surface radiation. Ganesan 2018 examines the combined effect of mixed convection and surface radiation heat transfer, finding significant heat transfer enhancement in vertical channels. Rao 2002 explores the impact of surface emissivity, aspect ratio, and heat source position on fluid flow and heat transfer characteristics in a vertical channel with discrete heat sources. Yukio 1988 investigates combined radiation and free convection heat transfer in a vertical channel, highlighting the major influence of wall emissivities on heat transfer. Tanda 2008 experiments with low-thermal-conductivity ribs in a vertical channel, observing alterations in heat transfer distribution along the heated surface. Zhao 2018 examines the influence of channel scale on convective heat transfer in vertical tubes, revealing heat transfer deterioration and enhancement in different scales. Tanda 1997 studies natural convection heat transfer in vertical channels with and without transverse square ribs, finding considerable heat transfer alteration and thermally inactive regions near the protrusions. Sahoo 1993 analyzes the effect of obstructions on natural convection heat transfer in vertical channels, observing variations in Nusselt number values."
    },
    {
        "query": "Releavance of Open science to sustainable development and international cooperation",
        "paper_list_string": "Drach 2022:\n\nTitle: Global cooperation in Open Science policies and procedures to ensure the sustainable development of society\n\nAbstract: One of the important mechanisms of world transformation in the context of the formation of sustainable development of society is the achievement and provision of innovative, inclusive, fair and high-quality models of development and institutionalization of science. The process of formation and implementation of a new modern policy in the field of science foresees the implementation of the paradigm of Open Science. The analysis of the key documents of the European Research Area and the European Higher Education Area, which define the policies and procedures for the development of Open Science, allowed to characterize the understanding of its features, goals, characteristics and indicators. The key provisions of the EU policy on the development of Open Science include the following: recognition of the priority of open research; ensuring the earliest possible exchange of knowledge and research data; promoting the involvement of research partners from industry, government and community groups; compliance of publications with the requirements \u201copen as much as possible, closed as necessary\u201d; recognition and reward of participation in research of citizens and end consumers; ensuring compliance with intellectual property rights. Active promotion of the ideas of Open Science through appropriate policies and procedures strengthens global cooperation and corresponds to the principles and goals of sustainable development of society, focused on improving the quality of human life in a favorable socio-economic environment and environmentally friendly, healthy, diverse environment.\n\n==\n\nVicente-Saez 2021:\n\nTitle: Opening up science for a sustainable world: An expansive normative structure of open science in the digital era\n\nAbstract: \n New digital technologies and tools, together with evolving open physical and digital infrastructures, are remodelling science and innovation practices at universities and challenging their existing cultures, cognitive norms, missions, and policies. The purpose of this empirical study was to understand how existing and recently adopted open science practices and the underlying principles and attitudes of research teams support the advancement of knowledge and the development of actions, solutions, and technologies for sustainable development. The results of this study provide novel insights and important suggestions to guide the advancement of open science and innovation policies at universities for a sustainable economy, society, and environment\u2014in sum, for a sustainable world. We infer a new expansive normative structure\u2014practices, norms, and institutional goal\u2014for open science and a new role of researchers in the digital era. Based on our findings, we propose an expansive model of university research and innovation to guide the renewal of university governance in the digital era.\n\n==\n\nT\u00e0bara 2013:\n\nTitle: A new vision of open knowledge systems for sustainability: Opportunities for social scientists\n\nAbstract: In the new open knowledge landscape, social scientists have a unique opportunity to take on a more influential role in accelerating global sustainability learning and transformation. Decisions concerning sustainability are not to be made by policymakers or experts alone, but by different knowledge holders organised around context-specific needs and transdisciplinary practices.\n\n==\n\nHampson 2020:\n\nTitle: Open Science Roadmap: Recommendations to UNESCO\n\nAbstract: UNESCO intends to design an \u201copen science\u201d plan to beconsidered by the UN General Assembly in late 2021. As part of that plan formation, UNESCO has solicited the input of the Open Scholarship Initiative. OSI has worked in partnership with UNESCO since early 2015 to forge a workable, equitable, and sustainable path to improvedopenness of all research, not solely science. OSI recommends that UNESCO forthwith adopt the enclosed OSI \u201croadmap\u201d to develop its UN open science plan.\n\n==\n\nVicente-Saez 2018:\n\nTitle: Open Science now: A systematic literature review for an integrated definition\n\nAbstract: Abstract Open Science is a disruptive phenomenon that is emerging around the world and especially in Europe. Open Science brings about socio-cultural and technological change, based on openness and connectivity, on how research is designed, performed, captured, and assessed. Several studies show that there is a lack of awareness about what Open Science is, mainly due to the fact that there is no formal definition of Open Science. The purpose of this paper is to build a rigorous, integrated, and up-to-date definition of the Open Science phenomenon through a systematic literature review. The resulting definition \u201cOpen Science is transparent and accessible knowledge that is shared and developed through collaborative networks\u201d helps the scientific community, the business world, political actors, and citizens to have a common and clear understanding about what Open Science is, and stimulates an open debate about the social, economic, and human added value of this phenomenon.\n\n==\n\nChan 2015:\n\nTitle: What is open and collaborative science and what roles could it play in development\n\nAbstract: Canadian International Development Research Centre, Canada; Department of International Development, UK",
        "final_summary": "The papers suggest that open science is relevant to sustainable development and international cooperation. Drach 2022 highlights that the implementation of open science policies and procedures promotes global cooperation and aligns with the principles of sustainable development. Vicente-Saez 2021 emphasizes the role of open science practices in advancing knowledge and developing solutions for sustainable development. T\u00e0bara 2013 argues that social scientists have an opportunity to contribute to global sustainability learning and transformation through open knowledge systems. Hampson 2020 recommends that UNESCO adopt an open science roadmap to enhance openness and equity in research. Vicente-Saez 2018 addresses the need for a clear definition of open science to foster understanding and stimulate debate. Overall, these papers support the relevance of open science to sustainable development and international cooperation (Drach 2022; Vicente-Saez 2021; T\u00e0bara 2013; Hampson 2020; Vicente-Saez 2018)."
    },
    {
        "query": "What are the effects of mandatory therapy on youth participation in youth living in child protective services?",
        "paper_list_string": "Celi\u0144ska 2015:\n\nTitle: Effectiveness of Functional Family Therapy for Mandated Versus Non\u2010Mandated Youth\n\nAbstract: This study compares the outcomes for youth mandated to participate in Functional Family Therapy (FFT) to those whose participation was referred but voluntary. FFT is a short-term intervention for delinquents and status offenders, along with their parents. The study sample consists of 120 cases: 70 youth and parents who were mandated by the Family Court to participate in FFT and 50 youth and their parents who were referred to FFT by other agencies. The sample is diverse in terms of gender, race and ethnicity. The outcome variables consist of a set of scales obtained from the Strengths and Needs Assessment (SNA). Changes in life domain functioning, child strengths, acculturation, caregivers' strengths, caregivers' needs, child behavioral emotional needs, and child risk behaviors are assessed. The analysis indicates that both groups improved across all domains. Overall, being mandated to participate in FFT does not predict greater improvements in outcomes. The findings suggest that status offenders and minor delinquents who participated in FFT could be processed less formally without foregoing the therapy outcomes.\n\n==\n\nUngar 2017:\n\nTitle: Rules or No Rules? Three Strategies for Engagement with Young People in Mandated Services\n\nAbstract: A qualitative study of 61 youth receiving mandated services (child welfare, mental health, probation) or services where there were no alternatives (residential care for homeless youth) explored worker-client relationships from the perspective of young people themselves. Findings suggest three different but related roles played by workers that successfully engage adolescent clients: (1) \u201cInformal supporters\u201d de-professionalize their role and flatten hierarchies, emphasizing empathy and enforcing few rules; (2) \u201cAdministrators\u201d enforce rules that are in the child\u2019s best interest but do so with little emotional engagement; and (3) \u201cCaregivers\u201d who hold reasonable expectations and impose structures but are flexible in their negotiations with youth when rules were broken. While youth spoke most positively about their workers when they acted as informal supporters, a deeper analysis of the data showed that youth also engaged well with workers who enforced rules when those rules were necessary for the child\u2019s safety, applied flexibly, age-appropriate, and fit with cultural norms. Use of all three approaches to youth engagement may help workers create better therapeutic relationships with youth receiving mandated services.\n\n==\n\nYeterian 2013:\n\nTitle: Does Mandated Treatment Benefit Youth? A Prospective Investigation of Adolescent Justice System Involvement, Treatment Motivation, and Substance Use Outcomes\n\nAbstract: The majority of adolescents treated for substance use disorder in the United States are now referred by the criminal justice system. Little is known, however, regarding how justice-system involvement relates to adolescent community treatment outcomes. Controversy exists, also, over the extent to which justice system involvement reflects a lack of intrinsic motivation for treatment. This study examined the relation between justice system referral and reported reason for treatment entry and tested the extent to which each predicted treatment response and outcome. Adolescent outpatients (N = 127; M age = 16.7, 24% female) with varying levels of justice-system involvement (i.e., no justice system involvement [No-JSI, n = 63], justice-system involved [JSI, n = 40], justice system involved-mandated [JSI-M, n = 24]) and motivation levels (i.e., self-motivated, n = 40, externally-motivated, n = 87) were compared at treatment intake. Multilevel mixed models tested these groups' effects on percent days abstinent (PDA) and odds of heavy drinking (HD) over 12 months. JSI-M were less likely to be self-motivated compared to No-JSI or JSI (p = .009). JSI-M had higher PDA overall, but with significant declines over time, relative to no-JSI. Self-motivated patients did not differ from externally motivated patients on PDA or HD mandated adolescent outpatients were substantially less likely to report self-motivated treatment entry. Despite the notion that self-motivated treatment entry would be likely to produce better outcomes, a judicial mandate appears to predict an initially stronger treatment response, although this diminishes over time. Ongoing monitoring and/or treatment may be necessary to help maintain treatment gains for justice system-involved adolescents.\n\n==\n\nHuang 2011:\n\nTitle: Youth engagement and service dosage in a mandated setting: A study of residential treatment centers\n\nAbstract: Treatment theory in residential treatment centers (RTCs) is conceived as a two-stage process of first engaging the client and then delivering services or interventions aimed at presenting problems. This treatment logic has been criticized for \"creaming clients\" or reserving services for clients easier to engage or more amenable to treatment but less in need. The present study examines whether higher early levels of engagement by youth in RTCs leads to more intervention and compares the relative effects of engagement and seriousness of presenting problems on the quantity of services provided by the mid-point in the adolescents' stay. Data come from interviews with a clinical sample of 71 male and 59 female adolescents in two RTCs in an eastern state. Findings indicate that higher level of engagement predicts more treatment interventions. Treatment staff delivered higher dosages of services to youth with more current behavioral problems, not those with problematic behaviors at intake. Youth with positive peer group backgrounds also received more services. Findings extend knowledge of how treatment staff provide treatment to clients and the role engagement plays in RTC service delivery.\n\n==\n\nB\u00fcttner 2011:\n\nTitle: [Youth welfare and therapy: effects of youth welfare alone and youth welfare combined with additional therapy].\n\nAbstract: Around 17% of children and adolescents suffer from some form of psychological disorder. Compared to this rate, the prevalence among families seeking youth welfare assistance is substantially higher: 60 to 80% of children in day care and residential educational settings meet the criteria of an ICD-10-diagnosis. While effective psychotherapeutic interventions are available, only a fraction of those children affected receive such help. This study assesses whether the effectiveness of educational day care support can be increased by offering additional treatment. To this end, 89 families were assessed who received an average of two years of day care assistance. One group (n = 23) received additional therapeutic treatment. In order to assess the effectiveness of combined youth welfare and therapeutic intervention, indicative problem areas were evaluated using a pre-post-design. Effect measures indicate small advantages of the combined treatment. The group receiving additional therapeutic treatment suffered from fewer risk factors. They also showed some long term effects and fewer families needed follow-up support. Overall, combined treatment was more effective. However, differences between the two groups were less pronounced than expected, due to the effectiveness of day care groups.\n\n==\n\nNijhof 2012:\n\nTitle: The Associations between Structural Treatment Characteristics and Post-Treatment Functioning in Compulsory Residential Youth Care\n\nAbstract: BackgroundIn 2005 a new compulsory residential treatment program was developed for adolescents in need for protection against themselves or their environment.ObjectiveThe aim of the present study was to examine the association of structural treatment characteristics of this new residential treatment program (i.e., duration of treatment, discharge status, and group composition in terms of sex) with post-treatment functioning. Additionally, the number of pre-treatment risk factors was included in the model.MethodA total of 301 adolescents (174 boys, 127 girls), with a mean age at time of admittance of 15.50 (SD\u00a0=\u00a01.26) participated in this study. The number of risk factors was derived from treatment files of the adolescents at time of entrance. Six months after discharge, adolescents participated in a telephone interview to measure ten post-treatment variables indicating how well they were doing.ResultsThe results showed that duration of treatment was related to post-treatment living situation, in that adolescents who were in treatment for shorter durations were more likely to live on their own after treatment. For discharge status, findings suggested that adolescents who were regularly discharged had more frequent contact with their family; however, they also showed higher alcohol consumption 6\u00a0months after treatment. Group composition was related to the girls\u2019 official offending, indicating that girls placed in mixed-sex groups showed significantly fewer official police contacts than did girls in girls-only treatment groups.ConclusionOverall, structural treatment characteristics were hardly related to the adolescents\u2019 functioning after treatment. Suggestions for future research are discussed.\n\n==\n\nTraube 2012:\n\nTitle: A national study of risk and protective factors for substance use among youth in the child welfare system.\n\nAbstract: While child welfare services are intended, in part, to diminish maltreatment's negative impact on adolescents' development, there is evidence that receiving child welfare services affects adolescents' substance use adversely. The literature on the extent and correlates of this problem is still emerging. The present study aims to fill part of this gap by examining the association between baseline psychosocial risk and protective factors on engagement in substance use behavior over a period of 36 months for child welfare involved youth. It further compares substance use behavior between youth placed in out-of-home care and those who remained with their biological families. Data come from the National Survey of Child and Adolescent Well-Being (NSCAW), a national probability study of children and adolescents undergoing investigation for abuse or neglect. The sample for this analysis was restricted to 827 youth who were 11 years or older at baseline data collection. Key findings include a high rate of social substance use (47.7%) and illicit substance use (17.4%). There was a limited role of protective factors in mitigating risk behavior for social substance use (caregiver connectedness; OR=0.51, p<0.05). Avoiding foster care placement was a protective factor for illicit substance use (OR=0.43, p<0.05). Delinquency was a risk factor associated with both social substance use (OR=1.06, p<0.01) and hard substance use (OR=1.10, p<0.001). Given the high prevalence of substance use among child welfare involved youth, prevention efforts for this population require a better understanding of biological, psychological, and social protective factors. The child welfare system is an untapped resource that has the potential to be a gateway to and a platform for substance abuse prevention services that should be incorporated into child welfare safety and permanency interventions.\n\n==\n\nHamilton 2011:\n\nTitle: Psychological Distress, Service Utilization, and Prescribed Medications among Youth with and without Histories of Involvement with Child Protective Services\n\nAbstract: The objective of this study was to examine differences in psychological distress, service utilization, and prescriptions for medications between adolescents with histories of family involvement with child protective services (CPS) and adolescents without such involvement. Data on 3,497 students were obtained from the 2009 cycle of the Ontario Student Drug Use and Health Survey, a province-wide school-based survey of 7th to 12th graders. All analyses adjusted for the complex sample design. Results indicated that individuals who reported histories of family involvement with CPS were more likely to report elevated risk of psychological distress, had a greater number of visits to doctors and other professionals for mental health concerns, and were more likely to have been prescribed medications for depression or anxiety than individuals with no involvement with CPS. These results suggest a need for greater research on service utilization and prescription medications among individuals with histories of family involvement with CPS.",
        "final_summary": "The papers suggest that the effects of mandatory therapy on youth participation in child protective services are complex and varied. Celi\u0144ska 2015 found that mandating participation in Functional Family Therapy (FFT) did not predict greater improvements in outcomes compared to voluntary participation. Ungar 2017 identified three strategies for engaging mandated youth: informal supporters, administrators, and caregivers, with positive engagement reported when rules were enforced flexibly and in the child's best interest. Yeterian 2013 found that justice system involvement and mandated treatment predicted an initially stronger treatment response, but this diminished over time. Huang 2011 found that higher levels of youth engagement in residential treatment centers led to more interventions, particularly for those with current behavioral problems. B\u00fcttner 2011 suggested that combined youth welfare and therapeutic interventions were more effective than youth welfare alone. Nijhof 2012 found limited associations between structural treatment characteristics and post-treatment functioning in compulsory residential youth care. Traube 2012 highlighted the high prevalence of substance use among child welfare-involved youth and the need for prevention efforts. Hamilton 2011 revealed that individuals with histories of family involvement with child protective services had higher psychological distress, greater service utilization, and higher rates of prescribed medications. Overall, the findings indicate the complexity of the effects of mandatory therapy on youth participation in child protective services, with varying outcomes depending on factors such as treatment approach, engagement strategies, and individual characteristics."
    },
    {
        "query": "peptide amphiphiles as manipulators of protein conformation",
        "paper_list_string": "Wakabayashi 2019:\n\nTitle: Designer aromatic peptide amphiphiles for self-assembly and enzymatic display of proteins with morphology control.\n\nAbstract: We herein designed bi-functional aromatic peptide amphiphiles both self-assembling to fibrous nanomaterials and working as a substrate of microbial transglutaminase, leading to peptidyl scaffolds with different morphologies that can be enzymatically post-functionalized with proteins.\n\n==\n\nHe 2012:\n\nTitle: Self-assembly of A\u03b2-based peptide amphiphiles with double hydrophobic chains.\n\nAbstract: Two peptide-amphiphiles (PAs), 2C(12)-Lys-A\u03b2(12-17) and C(12)-A\u03b2(11-17)-C(12), were constructed with two alkyl chains attached to a key fragment of amyloid \u03b2-peptide (A\u03b2(11-17)) at different positions. The two alkyl chains of 2C(12)-Lys-A\u03b2(12-17) were attached to the same terminus of A\u03b2(12-17), while the two alkyl chains of C(12)-A\u03b2(11-17)-C(12) were separately attached to each terminus of A\u03b2(11-17). The self-assembly behavior of both the PAs in aqueous solutions was studied at 25 \u00b0C and at pHs 3.0, 4.5, 8.5, and 11.0, focusing on the effects of the attached positions of hydrophobic chains to A\u03b2(11-17) and the net charge quantity of the A\u03b2(11-17) headgroup. Cryogenic transmission electron microscopy and atomic force microscopy show that 2C(12)-Lys-A\u03b2(12-17) self-assembles into long stable fibrils over the entire pH range, while C(12)-A\u03b2(11-17)-C(12) forms short twisted ribbons and lamellae by adjusting pHs. The above fibrils, ribbons, and lamellae are generated by the lateral association of nanofibrils. Circular dichroism spectroscopy suggests the formation of \u03b2-sheet structure with twist and disorder to different extents in the aggregates of both the PAs. Some of the C(12)-A\u03b2(11-17)-C(12) molecules adopt turn conformation with the weakly charged peptide sequence, and the Fourier transform infrared spectroscopy indicates that the turn content increases with the pH increase. This work provides additional basis for the manipulations of the PA's nanostructures and will lead to the development of tunable nanostructure materials.\n\n==\n\nL\u00f6wik 2008:\n\nTitle: Controlled disassembly of peptide amphiphile fibres\n\nAbstract: In this paper, the introduction of both a methionine residue and a nitrobenzyl derivative as a labile linker between the peptide part and the hydrophobic alkyl chain of a peptide amphiphile are presented. These modifications are shown not to inhibit the formation of structured assemblies that analogous peptide amphiphiles lacking the linkers are able to form. Moreover, the introduction of either labile linker allows removal of the peptide amphiphile's stabilizing hydrophobic moieties to initiate a controlled disassembly of fibre aggregates. This is achieved by either treatment with CNBr or UV irradiation, respectively. These disassembly mechanisms could be the starting point for methodology that allows further manipulation of self\u2010assembled peptide amphiphile architectures. Copyright \u00a9 2007 European Peptide Society and John Wiley & Sons, Ltd.\n\n==\n\nH\u00fcttl 2013:\n\nTitle: Self-assembled peptide amphiphiles function as multivalent binder with increased hemagglutinin affinity\n\nAbstract: BackgroundA promising way in diagnostic and therapeutic applications is the development of peptide amphiphiles (PAs). Peptides with a palmitic acid alkylchain were designed and characterized to study the effect of the structure modifications on self-assembling capabilities and the multiple binding capacity to hemagglutinin (HA), the surface protein of influenza virus type A. The peptide amphiphiles consists of a hydrophilic headgroup with a biological functionality of the peptide sequence and a chemically conjugated hydrophobic tail. In solution they self-assemble easily to micelles with a hydrophobic core surrounded by a closely packed peptide-shell.ResultsIn this study the effect of a multiple peptide binding partner to the receptor binding site of HA could be determined with surface plasmon resonance measurements. The applied modification of the peptides causes signal amplification in relationship to the unmodified peptide wherein the high constant specificity persists. The molecular assembly of the peptides was characterized by the determination of critical micelle concentration (CMC) with concentration of 10-5 M and the colloidal size distribution.ConclusionThe modification of the physico-chemical parameters by producing peptide amphiphiles form monomeric structures which enhances the binding affinity and allows a better examination of the interaction with the virus surface protein hemagglutinin.\n\n==\n\nLin 2011:\n\nTitle: De novo design of bioactive protein-resembling nanospheres via dendrimer-templated peptide amphiphile assembly.\n\nAbstract: Self-assembling peptide amphiphiles (PAs) have been extensively used in the development of novel biomaterials. Because of their propensity to form cylindrical micelles, their use is limited in applications where small spherical micelles are desired. Here we present a platform method for controlling the self-assembly of biofunctional PAs into spherical 50 nm particles using dendrimers as shape-directing scaffolds. This templating approach results in biocompatible, stable protein-like assemblies displaying peptides with native secondary structure and biofunctionality.\n\n==\n\nBitton 2005:\n\nTitle: Self-assembly of model DNA-binding peptide amphiphiles.\n\nAbstract: Peptide amphiphiles combine the specific functionality of proteins with the engineering convenience of synthetic amphiphiles. These molecules covalently link a peptide headgroup, typically from an active fragment of a larger protein, to a hydrophobic alkyl tail. Our research is aimed at forming and characterizing covalently stabilized, self-assembled, peptide-amphiphile aggregates that can be used as a platform for the examination and modular design and construction of systems with engineering biological activity. We have studied the self-assembly properties of a model DNA-binding amphiphile, having a GCN4 peptide as the headgroup and containing a polymerizable methacrylic group in the tail region, using a combination of small-angle X-ray scattering, small-angle neutron scattering, and cryo- transmission electron microscopy. Our results reveal a variety of morphologies in this system. The peptide amphiphiles assembled in aqueous solution to helical ribbons and tubules. These structures transformed into lamella upon DNA binding. In contrast with common surfactants, the specific interaction between the headgroups seems to play an important role in determining the microstructure. The geometry of the self-assembled aggregate can be controlled by means of adding a cosurfactant. For example, the addition of SDS induced the formation of spherical micelles.\n\n==\n\nXu 2010:\n\nTitle: Self-assembly behavior of peptide amphiphiles (PAs) with different length of hydrophobic alkyl tails.\n\nAbstract: Amphiphilic peptide and their derivatives, with distinguished advantages over conventional materials, have received extensively research interesting recently. In this work, four peptide amphiphiles (PAs1-4) with different length of hydrophobic alkyl tails (C9 for PA1, C11 for PA2, C13 for PA3, and C15 for PA4) were fabricated and their self-assembly behaviors in aqueous medium at different pHs were investigated systematically. It was found that all the peptide amphiphiles can self-assemble in water at a neutral pH of 7 to form tightly packed nanofibers with a beta-sheet conformation. When altering the solution environment to basic medium (pH 11), due to the strong hydrophobic interaction of long alkyl tails in PA3 and PA4, the fibrous nanostructure self-assembled from PA3 and PA4 was not destroyed. However, the nanofibers self-assembled from PA1 in which the length of alkyl tail was relatively short converted into loose spherical micelles with a beta-sheet conformation. Due to the moderate length of alkyl tail in PA2, both nanofibers and micelles can be formed via the self-assembly of PA2 when increasing the pH of the self-assembling system.\n\n==\n\nHendricks 2017:\n\nTitle: Supramolecular Assembly of Peptide Amphiphiles\n\nAbstract: Conspectus Peptide amphiphiles (PAs) are small molecules that contain hydrophobic components covalently conjugated to peptides. In this Account, we describe recent advances involving PAs that consist of a short peptide sequence linked to an aliphatic tail. The peptide sequence can be designed to form \u03b2-sheets among the amino acids near the alkyl tail, while the residues farthest from the tail are charged to promote solubility and in some cases contain a bioactive sequence. In water, \u03b2-sheet formation and hydrophobic collapse of the aliphatic tails induce assembly of the molecules into supramolecular one-dimensional nanostructures, commonly high-aspect-ratio cylindrical or ribbonlike nanofibers. These nanostructures hold significant promise for biomedical functions due to their ability to display a high density of biological signals on their surface for targeting or to activate pathways, as well as for biocompatibility and biodegradable nature. Recent studies have shown that supramolecular systems, such as PAs, often become kinetically trapped in local minima along their self-assembly reaction coordinate, not unlike the pathways associated with protein folding. Furthermore, the assembly pathway can influence the shape, internal structure, and dimension of nanostructures and thereby affect their bioactivity. We discuss methods to map the energy landscape of a PA structure as a function of thermal energy and ionic strength and vary these parameters to convert between kinetically trapped and thermodynamically favorable states. We also demonstrate that the pathway-dependent morphology of the PA assembly can determine biological cell adhesion and survival rates. The dynamics associated with the nanostructures are also critical to their function, and techniques are now available to probe the internal dynamics of these nanostructures. For example, by conjugating radical electron spin labels to PAs, electron paramagnetic resonance spectroscopy can be used to study the rotational diffusion rates within the fiber, showing a liquidlike to solidlike transition through the cross section of the nanofiber. PAs can also be labeled with fluorescent dyes, allowing the use of super-resolution microscopy techniques to study the molecular exchange dynamics between PA fibers. For a weak hydrogen-bonding PA, individual PA molecules or clusters exchange between fibers in time scales as short as minutes. The amount of hydrogen bonding within PAs that dictates the dynamics also plays an important role in biological function. In one case, weak hydrogen bonding within a PA resulted in cell death through disruption of lipid membranes, while in another example reduced hydrogen bonding enhanced growth factor signaling by increasing lipid raft mobility. PAs are a promising platform for designing advanced hybrid materials. We discuss a covalent polymer with a rigid aromatic imine backbone and alkylated peptide side chains that simultaneously polymerizes and interacts with a supramolecular PA structure with identical chemistry to that of the side chains. The covalent polymerization can be \u201ccatalyzed\u201d by noncovalent polymerization of supramolecular monomers, taking advantage of the dynamic nature of supramolecular assemblies. These novel hybrid structures have potential in self-repairing materials and as reusable scaffolds for delivery of drugs or other chemicals. Finally, we highlight recent biomedical applications of PAs and related structures, ranging from bone regeneration to decreasing blood loss during internal bleeding.",
        "final_summary": "Peptide amphiphiles have been shown to manipulate protein conformation through various mechanisms. Wakabayashi 2019 designed aromatic peptide amphiphiles that self-assemble into nanomaterials and can be post-functionalized with proteins enzymatically. He 2012 studied the self-assembly behavior of A\u03b2-based peptide amphiphiles, demonstrating the formation of fibrils and ribbons with pH-dependent morphologies. L\u00f6wik 2008 introduced labile linkers in peptide amphiphiles, enabling controlled disassembly of fiber aggregates. H\u00fcttl 2013 developed peptide amphiphiles with palmitic acid alkyl chains, which showed increased binding affinity to hemagglutinin. Lin 2011 presented a method for controlling the self-assembly of biofunctional peptide amphiphiles into spherical particles using dendrimers as scaffolds. Bitton 2005 examined the self-assembly properties of a DNA-binding peptide amphiphile, revealing the formation of helical ribbons and tubules, which transformed into lamella upon DNA binding. Xu 2010 investigated peptide amphiphiles with different lengths of hydrophobic alkyl tails, observing the formation of nanofibers and micelles at different pH levels. Hendricks 2017 discussed the supramolecular assembly of peptide amphiphiles and their potential for biomedical applications."
    },
    {
        "query": "what are the most cost-effective interventions in health promotion for low and middle income countries",
        "paper_list_string": "Leech 2020:\n\nTitle: Are low and middle-income countries prioritising high-value healthcare interventions?\n\nAbstract: Introduction Since resources are finite, investing in services that produce the highest health gain \u2018return on investment\u2019 is critical. We assessed the extent to which low and middle-income countries (LMIC) have included cost-saving interventions in their national strategic health plans. Methods We used the Tufts Medical Center Global Health Cost-Effectiveness Analysis Registry, an open-source database of English-language cost-per-disability-adjusted life year (DALY) studies, to identify analyses published in the last 10 years (2008\u20132017) of cost-saving health interventions in LMICs. To assess whether countries prioritised cost-saving interventions within their latest national health strategic plans, we identified 10 countries, all in sub-Saharan Africa, with the highest measures on the global burden of disease scale and reviewed their national health priority plans. Results We identified 392 studies (63%) targeting LMICs that reported 3315 cost-per-DALY ratios, of which 207 ratios (6%) represented interventions reported to be cost saving. Over half (53%) of these targeted sub-Saharan Africa. For the 10 countries we investigated in sub-Saharan Africa, 58% (79/137) of cost-saving interventions correspond with priorities identified in country plans. Alignment ranged from 95% (21/22 prioritised cost-saving ratios) in South Africa to 17% (2/12 prioritised cost-saving ratios) in Cameroon. Human papillomavirus vaccination was a noted priority in 70% (7/10) of national health prioritisation plans, while 40% (4/10) of countries explicitly included prenatal serological screening for syphilis. HIV prevention and treatment were stated priorities in most country health plans, whereas 40% (2/5) of countries principally outlined efforts for lymphatic filariasis. From our sample of 45 unique interventions, 36% of interventions (16/45) included costs associated directly with the implementation of the intervention. Conclusion Our findings indicate substantial variation across country and disease area in incorporating economic evidence into national health priority plans in a sample of sub-Saharan African countries. To make health economic data more salient, the authors of cost-effectiveness analyses must do more to reflect implementation costs and other factors that could limit healthcare delivery.\n\n==\n\nHorton 2017:\n\nTitle: Ranking 93 health interventions for low- and middle-income countries by cost-effectiveness\n\nAbstract: Background Cost-effectiveness rankings of health interventions are useful inputs for national healthcare planning and budgeting. Previous comprehensive rankings for low- and middle- income countries were undertaken in 2005 and 2006, accompanying the development of strategies for the Millennium Development Goals. We update the rankings using studies published since 2000, as strategies are being considered for the Sustainable Development Goals. Methods Expert systematic searches of the literature were undertaken for a broad range of health interventions. Cost-effectiveness results using Disability Adjusted Life-Years (DALYs) as the health outcome were standardized to 2012 US dollars. Results 149 individual studies of 93 interventions qualified for inclusion. Interventions for Reproductive, Maternal, Newborn and Child Health accounted for 37% of interventions, and major infectious diseases (AIDS, TB, malaria and neglected tropical diseases) for 24%, consistent with the priorities of the Millennium Development Goals. More than half of the interventions considered cost less than $200 per DALY and hence can be considered for inclusion in Universal Health Care packages even in low-income countries. Discussion Important changes have occurred in rankings since 2006. Priorities have changed as a result of new technologies, new methods for changing behavior, and significant price changes for some vaccines and drugs. Achieving the Sustainable Development Goals will require LMICs to study a broader range of health interventions, particularly in adult health. Some interventions are no longer studied, in some cases because they have become usual care, in other cases because they are no longer relevant. Updating cost-effectiveness rankings on a regular basis is potentially a valuable exercise.\n\n==\n\nJeet 2018:\n\nTitle: Effectiveness of targeting the health promotion settings for non-communicable disease control in low/middle-income countries: systematic review protocol\n\nAbstract: Introduction Settings-based approaches to health promotion, involving holistic and multidisciplinary methods, which integrate action across risk factors are important. Major advantage of focusing on these settings is the continuous and intensive contact with the participant. Despite the apparent advantages of addressing non-communicable diseases (NCDs) using targeted interventions for several developed country settings, a relative lack of evidence of effectiveness of such interventions in low/middle-income countries has led to poor allocation of resources towards these interventions. The focus is therefore on the settings rather than any one condition, and we therefore expect the findings to generalise to NCD prevention and control efforts. We intend to estimate the effectiveness of targeted interventions in low/middle-income countries. Methods and analysis We will search PubMed, Excerpta Medica Database, OVID, WHO Library and The Cochrane Library from the year 2000 to March 2018 without language restrictions. Study designs to be included will be randomised controlled trials. The primary outcome of effectiveness will be the percentage change in population having different behavioural risk factors. Subgroup analyses will be performed, and sensitivity analyses will be conducted to assess the robustness of the findings. Ethics and dissemination No ethical issues are foreseen. The Institute Ethics Committee of the Post Graduate Institute of Medical Education and Research approved the doctoral research protocol under which this review is being done. Dissemination will be done by submitting scientific articles to academic peer-reviewed journals. We will present the results at relevant conferences and meetings. Study design Systematic review. PROSPERO registration number CRD42016042647; Pre-results.\n\n==\n\nBeaglehole 2008:\n\nTitle: Improving the prevention and management of chronic disease in low-income and middle-income countries: a priority for primary health care\n\nAbstract: The burden of chronic diseases, such as heart disease, cancer, diabetes, and mental disorders is high in low-income and middle-income countries and is predicted to increase with the ageing of populations, urbanisation, and globalisation of risk factors. Furthermore, HIV/AIDS is increasingly becoming a chronic disorder. An integrated approach to the management of chronic diseases, irrespective of cause, is needed in primary health care. Management of chronic diseases is fundamentally different from acute care, relying on several features: opportunistic case finding for assessment of risk factors, detection of early disease, and identification of high risk status; a combination of pharmacological and psychosocial interventions, often in a stepped-care fashion; and long-term follow-up with regular monitoring and promotion of adherence to treatment. To meet the challenge of chronic diseases, primary health care will have to be strengthened substantially. In the many countries with shortages of primary-care doctors, non-physician clinicians will have a leading role in preventing and managing chronic diseases, and these personnel need appropriate training and continuous quality assurance mechanisms. More evidence is needed about the cost-effectiveness of prevention and treatment strategies in primary health care. Research on scaling-up should be embedded in large-scale delivery programmes for chronic diseases with a strong emphasis on assessment.\n\n==\n\nCecchini 2010:\n\nTitle: Tackling of unhealthy diets, physical inactivity, and obesity: health effects and cost-effectiveness\n\nAbstract: The obesity epidemic is spreading to low-income and middle-income countries as a result of new dietary habits and sedentary ways of life, fuelling chronic diseases and premature mortality. In this report we present an assessment of public health strategies designed to tackle behavioural risk factors for chronic diseases that are closely linked with obesity, including aspects of diet and physical inactivity, in Brazil, China, India, Mexico, Russia, and South Africa. England was included for comparative purposes. Several population-based prevention policies can be expected to generate substantial health gains while entirely or largely paying for themselves through future reductions of health-care expenditures. These strategies include health information and communication strategies that improve population awareness about the benefits of healthy eating and physical activity; fiscal measures that increase the price of unhealthy food content or reduce the cost of healthy foods rich in fibre; and regulatory measures that improve nutritional information or restrict the marketing of unhealthy foods to children. A package of measures for the prevention of chronic diseases would deliver substantial health gains, with a very favourable cost-effectiveness profile.\n\n==\n\nStokols 1996:\n\nTitle: The Social Ecology of Health Promotion: Implications for Research and Practice\n\nAbstract: Recent studies have documented the substantial health benefits and financial savings associated with many disease prevention and health promotion programs. ~5 Examples of effective strategies include employee health risk appraisal, counseling, and lifestyle change programs,6,r cultural change strategies within organizational settings, 8,9 and the provision of clinical preventive services to enhance maternal and child health? \u00b0,11 A mid-decade appraisal of progress toward meeting the Healthy People 2000 goals in the United States found substantial reductions in adult use of tobacco products and in alcohol-related automobile deaths and moderate gains in the proportion of adults exercising regularly and eating less fatty diets. Also noted was an increase in the proportion of workplaces providing health promotion programs for their employees. 12.13 Despite these encouraging trends, not all health promotion efforts have been met with unqualified success. Even the best-designed worksite health promotion programs reach only a small proportion of the total worktbrce. Participants in these programs tend to be healthier, better paid, more educated, and more motivated to change their health habits than nonparticipants.~.2Also, lifestyle-change programs that proceed in a \"linear\" fashion to modify specific health behaviors often neglect the contextual circumstances that lead to high relapse and attrition rates once the interventions have ended.14,15 And certain health risks--such as exposure to community violence, obesity, teen pregnancy, substance abuse, financial barriers to medical and preventive services, and lack of adequate health insurance-remain \"segmented in pockets of heightened prevalence, \u2019\u201916 particularly among low-income and minority groups in the population. 4,12,~v,~s To improve the health of vulnerable populations, and to reduce the self-selection biases and attrition rates associated with many intervention\n\n==\n\nRennie 2020:\n\nTitle: Contributing to health training in low and middle income countries \u2013 global health programmes' responsibility to be sustainable and impactful\n\nAbstract: www.jogh.org \u2022 doi: 10.7189/jogh.10.010310 1 June 2020 \u2022 Vol. 10 No. 1 \u2022 010310 There is a continued challenge of meeting the health needs of people living in low and middle-income countries that is often undermined by inadequate or inequitable health care systems within those settings [1]. These health inequalities have driven funding in settings such as sub-Saharan Africa especially in response to specific disease challenges such as the HIV epidemic [2].",
        "final_summary": "The papers suggest that there is a need for low and middle-income countries (LMICs) to prioritize cost-effective interventions in health promotion. Leech 2020 found that there is substantial variation among LMICs in incorporating cost-saving interventions into national health plans. Horton 2017 updated rankings of health interventions and found that many interventions cost less than $200 per Disability Adjusted Life Year (DALY), making them suitable for inclusion in Universal Health Care packages. Beaglehole 2008 emphasized the importance of an integrated approach to managing chronic diseases in primary health care, particularly in LMICs. Cecchini 2010 assessed public health strategies for tackling unhealthy diets, physical inactivity, and obesity in several countries, including LMICs, and found that these strategies can generate substantial health gains with favorable cost-effectiveness profiles. Stokols 1996 discussed the social ecology of health promotion and the need to consider contextual circumstances and vulnerable populations in intervention design. Rennie 2020 highlighted the responsibility of global health programs to contribute to sustainable and impactful health training in LMICs."
    },
    {
        "query": "Vietnam's economic growth has been driven by various factors, including industrialization, increased foreign investment, a growing services sector, and a relatively young and dynamic workforce. The country's strategic location in Southeast Asia and its efforts to attract foreign investments have also played a role in its economic expansion.",
        "paper_list_string": "Anwar 2011:\n\nTitle: Financial development and economic growth in Vietnam\n\nAbstract: By making use of a panel dataset that covers 61 provinces of Vietnam over the period 1997 to 2006, this paper examines the link between financial development and economic growth. Our analysis, which is based on endogenous growth theory, reveals that financial development has contributed to economic growth in Vietnam. We find that high ratio of credit to Gross Provincial Product (GPP) has accelerated economic growth in Vietnam. We also found a strong positive link between financial development and economic growth when alternative measures of financial development were used. The impact of foreign direct investment on economic growth will be stronger if more resources are invested in financial market development.\n\n==\n\nNgoc 2008:\n\nTitle: Sources of Vietnam's economic growth\n\nAbstract: This article first estimates Cobb-Douglas production functions for Vietnam's economy using annual data in 1975\u20132003. Then, the article measures the contribution of capital formation, labour, and technological progress to the growth of the economy, the effects of major internal and external shocks on output, the impact of economic reforms (doi moi) since the end of 1986, the rates of returns to capital and labour. Two major findings are: (1) technological progress was statistically absent in the growth of the Vietnamese economy throughout the study period; (2) the most important source of economic growth is capital accumulation.\n\n==\n\nAnwar 2010:\n\nTitle: Foreign direct investment and economic growth in Vietnam\n\nAbstract: By making use of a recently released panel dataset that covers 61 provinces of Vietnam from 1996\u20132005, this study examines the link between foreign direct investment and economic growth. Our analysis, which is based on a simultaneous equations model, reveals that in overall terms a mutually reinforcing two-way linkage between FDI and economic growth exists in Vietnam. However, this is not the case for each and every region of Vietnam. The results presented in this study suggest that the impact of foreign direct investment on economic growth in Vietnam will be larger if more resources are invested in education and training, financial market development and in reducing the technology gap between the foreign and local firms.\n\n==\n\nNgoc 2008:\n\nTitle: The roles of capital and technological progress in Vietnam's economic growth\n\nAbstract: Purpose - The purpose of this paper is to measure the contribution of capital formation, labor, and technological progress to the growth of the Vietnamese economy, the impact of economic reforms ( Design/methodologyapproach - Cobb-Douglas production functions are built for Vietnam's economy and then estimated using annual data for 1975-2005. Findings - The two major findings are that: technological progress was statistically absent in the growth of the Vietnamese economy throughout the period studied; and the most important source of economic growth is capital accumulation, accounting for between 84 percent and 89 percent of Vietnamese economic growth throughout the period 1975-2005, and between 85 percent and 90 percent in the reform period (1986-2005). Originality/value - This paper is the first of its kind in the Vietnamese literature that successfully sheds light on, among other things, the roles of capital and technological progress in the Vietnamese economy during the period 1975-2005. It also makes clear that Vietnam's economic growth has been fueled mainly by foreign funds and, thus, the continued heavy reliance of the economy on this financial source will make its growth unsustainable. In order to achieve sustainable growth in the coming decades, Vietnam must shift to rely more on productivity growth, which has been absent so far, and less on factor accumulation growth.\n\n==\n\nHoang 2010:\n\nTitle: Does Foreign Direct Investment Promote Economic Growth in Vietnam?\n\nAbstract: This study examines the effects of the foreign direct investment (FDI) on economic growth in Vietnam by using the panel data model across Vietnam\u2019s sixty-one provinces in 1995\u20132006. The study shows that there is a strong and positive effect of FDI on economic growth in Vietnam as a channel of increasing the stock of capital. Human capital and trade in Vietnam are not yet the channels that give access to advance technology and knowledge transfers from FDI inflows to increase Vietnam\u2019s economic growth.\n\n==\n\nThanh 2011:\n\nTitle: Revisiting Exports and Foreign Direct Investment in Vietnam\n\nAbstract: Since 1986, Vietnam has undertaken various reform measures in the trade and foreign investment area. This paper finds significant contributions of world trade, and competitiveness and liberalization effects to Vietnam's export growth over the period 1997\u20132008. Vietnam's exports became more competitive and better complemented the import demand of Vietnam's trade partners. In addition, dynamic comparative advantage became evident in many products, but significant room remains for improving export competitiveness. Foreign direct investment (FDI) inflows also increased and helped stimulate Vietnam's exports. FDI inflows have increased in both the short\u2010 and long\u2010term, yet are only of a limited magnitude. This necessitates more effective measures to enhance the linkages between FDI and domestic enterprises.\n\n==\n\nPincus 2009:\n\nTitle: Vietnam: Sustaining Growth in Difficult Times\n\nAbstract: Vietnam has achieved high rates of economic growth for a period of two decades. Growth slowed in 2008 as the government was forced to tighten credit in order to slow down price inflation. With the advent of the global recession, the government must now reverse course and find ways to support demand in the face of declining exports and foreign investment. However, as a small, open economy with a fixed exchange rate and large fiscal and trade deficits, Vietnam's options are limited. The most effective response would be to gradually depreciate the Vietnamese dong to slow the flood of imports and boost export prospects, while redirecting the public investment towards labour rather than import-intensive projects. The government must also find ways to impose discipline on the large state-owned enterprises and control their diversification into financial sector activities.\n\n==\n\nTarp 2003:\n\nTitle: Economic Structure and Development in an Emergent Asian Economy: Evidence from a Social Accounting Matrix for Vietnam\n\nAbstract: Vietnam started a wide-ranging process of economic reform in 1986 and is presently opening up its economy to regional and global economic forces. As a result, Vietnam faces significant challenges in the area of economic policy analysis in spite of a remarkable growth performance in recent years. This paper reviews insights emerging from a detailed social accounting matrix (SAM), compiled for the year 2000. The SAM reflects Vietnam\u2019s heavy reliance on primary sector activities, but we also find that agricultural potential could be expanded significantly. In other sectors, the critical importance of sustained commitments to human capital development is apparent. In this context, the international donor community can support the ongoing transformation process through concerted training and capacity building initiatives that have proven successful elsewhere in the region.",
        "final_summary": "The papers suggest that Vietnam's economic growth has been driven by multiple factors. Anwar 2011 finds that financial development, particularly a high credit-to-Gross Provincial Product ratio, has contributed to economic growth. Ngoc 2008 emphasizes the importance of capital accumulation as the primary source of economic growth in Vietnam, while technological progress has been limited. Anwar 2010 highlights the mutually reinforcing relationship between foreign direct investment (FDI) and economic growth, with the impact of FDI being enhanced by investments in education, training, and reducing the technology gap. Hoang 2010 supports the positive effect of FDI on economic growth through increased capital stock. Thanh 2011 emphasizes the role of trade liberalization and competitiveness in driving Vietnam's export growth, while FDI inflows have also stimulated exports. Pincus 2009 discusses the challenges Vietnam faces in sustaining growth, including the need to support demand amidst declining exports and foreign investment. Finally, Tarp 2003 highlights the importance of human capital development and the need for continued economic reform to support Vietnam's growth."
    },
    {
        "query": "Cognitive maps as universal organising principle of perception",
        "paper_list_string": "Behrens 2018:\n\nTitle: What is a cognitive map? Organising knowledge for flexible behaviour\n\nAbstract: It is proposed that a cognitive map encoding the relationships between entities in the world supports flexible behaviour, but the majority of the neural evidence for such a system comes from studies of spatial navigation. Recent work describing neuronal parallels between spatial and non-spatial behaviours has rekindled the notion of a systematic organisation of knowledge across multiple domains. We review experimental evidence and theoretical frameworks that point to principles unifying these apparently disparate functions. These principles describe how to learn and use abstract, generalisable knowledge and suggest map-like representations observed in a spatial context may be an instance of general coding mechanisms capable of organising knowledge of all kinds. We highlight how artificial agents endowed with such principles exhibit flexible behaviour and learn map-like representations observed in the brain. Finally, we speculate on how these principles may offer insight into the extreme generalisations, abstractions and inferences that characterise human cognition.\n\n==\n\nBehrens 2018:\n\nTitle: What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior\n\nAbstract: It is proposed that a cognitive map encoding the relationships between entities in the world supports flexible behavior, but the majority of the neural evidence for such a system comes from studies of spatial navigation. Recent work describing neuronal parallels between spatial and non-spatial behaviors has rekindled the notion of a systematic organization of knowledge across multiple domains. We review experimental evidence and theoretical frameworks that point to principles unifying these apparently disparate functions. These principles describe how to learn and use abstract, generalizable knowledge and suggest that map-like representations observed in a spatial context may be an instance of general coding mechanisms capable of organizing knowledge of all kinds. We highlight how artificial agents endowed with such principles exhibit flexible behavior and learn map-like representations observed in the brain. Finally, we speculate on how these principles may offer insight into the extreme generalizations, abstractions, and inferences that characterize human cognition.\n\n==\n\nSchmajuk 1998:\n\nTitle: Cognitive maps\n\nAbstract: The term, \" cognitive map, \" has various meanings. Some limit it to representations of the physical or spatial. Others restrict it to social matters. We use the term broadly to stand for what the idea of a \" theory \" \u2014a Principled Developmental-stage notion\u2014develops into at the Perspectivist Developmental-stage. The comparison between theory and cognitive map is explained below under the heading, \" Cognitive Maps and Developmental Stages. \" The word \" cognitive \" means \" related to knowing, thinking, or inquiry. \" Therefore \" cognitive maps \" are literally just \" thinking maps \" or mental or conceptual models. They are, metaphorically, analogous to geographic maps, but of cognitive \" territory \" instead of geographical territory. Geographic maps display the location of geographic features: mountains, valleys, rivers, lakes, and so on. They also show the spatial relationships among them: a river rising in the mountains and flowing into a lake, for example. Such maps reveal the lay of the land and enable you to locate where you are, where you want to go, and how to get there. They are useful for making a journey or for finding your way around. Of course, there are many other kinds of maps besides geographic ones: maps of population, economics, weather, and others. Cognitive maps are more abstract, but perform similar functions for concepts.\n\n==\n\nYeap 1988:\n\nTitle: Towards a Computational Theory of Cognitive Maps\n\nAbstract: Abstract A computational theory of cognitive maps is developed which can explain some of the current findings about cognitive maps in the psychological literature and which provides a coherent framework for future development. The theory is tested with several computer implementations which demonstrate how the shape of the environment is computed and how one's conceptual representation of the environment is derived. We begin with the idea that the cognitive mapping process should be studied as two loosely coupled modules: The first module, known as the raw cognitive map, is computed from information made explicit in Marr's 2 1 2 -D sketch and not from high-level descriptions of what we perceive. The second module, known as the full cognitive map, takes the raw cognitive map as input and produces different \u201cabstract representations\u201d for solving high-level spatial tasks faced by the individual.\n\n==\n\nImani 2012:\n\nTitle: Recreating mental image with the aid of cognitive maps and its role in environmental perception\n\nAbstract: Abstract People have some kind of internal model of a geographical space. This research is concerned with aspects of environmental cognition; that is our ability to imagine about our spatial world. In particular, this study makes an attempt to describe cognitive maps and the individual differences between people's map. Data was gathered from a sample of fifty persons. All of them walked through a historical region in Isfahan and were asked to draw a sketch map; the map was then assessed. The major finding was that our spatial knowledge is \u201cmap-like\u201d rather than, say, a sequence of instructions for various routes.\n\n==\n\nEpstein 2017:\n\nTitle: The cognitive map in humans: spatial navigation and beyond\n\nAbstract: The 'cognitive map' hypothesis proposes that brain builds a unified representation of the spatial environment to support memory and guide future action. Forty years of electrophysiological research in rodents suggest that cognitive maps are neurally instantiated by place, grid, border and head direction cells in the hippocampal formation and related structures. Here we review recent work that suggests a similar functional organization in the human brain and yields insights into how cognitive maps are used during spatial navigation. Specifically, these studies indicate that (i) the human hippocampus and entorhinal cortex support map-like spatial codes, (ii) posterior brain regions such as parahippocampal and retrosplenial cortices provide critical inputs that allow cognitive maps to be anchored to fixed environmental landmarks, and (iii) hippocampal and entorhinal spatial codes are used in conjunction with frontal lobe mechanisms to plan routes during navigation. We also discuss how these three basic elements of cognitive map based navigation\u2014spatial coding, landmark anchoring and route planning\u2014might be applied to nonspatial domains to provide the building blocks for many core elements of human thought.\n\n==\n\nPortugali 1996:\n\nTitle: The construction of cognitive maps\n\nAbstract: The Construction of Cognitive Maps: An Introduction J. Portugali. Part One: Theoretical Frameworks. Inter-representation Networks and Cognitive Mappings J. Portugali. Synergetics, Inter-representation Networks and Cognitive Maps H. Haken, J. Portugali. Connectionism and Neural Networks. Neural Network Models of Cognitive Maps S. Gopal. Connectionist Models in Spatial Cognition T. Ghiselli-Crippa, et al. The Ecological Approach. The Ecological Approach to Navigation: A Gibsonian Perspective H. Heft. Experiential Realism. Verbal Directions for Way-Finding: Space, Cognition and Language H. Couclelis. Part Two: Transformations. From Visual Information to Cognitive Maps. Vision: From Visual Information to Cognitive Maps J. Sholl. Constructing Cognitive Maps with Orientation Biases R. Lloyd, R. Cammack. Cognitive Maps by Visually Impaired People. Cognitive Mapping and Wayfinding by Adults Without Vision R.G. Golledge, et al. The Construction of Cognitive Maps by Children with Visual Impairments S. Ungar, et al. From Language to Cognitive Maps. Language as a Means of Constructing and Conveying Cognitive Maps N. Franklin. Modes of Linearization in the Description of Spatial Configurations M.-P. Daniel, et al. Part Three: Specific Themes. Spatial Reasoning. Modeling Directional Knowledge and Reasoning in Environmental Space: Testing Qualitative Metrics D.R. Montello, A.U. Frank. Cognitive Mapping and Culture. Mapping as a Cultural Universal D. Stea, et al. Subject Index.\n\n==\n\nLloyd 2000:\n\nTitle: Self-Organized Cognitive Maps\n\nAbstract: This paper argues cognitive mapping is a learning process that can be simulated by a self-organizing neural network. The learning of city locations was considered in two studies. One study focused on the learning of city locations on four continents. Results indicated the neural network aligned the cities producing systematic errors similar to those in human cognitive maps. A second study had a neural network learn a biased sample of city locations in the United States. Results indicated a non-linear relationship between cognitive and physical distances. Self-organized cognitive maps naturally produce this non-linear relationship when information from more than one scale is mapped into one space.",
        "final_summary": "Cognitive maps serve as a universal organizing principle of perception. Behrens 2018 posits that cognitive maps encode relationships between entities, supporting flexible behavior across multiple domains. Schmajuk 1998 broadens this definition, describing cognitive maps as mental models that organize knowledge, akin to geographic maps. Epstein 2017 delves into the neural basis of cognitive maps in humans, emphasizing their role not only in spatial navigation but also in other cognitive functions. Yeap 1988 introduces a computational theory of cognitive maps, focusing on how the environment's shape is computed and how it influences our conceptual representation of the environment."
    },
    {
        "query": "foundations of social network theory and its impact in innovation",
        "paper_list_string": "Yang 2017:\n\nTitle: Impact of social network heterogeneity and knowledge heterogeneity on the innovation performance of new ventures\n\nAbstract: Purpose \n \n \n \n \nFew studies investigated the influence of heterogeneous knowledge and social networks on the innovation performance of new ventures in emerging economics. Particularly, as the focus of global economic development is paying more attention to emerging economics such as China, Indonesia and India, innovation research in these countries becomes more and more important. As such, this paper aims to propose a theoretical model and designed a survey based on the Theory of Inventive Problem Solving (TRIZ). \n \n \n \n \nDesign/methodology/approach \n \n \n \n \nTo investigate the influence of network heterogeneity and knowledge heterogeneity on the innovation performance of new ventures, the authors designed a structured questionnaire based on the TRIZ methodology for innovative problem solving to collect data from new ventures established in the past eight years in eastern China. \n \n \n \n \nFindings \n \n \n \n \nThe empirical results verify the assumptions of our theoretical model: network heterogeneity has a positive impact on the innovation performance of new ventures; knowledge heterogeneity has a positive impact on the innovation performance of new ventures; network heterogeneity has a positive impact on knowledge heterogeneity. \n \n \n \n \nOriginality/value \n \n \n \n \nThe main contribution of this paper is that the proposed theoretical model determining the relationship of network heterogeneity and knowledge heterogeneity with the innovation performance of new ventures was empirically tested and verified. This study also empirically verified what types and what sources of social network and knowledge are critically linked to new venture innovation.\n\n==\n\nObstfeld 2005:\n\nTitle: Social Networks, the Tertius Iungens Orientation, and Involvement in Innovation\n\nAbstract: This study examines the microprocesses in the social networks of those involved in organizational innovation and their strategic behavioral orientation toward connecting people in their social network by either introducing disconnected individuals or facilitating new coordination between connected individuals. This tertius iungens (or \u201cthird who joins\u201d) strategic orientation, contrasts with the tertius gaudens orientation emphasized in structural holes theory, which concerns the advantage of a broker who can play people off against one another for his or her own benefit. Results of a multimethod study of networks and innovation in an engineering division of an automotive manufacturer show that a tertius iungens orientation, dense social networks, and diverse social knowledge predict involvement in innovation. Implications of the study for innovation and social networks, as well as for social skill and agency within firms are presented.\n\n==\n\nSteen 2012:\n\nTitle: The past, present and future of social network analysis in the study of innovation\n\nAbstract: Many years ago Joseph Schumpeter, the founder of the economics of innovation, described innovation as the process of searching for and recombining knowledge. Even this most fundamental model of innovation implies a process of finding the right connections through search and then consolidating those connections.\n\n==\n\nWineman 2009:\n\nTitle: Spatial and Social Networks in Organizational Innovation\n\nAbstract: Research on the enabling factors of innovation has focused on either the social component of organizations or on the spatial dimensions involved in the innovation process. But no one has examined the aggregate consequences of the link from spatial layout, to social networks, to innovation. This project enriches our understanding of how innovation works especially in highly innovative organizations by exploring the social dimensions of innovation as they are embedded in a specific spatial milieu. Workspace layout generates spatial boundaries that divide and reunite built space. These boundaries create relations of accessibility and visibility that integrate or segregate behaviors, activities, and people. As built space structures patterns of circulation, copresence, coawareness, and encounter in an organization, these interrelationships become fundamental to the development of social networks, especially those networks critical to the innovation process. This article presents a review of the knowledge bases of social network and spatial layout theories, and reports on a preliminary study of the effects of spatial layout on the formation and maintenance of social network structure and the support of innovation.\n\n==\n\nMuller 2019:\n\nTitle: The effect of social networks structure on innovation performance: A review and directions for research\n\nAbstract: Abstract Research on growth of innovations introduced to the market has gradually shifted its focus from aggregate-level diffusion to exploring how growth is influenced by a given social network structure's characteristics. In this paper, we critically review this branch of literature. We argue that the growth of an innovation in a social network is shaped by the network's structure. Borrowing from the field of industrial organization in economics, which defines itself as the study of the effect of market structure on market performance, we describe this new wave of research on growth of innovations as the effect of social network structure on innovation performance . Hence, social network structural characteristics should be incorporated into research on new product growth as well as into managerial marketing decisions such as targeting and new product seeding. We review how social network structure influences innovations' market performance. Specifically, we discuss (1) a networks' global characteristics, namely average degree, degree distribution, clustering, and degree assortativity; (2) dyadic characteristics, or the relationships between pairs of network members, namely tie strength and embeddedness; (3) intrinsic individual characteristics, namely opinion leadership and susceptibility; and (4) location-based individual characteristics, namely the degree centrality, closeness centrality, and betweenness centrality of an individual network member. Overall, we find that growth is particularly effective in networks that demonstrate the \u201c3 Cs\u201d: cohesion (strong mutual influence among its members), connectedness (high number of ties), and conciseness (low redundancy). We identify gaps in current knowledge, discuss the implications on managerial decision making, and suggest topics for future research.\n\n==\n\nCoulon 2005:\n\nTitle: The use of Social Network Analysis in Innovation Research: A literature review\n\nAbstract: The purpose of this paper is to review the innovation research literature which has made an explicit use of social network analysis methodology in order to provide empirical support to innovation theories or conceptual frameworks. The review introduces social network analysis then discusses why and how it has been used in innovation research so far. This paper argues that studies using social network analysis tend to focus too much on change in the relationships between interacting units or nodes of the network to the detriment of change within units/nodes. Therefore, a combination of case study and social network analysis can offer a solution to that problem by providing the best of both methodologies.\n\n==\n\nKolleck 2013:\n\nTitle: Social network analysis in innovation research: using a mixed methods approach to analyze social innovations\n\nAbstract: The importance of social networks for innovation diffusion and processes of social change is widely recognized in many areas of practice and scientific disciplines. Social networks have the potential to influence learning processes, provide opportunities for problem-solving, and establish new ideas. Thus, they can foster synergy effects, bring together key resources such as know-how of participating actors, and promote innovation diffusion. There is wide agreement regarding the usefulness of empirical methods of Social Network Analysis (SNA) for innovation and futures research. Even so, studies that show the chances of implementing SNA in these fields are still missing. This contribution addresses the research gap by exploring the opportunities of a mixed methods SNA approach for innovation research. It introduces empirical results of the author\u2019s own quantitative and qualitative investigations that concentrate on five different innovation networks in the field of Education for Sustainable Development.\n\n==\n\nDao-hong 2011:\n\nTitle: Generation and Evolution of Enterprise Innovation Network\u2014\u2014Based on Social Network Theory\n\nAbstract: The enterprise innovation network as an effective organization in promoting integrated technology and upgrading regional innovative strength has the attributes of social network and influenced by the structural embeddedness.On social network theory perspective,firm's investment in social capital is induced by the positive influence of network rent.The expansion of network structure and network scale is accelerated by the effect of structure hole.The network rent and network structures have different change during the evolution of enterprise innovation network.The paper analyses the enterprise innovation network generation mechanism and evolution model based on social network theory,finally carries out the case study using LED innovation network of Xi'an High-tech Zone.",
        "final_summary": "The papers suggest that social network theory plays a significant role in understanding the foundations of innovation. Yang (2017) found that network heterogeneity and knowledge heterogeneity positively impact the innovation performance of new ventures. Obstfeld (2005) highlighted the importance of a strategic orientation called tertius iungens, which involves connecting individuals in social networks to facilitate innovation. Steen (2012) emphasized the role of social networks in the process of finding and recombining knowledge for innovation. Wineman (2009) explored the interplay between spatial layout, social networks, and innovation, emphasizing the impact of workspace design on social network structure and innovation support. Muller (2019) discussed the influence of social network structure on innovation performance, highlighting the importance of cohesion, connectedness, and conciseness in networks. Coulon (2005) argued for the combination of social network analysis and case study methods to provide a comprehensive understanding of innovation. Kolleck (2013) emphasized the usefulness of social network analysis in studying innovation diffusion and social change. Dao-hong (2011) examined the generation and evolution of enterprise innovation networks based on social network theory. Overall, these papers demonstrate the significance of social networks in driving innovation and suggest various factors that contribute to innovation performance."
    },
    {
        "query": "Influence of Mn on properties of Co-Cr-Fe-Mn-Ni high entropy alloy systems",
        "paper_list_string": "Yao 2019:\n\nTitle: Effects of Mo Element on Microstructure and Mechanical Properties of CoCrFeMnNi High Entropy Alloys\n\nAbstract: In order to improve the mechanical properties of CoCrFeMnNi high entropy alloy, CoCrFeMnNiMox high entropy alloy were prepared by adding different content of Mo to re-alloying. The microstructure and phase composition were analyzed by OM, SEM, EDS and XRD, and the mechanical properties were tested by tensile test. The effects of Mo content on the phase composition and microstructure of CoCrFeMnNi high entropy alloy were investigated, and the mechanism of its influence on mechanical properties was studied. The results showed that with the increase of Mo element content, the crystalline morphology of CoCrFeMnNiMox high entropy alloy is change from dendrite crystals to cellular crystal, and the yield strength of the alloy is not change significantly. In the case of Mo content is less than 7%, Mo as substitutional atom is dissolves in the crystal structure of CoCrFeMnNi high entropy alloy substrate, which has function as solid solution strengthening, so that the strength of the alloy increases slightly, but the plasticity are not change significantly. In the case of Mo content is more than 7%, the \u03b6 (CrMo) phase is precipitates in the alloy substrate, which has plays effect of second phase strengthening, so that strength of the alloy is increase significantly, and the plasticity is decreases obviously.\n\n==\n\nSalishchev 2014:\n\nTitle: Effect of Mn and V on structure and mechanical properties of high-entropy alloys based on CoCrFeNi system\n\nAbstract: Microstructure and mechanical properties of equimolar composition alloys FeCrCoNi, FeCrCoNiV, FeCrCoNiMn and FeCrCoNiMnV were studied in as-solidified and annealed conditions. The FeCrCoNi and FeCrCoNiMn alloys were single-phase FCC solid-solutions in both conditions. However, the FeCrCoNiV and FeCrCoNiMnV alloys consisted of the intermetallic \u03c3-phase matrix with a tetragonal crystal lattice and precipitates of a disordered FCC phase. The crystal structures of these alloys were found to be not affected by annealing. A number of criteria were considered to explain phase composition of the studied alloys. It was shown that poor compatibility of V with other alloying elements caused significant distortions of FCC solid solution and thus promoted formation of the \u03c3 phase. Tensile and compressive properties of these alloys together with their microhardness were determined. Significant strengthening accompanied by the loss of ductility due to formation of the \u03c3 phase was demonstrated in the V containing alloys. The characteristics of the microstructure formation in the studied alloys were discussed.\n\n==\n\nSemenyuk 2021:\n\nTitle: Effect of carbon content on cryogenic mechanical properties of CoCrFeMnNi high entropy alloy\n\nAbstract: The effect of the carbon content (0-2 at.%) on the structure and mechanical properties at room and cryogenic temperatures of CoCrFeNiMn-based high entropy alloys with reduced Cr concentration was studied. The as-cast alloys were cold rolled to a thickness reduction of 80% and annealed at 800\u00b0C for 1 hour. As a result, a fully recrystallized microstructure with a grain size of 6.4 \u03bcm was produced in the carbon-free alloy. The recrystallized grain size was much smaller (1.5 \u03bcm in the alloy with 2.0 at.% of C) due to the pinning effect of the precipitated M23C6 carbides. The yield strength of the alloys increased with an increase in the carbon concentration from 313 MPa to 636 MPa, while the elongation to fracture slightly decreased from 56% to 43%, respectively, in the alloys with 0 and 2 at.% of C. A decrease in the test temperature to 77K resulted in a significant increase in both the strength and ductility of the alloys. The alloys had high values of impact toughness of 140 J/cm2 and 85 J/cm2, respectively, in the alloys with 0 and 2 at.% of C. A decrease in the testing temperature did not have a noticeable effect on the impact toughness.\n\n==\n\nLevenets 2017:\n\nTitle: INFLUENCE OF COMPOSITION AND MICROSTRUCTURE ON THE FEATURES OF MECHANICAL PROPERTIES OF Co-Cr-Fe-Mn-Ni HIGH ENTROPY ALLOYS\n\nAbstract: The structure and mechanical properties of high-entropy CoCrFeMnNi (equiatomic) and Co20Cr26Fe20Mn20Ni14 alloys, which differ significantly in the stacking-fault energy, are studied. The structure of the alloys was investigated in three states \u2013 as-cast, after homogenizing annealing at T = 1000 \u00b0C for 24h and after annealing at T = 850 \u00b0C. It was found that in the cast state and after homogenizing annealing at 1000 \u00b0C both alloys are single-phase solid solutions with a FCC lattice. Annealing at 850 \u00b0C preserves the single-phase state in the equiatomic CoCrFeMnNi alloy, but leads to the appearance of a \u03c3-phase in the Co20Cr26Fe20Mn20Ni14 alloy. The mechanical properties of these alloys were studied in a single-phase state by uniaxial compression tests and hardness measurements. It was discovered, that both alloys in a single-phase state had high plasticity and tendency to force strain hardening. The behavior of hardening coefficients in the area of true strains e = 0.05 \u2013 0.17 vary considerably. This may be due to a significant difference in the stacking-fault energy of the investigated alloys. As a result, in Co20Cr26Fe20Mn20Ni14 alloy along with dislocation deformation mechanism there is a high probability of implementation the twinning mechanism.\n\n==\n\nGu 2019:\n\nTitle: Microstructure and Mechanical Properties of CoCrFeMnNiSnx High-Entropy Alloys\n\nAbstract: The objective of this study is to investigate the effect of Sn content on the microstructure and mechanical properties of the CoCrFeMnNiSnx high entropy alloys. The microstructure and mechanical properties of the alloys have been studied by X-ray diffraction, scanning electron microscopy, room-temperature compressive tests and Vickers hardness tests. It has been found that Sn has a great effect on the microstructure and mechanical properties of the alloys. The CoCrFeMnNi alloy has a simple FCC phase structure, while a few white precipitations (MnNi2Sn-like) are observed in the Sn0.03 and Sn0.07 alloys. With further additions of Sn, the microstructure of Sn0.1, Sn0.3 and Sn0.5 changes to a typical dendrite structure, the dominated dendrite region has a lattice of FCC structure which is enriched with Cr, Co and Fe, and the interdendritic region is enriched with Mn, Ni and Sn. The volume of the MnNi2Sn-like phase increases with Sn content increasing. The addition of Sn enhances the formation of the MnNi2Sn-like phase in the CoCrFeMnNiSnx alloys, and improves the hardness and strength of as-cast alloys, but reduces the ductility of the alloys. The strengthening mechanism of alloys was solid solution strengthening and formation of the MnNi2Sn-like phase. In the CoCrFeMnNiSn0.03 and CoCrFeMnNiSn0.07 alloys, the solid solution is the dominant mechanism. With the Sn content increasing, the second phase strengthening is the dominant mechanism.\n\n==\n\nLevenets 2017:\n\nTitle: Ng INFLUENCE OF COMPOSITION AND MICROSTRUCTURE ON THE FEATURES OF MECHANICAL PROPERTIES OF Co-Cr-Fe-Mn-Ni HIGH ENTROPY ALLOYS\n\nAbstract: The structure and mechanical properties of high-entropy CoCrFeMnNi (equiatomic) and Co20Cr26Fe20Mn20Ni14 alloys, which differ significantly in the stacking-fault energy, are studied. The structure of the alloys was investigated in three states \u2013 as-cast, after homogenizing annealing at T = 1000 \u00b0C for 24h and after annealing at T = 850 \u00b0C. It was found that in the cast state and after homogenizing annealing at 1000 \u00b0C both alloys are single-phase solid solutions with a FCC lattice. Annealing at 850 \u00b0C preserves the single-phase state in the equiatomic CoCrFeMnNi alloy, but leads to the appearance of a \u03c3-phase in the Co20Cr26Fe20Mn20Ni14 alloy. The mechanical properties of these alloys were studied in a single-phase state by uniaxial compression tests and hardness measurements. It was discovered, that both alloys in a single-phase state had high plasticity and tendency to force strain hardening. The behavior of hardening coefficients in the area of true strains e = 0.05 \u2013 0.17 vary considerably. This may be due to a significant difference in the stacking-fault energy of the investigated alloys. As a result, in Co20Cr26Fe20Mn20Ni14 alloy along with dislocation deformation mechanism there is a high probability of implementation the twinning mechanism.\n\n==\n\nKlimova 2020:\n\nTitle: Effect of nitrogen on mechanical properties of CoCrFeMnNi high entropy alloy at room and cryogenic temperatures\n\nAbstract: Abstract Specimens of a CoCrFeMnNi high entropy alloy with different content of nitrogen (0.5\u20132.0\u00a0at.%) were prepared by induction melting. Microstructure and tensile mechanical behavior of the alloys in the as-cast condition were analyzed. The alloys with a low amount of N (0.5\u20131.0\u00a0at.%) had a single fcc phase coarse-grained microstructure. An increase in the content of N to 2.0\u00a0at.% resulted in the precipitation of a small amount (\u223c1%) of Cr-rich M2N nitride particles. The yield strength of the alloys increased in proportion to the percentage of nitrogen by 117\u00a0MPa/at% N at 293\u00a0K or by 316\u00a0MPa/at% N at 77\u00a0K. The observed increase in strength was attributed to solid solution hardening. Ductility of the alloy with 0.5 or 1.0\u00a0at.% of N increased with a decrease in the testing temperature while ductility of the alloy with 2\u00a0at.% dropped from 67% at 293\u00a0K to 8% at 77 K. The plastic deformation of the alloys at both 77\u00a0K or 293\u00a0K was mostly associated with planar dislocation slip. No signs of mechanical twinning were found even at 77\u00a0K. Thermo-Calc software was used to produce a quasi-binary CoCrFeMnNi\u2013N phase diagram for comparing the experimental and calculated results. The effect of N on strengthening and deformation mechanisms at different temperatures was discussed.\n\n==\n\nChristofidou 2018:\n\nTitle: On the influence of Mn on the phase stability of the CrMnxFeCoNi high entropy alloys\n\nAbstract: Abstract The fcc phase of the equiatomic high entropy alloy, CrMnFeCoNi, has been recently shown to be unstable at temperatures below 800\u00a0\u00b0C. However, the stability of the constituent CrFeCoNi quaternary alloy, which forms the basis of many other high entropy systems, remains under debate and the existing literature contains very little long duration heat treatment data. Here, the phase equilibria of CrFeCoNi and CrMn 0.5 FeCoNi are assessed following 1000\u00a0h exposures at 500, 700 and 900\u00a0\u00b0C. Prior to thermal exposure the cast alloys were homogenised and shown to exist as single phase fcc solid solutions. In line with previous reports, Cr rich particles were observed on the grain boundaries following the prolonged exposures but detailed electron microscopy showed that these features were M 23 C 6 carbides resulting from the unintentional incorporation of C during production. However, no evidence was found for any other phase formation during the heat treatments of either alloy, in direct contrast to the results for CrMnFeCoNi. Consequently, it is concluded that, within the limits of the temperature and times considered, the solid solution phases of both CrFeCoNi and CrMn 0.5 FeCoNi are stable and that Mn has a destabilising influence when present in sufficient concentrations. This change in behaviour occurs for a Mn content between 11.1 and 20\u00a0at.%.",
        "final_summary": "Salishchev 2014 found that the addition of Mn to CoCrFeNi high entropy alloys resulted in the formation of intermetallic phases, impacting the mechanical properties. Levenets 2017 studied the influence of composition and microstructure on the mechanical properties of Co-Cr-Fe-Mn-Ni high entropy alloys, highlighting the role of stacking-fault energy in deformation mechanisms. Christofidou 2018 focused on the influence of Mn on the phase stability of CrMnFeCoNi high entropy alloys, indicating that Mn destabilized the fcc phase at certain concentrations. These papers provide insights into the influence of Mn on the properties of Co-Cr-Fe-Mn-Ni high entropy alloy systems (Salishchev 2014; Levenets 2017; Christofidou 2018)."
    },
    {
        "query": "Preferences of health seeking for patients with diabetes and obesity in India",
        "paper_list_string": "Nimesh 2019:\n\nTitle: Patterns of healthcare seeking behavior among persons with diabetes in Central India: A mixed method study\n\nAbstract: Background: Management of diabetes is complex and requires multiple lifestyle modifications, drug therapy, and a sustained regular follow-up. Complexities of health-seeking pattern in individuals with diabetes have been poorly characterized. Objectives: To understand the health-seeking patterns, and reasons of provider preference and switching among persons with diabetes. Materials and Methods: We performed a mixed methods study in an urban slum setting of Bhopal. This urban slum was chosen as being a field practice area of the institute, a complete sampling frame with listing of households, and individuals with chronic disease conditions (including diabetes) was available. To be included in the study, the individual should have been an adult, aged \u226520 years, and diagnosed as type 2 diabetes mellitus. Descriptive statistical analysis of sociodemographic and disease management variables was performed. For qualitative component, interviews were transcribed and primary coding was done by two investigators followed by condensation of codes into themes or categories. The frequency of these content categories was presented with count and proportions. Results: In total, 60 individuals with diabetes were interviewed. Of all individuals, 36 (60%) were asymptomatic at the time of the first diagnosis, and 57 (95%) were currently under treatment from some healthcare provider. About 25 (41.6%) switched their first provider and remaining continued with the same provider. Second provider was sought by 9 (36%) of 25 patients. Reasons for switching were perceived nonrelief, cost of care, distance of facility, and behavior of care provider. Conclusions: Healthcare provider switching is common among persons with diabetes which has implications on continuity of care.\n\n==\n\nAbidin 2014:\n\nTitle: Prevalence and Determinants of Appropriate Health Seeking Behaviour among Known Diabetics: Results from a Community-Based Survey\n\nAbstract: Introduction. Living with diabetes requires patients to have good self-monitoring of their disease and treatment. Appropriate health seeking behavior is important to minimize complications and improve quality of life. Methodology. A community-based, cross-sectional study of disease events and experiences from diagnosis to the time of study was conducted among 460 known diabetics in Tanjong Karang district. The aim of this study was to describe the current pattern of health seeking behavior and its determinants among rural communities. Appropriate diabetic health services utilization was defined as using modern treatment either through oral hypoglycemics or insulin injections, obtained from either a public or private health facility. Result. 85.9% of respondents reported having appropriate health seeking behaviour at the time of the house-to-house community survey. Multivariate logistic regression analysis revealed that appropriate health seeking behaviour was significantly associated with age of respondent, presence of comorbidity, family history of diabetes, distance from health facilities, perceived family support, and history of early treatment seeking at diagnosis and duration of disease. Conclusion. The present population has better appropriate health seeking behavior and provision of knowledge with strong family support in diabetic care which are important in control and prevention of diabetic complication that need to be emphasized.\n\n==\n\nBhosale 2017:\n\nTitle: Healthcare-seeking behavior among diabetic patients in Kozhikode, Kerala -\n\nAbstract: Background: Diabetes mellitus is a global health problem. India has additional unique problem of treatment compliance of diabetic patients. Diabetic patients do not adhere to treatment and ultimately develop multiple chronic complications leading to irreversible disability and death. \n \nObjectives: To determine healthcare--seeking behavior and the health expenditure incurred for the treatment of diabetes in study population. \n \nMaterials and Methods: Community-based cross-sectional study was conducted in 2015. House-to-house survey was done for data collection. A structured questionnaire was used for collecting data after explaining purpose of the study. All individuals with diabetes residing in area who consented for participation in the study were included in the study. Elderly individuals who were critically ill and were unable to comprehend questions were excluded from the study. Collected data were entered in Microsoft Excel and statistical analysis was done using SPSS version 16. \n \nResults: A total of 79 diabetic patients were interviewed. Their mean age was 61.51 \u00b1 11.8 years. 63.3% individuals belonged to upper socioeconomic class. Family history of diabetes was seen in 39.2% of the individuals. Only 34.1% patients used Government health facilities. 82.3% individuals were taking treatment regularly. Comorbidities associated with diabetes were seen in 69.6% of the individuals. The mean total monthly expenditure required for diabetes care was Rs. 1793. \n \nConclusions: Healthcare-seeking behavior among diabetic patients in Kerala is still low compared to diabetic patients in developed countries. Diabetic patients have to spend significant amount for traveling, consultation, and laboratory investigations along with expenditure for drugs and hospitalization.\n\n==\n\nRani 2019:\n\nTitle: Does health seeking behaviour influences on quality of life of type II diabetes patients, an untouched area in diabetes, study done in JSS Hospital, Mysore\n\nAbstract: Background: Diabetes mellitus\u00a0is a multisystem disorder associated with number of complications. If patients are to contribute to the effective control of diabetes, awareness and practices can assist in reducing the incidence of its complications.Methods: A cross-sectional study of 200 type II diabetic patients at JSS Hospital, Mysuru was conducted, assessment of health seeking behaviour using semi structured questionnaire was done. Quality of life was assessed using WHO QOL-BREF Questionnaire.Results: Among 200 study participants 53.5% belongs to age group 41-60, 57.5% were males, 44.5% were having family history of diabetes, 68.5% were on oral hypoglycemic agents. 163 (81.5%) were anxious when they were diagnosed as diabetics, 123 (61.5%) were influenced by themselves to go for investigations, 68 (34%)were influenced by Doctors, 131 (65.5%) had no money as the barrier to attend hospital for check-up, 186 (93%) visit hospital once in less than 3 months, 180 (90%) of them believed oral drugs are the treatment for diabetes, 188 (94%) prefer Allopathic medicine. Association between initial response on diagnosis and interval for regular monitoring with Quality of life of diabetic patients was statically significant (p\u22640.05).Conclusions: Health seeking behaviour is an important determinant of controlled glycaemic status and Quality of life of people living with diabetes.\n\n==\n\nKapur 1997:\n\nTitle: Diabetes Care in India --- Patient's Perceptions, Attitudes and Practices\n\nAbstract: Diabetes is a major health care problem in India. There are an estimated 30 million persons with diabetes, majority living in urban areas. Over 98% are believed to have Type 2 diabetes. There are no reliable figures on how many are diagnosed and how many treated. Based on sales of anti-diabetic pharmaceuticals, we estimate that a meagre 10-12% receive modern pharmacological treatment. All involved with diabetes care agree that patients play a major role in the successful management of diabetes. To be able to provide comprehensive care including appropriate education and advice it is essential to understand perceptions, attitudes and practices amongst patients with diabetes. Such studies are rarely conducted, and if conducted rarely reported. Physician\u2019s often decide on what is good for the patient based on their better knowledge of the disease and their assumptions are at best empirical. The treatment of diabetes, particular Type 2 requires constant review and change as the disease progresses. Initial treatment with diet and exercise, gives way to oral medications and further - to insulin, as has been known for long[1, 2, 3] and is now documented in UKPD study. Hesitation of patients to insulin therapy is well known and recognised but rarely researched and reported. Preliminary results from the UKPD study[4] indicate that 27% of the 676 patients assigned to insulin refused it initially, but does not examine the issue further. A recent study [5] examined NIDDM patients\u2019 fears and hopes about insulin therapy and recommended that health care providers examine their patient\u2019s perceptions, actively elicit and respond to patient needs to avoid unwitting promotion of negative attitudes toward diabetes management. In a separate paper we will report on physician\u2019s perceptions, attitudes and practice (DIPPAP-2 study) [6] and how these can influence patient attitudes. In this study we report from a survey on 475 persons with diabetes living in urban India, their attitudes, perceptions and practices.\n\n==\n\nShrivastava 2016:\n\nTitle: Obesity, Diabetes and Cardiovascular Diseases in India: Public Health Challenges.\n\nAbstract: Non-communicable diseases (NCDs; including coronary heart disease and type 2 diabetes) are rapidly increasing in India causing nearly 5.8 million deaths per year. Primary reasons for rise in NCDs in India are nutrition and lifestyle transitions. Further, presence of higher body fat, abdominal fat, liver and pancreatic fat and lower lean mass than whites, contribute to heightened metabolic and cardiovascular risk in Asian Indians. Importantly, conversion from pre-diabetes to diabetes occurs more rapidly, and reversion to normal glucose regulation with appropriate lifestyle measures is more difficult in Asian Indians than white population. Huge number of patients with diabetes and with complications increase morbidity, mortality and pose substantial economic burden. It is difficult, though not impossible, to decrease pace of rapidly expanding juggernaut of NCDs in India. Only concerted efforts from multiple stakeholders, consistently sincere efforts and intensely focused attention from health officialdom and clear political will may help counter this increasingly difficult challenge. Finally, all prevention and management approaches should be cost-effective, pragmatic, and focused on children and underprivileged populations.\n\n==\n\nSilvanus 2022:\n\nTitle: Factors Influencing Health Seeking Behaviour among Persons with Diabetes Attending Urban Health Care Settings.\n\nAbstract: BACKGROUND\nHealth seeking behaviour can influence outcomes in chronic diseases such as diabetes. As diabetes burden is higher in urban areas and under half receive adequate diabetes care and support in Nepal, this study aimed to assess factors influencing health behaviour among persons with diabetes attending urban health care settings.\n\n\nMETHODS\nA cross-sectional study was conducted at two private healthcare settings with a pretested semi-structured questionnaire to assess health behaviour in particular with gender and regular diabetes follow-up. Bivariate analysis alongwith univariate and multivariate logistic regression was used to assess factors that influence HSB (P<0.05). Adjusted odds were reported within 95% confidence intervals.\n\n\nRESULTS\nAmong 385 PWDs (42.3% women) with a median diabetes duration of 7 years (IQR: 3 to 14 years), three fourths (75.1%, 95% CL: 70.5 to 79.1%) reported regular follow-up for diabetes care. After adjusting for confounders, the odds of regular follow-up were higher for those with a family history of diabetes (AOR: 1.82, 95% CI: 1.11 to 3.00) and non-smokers (AOR: 2.08, 95% CI: 1.34 to 3.61). The odds of follow-up were lower among the elderly (? 60 years) (AOR: 0.59, 95% CI: 0.35 to 0.97) and those with a family income below 35,000 Nepali rupees (AOR:0.39, 95% CI: 0.23 to 0.67).\n\n\nCONCLUSIONS\nA plurality of health seeking behaviours such as regular follow-up care, taking glucose lowering medications and diabetes information seeking was observed among persons with diabetes attending private urban heath care settings during the COVID-19 pandemic period. Older age, family history of diabetes, non-smoking status and low family income were found to influence regular follow-up. In particular, psychosocial mechanisms that influence behavior among persons who smoke may need exploration.\n\n==\n\nGulati 2014:\n\nTitle: Sugar Intake, Obesity, and Diabetes in India\n\nAbstract: Sugar and sweet consumption have been popular and intrinsic to Indian culture, traditions, and religion from ancient times. In this article, we review the data showing increasing sugar consumption in India, including traditional sources (jaggery and khandsari) and from sugar-sweetened beverages (SSBs). Along with decreasing physical activity, this increasing trend of per capita sugar consumption assumes significance in view of the high tendency for Indians to develop insulin resistance, abdominal adiposity, and hepatic steatosis, and the increasing \u201cepidemic\u201d of type 2 diabetes (T2DM) and cardiovascular diseases. Importantly, there are preliminary data to show that incidence of obesity and T2DM could be decreased by increasing taxation on SSBs. Other prevention strategies, encompassing multiple stakeholders (government, industry, and consumers), should target on decreasing sugar consumption in the Indian population. In this context, dietary guidelines for Indians show that sugar consumption should be less than 10% of total daily energy intake, but it is suggested that this limit be decreased.",
        "final_summary": "Nimesh 2019 reveals that patients with diabetes in India frequently switch healthcare providers, which can disrupt continuity of care. Abidin 2014 identifies several determinants of appropriate health seeking behavior among known diabetics, including age, presence of comorbidity, family history of diabetes, and perceived family support. Bhosale 2017 indicates that healthcare-seeking behavior among diabetic patients in Kerala is lower compared to developed countries, with significant costs associated with treatment. Rani 2019 emphasizes that health seeking behavior is a crucial determinant of controlled glycemic status and quality of life in people living with diabetes. Kapur 1997 underscores the importance of understanding patients' perceptions, attitudes, and practices to provide comprehensive diabetes care. Lastly, Gulati 2014 discusses the increasing sugar consumption in India and its link to obesity and diabetes, suggesting the need for prevention strategies such as taxation on sugar-sweetened beverages."
    },
    {
        "query": "antidepressant and withdrawal symptoms",
        "paper_list_string": "Jj 2005:\n\nTitle: Withdrawal symptoms of antidepressants\n\nAbstract: - Withdrawal symptoms are encountered with both the classical antidepressants, i.e. the tricyclic antidepressants and monoamineoxidase inhibitors, the modern antidepressants, i.e. the selective serotonin re-uptake inhibitors, and the new antidepressants such as venlafaxine and mirtazapine. - The symptoms that are reported following the withdrawal of these drugs can be classified into 8 groups: influenza-like symptoms, psychic symptoms, gastrointestinal symptoms, sleep disorders, equilibrium disorders, sensory disturbances, extrapyramidal symptoms and other symptoms. - It is characteristic of these symptoms that they appear 1-4 days after reduction of the dose or the last administration of the drug. They may also appear in the infants of mothers who used an antidepressant during the last phase of the pregnancy. - It is important that the patient be informed accurately and carefully at the start and termination of any treatment with an antidepressant. - In order to prevent withdrawal symptoms, it is advisable to reduce the dosage slowly if possible when the treatment is to be terminated.\n\n==\n\nVlaminck 2005:\n\nTitle: [Withdrawal symptoms of antidepressants].\n\nAbstract: Withdrawal symptoms are encountered with both the classical anti-depressants, i.e. the tricyclic antidepressants and monoamineoxidase inhibitors, the modern antidepressants, i.e. the selective serotonin re-uptake inhibitors, and the new antidepressants such as venlafaxine and mirtazapine. The symptoms that are reported following the withdrawal of these drugs can be classified into 8 groups: influenza-like symptoms, psychic symptoms, gastrointestinal symptoms, sleep disorders, equilibrium disorders, sensory disturbances, extrapyramidal symptoms and other symptoms. It is characteristic of these symptoms that they appear 1-4 days after reduction of the dose or the last administration of the drug. They may also appear in the infants of mothers who used an antidepressant during the last phase of the pregnancy. It is important that the patient be informed accurately and carefully at the start and termination of any treatment with an antidepressant. In order to prevent withdrawal symptoms, it is advisable to reduce the dosage slowly if possible when the treatment is to be terminated.\n\n==\n\nDilsaver 1984:\n\nTitle: Antidepressant withdrawal phenomena.\n\nAbstract: The authors review the antidepressant withdrawal literature. Withdrawal of tricyclic antidepressants may precipitate the development of discrete syndromes. The most common of these are general somatic or gastrointestinal distress with or without anxiety and agitation, sleep disturbance characterized by excessive and vivid dreaming and initial and middle insomnia, movement disorder, and psychic and behavioral activation extending on a continuum to frank mania. The etiology of these syndromes is discussed. The \"cholinergic overdrive hypothesis\" explains most antidepressant withdrawal phenomena, including infrequent manifestations. Some antidepressant withdrawal symptomatology may be due to an interaction between cholinergic overdrive and monoaminergic systems. A treatment program useful in ameliorating the distress of patients who develop antidepressant withdrawal symptoms and who cannot continue to take antidepressants is outlined. The theoretical significance of tricyclic withdrawal phenomena and the heuristic value of current hypotheses as to their pathophysiology are discussed.\n\n==\n\nLejoyeux 1996:\n\nTitle: Antidepressant Withdrawal Syndrome\n\nAbstract: SummaryWithdrawal from tricyclic antidepressants (TCAs), monoamine oxidase inhibitors (MAOIs) and selective serotonin (5-hydroxytryptamine; 5-HT) reuptake inhibitors (SSRIs) can produce somatic and psychological distress. Influenza-like syndromes, gastrointestinal adverse effects, arrhythmias, anxiety, sleep disturbances, movement disorders, mania or hypomania, panic attacks and delirium may follow antidepressant withdrawal. At present, the aetiology of withdrawal symptoms is not fully known.Withdrawal phenomena are usually prevented by gradually reducing the total daily dosage of the drug in question rather than abruptly discontinuing it. Antimuscarinic agents can be prescribed in order to alleviate the symptoms produced by the withdrawal of TCAs and MAOIs. To date, no drugs have been shown to be useful in the treatment of SSRI-associated withdrawal symptoms. The withdrawal syndrome associated with MAOIs may constitute a medical emergency.\n\n==\n\nTherrien 1997:\n\nTitle: Selective serotonin reuptake inhibitors and withdrawal symptoms: a review of the literature\n\nAbstract: There are accumulating reports of withdrawal symptoms emerging following the discontinuation of selective serotonin reuptake inhibitor antidepressants. This report summarizes published reports, characterizes the withdrawal syndrome, discusses potential mechanisms of withdrawal, and makes recommendations for prevention and management. A computerized search was conducted using MEDLINE (1985\u20131996) to retrieve all case reports and pertinent studies of antidepressant withdrawal. A total of 46 case reports and two drug discontinuation studies were retrieved. All of the selective serotonin reuptake inhibitors were implicated in withdrawal reactions with paroxetine most often cited in case reports. Withdrawal reactions were characterized most commonly by dizziness, fatigue/weakness, nausea, headache, myalgias and paresthesias. The occurrence of withdrawal did not appear to be related to dose or treatment duration. Symptoms generally appeared 1\u20134 days after drug discontinuation, and persisted for up to 25 days. Time of onset and duration of symptoms differed little among the agents. The pathophysiology/pharmacology of withdrawal is unclear but may involve multiple neurotransmitter systems. It is concluded that all of the SSRIs can produce withdrawal symptoms and if discontinued, they should be tapered over 1\u20132 weeks to minimize this possibility. Some patients may require a more extended tapering period. No specific treatment for severe withdrawal symptoms is recommended beyond reinstitution of the antidepressant with subsequent gradual tapering as tolerated. \u00a9 1997 John Wiley & Sons, Ltd.\n\n==\n\nTherrien 1997:\n\nTitle: Selective serotonin reuptake inhibitors and withdrawal symptoms: a review of the literature\n\nAbstract: There are accumulating reports of withdrawal symptoms emerging following the discontinuation of selective serotonin reuptake inhibitor antidepressants. This report summarizes published reports, characterizes the withdrawal syndrome, discusses potential mechanisms of withdrawal, and makes recommendations for prevention and management. A computerized search was conducted using MEDLINE (1985\u20131996) to retrieve all case reports and pertinent studies of antidepressant withdrawal. A total of 46 case reports and two drug discontinuation studies were retrieved. All of the selective serotonin reuptake inhibitors were implicated in withdrawal reactions with paroxetine most often cited in case reports. Withdrawal reactions were characterized most commonly by dizziness, fatigue/weakness, nausea, headache, myalgias and paresthesias. The occurrence of withdrawal did not appear to be related to dose or treatment duration. Symptoms generally appeared 1\u20134 days after drug discontinuation, and persisted for up to 25 days. Time of onset and duration of symptoms differed little among the agents. The pathophysiology/pharmacology of withdrawal is unclear but may involve multiple neurotransmitter systems. It is concluded that all of the SSRIs can produce withdrawal symptoms and if discontinued, they should be tapered over 1\u20132 weeks to minimize this possibility. Some patients may require a more extended tapering period. No specific treatment for severe withdrawal symptoms is recommended beyond reinstitution of the antidepressant with subsequent gradual tapering as tolerated. \u00a9 1997 John Wiley & Sons, Ltd.\n\n==\n\nRm 1997:\n\nTitle: Antidepressant Withdrawal Reactions\n\nAbstract: Antidepressants can cause a variety of withdrawal reactions, starting within a few days to a few weeks of ceasing the drug and persisting for days to weeks. Both tricyclic antidepressants and selective serotonin reuptake inhibitors cause similar syndromes, most commonly characterized by gastrointestinal or somatic distress, sleep disturbances, mood fluctuations and movement disorders. Most symptoms related to tricyclic antidepressant withdrawal are believed to be caused by rebound excess of cholinergic activity after prolonged anticholinergic effect on cholinergic receptors. (This situation is analogous to the adrenergic rebound that occurs after beta-blocker withdrawal.) Treatment involves restarting the antidepressant and tapering it more slowly. Alternatively, tricyclic antidepressant withdrawal symptoms often respond to anticholinergics, such as atropine or benztropine mesylate. Three case reports of antidepressant withdrawal are presented, including one featuring akathisia (motor restlessness) related to withdrawal of venlafaxine.\n\n==\n\nHenssler 2019:\n\nTitle: Antidepressant Withdrawal and Rebound Phenomena.\n\nAbstract: BACKGROUND\nAntidepressants are among the most commonly prescribed drugs worldwide. They are often discontinued, frequently without the knowledge of the prescribing physician. It is, therefore, important for physicians to be aware of the withdrawal and rebound phenomena that may arise, in order to prevent these phenomena, treat them when necessary, and counsel patients appropriately.\n\n\nMETHODS\nThis review is based on a comprehensive, structured literature search on antidepressant withdrawal phenomena that we carried out in the CENTRAL, PubMed (Medline), and Embase databases. We classified the relevant publications and reports by their methodological quality.\n\n\nRESULTS\nOut of a total of 2287 hits, there were 40 controlled trials, 38 cohort studies and retrospective analyses, and 271 case reports that met the inclusion criteria. Withdrawal manifestations are usually mild and self-limiting; common ones include dizziness, headache, sleep disturbances, and mood swings. More serious or pro- longed manifestations rarely arise. There is an increased risk with MAO inhibitors, tricyclic antidepressants, venlafaxine, and paroxetine; on the other hand, for agome- latine and fluoxetine, abrupt discontinuation seems to be unproblematic. There is also some evidence of rebound phenomena, i.e., of higher relapse rates or especially severe relapses of depression after the discontinuation of an anti- depressant.\n\n\nCONCLUSION\nA robust evidence base now indicates that there can be acute with- drawal phenomena when antidepressants are discontinued. Putative rebound phenomena have not been adequately studied to date. It is recommended that antidepressants should be tapered off over a period of more than four weeks.",
        "final_summary": "The papers suggest that withdrawal symptoms can occur when discontinuing antidepressants. These symptoms can manifest in various ways, including influenza-like symptoms, psychic symptoms, gastrointestinal symptoms, sleep disorders, equilibrium disorders, sensory disturbances, extrapyramidal symptoms, and others (Jj 2005, Vlaminck 2005, Dilsaver 1984). The onset of these symptoms typically occurs 1-4 days after reducing the dose or stopping the medication (Jj 2005, Vlaminck 2005). It is important for patients to be informed about these potential withdrawal symptoms at the start and termination of antidepressant treatment (Jj 2005, Vlaminck 2005). Gradually reducing the dosage when discontinuing the medication can help prevent withdrawal symptoms (Jj 2005, Vlaminck 2005). The severity and duration of withdrawal symptoms can vary, and they may differ among different antidepressants (Therrien 1997, Henssler 2019). It is recommended to taper off antidepressants over a period of more than four weeks to minimize the risk of withdrawal phenomena (Henssler 2019)."
    },
    {
        "query": "the cost of implementing advanced health management information systems in Nigeria is high, and funding is scarce amidst widespread government corruption which further hinders progress",
        "paper_list_string": "Benson 2011:\n\nTitle: Hospital Information Systems in Nigeria: A Review of Literature\n\nAbstract: This literature review was developed to examine empirically the factors hindering adoption of hospital information systems in Nigeria. The study was focused on the perceived paucity of health information technology policy in Nigeria and the causes of poor implementation of hospital information systems in the country. The findings of the literature review highlighted hindrances to the adoption of hospital information systems to include; the high cost of full implementation of a hospital information system, inadequate human capital, corruption, and problems associated with poor infrastructure in Nigeria. The recommendations were that, the Nigerian government needs to provide stable electricity, basic communication infrastructures, and Internet access to boost private initiatives in the adoption of health information technology across the country.\n\n==\n\nPantuvo 2011:\n\nTitle: Towards implementing a nationwide electronic health record system in Nigeria.\n\nAbstract: The World Health Organization identified inadequate Health Information Systems as a challenge in Nigeria. Many developed countries have either implemented or are in the process of implementing an Integrated Electronic Health Record (EHR) system because of its potential benefits. Pilot projects in many developing countries like Kenya, Malawi, Peru, and Haiti are demonstrating the viability of EHR in resource constrained areas. This study shows that the health system in Nigeria is pluralistic and complex with Federal, State and Local Governments, Health Related Agencies, Non-Governmental Organizations, private healthcare providers, patients, and researchers as the major stakeholders. The drivers for adoption of a nationwide EHR include the need to report data; improve patient safety, improve work place efficiency; comply with government reforms aimed at reducing the cost and increasing access to health services. Corruption, poor coordination among stakeholders, and lack of constant supply of electricity are some of the barriers to a successful implementation of a nationwide EHR. Factors considered critical to a successful implementation of a nationwide EHR include enforceable legislation, a trained and motivated workforce, and significant and sustainable funding.\n\n==\n\nObansa 2013:\n\nTitle: Health Care Financing in Nigeria: Prospects and Challenges\n\nAbstract: Fifty years after independence, Nigeria is still struggling with the provision of basic health services for its teeming population, now estimated at over 150million. The health care sector is still a labour-intensive handicraft industry, in spite of advances in technology, and health care provision has now become more complicated than in the past. Infant and under-five mortality rates are near the highest in the world, and maternal mortality is extremely high. It has the second largest number of people infected with HIV/AIDS in the world only next to South Africa and in 2008, between 3million and 3.5million people were estimated to be living with HIV/AIDS. Nigeria has the fourth highest number of TB cases in the world, with a 2004 estimate of 293 new cases per 100,000 population and 546 per 100,000 total cases. The reasons for a dysfunctional health system include: gross in adequate infrastructural support, electricity, potable water and diagnostic laboratories, very low per capita health spending, high out-of-pocket expenditure by citizens, and a total absence of a community-based integrated system for disease prevention, surveillance and treatment amongst others. Some strategies to tackle health sector challenges in Nigeria may include improved access to primary healthcare; strategic and purposeful leadership in health delivery services; increasing funding to the health sector; amongst others.\n\n==\n\nAdebayo 2014:\n\nTitle: Issues on E-health Adoption in Nigeria\n\nAbstract: E-health is the application of information technology for health care management. It includes all applications of information communication technologies to promote healthcare services support, delivery and education for improving efficiency in health care delivery to the citizens. Many factors contribute to the poor state of the medical sector of Nigeria, and in fact many developing countries, two of the most important being record keeping and accessibility. Nigeria still operates a paper based healthcare delivery system; meeting demands of citizens have been increasingly difficult due to her huge population, understaffed hospitals, apathy towards embracing ICTs and other issues. On the other hand, ICT applications have been used successfully in developed countries to address most of these problems. This paper presents an in-depth analysis of the healthcare system in Nigeria while highlighting confronting issues with a view to proposing a viable solution.\n\n==\n\nOnwujekwe 2019:\n\nTitle: Where Do We Start? Building Consensus on Drivers of Health Sector Corruption in Nigeria and Ways to Address It\n\nAbstract: Background: Corruption is widespread in Nigeria\u2019s health sector but the reasons why it exists and persists are poorly understood and it is often seen as intractable. We describe a consensus building exercise in which we asked health workers and policy-makers to identify and prioritise feasible responses to corruption in the Nigerian health sector. Methods: We employed three sequential activities. First, a narrative literature review identified which types of corruption are reported in the Nigerian health system. Second, we asked 21 frontline health workers to add to what was found in the review (based on their own experiences) and prioritise them, based on their significance and the feasibility of assessing them, by means of a consensus building exercise using a Nominal Group Technique (NGT). Third, we presented their assessments in a meeting of 25 policy-makers to offer their views on the practicality of implementing appropriate measures. Results: Participants identified 49 corrupt practices from the literature review and their own experience as most important in the Nigerian health system. The NGT prioritised: absenteeism, procurement-related corruption, under-the-counter payments, health financing-related corruption, and employment-related corruption. This largely reflected findings from the literature review, except for the greater emphasis on employment-related corruption from the NGT. Absenteeism, Informal payments and employment-related corruption were seen as most feasible to tackle. Frontline workers and policy-makers agreed that tackling corrupt practices requires a range of approaches. Conclusion: Corruption is recognized in Nigeria as widespread but often seems insurmountable. We show how a structured approach can achieve consensus among multiple stakeholders, a crucial first step in mobilizing action to address corruption.\n\n==\n\nAbel 2015:\n\nTitle: Funding E-Health in Nigeria by NGOS/Multinational Organization: Overview and Perspectives\n\nAbstract: Electronic-Health infrastructure tends to improve the quality of healthcare service delivery through timely access to information electronically with the aid of internet. Mmillions of people in the developed world use internet as a source of healthcare information to improve their lives. Despite the benefits of e-health infrastructure, people living in developing countries lack access to timely information and poor ehealthcare services. Foreign Ngo\u2019s and Multinational institutions (e g. world health organization) have been involved in the funding of e-health projects in some developing countries such as India, Uganda, Rwanda, Zambia and Ethopia e t c. In the case of Nigeria, these same organizations hardly fund or support the implementation of ehealth project. The factors preventing foreign NGOs and Multinational institutions from funding e-health projects in Nigeria were investigated. Data was obtained by conducting Semi-structured interview on prospective individuals of interest and from related academic journal papers. Individuals interviewed include, officials of World Health Organization (WHO), International Monetery Fund (IMF) and Swedish Support ICT Programs in Developing countries (SPIDER). Findings shows that lack of cooperation among healthcare stakeholders, political will and cultural barriers are some of the major factors preventing these Organizations from funding e-health projects. Recommendations were put forward in other to attracts these organizations to fund e-health projects in the conutry. Some of these recommendations include educating healthcare stakeholders what e-health is about, embarking on a sensitization program in the local, state and federal level.\n\n==\n\nOguntimilehin 2014:\n\nTitle: A Framework for Mobile Health Management for Diseases in Nigeria with Benefits and Challenges\n\nAbstract: World Health Organization (WHO) estimates that African Region has a shortfall of 817,992 health workers. Sub-Saharan Africa faces the greatest challenges with 11% of the world\u2019s population and 24% of the global burden of diseases; the region has only 3% of the world\u2019s health workers commanding less than 1% of world health expenditure. The ratio of available facilities to the teeming population is also grossly inadequate. These are among the major reasons why computer (Information Technology (IT)) has been employed over the years to help in different areas of health care- diagnosis, treatment, monitoring and medical records. It is very obvious that most of these computer based health management systems lack among other things the real time live interaction between health practitioners and patients. Mobile applications are now increasingly popular, though used mostly for instant messaging and social networking. In this paper we have proposed the use of mobile application to improve health care delivery system in Nigeria with real-time live chatting mode similar to the instant social chatting where diagnosis, treatment and monitoring of patients can be done through mobile technology. While readers\u2019 attention was drawn to this, a framework of the proposed system was developed for health practitioners and researchers to look into so as to move Nigeria and other developing countries in African forward in the health sector\n\n==\n\nYaqub 2012:\n\nTitle: PUBLIC HEALTH EXPENDITURE AND HEALTH OUTCOME IN NIGERIA: THE IMPACT OF GOVERNANCE\n\nAbstract: A key element of public policy is the promotion of good health in order to attain broad based economic growth. Base on this paradigm, many countries devote huge budgetary allocation to health, but in most developing countries especially, this huge health expenditure failed to translate into better health status. This paper thus investigates how the effectiveness of public health expenditure is affected by governance in Nigeria. Data on public health expenditure and governance variable captured by the corruption perception index were regressed on infant mortality, under-five mortality and life expectancy, using both the ordinary least squares and the two-stage least squares. The result obtained showed that public health expenditure has negative effect on infant mortality and under-5 mortalities when the governance indicators are included. The policy implication is that achieving the Millennium development goal of lowering infant mortality by two-thirds by 2015; reducing under-5 mortality rate and raising life expectancy in Nigeria may be unattainable if the level of corruption is not reduced considerably.",
        "final_summary": "The papers suggest that the cost of implementing advanced health management information systems in Nigeria is high, and funding is scarce amidst widespread government corruption, which further hinders progress. Benson (2011) and Pantuvo (2011) both highlight the high cost of implementation, inadequate human capital, corruption, and poor infrastructure as significant barriers. Obansa (2013) underscores the dysfunctional health system, marked by inadequate infrastructure, low per capita health spending, and high out-of-pocket expenditure. Adebayo (2014) points to the poor state of the healthcare system, emphasizing the need for viable solutions to issues like record keeping and accessibility. Onwujekwe (2019) identifies corruption as a pervasive issue in the Nigerian health sector. Abel (2015) discusses the lack of funding from foreign NGOs and multinational organizations for e-health projects, attributing this to factors such as lack of cooperation among stakeholders and cultural barriers. Oguntimilehin (2014) proposes mobile health management systems as a potential solution to the shortage of health workers and inadequate facilities. Lastly, Yaqub (2012) finds that corruption negatively impacts the effectiveness of health expenditure in Nigeria."
    },
    {
        "query": "enhanced care coordination, transitions, and patient education in oncology readmissions",
        "paper_list_string": "Montero 2016:\n\nTitle: Reducing Unplanned Medical Oncology Readmissions by Improving Outpatient Care Transitions: A Process Improvement Project at the Cleveland Clinic.\n\nAbstract: PURPOSE\nReducing 30-day unplanned hospital readmissions is a national policy priority. We examined the impact of a quality improvement project focused on reducing oncology readmissions among patients with cancer who were admitted to palliative and general medical oncology services at the Cleveland Clinic.\n\n\nMETHODS\nBaseline rates of readmissions were gathered during the period from January 2013 to April 2014. A quality improvement project designed to improve outpatient care transitions was initiated during the period leading to April 1, 2014, including: (1) provider education, (2) postdischarge nursing phone calls within 48 hours, and (3) postdischarge provider follow-up appointments within 5 business days. Nursing callback components included symptom management, education, medication review/compliance, and follow-up appointment reminder.\n\n\nRESULTS\nDuring the baseline period, there were 2,638 admissions and 722 unplanned 30-day readmissions for an overall readmission rate of 27.4%. Callbacks and 5-day follow-up appointment monitoring revealed a mean monthly compliance of 72% and 78%, respectively, improving over time during the study period. Readmission rates declined by 4.5% to 22.9% (P < .01; relative risk reduction, 18%) during the study period. The mean direct cost of one readmission was $10,884, suggesting an annualized cost savings of $1.04 million with the observed reduction in unplanned readmissions.\n\n\nCONCLUSION\nModest readmission reductions can be achieved through better systematic transitions to outpatient care (including follow-up calls and early provider visits), thereby leading to a reduction in use of inpatient resources. These data suggest that efforts focused on improving outpatient care transition were effective in reducing unplanned oncology readmissions.\n\n==\n\nEvans 2012:\n\nTitle: Patient-reported oncology readmission factors.\n\nAbstract: 48 Background: Reducing the risk of readmissions is an important quality goal for hospitals. Little is known of the perspectives of patients on underlying challenges that may lead to readmission. The objective of this study was to elicit perspectives of readmitted oncology patients on barriers to a successful transition from hospital to home.\n\n\nMETHODS\nAs part of a larger survey of readmitted inpatients, a 36-item survey was administered to 197 oncology patients readmitted to the hospital within 30 days of discharge to home. Surveys were administered at The Hospital of the University of Pennsylvania (an urban tertiary care academic medical center) and Penn Presbyterian Medical Center (an affiliated urban community hospital), both located in Philadelphia. Responses were entered in real-time into the electronic medical record (EMR) and used by the care team to address patient concerns and improve quality.\n\n\nRESULTS\n45.2% of readmitted oncology patients reported challenges during the transition from hospital to home which they perceived as contributing to readmission. The most commonly reported transition challenges within the oncology population included difficulty with activities of daily living (ADLs) (17.8%), feeling unprepared for discharge (14.2%) and difficulty adhering to medications (7.1%). 15.2% of patients could not identify any modifiable factor contributing to readmission and reported returning simply because of symptoms from progressive illness. After adjusting for potential confounders (age, gender and severity of illness) using multivariable logistic regression models, uninsured and Medicaid patients were more likely than other patients to attribute readmission to difficulty accessing medications (OR 4.5, 95%CI 1.0, 19.9) and performing ADLs (OR 2.7, 95%CI 1.18, 6.1).\n\n\nCONCLUSIONS\nUnderstanding challenges reported by readmitted oncology patients may enable inpatient oncologists to tailor transitions interventions to patient needs. Specifically, ensuring patients are able to perform necessary ADLs, are prepared for discharge and have assistance with medication adherence may help prevent unplanned readmissions. Uninsured and Medicaid patients may require additional assistance with accessing medications and ADLs.\n\n==\n\nMansukhani 2015:\n\nTitle: Exploring Transitional Care: Evidence-Based Strategies for Improving Provider Communication and Reducing Readmissions.\n\nAbstract: The coordination of care as patients move from one health care setting to another is crucial to treatment, but breakdowns in the process lead to poor transitions, fragmented care, hospital readmissions, and increased costs. This article discusses evidence-based strategies for improving communication and reducing readmissions.\n\n==\n\nBisognano 2009:\n\nTitle: Improving transitions to reduce readmissions.\n\nAbstract: Delivering high quality healthcare requires crucial contributions from many parts of the care continuum. However, as healthcare becomes increasingly specialized, coordination between providers and between settings is too often not conducted as a team effort. In the hospital setting, poor coordination of care often results in hospital readmissions, many of which are avoidable. In this article, we describe processes that hospitals can implement immediately to dramatically improve care transitions and reduce re-hospitalization rates. Readmission rates are a focus of interest for payers and policymakers seeking to promote efficiency and quality. Hospital executives may want to prepare their organization to excel on this performance metric in anticipation of payment, policy, and/or other forecasted changes in the market.\n\n==\n\nManzano 2015:\n\nTitle: Unplanned 30-Day Readmissions in a General Internal Medicine Hospitalist Service at a Comprehensive Cancer Center.\n\nAbstract: PURPOSE\nHospital readmissions are considered by the Centers for Medicare and Medicaid as a metric for quality of health care delivery. Robust data on the readmission profile of patients with cancer are currently insufficient to determine whether this measure is applicable to cancer hospitals as well. To address this knowledge gap, we estimated the unplanned readmission rate and identified factors influencing unplanned readmissions in a hospitalist service at a comprehensive cancer center.\n\n\nMETHODS\nWe retrospectively analyzed unplanned 30-day readmission of patients discharged from the General Internal Medicine Hospitalist Service at a comprehensive cancer center between April 1, 2012, and September 30, 2012. Multiple independent variables were studied using univariable and multivariable logistic regression models, with generalized estimating equations to identify risk factors associated with readmissions.\n\n\nRESULTS\nWe observed a readmission rate of 22.6% in our cohort. The median time to unplanned readmission was 10 days. Unplanned readmission was more likely in patients with metastatic cancer and those with three or more comorbidities. Patients discharged to hospice were less likely to be readmitted (all P values < .01).\n\n\nCONCLUSION\nWe observed a high unplanned readmission rate among our population of patients with cancer. The risk factors identified appear to be related to severity of illness and open up opportunities for improving coordination with primary care physicians, oncologists, and other specialists to manage comorbidities, or perhaps transition appropriate patients to palliative care. Our findings will be instrumental for developing targeted interventions to help reduce readmissions at our hospital. Our data also provide direction for appropriate application of readmission quality measures in cancer hospitals.\n\n==\n\nGrassman 1993:\n\nTitle: Development of inpatient oncology educational and support programs.\n\nAbstract: Support programs are needed to help patients manage the overwhelming emotions they experience when diagnosed and treated for cancer. Although many cancer support groups exist, most programs are designed for outpatients. Support groups for hospitalized patients and their families are an excellent way to provide greatly needed education and support to those who otherwise might not be able or willing to attend outpatient programs. Inpatient programs also offer the opportunity to provide support to people at the onset of diagnosis and treatment--a time when these services are particularly needed. This paper describes special considerations regarding the establishment of inpatient educational and support programs. The evolution of the Oncology Health Management Program of the Bay Pines VA Medical Center in Bay Pines, FL, also is presented. This multidisciplinary program for inpatients includes classes in spiritual support, family support, patient support, symptom management, stress management, and laughter therapy. Evaluation of the program revealed that classes that related symptom management were most useful and that laughter therapy and emotional support were reported to be highly beneficial as well. Nursing staff also gained from their participation in leading the program and expressed greater self-awareness and self-esteem. This has resulted in improved job satisfaction and staff retention as well as in more sensitive and compassionate delivery of patient care. These findings show that providing inpatient educational and support programs is an effective means of meeting the physical, mental, emotional, and spiritual needs of patients with cancer and their families.\n\n==\n\nBell 2017:\n\nTitle: Systematic Review of Hospital Readmissions Among Patients With Cancer in the United States\n\nAbstract: Purpose/Objectives: To review the existing literature on readmission rates, predictors, and reasons for readmission among adults with cancer. Data Sources: U.S.\u2010based empirical studies reporting readmission rates from January 2005 to December 2015 were identified using four online library databases\u2014PubMed, CINAHL\u00ae, EconLit, and the online bibliography of the National Cancer Institute\u2019s Surveillance Epidemiology and End Results Program. Some articles were identified by the authors outside the database and bibliography searches. Data Synthesis: Of the 1,219 abstracts and 271 full\u2010text articles screened, 56 studies met inclusion criteria. The highest readmission rates were observed in patients with bladder, pancreatic, ovarian, or liver cancer. Significant predictors of readmission included comorbidities, older age, advanced disease, and index length of hospital stay. Common reasons for readmission included gastrointestinal and surgical complications, infection, and dehydration. Conclusions: Clinical efforts to reduce the substantial readmission rates among adults with cancer may target high\u2010rate conditions, infection prevention, proactive management of nausea and vomiting, and nurse\u2010led care coordination interventions for older adult patients with multiple comorbid conditions and advanced cancer. Implications for Nursing: Commonly reported reasons for readmission were nursing\u2010sensitivepatient outcomes (NSPOs), amenable to nursing intervention in oncology settings. These findings underscore the important role oncology nurses play in readmission prevention by implementing evidence\u2010based interventions to address NSPOs and testing their impact in future research.\n\n==\n\nBalogh 2011:\n\nTitle: Patient-centered cancer treatment planning: improving the quality of oncology care. Summary of an Institute of Medicine workshop.\n\nAbstract: The Institute of Medicine's National Cancer Policy Forum recently convened a workshop on patient-centered cancer treatment planning, with the aim of raising awareness about this important but often overlooked aspect of cancer treatment. A primary goal of patient-centered treatment planning is to engage patients and their families in meaningful, thorough interactions with their health care providers to develop an accurate, well-conceived treatment plan, using all available medical information appropriately while also considering the medical, social, and cultural needs and desires of the patient and family. A cancer treatment plan can be shared among the patient, family, and care team in order to facilitate care coordination and provide a roadmap to help patients navigate the path of cancer treatment. There are numerous obstacles to achieving patient-centered cancer treatment planning in practice. Some of these challenges stem from the patient and include patients' lack of assertiveness, health literacy, and numeracy, and their emotional state and concurrent illnesses. Others are a result of physician limitations, such as a lack of time to explain complex information and a lack of tools to facilitate treatment planning, as well as insensitivity to patients' informational, cultural, and emotional needs. Potential solutions to address these obstacles include better training of health care providers and patients in optimal communication and shared decision making, and greater use of support services and tools such as patient navigation and electronic health records. Other options include greater use of quality metrics and reimbursement for the time it takes to develop, discuss, and document a treatment plan.",
        "final_summary": "Enhanced care coordination, transitions, and patient education can reduce oncology readmissions. Montero 2016 demonstrated that systematic transitions to outpatient care, including follow-up calls and early provider visits, reduced readmissions. Evans 2012 identified challenges faced by readmitted oncology patients, such as difficulty accessing medications and performing daily activities, particularly among uninsured and Medicaid patients. Mansukhani 2015 and Bisognano 2009 emphasized the importance of improving provider communication during care transitions. Manzano 2015 found that patients with metastatic cancer and multiple comorbidities were more likely to be readmitted, suggesting the need for improved coordination and management of comorbidities. Grassman 1993 highlighted the benefits of inpatient educational and support programs for cancer patients. Bell 2017 underscored the role of oncology nurses in preventing readmissions by addressing nursing-sensitive patient outcomes. Balogh 2011 advocated for patient-centered cancer treatment planning, involving meaningful interactions between patients, families, and healthcare providers."
    },
    {
        "query": "RESULTS - DRIVING FORCE BEHIND vietnam ECONOMIC GROWTH",
        "paper_list_string": "Anwar 2010:\n\nTitle: Foreign direct investment and economic growth in Vietnam\n\nAbstract: By making use of a recently released panel dataset that covers 61 provinces of Vietnam from 1996\u20132005, this study examines the link between foreign direct investment and economic growth. Our analysis, which is based on a simultaneous equations model, reveals that in overall terms a mutually reinforcing two-way linkage between FDI and economic growth exists in Vietnam. However, this is not the case for each and every region of Vietnam. The results presented in this study suggest that the impact of foreign direct investment on economic growth in Vietnam will be larger if more resources are invested in education and training, financial market development and in reducing the technology gap between the foreign and local firms.\n\n==\n\nHoang 2010:\n\nTitle: Does Foreign Direct Investment Promote Economic Growth in Vietnam?\n\nAbstract: This study examines the effects of the foreign direct investment (FDI) on economic growth in Vietnam by using the panel data model across Vietnam\u2019s sixty-one provinces in 1995\u20132006. The study shows that there is a strong and positive effect of FDI on economic growth in Vietnam as a channel of increasing the stock of capital. Human capital and trade in Vietnam are not yet the channels that give access to advance technology and knowledge transfers from FDI inflows to increase Vietnam\u2019s economic growth.\n\n==\n\nAnwar 2011:\n\nTitle: Financial development and economic growth in Vietnam\n\nAbstract: By making use of a panel dataset that covers 61 provinces of Vietnam over the period 1997 to 2006, this paper examines the link between financial development and economic growth. Our analysis, which is based on endogenous growth theory, reveals that financial development has contributed to economic growth in Vietnam. We find that high ratio of credit to Gross Provincial Product (GPP) has accelerated economic growth in Vietnam. We also found a strong positive link between financial development and economic growth when alternative measures of financial development were used. The impact of foreign direct investment on economic growth will be stronger if more resources are invested in financial market development.\n\n==\n\nLe 2021:\n\nTitle: The Influences of FDI, GFCF, OPEN on Vietnamese Economic Growth\n\nAbstract: The aim of study is to research the influences of Foreign Direct Investment (FDI), Gross Fixed Capital Formation (GFCF), Trade Openness of the Economy (OPEN) on Vietnam economic growth. This study uses the annual data for the period 1986 to 2019, obtained from World Bank and Vietnam General Statistics Office. The study shows that FDI, GFCF and OPEN together influence to Vietnam economic growth in the period 1986 \u2013 2019 at significant level of 5%; in which the FDI and GFCF determinants have influenced greatly. In the short\u2013run, the results indicate that there are bidirectional causality relationships running between FDI and GDP, OPEN and GDP, OPEN and GFCF, and there are undirectional causality relationships running from GDP to GFCF, from GFCF to FDI, from FDI to OPEN. The study\u2019s results confirm that FDI as more reliable and less violate source of capital and can extend the Vietnam economic growth. According to the study\u2019s results, the authors suggest some recommendations to increase the Vietnam economic growth.\n\n==\n\nBagnai 2015:\n\nTitle: Economic growth and balance-of-payments constraint in Vietnam\n\nAbstract: Abstract Our paper examines the long-run relation between economic growth and current account equilibrium in Vietnam, using a multicountry balance-of-payments-constrained growth model. We find that for the whole sample (1985\u20132010) Vietnam grew less than the rate predicted by the model. We also find that the balance-of-payments-constrained growth rate shifted after the 1997 Asian crisis. Since the relative price effect is neutral, the volume effects dominate in setting the balance-of-payments constraint. On the one hand, owing to the high income elasticities of exports, growth in advanced countries has a strong multiplier effect on the Vietnamese economy. On the other hand, this effect is hindered by a strong \u201cappetite\u201d for imports coming from Asia. Finally, we assess the impact of the current crisis on Vietnam\u2019s growth for the period 2011 to 2017.\n\n==\n\nLuan 2018:\n\nTitle: Vietnam\u2019s economy in 2017 - 2018: Growth on an efficient and sustainable foundation\n\nAbstract: 2017 is the first time after many years that Vietnam met and exceeded 13 social-economic indicators. Vietnam\u2019s economy experienced a high economic growth rate, stable and sustainable macroeconomic environment. GDP growth is 6.81% (target 6.7%), CPI 3.53% (target (4%), credit growth 18,1%, and FDI of 36 billion USD. Import-export turnovers remain a significant achievement and has been maintained at a high level. Early results from restructuring of state-own enterprises signal a successful restructuring. These positive indicators provide impetus for the Vietnam\u2019s economy in 2018. The targets in 2018 include maintaining a stable macro-economic environment, achieving a 6.7% GDP growth, focusing on growth quality and sustainability, 4% CPI, 3.7% state budget deficit, 63.9% public debt, 8-10% increase in export turnover against 2017, less than 3% trade deficit and a ratio of government investment to GDP at 34%. To successfully achieve 2018 economic indicators, it is necessary to strongly apply policies and solutions for a creative and innovative system, develop science and technology nationwide and in every industry. This is to create breakthrough for the changes of the structure and growth model. \n\u00a0\n\n==\n\nThai 2020:\n\nTitle: Vietnam Economic Issue: Looking from Enterprise Sector, Export and Import Situations\n\nAbstract: At first glance, Vietnam has relatively high growth rate in the region and in the world, the average growth in the period of 2011 - 2018 is about 6.2%. As soon as the Covid 19 pandemic became active, causing most countries to have a negative GDP growth rate, but Vietnam's GDP growth in the first quarter was still 3.82%. \nThis study tried to describe the situation in the context of the current economic situation in Viet Nam through the production results of the enterprise sector, import and export. \nThe study used official data sources from Vietnam General Statistics Office.",
        "final_summary": "The papers suggest that foreign direct investment (FDI) and financial development are driving forces behind Vietnam's economic growth.\n\nAnwar 2010 and Hoang 2010 find a positive relationship between FDI and economic growth in Vietnam. Anwar 2010 suggests that investing more resources in education, training, and reducing the technology gap between foreign and local firms can enhance the impact of FDI on economic growth.\n\nAnwar 2011 highlights the contribution of financial development to economic growth in Vietnam, particularly through a high ratio of credit to Gross Provincial Product (GPP). The study suggests that further investment in financial market development can strengthen the impact of FDI on economic growth.\n\nLe 2021 confirms the significant influence of FDI, Gross Fixed Capital Formation (GFCF), and trade openness on Vietnam's economic growth. The study emphasizes the reliability and positive impact of FDI as a source of capital for economic expansion.\n\nBagnai 2015 examines the balance-of-payments constraint on Vietnam's economic growth and finds that growth has been lower than predicted by the model. The study highlights the importance of volume effects, such as exports and imports, in setting the balance-of-payments constraint.\n\nLuan 2018 discusses the positive indicators of Vietnam's economy in 2017, including high GDP growth, stable macroeconomic environment, and successful restructuring of state-owned enterprises. These achievements provide momentum for continued economic growth.\n\nThai 2020 briefly mentions Vietnam's relatively high growth rate and positive GDP growth even during the Covid-19 pandemic, without providing specific insights into the driving forces behind the growth.\n\nIn summary, FDI, financial development, and factors such as education, training, technology, and trade openness play significant roles in driving Vietnam's economic growth."
    },
    {
        "query": "contemporary review of goodwin class struggle model",
        "paper_list_string": "Mehrling 1986:\n\nTitle: A Classical Model of the Class Struggle: A Game-Theoretic Approach\n\nAbstract: The class struggle is formalized as a differential game in a strictly supply-side model, an approach that synthesizes the models of Lancaster and Goodwin. Four different steady-state equilibria are derived, each corresponding to different assumptions about the degree to which each class is organized to promote its own interests. In particular, the Goodwin growth cycle is shown to emerge from a world characterized by unorganized capitalists and workers, in which individuals ignore the effects of their own actions on economywide variables. More relevant for discussion of modern capitalism are the hierarchical equilibria, especially the codetermination equilibrium in which the existence of a full-employment equilibrium turns out to be problematic. Finally, comparative-statics results suggest that the incentives for technological change differ widely among the four regimes.\n\n==\n\nSaville 1974:\n\nTitle: Class Struggle and the Industrial Revolution\n\nAbstract: Book review of John Foster, Class Struggle and the Industrial Revolution, Weidenfeld and Nicholson, 1974. This is an important book by a marxist historian which will have a considerable influence upon the work of social historians in the future. In a number of ways, this is a pioneering work, and its use of sophisticated quantitative techniques for the dissection of urban social structures will serve as a model for subsequent research workers. It is, however, a difficult volume to read; for one thing it is awkwardly organized around the experience of three towns-Oldham, Northampton and South Shields, with the greatest amount of space and attention being given to Oldham; and for another the analysis is densely argued but discontinuous, so that one has to jump from one section to another to obtain an overall understanding of the author's approach.\n\n==\n\nEdlund 2015:\n\nTitle: The democratic class struggle revisited\n\nAbstract: This paper attempts to resolve scholarly disagreements concerning how class conflicts are manifested in contemporary welfare states. An analytical distinction is made between social (tensions/antagonism between classes) and political (class-based differences in political preferences) manifestations of class conflict. Using International Social Survey Program data (1999/2009) from 20 countries, the results indicate that social conflict is more common in meager welfare states where material inequality is relatively high compared to encompassing highly redistributive welfare states where levels of material inequality are relatively low. When it comes to distributive struggles in the political sphere \u2013 political conflict \u2013 the pattern is reversed. The results do not support arguments emphasizing that class as an analytical concept is irrelevant for understanding socio-political phenomena in modern industrial democracies. Instead, the results suggest that the character of class conflict varies across national socio-economic contexts in tandem with between-country variation in the institutional setup of the welfare state. The results support Walter Korpi\u2019s theory outlined in The Democratic Class Struggle, which suggests that in modern welfare states, institutionalized political conflict tends to replace less institutionalized and unorganized social conflict. This is more the case in encompassing welfare states than in residual welfare states.\n\n==\n\nRandall 1974:\n\nTitle: Class Struggle and the Industrial Revolution: Early Industrial Capitalism in Three English Towns\n\nAbstract: Class Struggle and the Industrial Revolution represents both a continuation of, and a stark contrast to, the impressive tradition of social history which has grown up in Britain in the last two decades. Its use of sophisticated quantitative techniques for the dissection of urban social structures will serve as a model for subsequent research workers. This work examines the impact of industrialization on the social development of the cotton manufacturing town of Oldham from 1790-1860; in particular how the experience of industrial capitalism aided the formation of a coherent organized mass class consciousness capable by 1830 of controlling all the vital organs of local government in the town. This will be a useful study to any student of the industrial revolution.\n\n==\n\nFoster 1976:\n\nTitle: Class Struggle and the Industrial Revolution : Early Industrial Capitalism in Three English Towns\n\nAbstract: Class Struggle and the Industrial Revolution represents both a continuation of, and a stark contrast to, the impressive tradition of social history which has grown up in Britain in the last two decades. Its use of sophisticated quantitative techniques for the dissection of urban social structures will serve as a model for subsequent research workers. This work examines the impact of industrialization on the social development of the cotton manufacturing town of Oldham from 1790-1860; in particular how the experience of industrial capitalism aided the formation of a coherent organized mass class consciousness capable by 1830 of controlling all the vital organs of local government in the town. This will be a useful study to any student of the industrial revolution.\n\n==\n\nInnes 1978:\n\nTitle: Class struggle and the state\n\nAbstract: The authors argue that, although purporting to deal with class struggle, the article on \u2018Class Struggle and the Periodisation of the State in South Africa\u2019 in ReviewNo. 7 fails to focus its attention on the fundamental contradiction in South African society \u2010 that of the struggle between capital and labour. The reason for this inadequacy lies in the article's dependency on the Poulantzian method of analysis which incorrectly separates \u2018economics\u2019 and \u2018politics\u2019 from the \u2018fundamental relations of exploitation\u2019 in capitalist society. It is only by establishing the unity between the relations of exploitation and their manifestation as political and economic forms that the state's position in the class struggle can be correctly located.\n\n==\n\nLevine 1988:\n\nTitle: Class Struggle and the New Deal: Industrial Labor, Industrial Capital, and the State\n\nAbstract: In this reassessment of New Deal policymaking, Rhonda Levine argues that the major constraints upon and catalysts for Roosevelt's policies were rooted in class conflict. Countering neo-Marxist and state-centred theories, which focus on administrative and bureaucratic structures, she contends that too little attention has been paid to the effect of class struggle. Levine analyzes the balance of class forces during the Great Depression and the ways in which they shaped the formulation, implementation and consequences of federal policies.\n\n==\n\nLevine 1988:\n\nTitle: Class Struggle and the New Deal: Industrial Labor, Industrial Capital, and the State\n\nAbstract: In this reassessment of New Deal policymaking, Rhonda Levine argues that the major constraints upon and catalysts for Roosevelt's policies were rooted in class conflict. Countering neo-Marxist and state-centred theories, which focus on administrative and bureaucratic structures, she contends that too little attention has been paid to the effect of class struggle. Levine analyzes the balance of class forces during the Great Depression and the ways in which they shaped the formulation, implementation and consequences of federal policies.",
        "final_summary": "The papers collectively provide insights into the contemporary review of Goodwin's class struggle model. Mehrling 1986 presents a game-theoretic approach to formalize the class struggle as a differential game, deriving different equilibria based on the organization of classes. Saville 1974 provides a book review of Foster's work on class struggle during the Industrial Revolution, highlighting its use of quantitative techniques for analyzing urban social structures. Edlund 2015 explores the manifestations of class conflict in welfare states, finding that social conflict is more common in meager welfare states with high material inequality, while political conflict is more prevalent in highly redistributive welfare states. Innes 1978 criticizes an article for failing to focus on the fundamental contradiction of class struggle in South African society. Levine 1988 reassesses New Deal policymaking, emphasizing the influence of class conflict on Roosevelt's policies and their implementation. \n\nIn summary, the papers collectively discuss various aspects of class struggle, including its formalization, historical context, manifestations in welfare states, and its impact on policymaking."
    },
    {
        "query": "natural language processing to assess service quality",
        "paper_list_string": "Liu 2019:\n\nTitle: Natural language processing approach for appraisal of passenger satisfaction and service quality of public transportation\n\nAbstract: Owing to the limited-time relevance and labour cost of traditional traffic service surveys, the quantity and quality of the data collected are not satisfactory in many cases. As more people are becoming active on comment websites and given the advances in natural language processing (NLP) techniques, it is feasible to conduct the opinion mining of comments regarding transportation services at large scale and low cost. In this study, >25,900 comments on websites regarding various public transportation services in Shanghai were analysed using NLP techniques, including sentiment analysis to calculate people's satisfaction with different transit facilities, keyword extraction to understand the key point in comments, and co-occurrence analysis to understand the commonality and characteristics of different routes. The sentiment analysis shows that people are more satisfied with traffic hubs than vehicles, and people have the highest level of satisfaction with airports and the lowest level of satisfaction with buses. The keywords classified by sentiment values show the strengths and weaknesses of Bus No. 49. Finally, the authors interpret a few anomalies of the results by tracing the context around the keywords. This study designs an NLP approach of transportation survey to improve the quality of public transportation service.\n\n==\n\nVargas-Calder'on 2021:\n\nTitle: Machine learning for assessing quality of service in the hospitality sector based on customer reviews\n\nAbstract: The increasing use of online hospitality platforms provides firsthand information about clients preferences, which are essential to improve hotel services and increase the quality of service perception. Customer reviews can be used to automatically extract the most relevant aspects of the quality of service for hospitality clientele. This paper proposes a framework for the assessment of the quality of service in the hospitality sector based on the exploitation of customer reviews through natural language processing and machine learning methods. The proposed framework automatically discovers the quality of service aspects relevant to hotel customers. Hotel reviews from Bogot\u00e1 and Madrid are automatically scrapped from Booking.com. Semantic information is inferred through Latent Dirichlet Allocation and FastText, which allow representing text reviews as vectors. A dimensionality reduction technique is applied to visualise and interpret large amounts of customer reviews. Visualisations of the most important quality of service aspects are generated, allowing to qualitatively and quantitatively assess the quality of service. Results show that it is possible to automatically extract the main quality of service aspects perceived by customers from large customer review datasets. These findings could be used by hospitality managers to understand clients better and to improve the quality of service.\n\n==\n\nZhang 2008:\n\nTitle: A Model for Evaluating Service Quality Competitiveness Based on Linguistic Information\n\nAbstract: Service quality competitiveness is the focus of services quality management under economy globalization. This paper introduces a method based on linguistic approach to evaluate service quality competitiveness. Firstly, a service quality competitiveness evaluation index system is proposed, which is composed of five aspects such as quality stratagem, quality resources, quality capability, customers' satisfaction and quality performance. The linguistic information of evaluation is transformed into two-tuple linguistic information representation, and the multi-attribute decision-making model is developed to obtain the values from comprehensive group evaluation, further, service quality competitiveness evaluation and decision making can be achieved on the basis of above analysis. Finally, an example is shown to illustrate above method.\n\n==\n\nKowalski 2017:\n\nTitle: Application of Natural Language Processing to Determine User Satisfaction in Public Services\n\nAbstract: Research on customer satisfaction has increased substantially in recent years. However, the relative importance and relationships between different determinants of satisfaction remains uncertain. Moreover, quantitative studies to date tend to test for significance of pre-determined factors thought to have an influence with no scalable means to identify other causes of user satisfaction. The gaps in knowledge make it difficult to use available knowledge on user preference for public service improvement. Meanwhile, digital technology development has enabled new methods to collect user feedback, for example through online forums where users can comment freely on their experience. New tools are needed to analyze large volumes of such feedback. Use of topic models is proposed as a feasible solution to aggregate open-ended user opinions that can be easily deployed in the public sector. Generated insights can contribute to a more inclusive decision-making process in public service provision. This novel methodological approach is applied to a case of service reviews of publicly-funded primary care practices in England. Findings from the analysis of 145,000 reviews covering almost 7,700 primary care centers indicate that the quality of interactions with staff and bureaucratic exigencies are the key issues driving user satisfaction across England.\n\n==\n\nChen 2001:\n\nTitle: Applying Linguistic Decision-Making Method to Deal with Service Quality Evaluation Problems\n\nAbstract: Today the evaluation of service quality is one of the most important management issues for every organization. An important feature of the service process is the degree of customer influence on the service process. A characteristic of service quality evaluation is that it consists of both tangible and intangible factors. In this paper the customers' subjective opinions and the weights of factors are described by linguistic labels which can be expressed in trapezoidal fuzzy numbers. After aggregating these linguistic labels, a linguistic decision\u2013making method is proposed to evaluate the service quality level of each organization. Finally, a numerical example is shown to highlight the procedure of the proposed method at the end of this paper.\n\n==\n\nMashaabi 2022:\n\nTitle: Natural Language Processing in Customer Service: A Systematic Review\n\nAbstract: Artificial intelligence and natural language processing (NLP) are increasingly being used in customer service to interact with users and answer their questions. The goal of this systematic review is to examine existing research on the use of NLP technology in customer service, including the research domain, applications, datasets used, and evaluation methods. The review also looks at the future direction of the field and any significant limitations. The review covers the time period from 2015 to 2022 and includes papers from five major scientific databases. Chatbots and question-answering systems were found to be used in 10 main fields, with the most common use in general, social networking, and e-commerce areas. Twitter was the second most commonly used dataset, with most research also using their own original datasets. Accuracy, precision, recall, and F1 were the most common evaluation methods. Future work aims to improve the performance and understanding of user behavior and emotions, and address limitations such as the volume, diversity, and quality of datasets. This review includes research on different spoken languages and models and techniques.\n\n==\n\nBano 2015:\n\nTitle: Automated Service Selection Using Natural Language Processing\n\nAbstract: With the huge number of services that are available online, requirements analysts face an overload of choice when they have to select the most suitable service that satisfies a set of customer requirements. Both service descriptions and requirements are often expressed in natural language (NL), and natural language processing (NLP) tools that can match requirements and service descriptions, while filtering out irrelevant options, might alleviate the problem of choice overload faced by analysts. In this paper, we propose a NLP approach based on Knowledge Graphs that automates the process of service selection by ranking the service descriptions depending on their NL similarity with the requirements. To evaluate the approach, we have performed an experiment with 28 customer requirements and 91 service descriptions, previously ranked by a human assessor. We selected the top-15 services, which were ranked with the proposed approach, and found 53% similar results with respect to top-15 services of the manual ranking. The same task, performed with the traditional cosine similarity ranking, produces only 13% similar results. The outcomes of our experiment are promising, and new insights have also emerged for further improvement of the proposed technique.\n\n==\n\nLiou 2006:\n\nTitle: Subjective appraisal of service quality using fuzzy linguistic assessment\n\nAbstract: Purpose \u2013 This study proposes a conceptual model to assess the perceived service quality properly using fuzzy set theory, since customers' perceptions of service quality are generally expressed subjectively in vague linguistic termsDesign/methodology/approach \u2013 To demonstrate the proposed model, the exploratory prospect of empirical study with questionnaire is given in measuring the service quality. The customer first records his or her perception of service quality in linguistic terms. The reviewer then quantifies the perception with fuzzy numbers. By mutually comparing all the criteria, importance weights of criteria in assessing the service quality can be prioritized. The fuzzy perceived quality score is then calculated by combining the fuzzy numbers of criteria with the corresponding weights. The fuzzy scores are then transformed to linguistic terms to reflect the customer's satisfaction level of overall service quality as interpreted by the reviewer.Findings \u2013 The investigation shows that distinguish...",
        "final_summary": "Natural language processing (NLP) is a valuable tool for assessing service quality across various sectors. Liu (2019) used NLP to analyze public transportation services comments, revealing differing satisfaction levels with various transit facilities and routes. Vargas-Calder\u00f3n (2021) proposed a framework that employs NLP and machine learning to extract service quality aspects from customer reviews in the hospitality sector. Zhang (2008) introduced a linguistic approach for evaluating service quality competitiveness. Kowalski (2017) utilized NLP, specifically topic models, to analyze user reviews of primary care practices, identifying the quality of staff interactions and bureaucratic processes as key drivers of user satisfaction. Chen (2001) proposed a linguistic decision-making method for service quality evaluation, incorporating customers' subjective opinions and factor weights. Mashaabi (2022) conducted a systematic review of NLP in customer service, emphasizing the need for improved performance and understanding of user behavior. Bano (2015) suggested an NLP approach using knowledge graphs for automated service selection based on similarity with customer requirements. Lastly, Liou (2006) proposed a fuzzy linguistic assessment for subjective service quality appraisal."
    },
    {
        "query": "lidar measurement error for surfaces with a reflectivity higher than a lambertian surface",
        "paper_list_string": "Hanssen 1996:\n\nTitle: Effects of non-Lambertian surfaces on integrating sphere measurements.\n\nAbstract: The effects of non-Lambertian scattering of the interior wall of an integrating sphere are examined through a sphere simulation model. The model employs Monte Carlo techniques. A sphere used for measurement of directional-hemispherical reflectance is modeled. The simulation allows sphere wall scattering to vary from perfectly Lambertian to perfectly specular in steps. The results demonstrate that significant measurement error can result as the scattering deviates from the Lambertian ideal. The error is found to be a strong function of the wall reflectance value as well: it is minimized for reflectances approaching 1.0 and increases as the reflectance value decreases to the minimum value examined of 0.5. The magnitudes of the errors associated with non-Lambertian scattering are also shown to be relatively independent of the specific field of view of the detector used in the measurement.\n\n==\n\nJeffries 2023:\n\nTitle: Toward open benchmark tests for automotive lidars, year 1: static range error, accuracy, and precision\n\nAbstract: Abstract. This paper describes the initial results from the first of 3 years of planned testing aimed at developing methods, metrics, and targets necessary to develop standardized tests for these instruments. Here, we evaluate range error accuracy and precision for eight automotive grade lidars; a survey grade lidar is used as a reference. These lidars are tasked with detecting a static, child-sized, target at ranges between 5 and 200 m. Our target, calibrated to 10% reflectivity and Lambertian, is a unique feature of this test. We find that lidar range precision is in line with the values reported by each manufacturer. However, we find that maximum range and target detection can be negatively affected by presence of an adjacent strong reflector. Finally, we observe that design trade-offs made by each manufacturer lead to important performance differences that can be quantified by tests such as the ones proposed here. This paper also includes some lessons learned, planned improvements, and discussion of future iterations of this activity.\n\n==\n\nRagheb 2002:\n\nTitle: Lambertian reflectance correction for rough and shiny surfaces\n\nAbstract: For rough and smooth surfaces, there are departures from Lambertian reflectance at the occluding limb. This is due to backscattering from microfacets which protrude above the surface. These departures from Lambertian reflectance limit the applicability of shape-from-shading to the surface. We consider how to reconstruct the Lambertian reflectance component in order to improve the quality of surface normal information recoverable using shape-from-shading. We show that although this correction process is reasonable when the light source and viewing directions are close to one another, it is not applicable when the directions are perpendicular.\n\n==\n\nKimes 1982:\n\nTitle: Irradiance measurement errors due to the assumption of a Lambertian reference panel\n\nAbstract: Abstract Total and diffuse global spectral irradiances, which are often required field measurements in remote sensing, are commonly obtained by measuring the radiance from a horizontal reference panel with assumed Lambertian properties. A technique is presented for determining the error in diurnal irradiance measurements that results from the non-Lambertian behavior of a reference panel under various irradiance conditions. Spectral biconical reflectance factors of a spray-painted barium sulfate panel, along with simulated sky radiance data for clear and hazy skies at six solar zenith angles, were used to calculate the estimated panel irradiances and true-irradiances for a nadir-looking sensor in two wavelength bands. The inherent errors in total spectral irradiance (0.68 \u03bcm) for a clear sky were 0.60, 6.0, 13.0, and 27.0% for solar zenith angles of 0\u00b0, 45\u00b0, 60\u00b0, and 75\u00b0. The technique can be used to characterize the error of a specific panel used in field measurements and thus eliminate any ambiguity of the effects of the type, preparation, and aging of the paint.\n\n==\n\nOren 2005:\n\nTitle: Generalization of the Lambertian model and implications for machine vision\n\nAbstract: Lambert's model for diffuse reflection is extensively used in computational vision. It is used explicitly by methods such as shape from shading and photometric stereo, and implicitly by methods such as binocular stereo and motion detection. For several real-world objects, the Lambertian model can prove to be a very inaccurate approximation to the diffuse component. While the brightness of a Lambertian surface is independent of viewing direction, the brightness of a rough diffuse surface increases as the viewer approaches the source direction. A comprehensive model is developed that predicts reflectance from rough diffuse surfaces. The model accounts for complex geometric and radiometric phenomena such as masking, shadowing, and interreflections between points on the surface. Experiments have been conducted on real samples, such as, plaster, clay, sand, and cloth. All these surfaces demonstrate significant deviation from Lambertian behavior. The reflectance measurements obtained are in strong agreement with the reflectance predicted by the proposed model. The paper is concluded with a discussion on the implications of these results for machine vision.\n\n==\n\nDeems 2006:\n\nTitle: Lidar Measurement of Snow Depth: Accuracy and Error Sources\n\nAbstract: Airborne laser altimetry (lidar) is a remote sensing technology that holds tremendous promise for mapping snow depth in snow hydrology and avalanche applications. In recent years lidar has seen a dramatic widening of applications in the natural sciences, resulting in technological improvements and an increase in the availability of sensors. Modern sensors allow recording of multiple pulse returns, which allows mapping of vegetation heights and surface elevations below forest canopies. Typical reported vertical accuracies are on the order of 15 cm with an average ground point spacing of 1.5 m. However many parameters in the lidar acquisition process, such as laser scan angle, laser pulse rate, and flight geometry relative to terrain gradients require consideration to ensure adequate point coverage in forested and/or mountainous terrain. Additionally, laser light interaction with the snow surface has a significant volumetric scattering component, requiring different considerations for surface height error estimation than for other earth surface materials. The penetration depth of the laser pulse (NIR wavelength of 1064 nm) is dependent primarily on grain size, liquid water content, and the angle of incidence. Using published estimates of penetration depth, we estimate radiative transfer contribution to depth measurement errors to be on the order of 1 cm. In this paper, we present a review of lidar altimetry procedures and error sources, investigate potential errors unique to snow surface remote sensing in the NIR wavelengths, and make recommendations for projects using lidar for snow depth mapping.\n\n==\n\nFukuchi 2001:\n\nTitle: Evaluation of differential absorption lidar (DIAL) measurement error by simultaneous DIAL and null profiling\n\nAbstract: A differential absorption lidar system for measurement of trace atmospheric substances in the visible and UV regions is developed. The system is based on a pair of Nd:YAG pumped dye lasers, each capable of emitting two wavelengths on alternate shots. The measurement error of the system was evaluated by measurement of tropospheric O3 using two identical differential absorption lidar (DIAL) pairs, from which two null profiles and four DIAL profiles are obtained simultaneously. The null profiles, obtained for both the on and off wavelengths used in DIAL measurement, are useful to evaluate systematic DIAL measurement error due to beam misalignment. The null profiles show that this systematic error is at most 1.3%. The total DIAL measurement error is about 5%, most of which can be attributed to statistical error.\n\n==\n\nBelmonte 2006:\n\nTitle: Turbulence-induced measurement errors in coherent differential absorption lidar ground systems.\n\nAbstract: The presence of atmospheric refractive turbulence makes it necessary to use simulations of beam propagation to examine the uncertainty added to the differential absorption lidar (DIAL) measurement process of a practical heterodyne lidar. The inherent statistic uncertainty of coherent return fluctuations in ground lidar systems profiling the atmosphere along slant paths with large elevation angles translates into a lessening of accuracy and sensitivity of any practical DIAL measurement. This technique opens the door to consider realistic, nonuniform atmospheric conditions for any DIAL instrument configuration.",
        "final_summary": "Lidar measurement error can be significant for surfaces with reflectivity higher than a Lambertian surface. Hanssen (1996) demonstrates that non-Lambertian scattering within an integrating sphere can lead to measurement error, which increases as reflectance value decreases. Jeffries (2023) finds that lidar range precision is generally accurate, but maximum range and target detection can be negatively affected by the presence of adjacent strong reflectors. Ragheb (2002) discusses the limitations of shape-from-shading due to departures from Lambertian reflectance for rough and shiny surfaces. Kimes (1982) highlights the error in irradiance measurements resulting from the assumption of a Lambertian reference panel. Oren (2005) shows that the Lambertian model is often inaccurate for real-world objects, and proposes a comprehensive model that accounts for complex phenomena. Deems (2006) discusses lidar measurement of snow depth and the various error sources involved. Fukuchi (2001) evaluates the measurement error of a differential absorption lidar system, with null profiles obtained for both the on and off wavelengths used in DIAL measurement. Belmonte (2006) examines the impact of atmospheric refractive turbulence on coherent differential absorption lidar systems. These papers collectively emphasize the need to consider non-Lambertian surfaces and various error sources when conducting lidar measurements for surfaces with higher reflectivity."
    },
    {
        "query": "do glucocorticoids increase the risk of cardiovascular disease in rheumatoid arthritis?",
        "paper_list_string": "Davis 2007:\n\nTitle: Glucocorticoids and cardiovascular events in rheumatoid arthritis: a population-based cohort study.\n\nAbstract: OBJECTIVE\nTo determine the relationship between glucocorticoid exposure and cardiovascular (CV) events in patients with rheumatoid arthritis (RA).\n\n\nMETHODS\nA total of 603 adult residents of Rochester, Minnesota with incident RA between 1955 and 1995 were followed up through their medical records for a median of 13 years (total of 9,066 person-years). Glucocorticoid exposure was defined 3 ways: tertiles of cumulative exposure; recent use (< or =3 months) versus past use (>3 months); and average daily dosage (< or =7.5 mg/day or >7.5 mg/day). CV events, including myocardial infarction, heart failure, and death from CV causes, were defined according to validated criteria. Cox regression models were adjusted for demographic features, CV risk factors, and RA characteristics.\n\n\nRESULTS\nRheumatoid factor (RF)-negative patients with exposure to glucocorticoids were not at increased risk of CV events, irrespective of the glucocorticoid dosage or timing of use, as compared with the reference group of RF-negative patients who had never been exposed to glucocorticoids. In contrast, RF-positive patients were at increased risk of CV events, particularly with higher cumulative exposure, higher average daily dosage, and recent use of glucocorticoids. RF-positive patients with high cumulative exposure to glucocorticoids had a 3-fold increased risk of CV events (hazard ratio 3.06 [95% confidence interval 1.81-5.18]), whereas RF-negative patients with high cumulative exposure were not at increased risk (hazard ratio 0.85 [95% confidence interval 0.39-1.87]).\n\n\nCONCLUSION\nRF-positive but not RF-negative patients were at increased risk of CV events following exposure to glucocorticoids. These findings suggest that glucocorticoids interact with RF status to modulate the occurrence of CV events in patients with RA. The mechanisms underlying this interaction are unknown and should be the subject of further research.\n\n==\n\nSijl 2014:\n\nTitle: Confounding by Indication Probably Distorts the Relationship between Steroid Use and Cardiovascular Disease in Rheumatoid Arthritis: Results from a Prospective Cohort Study\n\nAbstract: Objective To evaluate the risk of cardiovascular disease in patients with rheumatoid arthritis exposed to glucocorticoids. Methods Retrospective analysis of exposure to glucocorticoids in a prospective cohort of 353 patients with rheumatoid arthritis followed from June 2001 up to November 2011 for incident cardiovascular disease in a hospital-based outpatient cohort in the Netherlands. Hazard ratios with 95%-confidence intervals were calculated for the association between different types of exposure to glucocorticoids and incident cardiovascular disease. Associations were adjusted for demographics, cardiovascular risk factors and disease related parameters. Results Recent and current exposure to glucocorticoids were associated with incident cardiovascular disease, as was a longer duration of exposure and cumulative exposure to glucocorticoids. Adjustment for disease activity and severity negated the association. Conclusion In observational studies the finding of incident cardiovascular disease in patients with rheumatoid arthritis exposed to glucocorticoids is strongly confounded by indication due to high disease activity. The adverse cardiovascular effects of glucocorticoids might be balanced by positive effects working through inflammation control.\n\n==\n\nDavis 2005:\n\nTitle: Use of low-dose glucocorticoids and the risk of cardiovascular morbidity and mortality in rheumatoid arthritis: what is the true direction of effect?\n\nAbstract: People with rheumatoid arthritis (RA) experience an increased burden of cardiovascular disease (CVD) and reduced survival compared to the general population1. Myocardial infarction (MI) and heart failure appear to be more prevalent in RA compared to subjects without RA2. In one study, the risk of congestive heart failure was 2-fold higher among RA subjects compared to non-RA controls3. Much of the best evidence suggests that systemic inflammation plays an important role in the pathogenesis of CVD in RA1. Still, a nagging question persists in the minds of many rheumatologists today: What is the role of glucocorticoids (GC)? A common view is that GC harm the cardiovascular system. We wish to examine an alternative hypothesis: that GC might actually reduce the risk of CVD in patients with RA. Our aim is not to provide a comprehensive review of the cardiovascular effects of GC but rather to bring balance to the debate of how use of GC might affect the development of CVD in patients with RA. A discussion of GC is highly relevant given current rheumatology practice. GC have been used to treat people with RA for the last half-century, and recently, there has been renewed interest in these medications4. Use of GC is highly prevalent among people with RA; for example, these are taken by 30\u201350% of patients enrolled in recent therapeutic trials5. Rheumatologists commonly use GC as \u201cbridge therapy\u201d awaiting efficacy of slow-acting disease modifying antirheumatic drugs (DMARD), and also for treating disease flares. Frequently, delays occur in initiating biologic agents due to time required for insurance company authorization, tuberculin skin testing, and/or patient education visits. For these reasons, GC will likely continue to be an important component of the armamentarium for patients with RA, at least for the foreseeable future. Despite years of use, relatively little is known regarding the longterm effects of GC, particularly on the development of CVD4. In light of the increased burden of CVD in RA, it is critical to better understand how GC affect the development of CVD in this population, whether beneficial or harmful. Inflammation plays a fundamental role in the pathogenesis of CVD in RA. For example, abnormalities in traditional cardiovascular risk factors alone cannot explain the increased risk of CVD in this population6. Also, clinical features in RA that are associated with high inflammatory disease activity such as elevated acute phase markers and extraarticular manifestations are associated with cardiovascular events and mortality3,7-10. Theoretically, GC could modulate the risk of CVD in RA in 2 competing ways. The risk could increase due to potentially deleterious effects of GC on lipids, glucose tolerance, hypertension, or obesity11. Alternatively, by alleviating systemic inflammation, GC may actually decrease the risk of atherosclerosis and CVD1,12. Evidence supporting both directions of effect will be reviewed. Furthermore, there is evidence that effective treatment of systemic inflammation with DMARD is associated with lower risk of cardiovascular mortality13. Treatment with antagonists of tumor necrosis factor-alpha (TNF-\u03b1) may lower the risk of heart failure14. Therefore, it is reasonable to consider the possibility that GC may also reduce the risk of cardiovascular events and mortality. A number of novel findings in epidemiological studies suggest that \u201cflares\u201d of inflammation in RA may be involved in triggering cardiovascular events. First of all, the risk of cardiovascular events is independent of the duration of RA8. Second, the risk of sudden death and silent MI appears to be increased very early, even prior to patients ful-\n\n==\n\nRinc\u00f3n 2014:\n\nTitle: Glucocorticoid Dose Thresholds Associated With All\u2010Cause and Cardiovascular Mortality in Rheumatoid Arthritis\n\nAbstract: To delineate daily and cumulative glucocorticoid dose thresholds associated with increased mortality rates in rheumatoid arthritis (RA).\n\n==\n\nBoers 2003:\n\nTitle: Influence of glucocorticoids and disease activity on total and high density lipoprotein cholesterol in patients with rheumatoid arthritis\n\nAbstract: Background: Glucocorticoids induce hypercholesterolaemia, a cardiovascular risk factor, in patients with diseases other than rheumatoid arthritis (RA), but the data in RA are contradictory. Objective: To determine the effects of antirheumatic treatment, including prednisolone (combination) therapy on total and high density lipoprotein (HDL) cholesterol levels in RA, taking disease activity into account. Methods: HDL cholesterol and total cholesterol levels were determined in:(a) established RA (b) two cohorts with early active RA, (c) a previously conducted 56 week trial among patients with early RA comparing the value of intensive combination therapy (that included glucocorticoids) with sulfasalazine alone (COBRA trial). Results: In established RA total cholesterol levels were only slightly raised, irrespective of disease activity. However, HDL cholesterol was significantly higher in patients in remission than in patients with active disease. In contrast, in active early RA at baseline total cholesterol was low normal: between 4.6 and 5.1 mmol/l in the different populations. The level of HDL cholesterol was highly dependent on the duration of storage. In both COBRA groups total cholesterol increased by a mean of 0.6 mmol/l. HDL cholesterol increased by more than 50% after treatment, leading to an improvement of the total cholesterol/HDL ratio (atherogenic index). This increase (and index improvement) was much more rapid in the group receiving combination treatment. A similar pattern was seen in the 2001 cohort with early RA. In all the groups with active disease HDL and total cholesterol levels correlated inversely with disease activity. Conclusion: In established, but especially in early RA, disease activity is accompanied by atherogenic lipid levels. This dyslipidaemia can be rapidly reversed by aggressive antirheumatic treatment including glucocorticoids.\n\n==\n\nRinc\u00f3n 2004:\n\nTitle: Effect of glucocorticoids on the arteries in rheumatoid arthritis.\n\nAbstract: OBJECTIVE\nGlucocorticoids are suspected to cause atherosclerosis. Because of the possibility that their antiinflammatory effect may be antiatherogenic, this study investigated the effect of glucocorticoids on the arteries of patients with rheumatoid arthritis (RA).\n\n\nMETHODS\nWe assessed the arteries of 647 patients with RA. Central atherosclerosis was measured using high-resolution carotid ultrasound for the presence of plaque and for the extent of carotid artery intima-media thickness (CaIMT). Peripheral atherosclerosis was assessed using the systolic pressures of the dorsal pedal, posterior tibial, and brachial arteries to obtain the ankle-brachial index (ABI). Cumulative glucocorticoid dose was determined using pharmacy records, supplemented by self-report. Cardiovascular (CV) risk factors and RA clinical manifestations were ascertained using clinical and laboratory methods.\n\n\nRESULTS\nAmong the RA patients studied, 427 (66%) had received glucocorticoids. Of those who had never received glucocorticoids, 100 (47%) of 215 had carotid plaque and 17 (8%) of 219 had > or =1 incompressible lower-limb artery (ABI >1.3). Among patients in the highest tertile of lifetime glucocorticoid exposure (>16.24 gm prednisone), the frequency of carotid plaque increased to 85 (62%) of 138 (P = 0.006) and that of lower-limb arterial incompressibility increased to 24 (17%) of 140 (P = 0.008), with differences remaining significant after adjustment for age at onset, disease duration, sex, CV risk factors, and RA clinical manifestations (tender, swollen, and deformed joint counts, subcutaneous nodules, rheumatoid factor seropositivity, and erythrocyte sedimentation rate). The CaIMT also displayed an increase with higher glucocorticoid exposure, but the differences did not reach significance. Lower-limb artery obstruction (ABI < or =0.9) was not associated with glucocorticoid exposure.\n\n\nCONCLUSION\nIn this RA sample, glucocorticoid exposure was associated with carotid plaque and arterial incompressibility, independent of CV risk factors and RA clinical manifestations. This supports a role for glucocorticoids in the CV complications that occur in RA.\n\n==\n\nWei 2004:\n\nTitle: Taking Glucocorticoids by Prescription Is Associated with Subsequent Cardiovascular Disease\n\nAbstract: Context Patients who take glucocorticoids appear to have an increased risk for cardiovascular disease. However, data about the magnitude of this increased risk are lacking Contribution In this large, population-based study, the use of glucocorticoids was associated with an increased risk for cardiovascular events, with a clear dose-response relationship. Patients who received high-dose glucocorticoids were more than 2.5 times as likely as patients who did not use glucocorticoids to experience a cardiovascular event. Implications These data will help clinicians estimate cardiovascular risk among patients who require glucocorticoids. The Editors Glucocorticoids are commonly used as anti-inflammatory and immunosuppressive therapy in diseases such as asthma, inflammatory bowel disease, and inflammatory arthritis. Well-known adverse effects of glucocorticoids include hypertension, diabetes mellitus, and obesity (1-3), all of which are independent risk factors for cardiovascular disease. The principal physiologic glucocorticoid is cortisol. Increased cortisol secretion and action, even within the physiologic range, is associated with several risk factors for cardiovascular disease (4, 5). Indeed, studies have proposed that subclinical Cushing syndrome may be an important cardiovascular risk factor (6, 7). However, whether, and to what extent, the adverse effects of exogenous glucocorticoids on these risk factors for cardiovascular disease cause cardiovascular morbidity and death has not been established (8, 9). This is not predictable, especially since glucocorticoids may also have cardioprotective effects mediated by their anti-inflammatory and antiproliferative actions in the vessel wall (10, 11). We tested the hypothesis that users of exogenous glucocorticoids have an increased risk for cardiovascular disease. In particular, we hypothesized that high doses will be associated with cardiovascular disease, while doses equivalent to or below the physiologic range of endogenous glucocorticoid secretion may be compensated for by decreased cortisol levels and will not incur excessive risk. We have tested this by using the MEdicines MOnitoring unit (MEMO) record linkage database to compare people who were exposed and not exposed to glucocorticoid therapy. Methods We performed this study in the Tayside region in Scotland by using the MEMO record linkage database. The MEMO database covers a geographically compact population and serves about 400000 patients in the National Health Service (NHS) in Scotland, 97% of whom are white. The NHS in Scotland is tax-funded and free at the point of consumption and covers the entire population. In Tayside, almost no health care is delivered without the NHS. The data collection methods for this database have previously been described (12). In brief, this database contains several data sets, including all dispensed community prescriptions, hospital discharge data, mortality data, biochemistry data, and other data, that are linked by a unique patient identifier, the community health index number. These data are made anonymous for the purposes of research, as approved by the Tayside Caldicott Guardians (a group appointed by the government to protect the confidentiality of medical records). The Tayside committee on research medical ethics also approved the project. We cleaned and validated all data before analysis. We included all patients who resided in Tayside and registered with a general practitioner in January 1993, who were 40 years of age or older, and who remained a resident in Tayside until December 1997 or who died during the study period. Definitions of Exposure to Glucocorticoids Exposed and Comparator Cohorts All patients who received 1 or more dispensed prescriptions for glucocorticoids (including inhaled steroids, topical steroids, oral and parenteral steroids, rectal application steroids, and nasal steroids) between July 1993 and December 1996 formed the exposed cohort. They entered the study at their date of first prescription. The rest of the study sample made up the comparator cohort. We excluded patients who were hospitalized for inflammatory bowel disease and chronic obstructive airways disease during the follow-up period because they may have been given glucocorticoids as hospital inpatients without the prescriptions being identified by MEMO. We generated a random date of entry to the study for each member of the comparator cohort by using a frequency-matched calendar year generated from the dates of entry to the study in the exposed cohort. We excluded patients from both cohorts if they were hospitalized for cardiovascular disease before study entry. Dose of Glucocorticoids For about two thirds of dispensed oral or systemic glucocorticoid prescriptions, we recorded the date of prescription, dose of tablets, amount dispensed, and instructions on how medication should be taken. For these prescriptions, we could determine daily exposure during prescribed courses. For the remainder, we knew the total dose dispensed but, because prescriptions were marked take as directed, could not accurately determine the daily dose and duration. We therefore calculated the average daily dose by dividing the total amount of glucocorticoid dispensed by the total number of days of observation. For multiple simultaneous formulations (oral and inhaled) of glucocorticoids, we used only the oral glucocorticoids to calculate the daily dose. We categorized glucocorticoid exposure according to average daily doses throughout the follow-up period for each patient as high (oral, parenteral, and rectal steroids with daily dosage 7.5 mg [that is, supraphysiologic doses]); medium (oral, parenteral, and rectal steroids with daily dosage <7.5 mg [that is, approximately equivalent to the physiologic range of endogenous glucocorticoid secretion]); or low (inhaled, topical, and nasal steroids with daily dosage less than the equivalent physiologic range). As a result of this calculation, we included patients who took large dosages for a short period in the medium-dose group. We calculated dose equivalents of prednisolone as follows: 1 mg of prednisolone = 5 mg of cortisone = 4 mg of hydrocortisone = 1 mg of prednisone = 0.8 mg of triamcinolone = 0.8 mg of methylprednisolone = 0.15 mg of dexamethasone = 0.15 mg of betamethasone (13). The MEMO database does not collect information for prescriptions dispensed in hospitals. We considered participants hospitalized for asthma, inflammatory bowel disease, or chronic obstructive pulmonary disease (disorders usually treated with high-dose glucocorticoids in the hospital) as being exposed to a typical dosage of glucocorticoids (30 mg/d, prednisolone) during that period. Analysis of Events during Glucocorticoid Exposure (On Treatment vs. Off Treatment) We did a subgroup analysis of those patients for whom we had data on daily dose of glucocorticoid exposure. For each patient, we divided exposure to glucocorticoid into the time that the patient was exposed (on treatment) and the time that the patient was not exposed (off treatment). We then temporally related these periods to the occurrence of cardiovascular events. Incident versus Prevalent Use For each patient in the cohort exposed to glucocorticoids, we used the 6 months before entry to the study as a screening period. We classified patients who did not receive glucocorticoids during this period as incident users of glucocorticoids and patients who received glucocorticoids during this period as prevalent users. Continuous versus Intermittent Use We did an analysis comparing cardiovascular risk in continuous use (180 days between prescriptions) versus intermittent use (>180 days between prescriptions). Exposure by Disease Indication We identified patients with chronic obstructive pulmonary disease if they were hospitalized for asthma or chronic obstructive pulmonary disease or were prescribed an inhaled steroid or bronchodilator drug before study entry. We identified patients with inflammatory bowel disease if they were hospitalized for colitis or were prescribed a rectal steroid preparation before study entry. We identified patients with inflammatory arthritis if they were hospitalized for inflammatory arthritis or were prescribed nonsteroidal anti-inflammatory drugs (NSAIDs) and disease-modifying antirheumatic drugs before study entry. Outcome Variables We collected the outcome data on each patient until 31 December 1997. The outcome of the study was a cardiovascular event defined as the composite end point of hospitalization with a primary diagnosis of myocardial infarction, angina, angioplasty or coronary revascularization, stroke, transient ischemic attack, congestive cardiac failure, or cardiovascular death during follow-up. We censored patients at their first event if they had several events. We ascertained diagnoses of myocardial infarction, angina, angioplasty and coronary revascularization, stroke, transient ischemic attack, and congestive cardiac failure from the hospital discharge diagnosis data, which were validated (14) in the Scottish Morbidity Record 1 by primary International Classification of Diseases, Ninth or Tenth Revisions, codes. We also ascertained diagnoses of angioplasty and coronary revascularization by the code of classification of surgical operations and procedures. We also obtained the death certification data for all Tayside residents who died. Statistical Analysis We counted events that occurred during the study period and compared rates of events between cohorts. We used the Poisson regression model to investigate the association between glucocorticoid exposure and cardiovascular outcome. We included the following covariates: age at study entry; sex; social deprivation; use of angiotensin-converting enzyme inhibitors, anticoagulants, antiplatelet agents, -blockers, -blockers, calcium-channel blockers, cardiac glycosides, diuretics, nitrates, lipid-lowering drugs, hormone replacement the\n\n==\n\nBoers 2015:\n\nTitle: Drugs and cardiovascular risk in inflammatory arthritis: another case of glucocorticoid-bashing?\n\nAbstract: I read with interest the meta-analysis by Roubille et al ,1 especially for the adverse effects noted with glucocorticoids (GCs). The data appear to confirm the risk of GC, which is surprising in view of the stated objective to include evidence from clinical trials. In a recent meta-analysis of randomised controlled trials (RCTs) that I participated in, no increased cardiovascular risk was found in 66 comparisons and a total \u2026",
        "final_summary": "The relationship between glucocorticoid use and cardiovascular disease (CVD) in rheumatoid arthritis (RA) is complex. Davis 2007 found that glucocorticoid exposure increased the risk of CVD in RF-positive patients with RA, but not in RF-negative patients. Sijl 2014 suggested that the observed association between glucocorticoid use and CVD in RA may be confounded by disease activity. Davis 2005 discussed the potential dual role of glucocorticoids in CVD, indicating they could have both harmful and beneficial effects. Boers 2003 found that disease activity in RA was associated with atherogenic lipid levels, which could be reversed by aggressive antirheumatic treatment including glucocorticoids. Rinc\u00f3n 2004 demonstrated an association between glucocorticoid exposure and carotid plaque and arterial incompressibility in RA patients. Wei 2004 reported an increased risk of cardiovascular events associated with glucocorticoid use, with a dose-response relationship. Lastly, Boers 2015 questioned the evidence of increased cardiovascular risk solely attributed to glucocorticoids. (Davis 2007; Sijl 2014; Davis 2005; Boers 2003; Rinc\u00f3n 2004; Wei 2004; Boers 2015)"
    },
    {
        "query": "impact of bank funding cost on credit risk",
        "paper_list_string": "Fabbro 2011:\n\nTitle: The Effects of Funding Costs and Risk on Banks' Lending Rates\n\nAbstract: After falling for over a decade, the major banks' net interest margins appear to have stabilised in a relatively narrow range in recent years. In the early part of the financial crisis, margins fell to the bottom of this range, reflecting an increase in debt funding costs. Margins have since recovered a little, to around the middle of the range, as a result of some repricing of lending rates relative to these costs. In addition to the increase in the cost of debt funding, there have been other drivers of the rise in lending rates relative to the cash rate. First, the banks have increased their equity funding, which is more costly than debt finance. Second, risk margins on loans have risen to account for higher expected losses.\n\n==\n\nDuquerroy 2022:\n\nTitle: Tracing Banks\u2019 Credit Allocation to their Funding Costs\n\nAbstract: We quantify how banks\u2019 funding costs affect their lending behavior directly, and indirectly by feeding back to their net worth. For identification, we exploit banks\u2019 heterogeneous liability structure and the existence of regulated deposits in France whose rates are set by the government. Using administrative credit-registry and regulatory bank data, we find that a one-percentage-point increase in funding costs reduces credit by 17%. To insulate their profits, banks reach for yield and rebalance their lending towards smaller and riskier firms. These changes are not compensated for by less affected banks at the aggregate city level, with repercussions for firms\u2019 investment. JEL classification: E23, E32, E44, G20, G21, L14\n\n==\n\nPanetta 2011:\n\nTitle: The impact of sovereign credit risk on bank funding conditions\n\nAbstract: The financial crisis and the ensuing recession have caused a sharp deterioration in public finances across advanced economies, raising investor concerns about sovereign risk. The concerns have so far mainly affected the euro area, where some countries have seen their credit ratings downgraded during 2009\u221211 and their funding costs rise sharply. Other countries have also been affected, but to a much lesser extent. Greater sovereign risk is already having adverse effects on banks and financial markets. Looking forward, sovereign risk concerns may affect a broad range of countries. In advanced economies, government debt levels are expected to rise over coming years, due to high fiscal deficits and rising pension and health care costs. In emerging economies, vulnerability to external shocks and political instability may have periodic adverse effects on sovereign risk. Overall, risk premia on government debt will likely be higher and more volatile than in the past. In some countries, sovereign debt has already lost its risk-free status; in others, it may do so in the future. The challenge for authorities is to minimise the negative consequences for bank funding and the flow-on effects on the real economy. This report outlines the impact of sovereign risk concerns on the cost and availability of bank funding over recent years. It then describes the channels through which sovereign risk affects bank funding. The last section summarises the main conclusions and discusses some implications for banks and the official sector. Two caveats are necessary before discussing the main findings. First, the analysis focuses on causality going from sovereigns to banks, as is already the case in some countries, and, looking forward, is a possible scenario for other economies. But causality may clearly also go from banks to sovereigns. However, even in this second case, sovereign risk eventually acquires its own dynamics and compounds the problems of the banking sector. Second, the report examines the link between sovereign risk and bank funding in general terms, based on recent experience and research. It does not assess actual sovereign risk and its impact on bank stability in individual countries at the present juncture.\n\n==\n\nBabihuga 2014:\n\nTitle: Bank Funding Costs for International Banks\n\nAbstract: This paper investigates the determinants of bank funding costs for a sample of internationally active banks from 2001\u201312. We find that changes in banks\u2019 unsecured funding costs are associated with bank-specific characteristics such as an institution\u2019s credit worthiness and the return on its market value, and importantly, on the level and quality of capital. Similarly, market factors such as the level of investor risk appetite, as well as shocks to financial markets\u2014notably the US subprime crisis and the Euro Area sovereign debt crisis\u2014have also been key drivers of the sharp rise in bank funding costs. We also find evidence that large systemically important institutions have enjoyed a funding advantage, and that this advantage has risen since the onset of the two crises. With the exception of Euro Area periphery banks, by end-2012 the rise in funding costs had generally been reversed for most major banks as a result of improvments in bank asset quality as well as steps taken to increase resilience, notably higher capitalization. Our results suggest increased capital buffers may potentially support bank lending to the real economy by reducing bank funding costs.\n\n==\n\nBabihuga 2014:\n\nTitle: Bank Funding Costs for International Banks\n\nAbstract: This paper investigates the determinants of bank funding costs for a sample of internationally active banks from 2001\u201312. We find that changes in banks\u2019 unsecured funding costs are associated with bank-specific characteristics such as an institution\u2019s credit worthiness and the return on its market value, and importantly, on the level and quality of capital. Similarly, market factors such as the level of investor risk appetite, as well as shocks to financial markets\u2014notably the US subprime crisis and the Euro Area sovereign debt crisis\u2014have also been key drivers of the sharp rise in bank funding costs. We also find evidence that large systemically important institutions have enjoyed a funding advantage, and that this advantage has risen since the onset of the two crises. With the exception of Euro Area periphery banks, by end-2012 the rise in funding costs had generally been reversed for most major banks as a result of improvments in bank asset quality as well as steps taken to increase resilience, notably higher capitalization. Our results suggest increased capital buffers may potentially support bank lending to the real economy by reducing bank funding costs.\n\n==\n\nDemirg\u00fc\u00e7-Kunt 2009:\n\nTitle: Bank Activity and Funding Strategies: The Impact on Risk and Return\n\nAbstract: This paper examines the implications of bank activity and short-term funding strategies for bank risk and return using an international sample of 1334 banks in 101 countries leading up to the 2007 financial crisis. Expansion into non-interest income generating activities such as trading increases the rate of return on assets, and it may offer some risk diversification benefits at very low levels. Non-deposit, wholesale funding in contrast lowers the rate of return on assets, while it can offer some risk reduction at commonly observed low levels of non-deposit funding. A sizeable proportion of banks, however, attract most of their short-term funding in the form of non-deposits at a cost of enhanced bank fragility. Overall, banking strategies that rely prominently on generating non-interest income or attracting non-deposit funding are very risky, consistent with the demise of the U.S. investment banking sector.\n\n==\n\nDemirg\u00fc\u00e7-Kunt 2009:\n\nTitle: Bank Activity and Funding Strategies: The Impact on Risk and Return\n\nAbstract: This paper examines the implications of bank activity and short-term funding strategies for bank risk and return using an international sample of 1334 banks in 101 countries leading up to the 2007 financial crisis. Expansion into non-interest income generating activities such as trading increases the rate of return on assets, and it may offer some risk diversification benefits at very low levels. Non-deposit, wholesale funding in contrast lowers the rate of return on assets, while it can offer some risk reduction at commonly observed low levels of non-deposit funding. A sizeable proportion of banks, however, attract most of their short-term funding in the form of non-deposits at a cost of enhanced bank fragility. Overall, banking strategies that rely prominently on generating non-interest income or attracting non-deposit funding are very risky, consistent with the demise of the U.S. investment banking sector.\n\n==\n\nJondeau 2020:\n\nTitle: A New Indicator of Bank Funding Cost\n\nAbstract: The cost of bank funding on money markets is typically the sum of a risk-free rate and a spread that reflects rollover risk, i.e., the risk that banks cannot roll over their short-term market funding. This risk is a major concern for policymakers, who need to intervene to prevent the funding liquidity freeze from triggering the bankruptcy of solvent financial institutions. We construct a new indicator of rollover risk for banks, which we call the forward funding spread. It is calculated as the difference between the three-month forward rate of the yield curve constructed using only instruments with a three-month tenor and the corresponding forward rate of the default-free overnight interest swap yield curve. The forward funding spread usefully complements its spot equivalent, the IBOR-OIS spread, in the monitoring of bank funding risk in real time. First, it accounts for market participants' expectations of how funding costs will evolve over time. Second, it identifies liquidity regimes, which coincide with the levels of excess liquidity supplied by central banks. Third, it has much higher predictive power for economic growth and bank lending in the United States and the euro area than the spot IBOR-OIS, credit default swap spreads or bank bond credit spreads.",
        "final_summary": "The impact of bank funding cost on credit risk is explored in the collected papers. Fabbro (2011) finds that increased debt funding costs during the financial crisis led to a decrease in net interest margins for banks. Duquerroy (2022) shows that higher funding costs result in a reduction in credit by 17% and a shift towards riskier lending. Panetta (2011) highlights that sovereign credit risk can affect bank funding conditions, leading to higher costs and potential adverse effects on banks and financial markets. Babihuga (2014) identifies various factors influencing bank funding costs, including creditworthiness, market conditions, and shocks such as the subprime and sovereign debt crises. Demirg\u00fc\u00e7-Kunt (2009) examines the implications of bank activity and funding strategies, finding that reliance on non-interest income and non-deposit funding can increase risk and fragility. Jondeau (2020) introduces a new indicator, the forward funding spread, which helps monitor bank funding risk and has predictive power for economic growth and bank lending. Overall, these papers demonstrate the complex relationship between bank funding costs and credit risk, influenced by various factors such as market conditions, sovereign risk, and funding strategies."
    },
    {
        "query": "how long can you store diesel before it goes bad?",
        "paper_list_string": "Lepera 1973:\n\nTitle: How stable is diesel in storage\n\nAbstract: The storage stability characteristics of four diesel fuels meeting the requirements of Federal Specification VV-F-800a (Fuel Oil, Diesel) were investigated. Two straight-run fuels and two fuels containing catalytically cracked fractions were stored in 100 bbl above-ground steel tanks for two years. The fuels were sampled initially and periodically during storage; each sample was analyzed for existent gum, particulates, and ASTM D2274 stability. Two of the fuels maintained satisfactory storage stability throughout the entire two-year storage period. One fuel was considered marginal with respect to storage stability since its initially low values for gum and particulate contamination were followed by gradually increasing values. The fourth fuel exhibited a cyclic pattern of gum and particulate values typical of unsatisfactory storage stability. These results indicated that production diesel fuels meeting the requirements of Federal Specification VV-F-800a might not have satisfactory storage stability.\n\n==\n\nBento 2001:\n\nTitle: Biodeterioration of stored diesel oil: studies in Brazil\n\nAbstract: Abstract The problems of hydrocarbon fuel storage in Brazil are particularly acute for diesel fuel. Visits to bus depots showed that many foremen did not understand the importance of draining water bottoms regularly and most systems were microbially contaminated. Common fungal isolates from refineries and distribution systems, Hormoconis resinae , Aspergillus niger , Aspergillus fumigatus , Paecilomyces variotii , and Candida silvicola , grew equally well in laboratory diesel/water systems with or without a chemical additive mixture, showing that this package of compounds neither promoted nor retarded fungal growth. Non-sterilised diesel was stored for 450 days over a water bottom, with or without an isothiazolone biocide, in the laboratory. The fungi most frequently detected in the non-biocide treated systems were H. resinae , A fumigatus , P. variotii , a Penicillium sp., and the yeasts, Rhodotorula glutinis and Candida silvicola . Bacterial isolates included oxidative Gram negative rods, sulphate-reducing bacteria and a Micrococcus sp. Biocide at 0.1 ppm maintained the systems clean for up to 30 days, and at 1 or 10 ppm for 400 days. After 400 days, the biomass (dry weight) from non-additive-containing diesel in control, 1 and 10 ppm biocide-containing systems was 24.6, 4.6 and 3.3 mg, respectively. The system treated with 0.1 ppm biocide yielded 38.2 mg biomass, indicating that sub-effective doses may lead to increased microbial growth. Within 24 h of addition of 10 ppm biocide to a highly contaminated control flask (145 days storage) there was a 2-log reduction in total aerobic bacterial and yeast population and the filamentous fungal count was >5 ml \u22121 .\n\n==\n\nBerrios 2012:\n\nTitle: Storage effect in the quality of different methyl esters and blends with diesel\n\nAbstract: Abstract Storage stability is an important aspect in the commercialization of biodiesel. Due to the chemical structure of methyl ester, long-term storage may lead to the degradation of fuel properties that can make it difficult to meet the EN 14214 Standard. This paper examines the degradation of several biofuels (sunflower methyl esters, used cooking oil methyl esters and blends with commercial diesel) under different storage conditions over a 6-month period. The storage conditions simulated real-life conditions by using similar temperatures to those found in commercial tanks, with little air turnover and in the absence of light. Extreme temperature conditions were also studied to determine if this variable has an effect on storage stability. Opaque glass or stainless steel storage containers were used to check the interaction between metal and biofuel. The results show that temperature plays an important role in the degradation of methyl esters. On the other hand, stainless steel was found to be a suitable material for the commercial storage tanks as the effect of this material on stability was almost negligible. The blends with commercial diesel performed better in long-term storage. This was probably due to the effect of the ester dilution and the presence of additives in commercial diesel.\n\n==\n\nMu\u017e\u00edkov\u00e1 2022:\n\nTitle: Oxidation stability of diesel fuel during storage\n\nAbstract: Over the last 25 years, the diesel fuel has undergone major changes in its composition, which have fundamentally affected its long-term storage possibilities. An oxidation stability is a main parameter characterising the storage of the diesel fuel and it is strongly affected by a diesel fuel composition. The oxidation stability decreases in a line saturated hydrocarbons \u2013 aromatic hydrocarbons \u2013 unsaturated hydrocarbons. The mandatory use of FAME as a biocomponent of the diesel fuel negatively affect its oxidation stability. The FAME contain unstable double bonds C=C and their mixtures with mineral diesel fuel cannot be storage for a long time. The use of antioxidants with FAME has not effect in the long time storage. A recommended usable life of diesel fuel with FAME accor-ding to \u010cSN 65 6500 is from 1 to 3 months according to the FAME content. However, in some cases it is ne-cessary to store diesel fuel for a long time. State material reserves or back-up diesel generators are examples, when the diesel fuel is stored for the long time.\nThe oxidation stability of the fuel expresses a resistance to an oxygen action. The oxygen, which is dissolved in the fuel, attacts molecules of the fuel and various oxidation products create. Hydroperoxides are the primary oxidation products. Secondary oxidation reactions give aldehydes, ketones, karboxylic acids and insoluble deposits. The oxidation products negatively affect the diesel fuel properties.\nBesides the composition the oxidation stability of the fuel is negatively affected by a high temperature,a high content of dissolved oxygen, an UV radiation and a presence of metals with a catalytic effect.\nAn overview of methods used for the measuring or the observing oxidation stability was prepared in the article. It means not only the oxidation stability measuring but also a measuring of the content of different oxidation products which are related to the various oxidation degree .\nThe aim of the article was to summarize the possibilities of a predicting the storage time of the diesel fuel and to propose a procedure for the monitoring and the predicting its longterm durability. Only one standardized storage test according to the ASTM D4625 can be found in the literature. The test is based on the storage of 400 ml of the diesel fuel at 43 \u00b0C for periods of 4, 8, 12, and 24 weeks. After aging for a selected period, a sample is analyzed for insolubles. The correlation of the test results is: a week at 43 \u00b0C is roughly equivalent to a month of the storage at the temperature of 21 \u00b0C. The test is a time and material consuming and the correaltion was determined for diesel fuels made up to 1990\u00b4s.\nThe new shorter storage test based on the standard test according to ASTM D4625 was proposed to predict diesel storage stability. The temperature was rised and the the time was shortened up to one month. During the test short laboratory analyses can be used for monitoring oxidation of the diesel fuel for example: the oxidation stability by PetroOxy, the peroxide number, the oxidation index by infrared spectroscopy, the acid number, the bromine number or the antioxidant content. The sample consumption is about 100 ml according to the selected analyses.\n\n==\n\nBezergianni 2012:\n\nTitle: Oxidative stability of waste cooking oil and white diesel upon storage at room temperature.\n\nAbstract: Renewable diesel fuels are alternative fuels produced from vegetable oils or animal fats. Catalytic hydrotreating of waste cooking oil (WCO) was carried out at pilot-plant scale and a paraffinic diesel, called \"white\" diesel was obtained. The white diesel and WCO samples were stored for one year at room temperature under normal atmospheric conditions, but not exposed to sunlight. Viscosity, total acid number (TAN), induction period (IP), carbonaceous deposits, density, cold flow properties, distillation and water content were monitored. TAN and density of the white diesel stored in conventional bottles changed from 0 to 0.221 mg KOH/g and from 787 to 838 kg/m(3), respectively. The remaining parameters did not vary significantly. Water content of WCO increased from 482 to 2491 mg/kg, TAN from 0.744 to 0.931 mg KOH/g, whereas viscosity, IP and carbon residues fluctuated mildly. The results are indicative of the white diesel's stability, rendering it suitable for prolonged storage.\n\n==\n\nBondioli 2004:\n\nTitle: The prediction of biodiesel storage stability. Proposal for a quick test\n\nAbstract: The results obtained during the set-up of a method for the prediction of biodiesel storage stability are shown. In order to speed up the rate of ageing processes, the test temperature was set to 80 \u00b0C, and tests were carried out within 24 h in a Rancimat modified cell apparatus. The effect of the quick ageing process can be monitored by means of the evaluation of changes in ester content, polymer content and peroxide value. By comparing the results obtained during a 1 -year experiment, carried out under real storage conditions, and during a second test, carried out at 43 \u00b0C for 24 wk according to ASTM D 4625 method, with the ones reported in this paper, several discrepancies regarding the different degradation reactions as well as the rate at which these reactions take place were evidenced. Finally, a tentative relationship between the results obtained with the proposed quick test and the real Rancimat induction period, shown by different samples after 12 months of real storage, are shown. From the results obtained during this work, the Oxidation Stability test carried out according to EN 14112:2003 and the Storage Stability test reported herein may provide different information for the evaluation of the overall stability of a biodiesel sample.\n\n==\n\nThompson 2013:\n\nTitle: Long term storage of biodiesel/petrol diesel blends in polyethylene fuel tanks\n\nAbstract: Abstract This paper explores the long-term effects that an ageing biodiesel fuel blended with low sulfur diesel fuel at concentrations of 0%, 15%, 50%, 85%, and 100% (v/v) can have on polyethylene materials. Rotationally molded fuel tanks made of linear low density polyethylene or crosslinked polyethylene were used to appropriately mimic the storage environment of fuels, though the study also considered immersed polymer specimens as this test approach is more commonly reported on in the literature. The extent of oxidation for the fuels was monitored by determination of its peroxide value and acid number over time, with both values being consistently higher for blends with higher biodiesel content. Storage at 80\u00a0\u00b0C for up to 56\u00a0days resulted in significant oxidation of the fuel while testing up to 380\u00a0days at 23\u00a0\u00b0C produced no change in peroxide or acid concentration. The change in mechanical properties for the polyethylenes due to fuel was determined by tensile testing. Comparing the tensile data to the oxidation species in the stored fuels revealed that noted mechanical losses did not correspond with higher concentration of hydroperoxides or acid species but rather occurred as a result of fuel absorption. No difference was found in this regards between the two polyethylene species.\n\n==\n\nMittelbach 2001:\n\nTitle: Long storage stability of biodiesel made from rapeseed and used frying oil\n\nAbstract: The degree of physical and chemical deterioration of biodiesel produced from rapeseed and used frying oil was studied under different storage conditions. These produced drastic effects when the fuel was exposed to daylight and air. However, there were no significant differences between undistilled biodiesel made from fresh rapeseed oil and used frying oil. The viscosity and neutralization numbers rose during storage owing to the formation of dimers and polymers and to hydrolytic cleavage of methyl esters into fatty acids. However, even for samples studied under different storage conditions for over 150 d the specified limits for viscosity and neutralization numbers had not been reached. In European biodiesel specifications there will be a mandatory limit for oxidative stability, because it may be a crucial parameter for injection pump performance. The value for the induction period of the distilled product was very low. The induction period values for the undistilled samples decreased very rapidly during storage, especially with exposure to light and air.",
        "final_summary": "Diesel fuel storage stability varies significantly. Lepera (1973) found that while some diesel fuels maintained satisfactory storage stability over two years, others did not. Bento (2001) highlighted microbial contamination in stored diesel fuel and the effectiveness of biocides in maintaining cleanliness. Berrios (2012) emphasized the role of temperature in methyl ester degradation, noting that blends with commercial diesel showed better long-term stability. Mu\u017e\u00edkov\u00e1 (2022) discussed the negative impact of FAME on diesel fuel oxidation stability and suggested a new shorter storage test based on the standard test according to ASTM D4625. Bezergianni (2012) found that white diesel, a paraffinic diesel obtained from waste cooking oil, remained stable over a year. Bondioli (2004) proposed a method for predicting biodiesel storage stability. Thompson (2013) noted higher oxidation levels in biodiesel blends with higher biodiesel content. Lastly, Mittelbach (2001) found that biodiesel made from rapeseed and used frying oil did not reach the specified limits for viscosity and neutralization numbers even after 150 days of storage."
    },
    {
        "query": "Critical discussion of the influence of technological, environmental, social and economic determinates on business model transformation (BMT) towards sustainable business model.",
        "paper_list_string": "Arifiani 2021:\n\nTitle: Environmental Turbulence Capability Sparks Firm Performance by Embracing Business Model Transformation: An Empirical Study on the Internet Industry in Indonesia\n\nAbstract: This study assesses solutions to several questions regarding the effect of Environmental Turbulence on Firm Performance directly and indirectly through Business Model Transformation on Internet Service Provider Companies in Indonesia. The research approach used is an explanatory survey, verification research with a quantitative approach by measuring a sample of variables built on the population's constructs representing 239 companies. The author finds that novelty B.M. transformation design is significantly related to what factors drive the business transformation. Although, this study also finds that BMT positively mediates the relationship between environmental turbulence to influence performance. Environmental turbulence alone is no longer adequate, and organizations need to revamp the B.M. to achieve firm performance. This study also contributed to the movement of research trends targeted at B.M. Transformation. This study provides an appropriate riposte for quantitative empirical studies that elucidate the correlation between BMT on company performance by simultaneously embracing and managing environmental turbulence. The relationship between BMT and the factors that support it will be higher comprehend when contextualized. The author locates different designs and impacts of BMT on business transformation results that culminate in firm business performance since it faces an uncertain business environment that grows dynamically. Although this novelty is important, the authors found no support for effectiveness. However, this study highlights the robustness of the important factors that drive BMT to work effectively to improve business performance.\n\n==\n\nKutscha 2016:\n\nTitle: What drives Business Model Transformation in small and medium sized enterprises? : empirically assessing the roles of business environment and strategic agility\n\nAbstract: In recent years, firms are experiencing challenging shifts in their business environments. Technological developments and changing patterns of customer demand severely shorten the live spans of the prevailing business models. In order to survive and expand in such turbulent environments, firms must constantly question their established routines and processes. However, small and medium sized enterprises (SMEs) are alarmingly unaware of the business model concept and dedicate little attention to business model innovations (BMIs). As a starting point to overcome this challenge, it is crucially important to accurately identify both, the environmental factors that necessitate business model changes, and the internal firm capabilities to conduct them. \n \nBy referring to the Dynamic Capabilities View (DCV) literature as well as to the largely case study driven literature stream on business model innovations, the study in hand recognizes seven potential influence factors for business model transformations in incumbent SMEs. To empirically verify these factors, an internet-mediated, quantitative survey among executive managers of 89 German SMEs (net response rate: 10.80\\%) was conducted. Subsequently, the resulting data was analyzed using a Partial Least Squares Regression (PLS-R) approach to handle multicollinearity related problems. \n \nThe results indicate that four factors, namely, Strategic Sensitivity, Resource Fluidity, Market Turbulence and Technological Turbulence, are significantly related to the degree of Business Model Innovativeness for at least one size class of firms (differentiating micro, small and medium sized firms). Given the lack of quantitative studies in the field, this thesis contributes to the literature by supplying empirical findings that are not limited to certain cases or specific industries. Also, the originality of this work is expressed by the fact that it is the first to empirically conceptualize environmental turbulence in the context of business model innovation.\n\n==\n\nShah 2020:\n\nTitle: Transformation towards Sustainable Business Models in Production\n\nAbstract: A global transformation from Industry 4.0 towards Industry 5.0 will soon take place. Such transformations are intrinsic to human and sustainable value creation. An increasing number of companies, recognising this imminent trend, will need to transform their current classical, solely profit-oriented business models into sustainable business models that also target human, social, and environmental aspects. Various frameworks already exist that support the design of sustainable business models. Practical applications of these frameworks and research on the transformation process in the production domain, which places a special focus on small and medium enterprises, is still scarce. This paper presents the first results from a sustainable business model transformation project on a European SME, which produces 3D printers. The research method applies a single case study design. The study was based on publicly available information and qualitative data, obtained through interviews and workshops carried out on an executive management level.\n\n==\n\nM\u00fcller 2014:\n\nTitle: Business transformation towards sustainability\n\nAbstract: Sustainability is becoming increasingly important in today\u2019s corporate world and can contribute to the current and future success of organizations. Integrating ecological, social, and economic objectives into corporate decisions is a key success factor for transformation towards sustainability. As sustainability is not achieved by single actions, but rather is an on-going process, decision-makers must have means to analyze the current state of an organization. For this, we first illustrate how companies can structure the field of action for the transformation towards sustainability. Furthermore, we propose a decision model to determine how sustainability actions should be implemented in accordance with the paradigm of value-based management, i.e., considering their economic effects. We illustrate the application of the approach using the example of a German medium-sized company.Executive summaryRecently, organizations have recognized sustainability as an emerging mega-trend and as an increasingly important strategic goal. Its integration into the business model can be a key success factor, but also a challenge that requires a systematic approach. In order to comprehensively steer corporate sustainability, with the aim of minimizing negative externalities while maximizing positive effects, companies first need to structure their processes to achieve transparency on where sustainability actions can be incorporated. By furthermore considering the three dimensions of sustainability, possible starting points for sustainability actions can be identified. These two perspectives are complemented by adapting the basic idea of stages of development and maturity to sustainability context, as a way to capture the progress of sustainability actions within each corporate activity. The resulting \u201cSustainability Maturity Cube\u201d serves as a blueprint, i.e., a first generic approach, of how an organization can structure the field of action for the transformation towards sustainability. Considering the paradigm of value-based management in business context, economic effects of the transformation towards sustainability have to be regarded. We therefore, also propose a decision model, which allows aligning ecological, social and economic objectives in order to draw economically useful conclusions by determining the optimal increase of the sustainability maturity level. To evaluate whether our approach proves useful for subject matter experts who are involved in sustainability decisions, we provide a first example of how a specific company can transform towards sustainability.\n\n==\n\nGorissen 2016:\n\nTitle: Transition Thinking and Business Model Innovation\u2013Towards a Transformative Business Model and New Role for the Reuse Centers of Limburg, Belgium\n\nAbstract: The current dynamics of change, including climate change, resource depletion, energy transition, internet of things, etc. will have substantial impacts on the functioning of contemporary business models. Most firms and companies, however, still largely focus on efficiency strategies leading to sub-optimal solutions (reducing bad impact), rather than radically changing their business model to develop new transaction models more appropriate for today\u2019s world (doing better things). However, persistent sustainability issues arising from our changing societal systems, require more radical and structural changes both within and between organizations to change the way we do business. There is limited empirically established research literature on how businesses can be more proactive in this regard and how they can move beyond \u201cmanagement of unsustainability\u201d. In this paper, we present a transformative business model innovation approach inspired by transition theory combining elements of systems thinking, value-inspired envisioning, reflexive design and co-creation that was pioneered for a consortia of reuse centers in the province of Limburg, Belgium. We demonstrate that this approach contributed to the development of new business model concepts, was conducive for mobilizing support and resources to ensure follow up activity and holds potential to promote a more proactive role for businesses as agents of transformative change.\n\n==\n\nSchneider 2018:\n\nTitle: Challenges of transforming a Business Model to a Sustainable Business Model - A case study based on IKEA and Tetra Pak\n\nAbstract: Climate change, poverty, inequality and the increase of strict regulations - these are just a few \nof the factors contributing to the rise of sustainability in today\u2019s corporate environment. As \nsociety and the worldwide population are affected by these omnipresent changes, it becomes \ninevitable for companies to transform their business models in order to respond to demands \nconcerning sustainability. However, this process implies challenges that have so far not been \nfurther examined by literature - neither have the actions that companies need to implement to \novercome them. Thus, this research aims at providing deeper insights into the emerging field of \nSustainable Business Models. In particular, the following thesis envisions to contribute in terms \nof gaining understandings about the challenges of this process and to provide insights about \nhow companies can overcome them. In order to reach this aim, a qualitative approach was \nselected and a multiple case study based on Tetra Pak and IKEA was conducted. To provide a \ntheoretical background about the most relevant topics, a traditional literature review was \nconducted. Subsequently, these were confronted with the results collected from interviews and \nsecondary data. Our findings suggest that challenges may arise in every element of the SBM \nwhile others may affect the whole system. In addition, actions to overcome these challenges \nrequire profound consideration regarding different factors such as the type of business and the \nownership structure of the company.\n\n==\n\nKrmela 2020:\n\nTitle: Business Model Adaptation Through Inclusion of Environmental Sustainability Measures\n\nAbstract: Purpose of the article: The article aims at understanding and description of how and why the business model (BM) of an incumbent manufacturing B2B company adapts, collaboratively with a dominant customer, through measures leading to an improved environmental sustainability performance. The research question was: How is a business model of an incumbent, businessto-business company adapted through the inclusion of environmental sustainability measures, with the customer\u2019s involvement? Methodology/Methods: A qualitative research approach, drawing on a descriptive-explanatory, single case study conducted on a Czech manufacturing company active in the field of steel processing was performed. The study draws on the analysis of the focal company\u2019s internal and external documents related to sustainability, as well as on semi-structured interviews with the examined company\u2019s executives. The analysis also include the externally available sustainability related documents of a dominant customer. Scientific Aim: Understanding the impact of the environment-related sustainability measures on the adaptation of the extant BM of the focal company, and the role a dominant customer plays in the process. Findings: The inclusion of environment-related sustainability measures leads to an adaptation of the process and the product, as well as of the focal company\u2019s BM and most of its key elements. A value capture by the focal company has been identified as the mainly affected element of a BM through both cost savings and a more business gained. The element value proposition adapts towards a \u201cgreener company\u201d. The core logic of the BM remains unchanged. Conclusions: B2B incumbent companies implement environmental sustainability measures into their BMs, aiming to contribute to the preservation of the environment. The economic side continues to be carefully considered. It can even be a trigger of implementation. The large customers get strongly involved in the process, contributing to the adaptation of all four key elements of the BM. The BM\u2019s adaptation happens dynamically, in incremental steps.\n\n==\n\nStubbs 2008:\n\nTitle: Conceptualizing a \u201cSustainability Business Model\u201d\n\nAbstract: According to one perspective, organizations will only be sustainable if the dominant neoclassical model of the firm is transformed, rather than supplemented, by social and environmental priorities. This article seeks to develop a \u201csustainability business model\u201d (SBM)\u2014a model where sustainability concepts shape the driving force of the firm and its decision making. The SBM is drawn from two case studies of organizations considered to be leaders in operationalizing sustainability and is informed by the ecological modernization perspective of sustainability. The analysis reveals that organizations adopting a SBM must develop internal structural and cultural capabilities to achieve firm-level sustainability and collaborate with key stakeholders to achieve sustainability for the system that an organization is part of.",
        "final_summary": "Technological, environmental, social, and economic determinants significantly influence business model transformation (BMT) towards sustainable business models. Arifiani (2021) demonstrates that environmental turbulence necessitates BMT, which in turn positively impacts firm performance in Indonesia's internet industry. Kutscha (2016) identifies strategic sensitivity, resource fluidity, market turbulence, and technological turbulence as factors related to the degree of Business Model Innovativeness across various firm sizes, including micro, small, and medium-sized enterprises. Shah (2020) underscores the importance of practical applications of sustainable business model transformation frameworks, particularly for SMEs in the production domain. M\u00fcller (2014) advocates for the integration of ecological, social, and economic objectives in corporate decisions as a crucial factor for sustainability transformation. Gorissen (2016) proposes transformative business model innovation to address sustainability issues and foster proactive change. Schneider (2018) explores the challenges of transforming business models towards sustainability, using IKEA and Tetra Pak as case studies. Krmela (2020) discusses how a manufacturing B2B company adapts its business model through environmental sustainability measures, with customer involvement. Lastly, Stubbs (2008) introduces a \"sustainability business model\" where social and environmental priorities shape the firm's driving force."
    },
    {
        "query": "How is  personalization becoming more ubiquitous?",
        "paper_list_string": "Madeira 2014:\n\nTitle: Building a platform for pervasive personalization in a ubiquitous computing world\n\nAbstract: We propose a platform with the goal of helping developers when they need to apply personalization to ubiquitous computing applications, ranging from simple mobile apps to rich multimodal systems. The platform's core is a general personalization model, which provides the orientation and required tools to simplify and accelerate the creation of personalized applications. It also includes a context-aware clustering module fed by user data and context data in order to obtain users profiles for each personalization.\n\n==\n\nLewis 2004:\n\nTitle: From customization to ubiquitous personalization: digital identity and ambient network intelligence\n\nAbstract: proliferation of new electronic technologies: devices with heterogeneous interfaces and advanced services available at home, in the car, in the school, in public spaces. This connected world poses many challenges, opportunities, and risks, the greatest being that its full benefits will not reach most of the intended users. As computers and networks are increasingly integrated with everyday environments, this \u201cambient intelligence\u201d creates new synergies and dependencies between individual users and a multitude of services and applications, which should become accessible through easy-to-use, almost instinctive, human interfaces. This is a new complex and interdependent ecosystem that will make or break the realization of a truly connected society. However, building and instituting this landscape require greater clarity about the interdependencies (conceptually and technologically) that constitute this ecosystem. Along with interface design, one of the key enablers in the foregoing scenario is the question of digital identity. The need to identify individual users and provide them with intelligent personalized assistance based on their needs in specific locations and contexts remains the basic condition for reaching this new landscape. Intelligent personalized assistance (as opposed to customized customer relations management garnered through inferred behavior) must fulfill three conditions. It must:\n\n==\n\nTam 2006:\n\nTitle: Understanding the Impact of Web Personalization on User Information Processing and Decision Outcomes\n\nAbstract: Personalized information technology services have become a ubiquitous phenomenon. Companies worldwide are using the web to provide personalized offerings and unique experiences to their customers. While there is a lot of hype about delivering personalized services over the web, little is known about the effectiveness of web personalization and the link between the IT artifact (the personalization agent) and the effects it exerts on a user's information processing and decision making. To address the impact of personalized content, this article theoretically develops and empirically tests a model of web personalization. The model is grounded on social cognition and consumer research theories adapted to the peculiar features of web personalization. The influence of a personalization agent is mediated by two variables: content relevance and self reference. Hypotheses generated from the model are empirically tested in a laboratory experiment and a field study. The findings indicate that content relevance, self reference, and goal specificity affect the attention, cognitive processes, and decisions of web users in various ways. Also, users are found to be receptive to personalized content and find it useful as a decision aid. Theoretical and practical implications of the findings are discussed.\n\n==\n\nShacham 2007:\n\nTitle: Ubiquitous device personalization and use: The next generation of IP multimedia communications\n\nAbstract: Service usage in emerging ubiquitous environments includes seamless and personalized usage of public and private devices discovered in the vicinity of a user. In our work, we describe an architecture for device discovery, device configuration, and the transfer of active sessions between devices. The presented architecture uses the Session Initiation Protocol (SIP) as a standardized, widely used signaling protocol for IP-based multimedia services. Our solution includes support of simple existing devices, split of sessions between devices, user-control of location-based behavior, and handling of security and privacy concerns. We present the implementation and show the feasibility of our work with analytical evaluation and measurements.\n\n==\n\nMouri 2016:\n\nTitle: Context-aware and Personalization Method based on Ubiquitous Learning Analytics\n\nAbstract: In the past decades, ubiquitous learning (u-learning) has been the focus of attention in educational research across the world. Majority of u-learning systems have been constructed using ubiquitous technologies such as RFID tags and cards, wireless communication, mobile phones, and wearable computers. There is also a growing recognition that it can be improved by utilizing ubiquitous learning logs collected by the u-learning system to enhance and increase the interactions among a learner, contexts, and context-based knowledge. One of the issues of analytics based on u-learning is how to detect or mine learning logs collected by u-learning systems. Moreover, it is necessary to evaluate whether the recommendations detected by analysis are appropriate in terms of learning levels, contexts and learners\u2019 preference. To tackle the issues, we developed a system that could recommend useful learning logs at the right place in the right time in accordance with personalization of learners. An experiment was conducted to evaluate the system\u2019s performance and the recommendations\u2019 usefulness for learning. In the evaluation experiment, we found important criteria for recommending in the real-world language learning. In addition, the participants were able to increase their learning opportunities by our recommendation method.\n\n==\n\nPark 2009:\n\nTitle: Personalized Service Discovery in Ubiquitous Computing Environments\n\nAbstract: In ubiquitous computing environments, users want to discover the most appropriate service to support their tasks. Because the most appropriate service depends on user preferences and context, service discovery protocols should personalize results. A service discovery framework based on the virtual personal space (VPS)-that is, a virtual administrative domain of services managed for the user-aims to provide this personalization. In this framework, personal operating middleware embedded in a personal device manages a set of contextually close services in the user's VPS. An inference module supports this management. Laboratory evaluations show that the VPS framework helps users find high-quality, appropriate services.\n\n==\n\nMahony 2012:\n\nTitle: User-Centric Personalization and Autonomous Reconfiguration Across Ubiquitous Computing Environments\n\nAbstract: In the era of Ubiquitous Computing (UbiComp), during our typical daily routines we may encounter multiple, shared and heterogeneous UbiComp environments across various locations. As these environments are meant to be shared between multiple users, interaction control methods (gestural, touch, voice commands, etc.) and the contexts (light, temperature, sound, informational services, etc.) of the environment are not personalized for individual users naturally. Typically, users are required to manually configure interaction preferences and conditions each time they encounter UbiComp systems. This however, refers to a tedious and redundant reconfiguration procedure, which is against the concept of UbiComp. In this paper, we present our work targeting on improving the personalization and reconfiguration procedure. Firstly, a usercentric personalization approach is proposed for facilitating users in determining how an UbiComp environment should adapt to their own preferred configurations. Then, an autonomous reconfiguration procedure is proposed, ensuring that a user\u2019s preferences are maintained and accessible across multiple ubiquitous computing environments seamlessly. Keywords-Ubiquitous Computing; personalization; reconfiguration; human computer interaction.\n\n==\n\nPotonni\u00e9e 2002:\n\nTitle: Ubiquitous Personalization : a Smart Card Based Approach\n\nAbstract: This paper presents a innovative approach to build an application personalization framework. The use of a smart card to host the personal profile engine answers both privacy and ubiquity requirements. The proposed framework includes a generic part, applicable for many applications in many contexts (e.g., browsing the web on PC, shopping on mobile phone, or buying movies on TV), and a set domain specific services. For illustration purpose, we present how this framework can be implemented in the context of interactive television.",
        "final_summary": "The papers suggest that personalization is becoming more ubiquitous in various domains. Madeira 2014 proposes a platform for pervasive personalization in ubiquitous computing applications, aiming to simplify and accelerate the creation of personalized applications. Lewis 2004 discusses the importance of digital identity in achieving ubiquitous personalization and ambient network intelligence. Tam 2006 explores the impact of web personalization on user information processing and decision outcomes, highlighting the effectiveness of personalized content as a decision aid. Shacham 2007 presents an architecture for ubiquitous device personalization and use, focusing on seamless and personalized usage of public and private devices in emerging ubiquitous environments. Mouri 2016 focuses on context-aware and personalized recommendations in ubiquitous learning, utilizing learning logs to enhance interactions between learners, contexts, and context-based knowledge. Park 2009 proposes a personalized service discovery framework in ubiquitous computing environments, aiming to provide personalized results based on user preferences and context. Mahony 2012 addresses user-centric personalization and autonomous reconfiguration in ubiquitous computing environments, aiming to improve the personalization and reconfiguration procedure for individual users. Potonni\u00e9e 2002 presents an innovative approach to ubiquitous personalization using smart cards, offering a generic framework applicable to various applications and contexts. Overall, these papers highlight the increasing presence and importance of personalization in ubiquitous computing, web services, learning environments, and other domains."
    },
    {
        "query": "factors hindering information sharing in organisations",
        "paper_list_string": "Barua 2007:\n\nTitle: Enabling information sharing within organizations\n\nAbstract: Organizations which have invested heavily in Enterprise Resource Planning (ERP) systems, intranets and Enterprise Information Portals (EIP) with standardized workflows, data definitions and a common data repository, have provided the technlogical capability to their workgroups to share information at the enterprise level. However, the responsibility of populating the repository with relevant and high quality data required for customized data analyses is spread across workgroups associated with specific business processes. In an information interdependent setting, factors such as short-term organizational focus and the lack of uniformity in information management skills across workgroups can act as impediments to information sharing. Using an analytical model of information exchange between two workgroups, we study the impact of measures (e.g., creating a perception of continuity and persistence in interactions, benefit sharing, etc.) on the performance of the workgroups and the organization. The model considers a setting we describe as information complementarity, where the payoff to a workgroup depends not only on the quality of its own information, but also on that of the information provided by other workgroups. We show how a long-term vision combined with homogeneity in information management capabilities across workgroups can lead to organizationally desirable levels of information exchange, and how benefit sharing can either help or hurt individual and organizational information exchange outcomes under different circumstances. Our analysis highlights the need for appropriate organizational enablers to realize the benefits of enterprise systems and related applications.\n\n==\n\nAviram 2010:\n\nTitle: Overcoming Impediments to Information Sharing\n\nAbstract: When deciding whether to share information, firms consider their private welfare. Discrepancies between social and private welfare may lead firms excessively to share information to anti-competitive ends - in facilitating of cartels and other harmful horizontal practices - a problem both antitrust scholarship and case law have paid much attention to. On the other hand, legal scholars have paid far less attention to the opposite type of inefficiency in information sharing among competitors - namely, the problem of sub-optimal information sharing. This phenomenon can generate significant social costs and is of special importance in network industries because the maintenance of compatibility, a key to producing positive network effects, typically requires information sharing. Understanding the hitherto neglected impact of sub-optimal information sharing is important not only for many areas of antitrust law, but also for developing effective policies towards network industries and critical infrastructures more generally, as well as for improving those procedural rules that concern information exchange among litigating parties. This paper therefore advances the legal analysis of impediments to efficient information sharing in a number of significant ways: First, it shows that the strategic behavior of competitors may erect an economic barrier to information sharing that has not been previously addressed in the literature - the fear of degradation. This form of strategic behavior involves the strategic refusal to share information when the refusal inflicts a greater harm on one's rivals than on oneself, and thus generates a competitive advantage. Second, the paper reveals a hitherto unrecognized set of behavioral impediments to information sharing, wherein rivalry norms and managers' risk attitudes bias competitors' judgments of the prospects of information sharing and the status-quo bias and ambiguity aversion lead these decision makers to avoid such arrangements. Third, it integrates these economic and behavioral insights with the findings of the extant literature to create a new framework for predicting when private information sharing will be suboptimal. Finally, we suggest how the alignment of private information sharing with social optimality may be promoted, based on the framework developed here.\n\n==\n\nLage 2012:\n\nTitle: Factors Influencing Information Sharing in Four SME Networks in Portugal - A Coordination Perspective\n\nAbstract: Sharing a minimum amount of information is a basic condition for working in cooperation. The aim of this study was to identify aspects that influence information sharing within inter-organizational networks through an exploratory and qualitative approach. The coordination\u2019s representatives of four networks were interviewed: the Majority-Partner of a Purchase Network (RAVT), the Technical Consultant of an Innovation Network (COTEC), the General Secretary of a cluster representation network (CEFAMOL) and the President of a Network of Associations (UACS). Aspects related to the information sharing were identified, such as information quality, top management commitment, trust, network coordination\u2019s role, reciprocity and age. Further researches can deepen these results by comparing them with network companies\u2019 view.\n\n==\n\nAlsalamah 2010:\n\nTitle: Towards Information Sharing in Virtual Organisations: The Development of an Icon-based Information Control Model\n\nAbstract: Today, innovation in information communication technology has encouraged contribution among different fields to tackle large-scale scientific problems or introduce novel inventories that, in both cases, demand extensive sharing of information among collaborating organisations in order to achieve the overall goal. Sharing information across different physical organisations, working as a single virtual organisation, raises a number of information security issues that limit the effectiveness, dynamism, and potential of collaborative working. Although extensive research has been conducted to provide secure information-sharing solutions within a single organisation, little research has investigated multi- organizational information-sharing environments where information requires to be protected but there are variations in information security needs and, in some cases, conflicts in applied information security controls. A key obstacle, the majority of research conducted in this area has overlooked, is not only the ability to govern remote access of users from one organisation to sensitive information stored in another organisation, but also having persistent control over owned information even after access has been granted and the information is either disseminated electronically, transformed into paper format, or even shared verbally. In addition, research was tailored to meet only specific research needs and address particular issues. Therefore, there is a lack of comprehensive, systematic approaches for controls on information usage shared electronically, regardless of specific circumstances. This paper aims to present a novel information control model that could keep information self-protected in dynamic collaborative environments by communicating information security needs along with the exchanged information using an Information Labelling Scheme. Based on SPIDER solution and Protective Commons, this scheme uses nine labelling icons (reflecting the protection type and level) associated with different information security controls (representing the information security mechanisms used to provide the protection). The model is demonstrated in the Microsoft Word 2007 application and a prototype has been developed as a plug-in software named Information Labelling Palette. It displays the nine self-explanatory icons in order for an information owner/user to label any information range within a single document using any icon. This consequently enforces the information security controls associated with the selected icon only into that particular range of electronic information, and secondly, communicates the information security needs to the recipient in a human-readable format, which would help keep recipients informed about how this information should be managed if printed out or shared verbally. Finally, the wide range of information security controls used in this proposed solution makes it widely applicable to meet the considerable diversity of organisations\u2019 information security needs. Furthermore, it is believed to lay a solid foundation for future work in the area of information access control and control policy enforcement in collaborative environments.\n\n==\n\nBata 2020:\n\nTitle: Information Sharing Behaviour of Complex and Extended Organisations\n\nAbstract: The information sharing behaviours observed in extended organisations involve the use of dedicated specialised teams and groups. This study presents theoretical evidence, drawing on Activity Theory as an analytical lens, linking specialised teams in such complex and extended organisations to conditions which may lead to information sharing failures. These failures are apparent through tensions and contradictions examined in the activity system which helped in identifying failures in the formal systems and emergent behaviours to (or \u201cintending to\u201d) resolve them. Results from this study show that information sharing behaviours in an extended organisation are driven in part at least by the team\u2019s need and ability to share information with members of the team (formed to) dealing with an information sharing failure resulting in an organisational problem and aimed at achieving a common shared goal in the relationship. There is an established link showing that information sharing in the setting supports and enhances the exchange of ideas and information between partners due to the complementing needs of services thus helping in reducing extension problems and, thereby, providing a coping strategy for the central organisation. Amongst the importance of information sharing in the setting is the ability to forecast uncertainties as a result of better understanding of each partner\u2019s capabilities and strength. Complexity was observed to be a factor that triggers information sharing failure in extended relationships and dependencies. This occurs when there is no congruence between information deliveries amongst all the collaborating partners.\n\n==\n\nJahanyan 2011:\n\nTitle: Factors influencing on knowledge-sharing behaviour in organisations\n\nAbstract: In this paper, using structural equation modelling, significance of relationships between the affecting factors and knowledge-sharing behaviour was examined. To test the proposed model of research, an Iranian leading company in the field of ICT was selected. Using the structural model as an output of data analysis, the significant influentive constructs of model were identified. According to the results of data analysis, it was clear that Managerial Factors (MFs) and Personality Traits (PTs) have significant influence on knowledge-sharing behaviour in the organisation.\n\n==\n\nYang 2011:\n\nTitle: Information-sharing in public organizations: A literature review of interpersonal, intra-organizational and inter-organizational success factors\n\nAbstract: Abstract Information sharing is considered an important approach to increasing organizational efficiency and performance. With advances in information and communication technology, sharing information across organizations has become more feasible. In the public sector, government agencies are also aware of the importance of information sharing for addressing policy issues such as anti-terrorism and public health. However, information sharing can be a complex task. Identifying factors that influence information sharing is critical. In the literature, research in information sharing focuses on the interpersonal, intra-organizational, and inter-organizational levels. This paper reviews the current information-sharing research, discusses the factors affecting information sharing at the three levels, and provides summative frameworks. These frameworks provide a means to discover future research opportunities, and a systematic way for practitioners to identify key factors involved in successful information sharing.\n\n==\n\nHart 2006:\n\nTitle: Comparing Cultural and Political Perspectives of Data, Information, and Knowledge Sharing in Organisations\n\nAbstract: This paper raises issues concerning data, information, and knowledge sharing in organisations and, in particular, compares an organisational cultural analysis of why such sharing is often difficult to achieve with an organisational political one. The issues raised are often insufficiently attended to by practitioners who are attempting to build technological information and knowledge management systems. The driver for the paper is that despite impressive advances in technology and its now almost ubiquitous presence in organisations, as well as academic study over several decades, many of the benefits originally expected concerning improved data, information, and knowledge sharing have not materialised as expected. Basic reasons for this lie in the lack of attention to the cultural foundations of organisations and because matters relating to organisational power and political matters are often misunderstood, overlooked, or ignored. These different perspectives are discussed and contrasted in order to tease out the important differences between them and assess the prospects for a synthesis. It is concluded that while there are important commonalities between the two perspectives there are also fundamental differences, notably regarding what are causes and what are effects and, therefore, how to go about effecting change regarding data, information, and knowledge sharing.",
        "final_summary": "The papers suggest several factors that hinder information sharing in organizations. Barua 2007 highlights short-term organizational focus and the lack of uniformity in information management skills as impediments to information sharing. Aviram 2010 discusses the fear of degradation and behavioral biases such as status quo bias and ambiguity aversion as barriers to sharing information among competitors. Lage 2012 identifies aspects such as information quality, top management commitment, trust, network coordination's role, reciprocity, and age as factors influencing information sharing within inter-organizational networks. Alsalamah 2010 focuses on the challenges of information sharing in virtual organizations and the need for effective information security controls. Bata 2020 emphasizes the importance of specialized teams and the need for information sharing to resolve failures in extended organizations. Jahanyan 2011 highlights managerial factors and personality traits as influential factors in knowledge-sharing behavior. Yang 2011 reviews factors affecting information sharing at interpersonal, intra-organizational, and inter-organizational levels. Hart 2006 compares cultural and political perspectives, emphasizing the importance of organizational culture and power dynamics in data, information, and knowledge sharing."
    },
    {
        "query": "Time-use homogeneity",
        "paper_list_string": "Gonz\u00e1lez-Chapela 2006:\n\nTitle: On measuring heterogeneity in the use of time\n\nAbstract: This paper discusses methods that permit evaluating the degree of heterogeneity in the use of time. In particular, it is argue that scalar measures of multivariate scatter constitute a general means to assess the degree of withingroup time-use heterogeneity. The techniques are then illustrated by inquiring into the heterogeneity in the allocation of time of retired and employed older adults.\n\n==\n\nMerz 2008:\n\nTitle: Representative time use data and new harmonised calibration of the American Heritage Time Use Data (AHTUD) 1965-1999\n\nAbstract: Representative and reliable individual time use data, in connection with a proper set of socio-economic back-ground variables, are essential elements for the empirical foundation and evaluation of existing and new theories in general and in particular for time use analyses. Within the international project Assessing Time Use Survey Datasets several potentially useful individual US time use heritage datasets have been identified for use in de-veloping an historical series of non-market accounts. In order to evaluate the series of American Heritage Time Use Data (AHTUD) (1965, 1975, 1985, 1992-94, 1998-99) this paper analyses the representativeness of this data when using given weights and provides a new harmonised calibration of the AHTUD for sound time use analyses. Our calibration procedure with its ADJUST program package is theoretically founded on information theory, consistent with a simultaneous weighting including hierarchical data, ensures desired positive weights, and is well-suited and available for any time use data calibration of interest. We present the calibration approach and provide new harmonised weights for all AHTUD surveys based on a substantially driven calibration frame-work. To illustrate the various application possibilities of a calibration, we finally disentangle demographic vs. time use behavioural changes and developments by re-calibrating all five AHTUD surveys using 1965 popula-tion totals as a benchmark.\n\n==\n\nVagni 2020:\n\nTitle: The social stratification of time use patterns.\n\nAbstract: Time use is both a cause of social inequality and a consequence of social inequality. However, how social class stratifies time use patterns is seldom studied. In this paper, I describe the time use patterns in the years 1983 and 2015 by social class, and\u00a0gender in the British context. Using sequence analysis methods, I show how the diversity of time use patterns in British society is socially stratified. I find that 13 clusters capture the heterogeneity of time use patterns and that these clusters are associated with social class, gender, and day of the week. These clusters capture patterns of paid and unpaid work schedules, as well as leisure patterns. The results show that men have experienced a reduction of the standard Monday to Friday 8-hr working day, while women have experienced a general increase in this type of schedule. On the other hand, patterns of domestic working days have reduced for women and increased for men. Important differences exist in paid and unpaid work patterns between social classes. Working-class women have experienced an important increase in shift work on weekends. They are also much more likely to be doing unpaid work on weekdays compared to upper-class and middle-class women. Working-class men are more likely to experience non-working days and leisure days on both weekdays and weekends and are more likely to be doing shift work. They are also more often doing unpaid work on weekdays compared to men in upper-class households. Patterns of childcare indicate that all families have increased their childcare time. Men in upper-class households in particular have experienced an important growth in childcare time between 1983 and 2015. I conclude by discussing how time use can further our understanding of social stratification.\n\n==\n\nSayer 2016:\n\nTitle: Trends in Women\u2019s and Men\u2019s Time Use, 1965\u20132012: Back to the Future?\n\nAbstract: Women\u2019s and men\u2019s time use is more similar today than it was in the 1960s, when specialization in adult roles was at its peak, but convergence remains stubbornly out of sight. This chapter updates earlier trend studies of time use and finds that recent data confirm the most consistent findings from earlier analyses. The greater similarity of women\u2019s and men\u2019s time use today is due much more to changes among women than among men. Further, despite declines in women\u2019s housework time, the increase in women\u2019s childcare time and paid work time has resulted in a gender gap in leisure time. New findings from this analysis reveal the gender gap in leisure is accounted for by men\u2019s higher levels of television time.\n\n==\n\nFisher 2007:\n\nTitle: Gender Convergence in the American Heritage Time use Study (AHTUS)\n\nAbstract: We present evidence from a new comprehensive database of harmonized national time-diary data that standardizes information on almost 40\u00a0years of daily life in America. The advantages of the diary method over other ways of calculating how time is spent are reviewed, along with its ability to generate more reliable and accurate measures of productive activity than respondent estimates or other alternatives. We then discuss the various procedures used to develop these harmonized data, both to standardize reporting detail and to match with Census Bureau population characteristics. We then use these data to document historical shifts in Americans\u2019 use of time, particularly focusing on gendered change in paid and unpaid work. We explore these data to find new and more complex evidence of continuing gender convergence, not just in aggregated totals of hours worked, but also in (1) the distributions of activity through the day and the week, (2) the sorts of activities that marital partners do together, as well as (3) the processes of construction of the diary accounts themselves.\n\n==\n\nKamakura 2009:\n\nTitle: American Time-Styles: A Finite-Mixture Allocation Model for Time-Use Analysis\n\nAbstract: Time-use has already been the subject of numerous studies across multiple disciplines such as economics, marketing, sociology, transportation and urban planning. However, most of this research has focused on comparing demographic groups on a few broadly defined activities (e.g., work for pay, leisure, housework, etc.). In this study we take a holistic perspective, identifying a typology of latent \u201ctime-styles,\u201d that defines the different ways people allocate the 24 hr in a day across multiple competing daily activities. We propose a finite-mixture time-allocation model that accounts for differences in life priorities across individuals, taking into consideration the fact that we all have the same \u201cbudget\u201d of 24 hr to spend every day and that this allocation leads to highly sparse, truncated data. This model is then applied to time-use data from the American Time Use Survey collected by the U.S. Bureau of Labor Statistics in 2006.\n\n==\n\nGauthier 2000:\n\nTitle: Patterns of time use of people age 55 to 64 years old: Some cross-national comparisons\n\nAbstract: This paper examines the patterns of time use of adults age 55 to 64 years old in six countries: Austria, Canada, Finland, Italy, Sweden, and the United States. It examines the discontinuity in daily activities by employment status and gender.The paper uses nationally representative samples from time use surveys carried out in each country. We compute aggregate patterns of time use by employment status and gender for seven categories of activities: personal activities, paid work, unpaid work, housework, social leisure, active leisure, and passive leisure. We also compute dissimilarity indices to measure the degree of discontinuity in patterns of time use by employment status and gender.We find that the pattern of time use of non-employed adults resemble that of full-time employed people on their non-workdays. We also find evidence that the transition out of the labor force is associated with a convergence in pattern of time use of men and women in the USA, Canada, and Finland, but not in other countries.There appears to be continuities in the way people use their time as they grow older and retire from the labor force. We however raise the possibility that these results may hold only for the `young-old'. Decreasing health and physical endurance at older ages may introduce significant discontinuities in patterns of time at a later stage of the life-cycle. Our future work will examine the impact of health and daily limitation on patterns of time use at older ages.\n\n==\n\nGimenez-Nadal 2012:\n\nTitle: Trends in time allocation: A cross-country analysis\n\nAbstract: Using detailed time-use data for seven industrialized countries from the 1970s until today we document general decreases in men's market work coupled with increases in men's unpaid work and child care, and increases in women's paid work and child care coupled with decreases in unpaid work. We also find almost universal increases in the time devoted to watching television over this period, and uncover a widespread increase in leisure inequality in favor of lower educated adults. Trends in leisure inequality mirror the general increase in income and earnings inequality experienced in most countries over this period, especially after the mid-1980s. All these findings are consistent with previous results for the US. However, in contrast to the increases in leisure found for the US, we fail to find common trends in leisure time across countries over the period analyzed.",
        "final_summary": "The papers collectively provide insights into the research question of time-use homogeneity. Gonz\u00e1lez-Chapela 2006 discusses methods to measure heterogeneity in time use, focusing on retired and employed older adults. Merz 2008 analyzes the representativeness of American Heritage Time Use Data (AHTUD) and provides a new harmonized calibration for time use analyses. Vagni 2020 examines the social stratification of time use patterns, finding that time use is stratified by social class, gender, and day of the week. Sayer 2016 shows that while women's and men's time use has become more similar over time, there is still a gender gap in leisure time due to differences in childcare and paid work. Fisher 2007 presents evidence of gender convergence in time use, particularly in paid and unpaid work, and explores the construction of time diary accounts. Kamakura 2009 proposes a finite-mixture time-allocation model to identify different time styles and applies it to American Time Use Survey data. Gauthier 2000 compares patterns of time use among adults aged 55 to 64 in six countries, highlighting the discontinuity in daily activities by employment status and gender. Gimenez-Nadal 2012 analyzes trends in time allocation across seven industrialized countries, finding changes in market work, unpaid work, child care, and leisure, as well as an increase in leisure inequality."
    },
    {
        "query": "What is the causal effect of judicial decisions on political attitudes?",
        "paper_list_string": "Carrubba 2008:\n\nTitle: Judicial Behavior under Political Constraints: Evidence from the European Court of Justice\n\nAbstract: The actual impact of judicial decisions often depends on the behavior of executive and legislative bodies that implement the rulings. Consequently, when a court hears a case involving the interests of those controlling the executive and legislative institutions, those interests can threaten to obstruct the court's intended outcome. In this paper, we evaluate whether and to what extent such constraints shape judicial rulings. Specifically, we examine how threats of noncompliance and legislative override influence decisions by the European Court of Justice (ECJ). Based on a statistical analysis of a novel dataset of ECJ rulings, we find that the preferences of member-state governments\u2014whose interests are central to threats of noncompliance and override\u2014have a systematic and substantively important impact on ECJ decisions.\n\n==\n\nZink 2009:\n\nTitle: Courting the Public: The Influence of Decision Attributes on Individuals\u2019 Views of Court Opinions\n\nAbstract: Scholars and judges commonly maintain that courts require institutional legitimacy in order to be an effective institution of government and that such legitimacy depends on judges making legally principled, neutral decisions. Two principal ways judges can signal the neutrality of decisions are the size of the majority coalition and the treatment of precedent: opinions with larger majorities or grounded in precedent project to the public that they were decided in accordance with the rule of law and thus based on impartial decision-making criteria. We use an experimental design to test whether these two decision attributes influence attitudes toward decisions, presenting individuals with mock newspaper articles reporting on Court decisions in which we systematically vary majority coalition size and treatment of precedent. Our data show that when the Court produces a unanimous (rather than divided) decisional coalition and when it follows (rather than overrules) precedent, individuals are more likely to agree with and accept a decision, even if they are ideologically predisposed to disagree with a given Court outcome.\n\n==\n\nZinkJames 2015:\n\nTitle: Courting the Public: The Influence of Decision Attributes on Individuals\u2019 Views of Court Opinions\n\nAbstract: Scholars and judges commonly maintain that courts require institutional legitimacy in order to be an effective institution of government and that such legitimacy depends on judges making legally principled, neutral decisions. Two principal ways judges can signal the neutrality of decisions are the size of the majority coalition and the treatment of precedent: opinions with larger majorities or grounded in precedent project to the public that they were decided in accordance with the rule of law and thus based on impartial decision-making criteria. We use an experimental design to test whether these two decision attributes influence attitudes toward decisions, presenting individuals with mock newspaper articles reporting on Court decisions in which we systematically vary majority coalition size and treatment of precedent. Our data show that when the Court produces a unanimous (rather than divided) decisional coalition and when it follows (rather than overrules) precedent, individuals are more likely to agre...\n\n==\n\nLink 1995:\n\nTitle: Tracking Public Mood in the Supreme Court: Cross-Time Analyses of Criminal Procedure and Civil Rights Cases\n\nAbstract: Representational models of judicial decision making posit that the Supreme Court is a permeable institution, subject to external pressures-namely elite and mass opinion. Court rulings, therefore, should be demonstrably congruent with the views of the majoritarian electoral/political alliance. Building upon an innovative study by Mishler and Sheehan (1993), the relationship between public and elite attitudes and Supreme Court rulings are examined in two important issue areas: criminal procedure and race- related civil rights. The results indicate that permeability of the Court varies to some degree according to issue area. While public opinion (in the pre- Reagan years) has a direct affect on Supreme Court liberalism in both issue areas, the direct impact of elite opinion is evidenced only in the area of criminal procedure, not in the area of race-related civil rights. The relation ship between public opinion and Court liberalism during the Reagan years, however, is negative as the gap between liberalism in the public \"mood\" and in Court decisions has widened. These nuances in the relationship between the Supreme Court and the broader sociopolitical environment indicate that the Court does follow changes in the dominant political alliance more readily in some issue areas than it does in others.\n\n==\n\nGibson 2010:\n\nTitle: Can Judicial Symbols Produce Persuasion and Acquiescence? Testing a Micro-Level Model of the Effects of Court Legitimacy*\n\nAbstract: How is it that the United State Supreme Court \u2013 perhaps the least majoritarian of all American political institutions \u2013 is capable of getting most citizens to accept rulings with which they disagree? This analysis addresses the role of the symbols of judicial authority and legitimacy \u2013 the robe, the gavel, the cathedrallike court building \u2013 in contributing to this willingness of ordinary citizens to acquiesce to disagreeable court. Based on an experimental design, our analysis shows that the mere exposure to seemingly innocuous symbols of judicial power increases the likelihood of acquiescence. The effect is not strong, but neither are the symbols we use in the experimental treatment. Additional analysis reveals that the consequences of judicial symbols vary with factors such as the intensity of initial policy attitudes, knowledge of the judiciary, and independent measures of institutional support for the Supreme Court. We understand these various empirical findings with the assistance of three bodies of theory: Legitimacy Theory, Positivity Theory, and the Theory of Motivated Political Reasoning. The Theory of Motivated Political Reasoning explains how non-conscious stimuli such as background symbols can affect the reasoning of ordinary citizens, providing the micro-level underpinning for the asymmetrical effects of exposure to the judiciary that is addressed by Positivity Theory. Because symbols influence citizens, and influence them in ways that reinforce the legitimacy of courts, the connection between institutional attitudes and acquiescence posited by Legitimacy Theory is both supported and explained.\n\n==\n\nAshenfelter 1995:\n\nTitle: Politics and the Judiciary: The Influence of Judicial Background on Case Outcomes\n\nAbstract: It is widely believed that the background and worldview of judges influence their decisions. This article uses the fact that judges are assigned their cases randomly to assess the effect of judicial background on the outcome of cases from the day-to-day docket in three federal trial courts. Unlike the political science findings of ideological influence in published opinions, we find little evidence that judges differ in their decisions with respect to the mass of case outcomes. Characteristics of the judges or the political party of the judge's appointing president are not significant predictors of judicial decisions.\n\n==\n\nTiede 2015:\n\nTitle: The political determinants of judicial dissent: evidence from the Chilean Constitutional Tribunal\n\nAbstract: Many judicial scholars argue that judicial dissent stems from partisanship or political differences among judges on courts. These arguments are evaluated using the variation in political backgrounds on a constitutional court, Chile\u2019s Constitutional Tribunal, using case-level and vote-level data from 1990 until 2010. The analysis shows that the rate of dissent rises after major reforms to the powers and judicial selection mechanism of the Tribunal in 2005 and that the dissent rate corresponds to periods of greater partisanship on the court. Further, decisions regarding the unconstitutionality of laws intensify the propensity to dissent at both the case and judge level. In further examination of variation across judges\u2019 voting records, judges who have identifiable partisan associations of any kind are generally more likely to dissent than those with limited political backgrounds.\n\n==\n\nGoldman 1969:\n\nTitle: Backgrounds, Attitudes and the Voting Behavior of Judges: A Comment on Joel Grossman's Social Backgrounds and Judicial Decisions\n\nAbstract: SHELDON GOLDMAN University of Massackusetts T HE DIALOGUE WITHIN the profession concerning judicial behavior research has generally moved from disapproving polemic to sympathetic indeed constructive criticism. A recent valuable contribution to this dialogue was The Journal of Politics article by Joel B. Grossmanl in which he assessed three judicial decisionmaking models which either expressly or implicitly consider social/ political backgrounds insofar as they are at the root of attitudes and values as variables for judicial decision-making. Since I am responsible for one of the models, I would like to respond to Grossman's points of criticism and cite some evidence which supports ome of the assumptions questioned by Grossman. In so doing, the attempt shall be made to clarify the research problems at issue.",
        "final_summary": "The papers suggest that judicial decisions can have a causal effect on political attitudes, but this effect is influenced by various factors.\n\nCarrubba 2008 finds that the preferences of member-state governments can shape decisions by the European Court of Justice (ECJ), indicating that political constraints can influence judicial rulings.\n\nZink 2009 and ZinkJames 2015 show that decision attributes such as the size of the majority coalition and adherence to precedent can influence individuals' views of court opinions. Unanimous decisions and decisions that follow precedent are more likely to be accepted by individuals, even if they initially disagree with the outcome.\n\nLink 1995 examines the relationship between public opinion, elite opinion, and Supreme Court rulings. The study finds that public opinion has a direct impact on Supreme Court liberalism in criminal procedure cases, but not in race-related civil rights cases. The relationship between public opinion and Court liberalism also varies during different time periods.\n\nGibson 2010 explores the role of judicial symbols in influencing citizens' acquiescence to court decisions. The study finds that exposure to symbols of judicial power increases the likelihood of acquiescence, although the effect is not strong. Factors such as initial policy attitudes, knowledge of the judiciary, and institutional support for the Supreme Court can moderate the consequences of judicial symbols.\n\nAshenfelter 1995 examines the influence of judicial background on case outcomes. The study finds little evidence that judges differ in their decisions based on their background or the political party of the appointing president.\n\nTiede 2015 focuses on judicial dissent and its political determinants in the Chilean Constitutional Tribunal. The study finds that major reforms and periods of greater partisanship on the court are associated with higher rates of dissent. Judges with identifiable partisan associations are more likely to dissent than those with limited political backgrounds.\n\nGoldman 1969 responds to Joel Grossman's article on social backgrounds and judicial decisions, providing additional evidence that supports the assumption that social and political backgrounds can influence judicial decision-making.\n\nIn summary, the papers suggest that political constraints, decision attributes, public opinion, judicial symbols, and judicial backgrounds can all play a role in the causal effect of judicial decisions on political attitudes."
    },
    {
        "query": "List me the works of Crenshaw on intersectionality.",
        "paper_list_string": "Rodriguez 2016:\n\nTitle: The Theory and Praxis of Intersectionality in Work and Organisations: Where Do We Go From Here?\n\nAbstract: Intersectionality is understood as a metaphor (Cuadraz and Uttal, 1999; Acker, 2011), a concept (Knapp, 2005; Styhre and Ericksson-Zetterquist, 2008), a research paradigm (Hancock, 2007a; Dhamoon, 2011), an ideograph (Alexander-Floyd, 2012), a broad-based knowledge project (Collins, 2015), and an analytical sensibility (Crenshaw, 2015). In spite of these diverse definitions, intersectionality has been central to the study of inequality, identity and power relations in recent history (Cho et al., 2013), highlighting the inseparability of categories of social differences such as race, gender, class, ethnicity, sexuality and nation, and calling attention to the systemic power dynamics that arise as multiple dimensions of social difference interact across individual, institutional, cultural and societal spheres of influence (Collins, 2000; McCall, 2005; Yuval-Davis, 2006, 2011; Weber, 2010). Coined as a term by Kimberl\u00e9 Crenshaw in 1989 to \u201ccounter the disembodiment of Black women from Law\u201d (Crenshaw, 2014), intersectionality captured the\n\n==\n\nHaynes 2020:\n\nTitle: Toward an Understanding of Intersectionality Methodology: A 30-Year Literature Synthesis of Black Women\u2019s Experiences in Higher Education\n\nAbstract: Kimberl\u00e9 Crenshaw\u2019s scholarship on Black women has been the springboard for numerous education studies in which researchers use intersectionality as a theoretical framework; however, few have considered the possibilities of intersectionality as a methodological tool. In this literature synthesis, the authors (a) examined studies about Black women in higher education that had been published in the past 30 years to understand how those scholars applied intersectionality across Crenshaw\u2019s three dimensions (i.e., structural, political, and representational) and (b) advanced a set of four strategies, arguably providing a guide for engaging \u201cintersectionality methodology,\u201d what the authors coin as \u201cIM.\u201d Implications for higher education research and social science research broadly are also presented.\n\n==\n\nMackinnon 2013:\n\nTitle: Intersectionality as Method: A Note\n\nAbstract: This brief note clarifies and expands upon the power and implications of intersectionality on the level of method, focusing upon its use in the hands of Kimberl\u00e9 Williams Crenshaw, its originator and premier practitioner.\n\n==\n\nSimien 2011:\n\nTitle: Mini-Symposium: Intersectionality Research\n\nAbstract: Since legal theorist Kimberle Crenshaw first spoke of intersectionality in the late 1980s, scholars in the social sciences and humanities have debated its relative strengths and weaknesses in theoretical, methodological, and policy terms. Intersectionality research is defined principally by its focus on the simultaneous and interactive effects of race, gender, class, sexual orientation, and national origin as categories of difference in the United States and beyond. This symposium is a conscious attempt to draw on our expertise in intersectionality and our identities as political scientists to spotlight different approaches to intersectionality. Part of our goal is thus to facilitate the development of innovative theoretical arguments and new empirical research designs in this area of scholarship by featuring examples of intersectionality research from distinct subfields. The essays that follow this introduction speak to a broad audience, as they represent four distinct subfields: political theory, American politics, comparative politics, and public policy. Each essay addresses the growing body of research focusing on what Dara Strolovitch (2007) calls intersectionally stigmatized populations at different levels of analysis. Each article also brings the tools of political science to bear on topics ripe for intersectional analysis, again reinforcing the notion that intersectionality theory and political science have much to offer each other. Much of the early scholarship on intersectionality has focused almost entirely on the United States and is often critiqued as having utility solely in the American context. Later scholarship, in attempting to address this critique, opened another avenue of questions, such as, what happens to a concept when it \"goes global\" and is applied to contexts outside of the United States? Comparative politics scholar Erica Townsend-Bell provides one answer to this question. Her essay, \"What is Relevance? Defining Intersectional Praxis in Uruguay,\" is less focused on intersectionality as a theoretical construct and more concerned with its development as political praxis. Using the 2005 International Women's Day March and the 2003 domestic violence coalition as illustrative examples, she identifies the conditions under which a normative philosophical commitment to intersectionality does not square with its adaption and translation on the ground. Through analysis of field interviews with activists Townsend-Bell attends to the ideological disputes over which identity categories are most relevant to single-issue and multiissue women's organizations in Uruguay. Linking her interests in intersectionality with traditional questions of social movements and group formation, Townsend-Bell exposes real-life circumstances that arise when social movement actors attempt to find the most productive and usable ways of provoking social change collaboratively. Through her critical analysis of intersectionality's applicability in the Uruguayan context, she provides further evidence of the need for intersectional consciousness (Greenwood 2008) prior to movement building, and her essay reminds us that intersectionality as a term travels across time and geographic regions. Like Townsend-Bell, Nancy Wadsworth's essay, \"Intersectionality in California's Same-Sex Marriage Battles: A Complex Proposition,\" also focuses on a longstanding area of interest shared by intersectionality scholars and political scientists: the tensions and limitations of majoritarian efforts to protect the rights of minorities. From a very different political context Wadsworth examines the coalitional and rhetorical strategies used to mobilize support in favor of a ballot initiative in the United States, Proposition 8, which eliminated the right to samesex marriage in California. Wadsworth makes the case for a broader, more inclusive conceptualization of intersectionality, insisting that religion qualifies as yet another identity category of difference. \u2026\n\n==\n\nHarris 2018:\n\nTitle: Un/Doing Intersectionality through Higher Education Research\n\nAbstract: ABSTRACT Grounded in Black feminist and critical race theories, legal scholar Kimberl\u00e9 Crenshaw introduced the term \u201cintersectionality\u201d to the academy in 1989 to demonstrate how U.S. structures, such as the legal system, and discourses of resistance, such as feminism and anti-racism, often frame identities as isolated and mutually exclusive, resulting in the \u201ctheoretical erasure\u201d of Black women who hold multiple minoritized identities. Since 1989, intersectionality has become a \u201ctraveling theory,\u201d that has crossed into and influenced almost every academic discipline, including higher education. Through this study, we examined how researchers in higher education do and undo intersectionality and, subsequently, how intersectional analyses may advance a radical social justice agenda in higher education. To explore how scholars un/do intersectionality in higher education, we conducted a summative content analysis of 97 higher education articles that used the term \u201cintersectionality\u201d in some manner. The goal of the study was not to offer a prescriptive way to use intersectionality. In fact, theoretically musing over the precise way in which intersectionality should be done may confine the concept to an overly academic contemplative exercise and therefore, undo intersectionality. Instead, through this research, we aimed to explore and use intersectionality in a manner that advances a transformative social justice agenda.\n\n==\n\nGopaldas 2013:\n\nTitle: Intersectionality 101\n\nAbstract: The concept of \u201cintersectionality\u201d refers to the interactivity of social identity structures such as race, class, and gender in fostering life experiences, especially experiences of privilege and oppression. This essay maps out the origins, evolution, and many contemporary meanings of intersectionality to make a notoriously ambiguous idea more concrete. In addition, the author clarifies the tenets of the intersectionality literature by contrasting traditional and intersectional research on marketplace diversity along three dimensions: ontology, methodology, and axiology. The essay concludes with implications for radicalizing diversity research, marketing, and advocacy.\n\n==\n\nHancock 2015:\n\nTitle: Intersectionality: An Intellectual History\n\nAbstract: Intersectionality theory has emerged over the past thirty years as a way to think about the avenues by which inequalities (most often dealing with, but not limited to, race, gender, class and sexuality) are produced. Rather than seeing such categories as signaling distinct identities that can be adopted, imposed or rejected, intersectionality theory considers the logic by which each of these categories is socially constructed as well as how they operate within the diffusion of power relations. In other words, social and political power are conferred through categories of identity, and these identities bear vastly material effects. Rather than look at inequalities as a relationship between those at the center and those on the margins, intersectionality maps the relative ways in which identity politics create power. Though intersectionality theory has emerged as a highly influential school of thought in ethnic studies, gender studies, law, political science, sociology and psychology, no scholarship to date exists on the evolution of the theory. In the absence of a comprehensive intellectual history of the theory, it is often discussed in vague, ahistorical terms. And while scholars have called for greater specificity and attention to the historical foundations of intersectionality theory, their idea of the history to be included is generally limited to the particular currents in the United States. This book seeks to remedy the vagueness and murkiness attributed to intersectionality by attending to the historical, geographical, and cross-disciplinary myopia afflicting current intersectionality scholarship. This comprehensive intellectual history is an agenda-setting work for the theory.\n\n==\n\nGillborn 2015:\n\nTitle: Intersectionality, Critical Race Theory, and the Primacy of Racism\n\nAbstract: The article explores the utility of intersectionality as an aspect of critical race theory (CRT) in education. Drawing on research with Black middle-class parents in England, the article explores the intersecting roles of race, class, and gender in the construction and deployment of dis/ability in education. The author concludes that intersectionality is a vital aspect of understanding race inequity but that racism retains a primacy for critical race scholars in three key ways: namely, empirical primacy (as a central axis of oppression in the everyday reality of schools), personal/autobiographical primacy (as a vital component in how critical race scholars view themselves and their experience of the world), and political primacy (as a point of group coherence and activism).",
        "final_summary": "Crenshaw's works on intersectionality have had a significant impact on various fields of study. Rodriguez 2016 highlights that intersectionality is seen as a metaphor, concept, research paradigm, ideograph, knowledge project, and analytical sensibility. Crenshaw coined the term in 1989 to address the exclusion of Black women from the law. Haynes 2020 explores how scholars have applied intersectionality as a theoretical framework in higher education research, while Mackinnon 2013 emphasizes Crenshaw's role as the originator and premier practitioner of intersectionality. Simien 2011 discusses the ongoing debates surrounding intersectionality's strengths and weaknesses in theoretical, methodological, and policy terms. Harris 2018 examines how researchers in higher education do and undo intersectionality, aiming to advance a transformative social justice agenda. Gopaldas 2013 provides an overview of the origins, evolution, and contemporary meanings of intersectionality, emphasizing its potential for radicalizing diversity research and advocacy. Hancock 2015 calls for a comprehensive intellectual history of intersectionality to address the vagueness and historical limitations in current scholarship. Finally, Gillborn 2015 explores the utility of intersectionality as an aspect of critical race theory, highlighting its importance in understanding race inequity while emphasizing the primacy of racism."
    },
    {
        "query": "urban ethnicity african politics afrobarometer data",
        "paper_list_string": "Conroy-Krutz 2009:\n\nTitle: Who are Africa's (Non) Ethnic Voters? Evaluating Theories on the Salience of Ethnicity in African Electoral Politics\n\nAbstract: The political salience of ethnicity in Africa is well-established, but these identities are by no means the sole determiners of individuals\u2019 political preferences and behaviors, either. Substantial variation exists within and between countries in terms of individuals\u2019 likelihood of voting for co-ethnics. In an attempt to identify factors contributing to this variation, this paper draws upon three theoretical approaches. First, some versions of modernization theory suggest that educated, urbanized voters with greater access to mass media will be more likely to embrace global, rather than parochial, identities, and will therefore be more open to voting across ethnic lines. Second, theories of strategic voting would suggest that the relative size of an individual\u2019s ethnic group should impact tendencies toward ethnic voting. Finally, ethnic voting might be more common under some electoral institutions than under others. Hypotheses are tested using data drawn from the third round of the Afrobarometer survey. The dependent variable - ethnic voting - is operationalized according to individuals\u2019 support for the consensus candidate of their ethnic group. The analysis finds support for \u201cclassical\u201d modernization theories, and finds significant correlations between the size of an individual\u2019s ethnic group at both the national and local levels and ethnic voting. Finally, ethnic voting seems to be more common, ceteris paribus, under proportional representation rules and less common under majoritarian presidential election rules.\n\n==\n\nCohen 1969:\n\nTitle: Custom and Politics in Urban Africa: A Study of Hausa Migrants in Yoruba Towns\n\nAbstract: Based on Cohen's fieldwork in the 1960s among the Hausa migrants, a people of the Yoruba area (then the western region of the Federation of Nigeria), Custom and Politics in Urban Africa looks at how ethnic groups use elements of tradition in jostling for power and privilege in new urban situations. This is a landmark work in urban anthropology and provides a comparative framework for studying political processes in African societies.\n\n==\n\nEifert 2010:\n\nTitle: Political Competition and Ethnic Identification in Africa\n\nAbstract: This article draws on data from over 35,000 respondents in 22 public opinion surveys in 10 countries and finds strong evidence that ethnic identities in Africa are strengthened by exposure to political competition. In particular, for every month closer their country is to a competitive presidential election, survey respondents are 1.8 percentage points more likely to identify in ethnic terms. Using an innovative multinomial logit empirical methodology, we find that these shifts are accompanied by a corresponding reduction in the salience of occupational and class identities. Our findings lend support to situational theories of social identification and are consistent with the view that ethnic identities matter in Africa for instrumental reasons: because they are useful in the competition for political power.\n\n==\n\nNathan 2016:\n\nTitle: Electoral Politics Amid Africa's Urban Transition: A Study of Urban Ghana\n\nAbstract: Africa is rapidly urbanizing. With so many African voters now living in cities, understanding African electoral politics now requires understanding the politics of urban areas. How does urbanization affect the accountability relationships between voters and politicians? Answering this question means answering a series of more specific empirical questions: what do urban voters want from the government? Which types of urban voters participate in politics and which do not? How do urban voters choose which candidates to support? How do politicians campaign in cities? Which types of urban voters do politicians seek to favor with state resources? Electoral politics in African cities received significant attention in the independence era, but little political science research has examined these cities in the contemporary democratic period. The small literature that has is largely supportive of modernization approaches. Modernization theories expect a series of socioeconomic transformations created by urbanization to reduce the political importance of ethnicity and the prevalence of clientelism and other forms of patronage-based politics. But I argue that urbanization also simultaneously creates conditions that reinforce incentives for patronage distribution, clientelism, and ethnic voting. Scarcity in the provision of basic services in contexts of low state capacity encourages politicians to continue employing patronage-based appeals. This solidifies many voters\u2019 incentives to support ethnicallyaligned parties and drives the new urban middle class away from active political participation, lowering pressure on urban politicians to engage in programmatic, policy-based competition. I explore these incentives through a detailed study of Greater Accra, the largest metropolitan area in Ghana. I combine original survey data and survey experiments, fine-grained geo-coded census data, and extensive qualitative evidence to explore voters\u2019 policy preferences, vote choices, and patterns of political\n\n==\n\nScarritt 1999:\n\nTitle: The specification of ethnic cleavages and ethnopolitical groups for the analysis of democratic competition in contemporary Africa\n\nAbstract: Ethnicity remains an important (but not the only) cost\u2010effective strategic resource for organizing collective political action in Africa's emerging democracies. To advance systematic analysis of the impact of ethnicity on current patterns of democratic politics and the potential for democratic consolidation, this article describes and presents a comprehensive data set on ethnopolitical groups in all 48 African countries. It explicates the theoretical orientation that informs the data set and the methodology used in defining, identifying and coding ethnopolitical groups.\n\n==\n\nSiachiwena 2022:\n\nTitle: The urban vote in Zambia\u2019s 2021 elections: popular attitudes towards the economy in Copperbelt and Lusaka\n\nAbstract: ABSTRACT\n This article analyses Afrobarometer survey data to understand popular attitudes toward the economy of Zambia amongst residents in the ruling party strongholds. The Patriotic Front (PF) won the most votes in urban provinces from 2006 to 2016 but crucially lost to the opposition in 2021 while retaining majorities in its rural base. Historically, opposition parties have won the most votes in urban regions on every occasion that Zambia has experienced an electoral turnover, demonstrating the significance of the urban vote. The evidence presented in this article reveals that respondents in the two urban provinces, Copperbelt and Lusaka, were better educated, had more access to essential services, and had higher levels of access to Information relative to their compatriots in the PF\u2019s rural strongholds. While both urban and rural residents were dissatisfied with the government\u2019s handling of the economy, the PF\u2019s urban base was more likely to punish the ruling party at the polls. Drawing on survey evidence and literature on voting in urban Africa, this article shows that the divergent geographic, social, and economic characteristics of urban and rural areas produce distinct grievances which have implications for policy expectations, political mobilization, and how voters evaluate candidates.\n\n==\n\nNathan 2016:\n\nTitle: Local Ethnic Geography, Expectations of Favoritism, and Voting in Urban Ghana\n\nAbstract: African democracies are increasingly urban. While ethnicity is generally correlated with vote choice, recent research suggests there may be less ethnic voting in cities. But I show that voting for ethnically affiliated parties is as common in some neighborhoods in urban Ghana as in rural areas, while virtually non-existent in other neighborhoods elsewhere within the same city. This intra-urban variation is not explained by differences in the salience of ethnic identities or other individual-level characteristics of voters themselves. Instead, it is influenced by the diversity and wealth of the local neighborhoods in which parties and voters interact. These neighborhood characteristics change otherwise similar voters\u2019 expectations of the benefits they will receive from an ethnically affiliated party when living in different places, producing intra-urban differences in the importance of ethnicity for vote choice.\n\n==\n\nSimone 2001:\n\nTitle: Africities: Popular Engagements of the Urban in Contemporary Africa\n\nAbstract: Accelerated urbanization in Africa has produced cities whose formal physical, political and social infrastructures are largely unable to absorb, apprehend or utilize the needs, aspirations and resourcefulness of those who live within them. As a result, the efforts to secure livelihood depend upon largely informalised processes and a wide range of provisional and ephemeral institutions which cultivate specific orientations toward, knowledge of, and practices for dealing with urban life. Soon, the majority of Africans will live in peri-urban and informal settlements, often at the physical and social margins of the city. Yet, marginalization from security of tenure, formal livelihood, and, frequently, participation in viable urban political institutions converges with new modalities of urban social production which frequently confound clear readings of the motivations and effects of various forms of collective action undertaken in the public sphere. \nAt the same time, African cities are sites for multifaceted development and governance interventions which make largely uninspired efforts to engage such practices and institutions. \nEven the efforts of civil society organizations operating under the umbrella of their promotion of \u2018active citizenship\u2019 tend to impose an ineffectual normative. What I attempt to do here is discuss a range of orientations to the city and an ambivalence about the city which seems to pervade how residents in many African cities use it and what they do within it. This is an ambivalence largely \nabout whether the city belongs to them, where the African city can best be made, and to what extent the city is a place for the reinvention of social solidarity or a circumvention of solidarity. \nWhile urban analysts have tended to focus on how such African cities are marginal within urban systems, the primary focus in this article is to begin, in a very provisional way, to identify strategic operations and sensibilities being put together to help African urban residents \u2018navigate\u2019 disparate positionalities within both local and global networks of power. These efforts at \nnavigation give rise to particular notions of citizenship and collective responsibility whose \u2018containment\u2019 within clearly demarcated urban identities, municipal domains, and practices of urban politics becomes increasingly problematic.",
        "final_summary": "The papers suggest that ethnicity plays a significant role in African politics, particularly in relation to electoral behavior and voting patterns in urban areas. Conroy-Krutz (2009) finds that while ethnicity is politically salient, other factors such as education, urbanization, and media access can influence individuals' likelihood of voting for co-ethnics. Eifert (2010) supports this, showing that exposure to political competition strengthens ethnic identities in Africa. Scarritt (1999) emphasizes the importance of ethnicity as a strategic resource for political action in emerging democracies. Nathan (2016) explores the impact of urbanization on electoral politics in Ghana, highlighting the persistence of ethnic voting in some urban neighborhoods. Additionally, Simone (2001) discusses the challenges faced by urban residents in Africa, including marginalization and the need to navigate complex networks of power. Overall, these papers underscore the significance of ethnicity and urban dynamics in shaping African politics, particularly in relation to voting behavior and political mobilization."
    },
    {
        "query": "HI",
        "paper_list_string": "Zhang 2020:\n\nTitle: HI hotel eco spa & beach \u2013 We Dream Business\n\nAbstract: a) The main facts about the activities of the company Hi hotel is a 38 room urban luxury hotel in Nice located 200 meters from the sea. It is has been founded by Patrick Elouarghi and Philippe\u2026\n\n==\n\nBhatia 2004:\n\nTitle: HIAP Health Information Access Project\n\nAbstract: The Health Information Access Project (HIAP) is a joint project of the ACT Health Library and Health Information Networking (HIN) Program. The HIAP website provides health information resources on the desktop of GPs, specialists, health professionals, and health care consumers engaged in research projects to support evidence-based practice at the point of care.\n\n==\n\nMaiti 2021:\n\nTitle: HI (HARLEQUIN ICHTHYOSIS) BABY\n\nAbstract: HI is a very rare type of genetical abnormality but can be seen and scared by seeing various social media footages. It is associated with deletion and\ntruncation mutations of a keratinocyte lipid transporter. Harlequin disorder is categorized by diffuse epidermal hyperkeratinization along with\ndefective desquamation. During birth, the HI phenotype is conspicuous with thick hyperkeratotic plate-like scales with deep dermal \uf001ssures,\nsevere ectropion and eclabium, among other \uf001ndings. In this ichthyosis marked eclabium and ectropion are present secondary to the taut as well as\nin the unyielding skin. The ears may be inattentive, absent or poorly developed. Even the arms, feet, and digits have \uf002exion contractures and may be\nhypoplastic. The skin wall is harshly compromised, leading to undue water loss, causes electrolyte abnormalities, temperature alteration and an\nincreased risk of deadly infection. Some of the primary treatments are smearing retinoids application for shedding the hard and scaly skin, topical\nantibiotics application can prevent infection, insertion an ET tube in the airway to assistance with breathing, applying lubricating eye drops or\nprotective devices on the eyes. Following ongoing treatment like humidi\uf001ed incubator (for premature infants), continuous monitoring of TPR and\nSpO2, early intubation(optional), frequent cultures of the skin should be taken for lab testing, monitoring serum electrolyte levels, maintaining a\ngerm-free environment to evade infection etc.\n\n==\n\nBrinks 1996:\n\nTitle: HI in HII-Galaxies and Their Companions\n\nAbstract: HII galaxies are dwarf galaxies which are currently actively forming stars. We speculated that an interaction with an optically faint, but gas-rich object might be responsible for their enhanced star formation. This prompted us to search for companions with the VLA in the 21-cm line of HI. This has several advantages over optical searches (Campos-Aguilar et al. 1991, AJ, 106, 1784; Telles & Terlevich 1995, MNRAS, 275, 1), e.g., the direct availability of redshifts.\n\n==\n\nHo 2006:\n\nTitle: HiQ: a hierarchical Q-learning algorithm to solve the reader collision problem\n\nAbstract: HiQ is a hierarchical, online learning algorithm that finds dynamic solutions to the reader collision problem in RFID systems. When the transmissions from one reader interfere with the operation of another reader, a reader collision occurs. The objective of the reader collision problem is to minimize the reader collisions experienced by RFID readers while using the minimum number of frequencies and using the minimum total time for all readers to communicate successfully. HiQ attempts to minimize reader collisions by learning the collision patterns of the readers and by effectively assigning frequencies over time to ensure neighboring readers do not experience collisions from one another. HiQ is arranged hierarchically with distributed, local control. The algorithm is based on a type of reinforcement learning called Q-learning, which is used to determine frequency and time assignments. Through repeated interaction with the system, Q-learning attempts to discover an optimum frequency assignment over time. We show that HiQ finds optimal or near optimal solutions to the reader collision problem.\n\n==\n\nPrivon 2015:\n\nTitle: GOALS: HI Mapping of Local (U)LIRGs\n\nAbstract: Neutral hydrogen (HI) is one of the major constituents of the interstellar medium in local luminous and ultraluminous infrared galaxy ((U)LIRGs; L_IR > 10^11 L_sun) interactions and mergers. Observations of HI can be used to obtain information on the kinematic state of the interaction, search for evidence of interaction in seeminly unperturbed systems, and constrain the amount of fuel available for future star formation. We present preliminary results from an interferometric HI survey of local (U)LIRGs from the Great Observatories All-sky LIRG Survey (GOALS). Here we focus on observations of a subset of systems with undisturbed stellar morphologies - interpreted as suggesting these galaxies have not been significantly perturbed by tidal interactions. Despite this, they show elevated star formation rates. HI observations of these systems show extended features with mophologies and kinematics consistent with ongoing tidal interaction, suggesting these systems are already experiencing tidal perturbation - likely the cause of the enhanced star formation.\n\n==\n\nCowell 2004:\n\nTitle: The Human-Information Workspace (HI-Space): Ambient Table Top Entertainment\n\nAbstract: This paper introduces the Human Information Workspace (HI-Space) as a test-bed for evaluating new information exploration mechanisms. In moving from dated interaction devices and small computer monitors, we aim to utilize more natural surfaces such as tables and walls as our interaction space. In testing our theories, we have produced a number of gaming applications as test cases. Here, we report on our most popular application, Virtual Hockey.\n\n==\n\nDreavesHilary 2016:\n\nTitle: How Health Impact Assessments (HIAs) Help Us to Select the Public Health Policies Most Likely to Maximise Health Gain, on the Basis of Best Public Health Science\n\nAbstract: Health Impact Assessment (HIA) is a decision support tool intended to present timely, evidence-based recommendations to decision makers in all sections of society in order to accentuate potential positive health and well-being impacts (and mitigate potential negative impacts) of policies, plans (including local and neighbourhood plans), programmes and projects (including infrastructure and local development proposals), in order to reduce health inequalities/disparities. HIA is a well established and proven means of linking research evidence from public health and the environmental sciences with equitable decision making processes at all levels, from local to global. It may also provide a platform for examination of research proposals to strengthen the impact statement therein, identifying potential for future public benefit. This paper highlights some of the main drivers for a timely re-emphasis on the use of best scientific evidence and systematic HIA to inform decision making for future public benefit, citing the example of air pollution.",
        "final_summary": "The papers collected do not directly address the research question \"HI.\" The papers cover various topics such as a hotel in Nice (Zhang 2020), a health information access project (Bhatia 2004), a rare genetic abnormality called Harlequin Ichthyosis (Maiti 2021), HII galaxies and their companions (Brinks 1996), a hierarchical Q-learning algorithm for RFID systems (Ho 2006), HI mapping of local (U)LIRGs (Privon 2015), a human information workspace for interactive surfaces (Cowell 2004), and the use of Health Impact Assessments in decision-making (DreavesHilary 2016). None of these papers directly address the research question."
    },
    {
        "query": "Form of Science Journalism",
        "paper_list_string": "Secko 2013:\n\nTitle: FOUR MODELS OF SCIENCE JOURNALISM\n\nAbstract: Much of the science communication and journalism studies literature continues to reiterate the same critiques about science journalism. This literature accuses science journalists of inaccuracy, sensationalism, oversimplification and failing to engage audiences in meaningful debate about scientific issues. However, research has yet to offer concrete solutions to journalists that connect theory to practice in an effort to counter these criticisms. In this paper, we approach this gap through the development of clearly articulated models of science journalism that are supported by theoretical considerations of the varying purposes of science communication, and then, importantly, tied to practical story development criteria. Four models are presented: science literacy, contextual, lay-expertise and public participation. These models are clear representations of how science journalism can be produced from within different theoretical frameworks and thereby provide a theoretically-informed but practical guide for nuanced evaluations of the quality of science journalism.\n\n==\n\nVadapalli 2018:\n\nTitle: Sci-Blogger: A Step Towards Automated Science Journalism\n\nAbstract: Science journalism is the art of conveying a detailed scientific research paper in a form that non-scientists can understand and appreciate while ensuring that its underlying information is conveyed accurately. It plays a crucial role in making scientific content suitable for consumption by the public at large. In this work, we introduce the problem of automating some parts of the science journalism workflow by automatically generating the 'title' of a blog version of a scientific paper. We have built a corpus of $87,328$ pairs of research papers and their corresponding blogs from two science news aggregators and have used it to buildSci ence-Blogger - a pipeline-based architecture consisting of a two-stage mechanism to generate the blog titles. Evaluation using standard metrics indicate viability of the proposed system.\n\n==\n\nFahy 2011:\n\nTitle: The science journalist online: Shifting roles and emerging practices\n\nAbstract: Science reporters today work within an evolving science media ecosystem that is pluralistic, participatory and social. It is a mostly online environment that has challenged the historically dominant and exceptional role of science reporters as privileged conveyers of specialist information to general audiences. We map this science media environment, drawing on interviews with journalists and writers from nationally prominent US and UK media organizations, describing the shifting roles and emerging practices of science journalists online. Compared to a decade ago, this occupational group, driven by economic imperatives and technological changes, is performing a wider plurality of roles, including those of curator, convener, public intellectual and civic educator, in addition to more traditional journalistic roles of reporter, conduit, watchdog and agenda-setter. Online science journalists have a more collaborative relationship with their audiences and sources and are generally adopting a more critical and interpretative stance towards the scientific community, industry, and policy-oriented organizations.\n\n==\n\nAllan 2011:\n\nTitle: Introduction: Science Journalism in a Digital Age\n\nAbstract: The people who run the media are humanities graduates with little understanding of science, who wear their ignorance as a badge of honour. Secretly, deep down, perhaps they resent the fact that they have denied themselves access to the most significant developments in the history of Western thought from the past 200 years; but there is an attack implicit in all media coverage of science: in their choice of stories, and the way they cover them, the media create a parody of science. On this template, science is portrayed as groundless, incomprehensible, didactic truth statements from scientists, who themselves are socially powerful, arbitrary, unelected authority figures. They are detached from reality; they do work that is either whacky or dangerous, but either way, everything in science is tenuous, contradictory, probably going to change soon and, most ridiculously, \u2018hard to understand.\u2019 Having created this parody, the commentariat then attack it, as if they were genuinely critiquing what science is all about. (2009: 207\u20138)\n\n==\n\nBennet 2013:\n\nTitle: Science Service and the origins of science journalism, 1919--1950\n\nAbstract: In 1919, just after the end of World War I, Edward W. Scripps and William E. Ritter joined to form a science news organization dedicated to pioneering the dissemination of accurate, accessible, and engaging news of science to the public primarily through the mainstream media. Scripps, a longtime journalist and successful entrepreneur, and Ritter, a respected biologist and teacher, were convinced of the importance of science knowledge to the American public. They also were concerned about limits and abuses under other political systems where science research was abridged or threatened. They sought to create a \"scientific habit of mind\" among the general public to increase the average person's awareness of the role of science in his or her daily life, to gain support for science research, and to help protect American democracy through an intelligent\u2014meaning scienceeducated\u2014citizenry. The result of this collaboration was the organization Science Service, established in 1921 for the popularization of science, with the support and participation of the leading science organizations, including the American Association for the Advancement of Science, National Academy of Science, and the National Research Council, as well as leaders of the journalism community. Reaching the public also meant navigating the contentious relationship between scientists and the press to create new ways of translating science information, and overcoming scientists' reticence about sharing their research interests with the lay public. Additionally, Science Service weighed capturing the public's attention through enticing but only fact-tinged stories, versus adhering to higher scientific and journalistic standards of fact-based but less sensational articles. Through the post-war twenties, the Depression thirties, and the war-plagued forties, Science Service forged relationships with\n\n==\n\nNisbet 2015:\n\nTitle: The Need for Knowledge-Based Journalism in Politicized Science Debates\n\nAbstract: Largely overlooked by researchers studying the science of science communication are the specific journalistic practices and media structures that might enable more constructive public debate in politicized science controversies. In this commentary, we discuss the role that journalists can play as influential knowledge professionals, drawing on insights from the studies in this section of the special issue. In doing so, we outline three complementary approaches to what Thomas Patterson calls \u201cknowledge-based journalism.\u201d By way of these approaches, journalists and their news organizations can contextualize and critically evaluate expert knowledge; facilitate discussion that bridges entrenched ideological divisions; and promote consideration of a broader menu of policy options and technologies. We conclude by discussing the implications for journalism education.\n\n==\n\nGuenther 2017:\n\nTitle: Scientific Evidence and Science Journalism\n\nAbstract: Science journalism in print and online media is an important source of information for laypersons. While science journalists established their own professional routines and standards, different actors (and scientists in particular) often criticise science journalism for not being transparent and accurate enough. One critical point raised concerns journalists\u2019 representation of scientific evidence. Against this background, the current study investigated which kind of information about scientific evidence is represented in science articles in German print and online media. The results of a quantitative content analysis (N\u2009=\u2009128), in which variables were coded for the represented scientific results (N\u2009=\u2009225), showed that scientific evidence is rarely part of science articles, and scientific findings are predominantly depicted as scientifically certain. However, information about uncertainty is more common within sections in the media specifically dedicated to science, as well as when the article has natural scientific or medical scientific content. In addition, the scientific results that are presented first in an article contain the most information about scientific evidence.\n\n==\n\nPolman 2014:\n\nTitle: Adapting Practices of Science Journalism to Foster Science Literacy\n\nAbstract: In this paper, the authors describe how the practices of expert science journalists enable them to act as \u201ccompetent outsiders\u201d to science. We assert that selected science journalism practices can be used to design reform-based science instruction; these practices not only foster science literacy that is useful in daily life, but also complement practices of scientists that already have been incorporated into science education. We describe and illustrate science journalism practices for educational activities that can be adapted and used. We argue that such experiences enable students to better use science information for personal decision-making and help them contribute meaningfully to public discourse long after high school graduation. Finally, we describe the design of a learning environment that facilitates research and development based on this model.",
        "final_summary": "The papers collectively provide insights into the form of science journalism. Secko 2013 proposes four models of science journalism: science literacy, contextual, lay-expertise, and public participation, which offer theoretical frameworks and practical criteria for evaluating the quality of science journalism. Vadapalli 2018 explores the automation of science journalism by generating blog titles for scientific papers. Fahy 2011 discusses the evolving roles of science journalists in the online environment, including curator, convener, public intellectual, and civic educator. Allan 2011 criticizes the media's portrayal of science as groundless and difficult to understand. Bennet 2013 traces the origins of science journalism and the efforts to popularize science knowledge among the public. Nisbet 2015 emphasizes the need for knowledge-based journalism to foster constructive public debate in politicized science controversies. Guenther 2017 examines the representation of scientific evidence in science articles, highlighting the rarity of such information and the predominance of depicting findings as certain. Polman 2014 suggests adapting science journalism practices for science instruction to enhance science literacy."
    }
]