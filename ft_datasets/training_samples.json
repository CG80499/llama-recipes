[{"query": "Autoimmune disease", "paper_list_string": "Panayi 1976:\n\nTitle: Auto-immune disease.\n\nAbstract: Auto-immune disease may result from the interaction of the genetic load of the individual, modification of self-tissue antigens by environmental agents such as virus or drugs and abnormalities of the immunological system itself such as the loss of controlling or suppressor T cells with age. In the majority of people the outcome is tolerance, maintenance of normal tissue architecture and function. In the unfortunate few the outcome is auto-immune disease, that is, failure to recognize \"self\".\n\n==\n\nDavidson 2014:\n\nTitle: General Features of Autoimmune Disease\n\nAbstract: Abstract Autoimmune diseases are common diseases in which loss of tolerance within the immune system results in pathologic immune responses that target either cellular or organ-specific self-antigens. There is a genetic tendency toward autoreactivity in affected individuals, and both innate and adaptive immune activation may contribute to disease. An important recent advance is the identification of genetic polymorphisms that contribute to risk in most autoimmune diseases and are associated with a variety of immune activation pathways; these associations have been informative about disease-specific pathogenesis and have led to the development of successful therapies for some diseases. A deeper understanding of the components of the innate and adaptive immune system has led to highly effective therapeutic targeting of cytokines, cell surface molecules, and intracellular signaling molecules with marked improvements in outcome in several autoimmune diseases. Fertile new avenues for research include mechanisms of regulation by regulatory immune cells, metabolic regulation of immune cell function, and immune activation by commensal bacteria that colonize humans. The goal of regulating autoimmunity without causing excessive immunosuppression remains elusive but is the holy grail of current research efforts.\n\n==\n\nBaker 1997:\n\nTitle: Autoimmune endocrine disease.\n\nAbstract: Autoimmune endocrine diseases are serious disorders that utilize immense health care resources and cause tremendous disability. They include type 1 diabetes mellitus, thyroiditis, Graves disease, Addison disease, and polyglandular syndromes. Analysis of the basis of autoimmune diseases has been aided by the application of new knowledge in immunologic physiology. Recent investigations using these techniques have revealed complicated disorders that have varied pathogenesis and complex genetic predispositions. While the mainstay of treatment for these diverse diseases remains the replacement of hormones produced by the damaged endocrine organ, investigations into the pathogenesis of these disorders provide hope for the development of specific therapeutic measures to block their pathologic basis.\n\n==\n\nKing 1963:\n\nTitle: Autoimmune diseases; pathogenesis, chemistry and therapy.\n\nAbstract: Medical scientists, meeting the challenge of socalled \"autoimmune\" diseases, have created new techniques and achieved new concepts which are altering markedly our outlook in immunology, immunochemistry, and pathology. Burnet, winner of the Nobel prize for medicine, collaborates with Mackay to present an exposition of this field. They discuss a group of diseases \"for which an autoimmune etiology has been established either as a primary or an associated factor, or in which there is some likelihood that autoimmune processes are concerned.\" Analyzed at length are thyroiditis, systemic lupus erythematosis, certain hemolytic conditions, rheumatoid arthritis, glomerulonephritis, and certain liver and nervous system diseases, while numerous other conditions are treated more cursorily. To diagnose an autoimmune state the authors invoke the following criteria: an increase in gamma globulin, demonstrable autoantibodies against some body component, deposition of denatured gamma globulin at certain sites of elections, accumulation of lymphocytes and plasma cells, some degree of\n\n==\n\nSinha 1990:\n\nTitle: Autoimmune diseases: the failure of self tolerance.\n\nAbstract: The ability to discriminate between self and nonself antigens is vital to the functioning of the immune system as a specific defense against invading microorganisms. Failure of the immune system to \"tolerate\" self tissues can result in pathological autoimmune states leading to debilitating illness and sometimes death. The induction of autoimmunity involves genetic and environmental factors that have focused the attention of researchers on the trimolecular complex formed by major histocompatibility complex molecules, antigen, and T cell receptors. Detailed molecular characterization of these components points to potential strategies for disease intervention.\n\n==\n\nAsherson 1967:\n\nTitle: Growing Points: Autoimmune Disease\u2014I\n\nAbstract: In some human diseases antibodies react with human tissue in vitro. These antibodies are called autoantibodies, and are found in rheumatic fever, Hashimoto's thyroiditis, systemic lupus erythematosus, and other diseases. The presence of autoantibodies is compatible with the view that tissues are damaged by an autoimmune process directed against components of the patient's own tissues. For this reason Hashimoto's thyroiditis and systemic lupus erythematosus are called putative autoimmune diseases. For readers wishing to learn more about autoimmunity the recent books by Humphrey and White (1964) and Glynn and Holborow (1965) are recommended. The bibliography for the statements made in this article is given by Asherson (1965).\n\n==\n\nAgarwal 2009:\n\nTitle: Autoimmunity in common variable immunodeficiency\n\nAbstract: Common variable immunodeficiency (CVID) is the most common clinically significant primary immune defect. Although the hallmark of CVID is hypogammaglobulinemia, the intrinsic dysregulation of the immune system leads to defective T-cell activation and proliferation, as well as dendritic cell and cytokine defects. Although 70% to 80% of patients have had recurrent sinopulmonary infections, autoimmunity and inflammatory complications are also common. The most common autoimmune conditions are immune thrombocytopenic purpura and hemolytic anemia, but other autoimmune complications arise, including rheumatoid arthritis, pernicious anemia, primary biliary cirrhosis, thyroiditis, sicca syndrome, systemic lupus, and inflammatory bowel disease. Treatment of autoimmunity includes highdose immunoglobulins, corticosteroids, selected immu no suppressants, and other immune modulators. This review focuses on autoimmune conditions associated with CVID, potential mechanisms of immune dysregulation, and therapeutic strategies.\n\n==\n\nKatsorida 2001:\n\nTitle: Autoimmunity and autoimmune disease.\n\nAbstract: A disease is defined as autoimmune if the tissue damage is caused by an adaptive immune response to self-antigen. Development of the disease by transferring the active component of the immune response to an appropriate recipient is the best proof that the disease is caused by autoimmunity. The body contains large numbers of potentially autoreactive B and T cells, although there are a variety of mechanisms operating to establish self-tolerance during lymphocyte development. This is particularly true for thymic T cells that are not deleted by selfepitopes. The presence of autoreactive B cells in the normal population is well demonstrated by the development of autoantibodies when autoantigens are injected with adjuvants in normal animals (e.g. anti-thyroglobulin). Existence of autoreactive T cells in normal individuals is demonstrated by the production of autoimmune lines of T cells when normal circulating T cells are stimulated by the appropriate autoantigen (e.g. myelin basic protein) and interleukin-2. Autoimmune disease can be classified as organ specific, when the response is primarily against antigens localized to particular organs or as non-organ specific, when the response is directed against widespread antigens. Hashimoto\u2019s thyroiditis and SLE represent the extremes of autoimmune disease spectrum. In organ specific diseases, the thyroid, adrenals, stomach, and pancreas are the usual target organs. The non-organ specific diseases, which include the rheumatic disorders, characteristically involve the skin, kidney, brain, lungs, joints, and muscles w1x.", "final_summary": "Autoimmune diseases are complex conditions where the immune system mistakenly attacks the body's own cells, viewing them as foreign invaders. These diseases result from a combination of genetic predisposition and environmental factors, leading to a loss of immune tolerance to self-antigens (Panayi, 1976; Davidson, 2014; Baker, 1997; Sinha, 1990). \n\nThe pathogenesis of autoimmune diseases is multifaceted and involves both the innate and adaptive immune systems. Genetic polymorphisms associated with immune activation pathways contribute to the risk of developing these diseases (Davidson, 2014). The interaction of major histocompatibility complex molecules, antigen, and T cell receptors is a focal point in the induction of autoimmunity (Sinha, 1990). \n\nAutoimmune diseases can be organ-specific, such as Hashimoto's thyroiditis, where the immune response primarily targets specific organs, or non-organ specific, like systemic lupus erythematosus, where the immune response is directed against widespread antigens (Katsorida, 2001). \n\nAutoimmune endocrine diseases, including type 1 diabetes mellitus and Graves disease, are serious disorders that cause significant disability and utilize immense health care resources (Baker, 1997). Autoimmunity is also associated with common variable immunodeficiency (CVID), a primary immune defect that leads to defective T-cell activation and proliferation (Agarwal, 2009). \n\nIn conclusion, autoimmune diseases are a result of the immune system's failure to recognize \"self,\" leading to pathologic immune responses. These diseases are complex, involving genetic predisposition, environmental factors, and immune system dysregulation. They can be organ-specific or non-organ specific and are associated with significant morbidity and healthcare utilization. Further research is needed to develop specific therapeutic measures to block their pathologic basis (Baker, 1997; Davidson, 2014)."}, {"query": "genomic imprinting", "paper_list_string": "Falls 1999:\n\nTitle: Genomic imprinting: implications for human disease.\n\nAbstract: Genomic imprinting refers to an epigenetic marking of genes that results in monoallelic expression. This parent-of-origin dependent phenomenon is a notable exception to the laws of Mendelian genetics. Imprinted genes are intricately involved in fetal and behavioral development. Consequently, abnormal expression of these genes results in numerous human genetic disorders including carcinogenesis. This paper reviews genomic imprinting and its role in human disease. Additional information about imprinted genes can be found on the Genomic Imprinting Website at http://www.geneimprint.com.\n\n==\n\nBarlow 2011:\n\nTitle: Genomic imprinting: a mammalian epigenetic discovery model.\n\nAbstract: Genomic imprinting is an epigenetic process leading to parental-specific expression of one to two percent of mammalian genes that offers one of the best model systems for a molecular analysis of epigenetic regulation in development and disease. In the twenty years since the first imprinted gene was identified, this model has had a significant impact on decoding epigenetic information in mammals. So far it has led to the discovery of long-range cis-acting control elements whose epigenetic state regulates small clusters of genes and of unusual macro noncoding RNAs (ncRNAs) that directly repress genes in cis, and critically, it has demonstrated that one biological role of DNA methylation is to allow expression of genes normally repressed by default. This review describes the progress in understanding how imprinted protein-coding genes are silenced; in particular, it focuses on the role of macro ncRNAs that have broad relevance as a potential new layer of regulatory information in the mammalian genome.\n\n==\n\nBartolomei 2009:\n\nTitle: Genomic imprinting: employing and avoiding epigenetic processes.\n\nAbstract: Genomic imprinting refers to an epigenetic mark that distinguishes parental alleles and results in a monoallelic, parental-specific expression pattern in mammals. Few phenomena in nature depend more on epigenetic mechanisms while at the same time evading them. The alleles of imprinted genes are marked epigenetically at discrete elements termed imprinting control regions (ICRs) with their parental origin in gametes through the use of DNA methylation, at the very least. Imprinted gene expression is subsequently maintained using noncoding RNAs, histone modifications, insulators, and higher-order chromatin structure. Avoidance is manifest when imprinted genes evade the genome-wide reprogramming that occurs after fertilization and remain marked with their parental origin. This review summarizes what is known about the establishment and maintenance of imprinting marks and discusses the mechanisms of imprinting in clusters. Additionally, the evolution of imprinted gene clusters is described. While considerable information regarding epigenetic control of imprinting has been obtained recently, much remains to be learned.\n\n==\n\nFerguson-Smith 2011:\n\nTitle: Genomic imprinting: the emergence of an epigenetic paradigm\n\nAbstract: The emerging awareness of the contribution of epigenetic processes to genome function in health and disease is underpinned by decades of research in model systems. In particular, many principles of the epigenetic control of genome function have been uncovered by studies of genomic imprinting. The phenomenon of genomic imprinting, which results in some genes being expressed in a parental--origin-specific manner, is essential for normal mammalian growth and development and exemplifies the regulatory influences of DNA methylation, chromatin structure and non-coding RNA. Setting seminal discoveries in this field alongside recent progress and remaining questions shows how the study of imprinting continues to enhance our understanding of the epigenetic control of genome function in other contexts.\n\n==\n\nHall 1997:\n\nTitle: Genomic imprinting: nature and clinical relevance.\n\nAbstract: Molecular genetic techniques allow investigators to trace chromosomes and genes from parent to child and, in a single individual, from tissue to tissue. These techniques have uncovered a new type of gene control in which the allele from one parent is expressed and the allele from the other parent is not. This differential expression is called genomic imprinting. It may lead to phenotypic differences when inheritance is from the mother versus the father. Genomic imprinting has been observed in a number of disorders having to do with growth, behavior, and abnormal cell growth. It is important to be aware that such a phenomenon exists and to consider it when making diagnoses and determining therapy.\n\n==\n\nReik 2001:\n\nTitle: Genomic imprinting: parental influence on the genome\n\nAbstract: Genomic imprinting affects several dozen mammalian genes and results in the expression of those genes from only one of the two parental chromosomes. This is brought about by epigenetic instructions \u2014 imprints \u2014 that are laid down in the parental germ cells. Imprinting is a particularly important genetic mechanism in mammals, and is thought to influence the transfer of nutrients to the fetus and the newborn from the mother. Consistent with this view is the fact that imprinted genes tend to affect growth in the womb and behaviour after birth. Aberrant imprinting disturbs development and is the cause of various disease syndromes. The study of imprinting also provides new insights into epigenetic gene modification during development.\n\n==\n\nBarlow 2014:\n\nTitle: Genomic imprinting in mammals.\n\nAbstract: Genomic imprinting affects a subset of genes in mammals and results in a monoallelic, parental-specific expression pattern. Most of these genes are located in clusters that are regulated through the use of insulators or long noncoding RNAs (lncRNAs). To distinguish the parental alleles, imprinted genes are epigenetically marked in gametes at imprinting control elements through the use of DNA methylation at the very least. Imprinted gene expression is subsequently conferred through lncRNAs, histone modifications, insulators, and higher-order chromatin structure. Such imprints are maintained after fertilization through these mechanisms despite extensive reprogramming of the mammalian genome. Genomic imprinting is an excellent model for understanding mammalian epigenetic regulation.\n\n==\n\nVerona 2003:\n\nTitle: Genomic imprinting: intricacies of epigenetic regulation in clusters.\n\nAbstract: An intriguing characteristic of imprinted genes is that they often cluster in large chromosomal domains, raising the possibility that gene-specific and domain-specific mechanisms regulate imprinting. Several common features emerged from comparative analysis of four imprinted domains in mice and humans: (a) Certain genes appear to be imprinted by secondary events, possibly indicating a lack of gene-specific imprinting marks; (b) some genes appear to resist silencing, predicting the presence of cis-elements that oppose domain-specific imprinting control; (c) the nature of the imprinting mark remains incompletely understood. In addition, common silencing mechanisms are employed by the various imprinting domains, including silencer elements that nucleate and propagate a silent chromatin state, insulator elements that prevent promoter-enhancer interactions when hypomethylated on one parental allele, and antisense RNAs that function in silencing the overlapping sense gene and more distantly located genes. These commonalities are reminiscent of the behavior of genes subjected to, and the mechanisms employed in, dosage compensation.", "final_summary": "Genomic imprinting, an intriguing epigenetic phenomenon, results in the monoallelic, parent-specific expression of certain genes, a deviation from the traditional Mendelian inheritance (Falls, 1999; Barlow, 2011; Bartolomei, 2009; Ferguson-Smith, 2011; Hall, 1997; Reik, 2001; Barlow, 2014; Verona, 2003). This process is crucial for normal mammalian growth, development, and behavior, with aberrations leading to various genetic disorders and diseases, including carcinogenesis (Falls, 1999; Hall, 1997; Reik, 2001).\n\nImprinted genes are marked epigenetically in parental germ cells, with DNA methylation being a key player in this process (Bartolomei, 2009; Ferguson-Smith, 2011; Barlow, 2014). These marks are maintained post-fertilization despite extensive genome reprogramming, a testament to the robustness of this mechanism (Bartolomei, 2009; Barlow, 2014).\n\nInterestingly, imprinted genes often cluster in large chromosomal domains, suggesting the existence of both gene-specific and domain-specific regulatory mechanisms (Verona, 2003). These clusters are regulated through the use of insulators or long noncoding RNAs (lncRNAs), which play a significant role in gene silencing (Barlow, 2011; Bartolomei, 2009; Barlow, 2014; Verona, 2003).\n\nIn conclusion, genomic imprinting is a complex and fascinating epigenetic process that plays a pivotal role in mammalian development and disease. It offers a unique model for understanding mammalian epigenetic regulation, with its intricate interplay of DNA methylation, lncRNAs, and chromatin structure (Ferguson-Smith, 2011; Barlow, 2014). However, much remains to be discovered about the precise mechanisms and full implications of this process."}, {"query": "why does pulmonary edema tend to occur in hilar area rather than peripheral?", "paper_list_string": "Fleischner 1967:\n\nTitle: The butterfly pattern of acute pulmonary edema.\n\nAbstract: Abstract Among the factors causing uneven distribution of pulmonary edema, defective lymph drainage favors the accumulation of edematous fluid in areas of previous disease and scars, both pulmonary and pleural. The medulla and cortex of the lung are structurally different, and there are convincing reasons to assume nonuniformity of ventilation between them. Radiologic evidence shows that the cortex participates in the ventilatory play to a greater extent than the medulla; being readily distensible, it provides probably the greatest part of the ventilatory volume changes at rest. Lymphatic drainage, dependent on movements of the adjacent structures, is greatly promoted in the well ventilated parts, and the poorly moving central parts (medulla) seem to have a sluggish lymph flow. This would explain the frequently encountered preference of pulmonary edema for the central part of the lung or individual lobe. The involved central wet portion is occasionally set off abruptly from the dry cortex, because of the sharp morphologic and functional demarcation between medulla and cortex. The appearance of this demarcation is obviously dependent on factors of ventilation, perfusion, and others. It is, therefore, usually transient in cardiovascular edema.\n\n==\n\nGluecker 1999:\n\nTitle: Clinical and radiologic features of pulmonary edema.\n\nAbstract: Pulmonary edema may be classified as increased hydrostatic pressure edema, permeability edema with diffuse alveolar damage (DAD), permeability edema without DAD, or mixed edema. Pulmonary edema has variable manifestations. Postobstructive pulmonary edema typically manifests radiologically as septal lines, peribronchial cuffing, and, in more severe cases, central alveolar edema. Pulmonary edema with chronic pulmonary embolism manifests as sharply demarcated areas of increased ground-glass attenuation. Pulmonary edema with veno-occlusive disease manifests as large pulmonary arteries, diffuse interstitial edema with numerous Kerley lines, peribronchial cuffing, and a dilated right ventricle. Stage 1 near drowning pulmonary edema manifests as Kerley lines, peribronchial cuffing, and patchy, perihilar alveolar areas of airspace consolidation; stage 2 and 3 lesions are radiologically nonspecific. Pulmonary edema following administration of cytokines demonstrates bilateral, symmetric interstitial edema with thickened septal lines. High-altitude pulmonary edema usually manifests as central interstitial edema associated with peribronchial cuffing, ill-defined vessels, and patchy airspace consolidation. Neurogenic pulmonary edema manifests as bilateral, rather homogeneous airspace consolidations that predominate at the apices in about 50% of cases. Reperfusion pulmonary edema usually demonstrates heterogeneous airspace consolidations that predominate in the areas distal to the recanalized vessels. Postreduction pulmonary edema manifests as mild airspace consolidation involving the ipsilateral lung, whereas pulmonary edema due to air embolism initially demonstrates interstitial edema followed by bilateral, peripheral alveolar areas of increased opacity that predominate at the lung bases. Familiarity with the spectrum of radiologic findings in pulmonary edema from various causes will often help narrow the differential diagnosis.\n\n==\n\nB\u00e4rtsch 2005:\n\nTitle: Physiological aspects of high-altitude pulmonary edema.\n\nAbstract: High-altitude pulmonary edema (HAPE) develops in rapidly ascending nonacclimatized healthy individuals at altitudes above 3,000 m. An excessive rise in pulmonary artery pressure (PAP) preceding edema formation is the crucial pathophysiological factor because drugs that lower PAP prevent HAPE. Measurements of nitric oxide (NO) in exhaled air, of nitrites and nitrates in bronchoalveolar lavage (BAL) fluid, and forearm NO-dependent endothelial function all point to a reduced NO availability in hypoxia as a major cause of the excessive hypoxic PAP rise in HAPE-susceptible individuals. Studies using right heart catheterization or BAL in incipient HAPE have demonstrated that edema is caused by an increased microvascular hydrostatic pressure in the presence of normal left atrial pressure, resulting in leakage of large-molecular-weight proteins and erythrocytes across the alveolarcapillary barrier in the absence of any evidence of inflammation. These studies confirm in humans that high capillary pressure induces a high-permeability-type lung edema in the absence of inflammation, a concept first introduced under the term \"stress failure.\" Recent studies using microspheres in swine and magnetic resonance imaging in humans strongly support the concept and primacy of nonuniform hypoxic arteriolar vasoconstriction to explain how hypoxic pulmonary vasoconstriction occurring predominantly at the arteriolar level can cause leakage. This compelling but as yet unproven mechanism predicts that edema occurs in areas of high blood flow due to lesser vasoconstriction. The combination of high flow at higher pressure results in pressures, which exceed the structural and dynamic capacity of the alveolar capillary barrier to maintain normal alveolar fluid balance.\n\n==\n\nDenomme 1988:\n\nTitle: Pulmonary edema.\n\nAbstract: Pulmonary edema is the accumulation of fluids in the interstitium and alveoli of the lung. There are two main basic mechanisms for edema development: increased hydrostatic pressure in the lung capillaries (\u201chigh-pressure edema\u201d) and increase vascular permeability (\u201clow-pressure edema). This classification helps understand the basic pathophysiological differences between the two types of pulmonary edema, but has limitations. Disruption of some or all layers of the alveolar-capillary unit occurs during elevated capillary hydrostatic pressures, a phenomenon termed \u201cpulmonary capillary stress failure\u201d. Pulmonary capillary stress failure represents a process that blurs the distinction between high-pressure and low-pressure pulmonary edema, as the disruption of the alveolar-capillary membrane by high hydrostatic pressures may render it more permeable to fluid and proteins. The resulting edema fluid has a higher concentration of protein than would be expected in conventional high-pressure pulmonary edema. These observations may explain some features seem in high-altitude pulmonary edema and neurogenic pulmonary edema.\n\n==\n\nGleason 1966:\n\nTitle: The lateral roentgenogram in pulmonary edema.\n\nAbstract: 1. Lateral roentgenograms demonstrate that the edema which appears central on the frontal roentgenogram may, in fact, lie in anterior or posterior segments of any lobe of the lung.2. Interstitial and intra-alveolar pulmonary edema accompany each other in most patients irrespective of the cause.3. There are specific signs of interstitial edema, such as septal lines and peribronchial and perivascular cuffing. but others, such as perihilar haze, can be confused with intra-alveolar edema.4. Although gravity probably plays a part in the distribution of edema, it is difficult to obtain conclusive evidene of this in all cases.5. Pulmonary edema appears to shift easily and rapidly from lobe to lobe and from one lung to the other.\n\n==\n\nGurney 1989:\n\nTitle: Pulmonary edema localized in the right upper lobe accompanying mitral regurgitation.\n\nAbstract: Focal patterns of pulmonary edema are confusing and often mistaken for the more common causes of focal lung disease, pneumonia, infarction, or aspiration. The authors report four cases of right upper lobe edema secondary to mitral regurgitation. The pathogenesis believed to be responsible for this condition is the vector of blood flow from the left ventricle to left atrium, which may be targeted at the right superior pulmonary vein, locally accentuating the forces for edema formation in the right upper lobe. Pulmonary edema accompanying mitral regurgitation should be suspected whenever right upper lobe consolidation develops in a patient with known or suspected mitral valve disease. The presence of interstitial edema in the remainder of the lungs can help in the differentiation of this condition from pneumonia and other disorders.\n\n==\n\nWillms 1988:\n\nTitle: Pulmonary edema due to upper airway obstruction in adults.\n\nAbstract: A report of pulmonary edema following acute upper airway obstruction in an adult is presented, and the literature involving 25 additional cases is reviewed. This form of pulmonary edema appears to be related to markedly negative intrathoracic pressure due to forced inspiration against a closed upper airway resulting in transudation of fluid from pulmonary capillaries to the interstitium. Postanesthetic laryngospasm is the most common cause of pulmonary edema in adults (11/26 cases). The edema usually clears rapidly with supportive care. Aggressive diagnostic and therapeutic interventions may be avoided if the syndrome is recognized. Maintenance of oxygenation and a patent airway are the mainstays of treatment.\n\n==\n\nSzidon 1972:\n\nTitle: The alveolar-capillary membrane and pulmonary edema.\n\nAbstract: CONVENTIONAL teaching, based on light microscopy, has dealt with the physiopathology of pulmonary edema in terms of a simple barrier separating pulmonary capillaries from the alveolar spaces. According to this simplistic view, pulmonary edema represents the extravasation of a filtrate of plasma directly from pulmonary capillaries into alveoli as the result of one of two alternative mechanisms: an imbalance of forces operating across a normal alveolar-capillary barrier so that reabsorptive forces cannot cope with the high transcapillary filtration rates; or, despite normal transcapillary forces, excessive passage of plasma filtrate out of the capillary because of a breakdown of the barrier. .\u00a0.\u00a0.", "final_summary": "Pulmonary edema, the accumulation of fluid in the lungs, tends to occur in the hilar area rather than the peripheral regions due to a variety of physiological factors. \n\nFleischner (1967) suggests that the uneven distribution of pulmonary edema is due to defective lymph drainage and nonuniform ventilation between the medulla and cortex of the lung. The central parts of the lung, or the medulla, have a sluggish lymph flow and are less involved in ventilation, leading to a preference for edema in these areas. \n\nGluecker (1999) provides a comprehensive overview of the various manifestations of pulmonary edema, but does not specifically mention the manifestation of postobstructive pulmonary edema as central alveolar edema.\n\nB\u00e4rtsch (2005) discusses high-altitude pulmonary edema, suggesting that nonuniform hypoxic arteriolar vasoconstriction can cause leakage, leading to edema in areas of high blood flow due to lesser vasoconstriction. \n\nDenomme (1988) explains that pulmonary capillary stress failure, a process that blurs the distinction between high-pressure and low-pressure pulmonary edema, may explain some features seen in high-altitude pulmonary edema and neurogenic pulmonary edema. \n\nGleason (1966) notes that edema can shift easily and rapidly from lobe to lobe and from one lung to the other, suggesting a dynamic nature to the distribution of pulmonary edema. \n\nGurney (1989) reports cases of right upper lobe edema secondary to mitral regurgitation, suggesting that the vector of blood flow from the left ventricle to left atrium may locally accentuate the forces for edema formation in the right upper lobe. \n\nWillms (1988) discusses pulmonary edema following acute upper airway obstruction, suggesting that this form of edema is related to markedly negative intrathoracic pressure due to forced inspiration against a closed upper airway. \n\nFinally, Szidon (1972) discusses the conventional view of pulmonary edema, suggesting that it represents the extravasation of a filtrate of plasma directly from pulmonary capillaries into alveoli due to an imbalance of forces or a breakdown of the barrier. \n\nIn conclusion, the central occurrence of pulmonary edema can be attributed to a variety of factors including defective lymph drainage, nonuniform ventilation, nonuniform hypoxic arteriolar vasoconstriction, pulmonary capillary stress failure, and the dynamics of blood flow and pressure in the lungs."}, {"query": "Antioxidant enzymes and Skin Cancer risk", "paper_list_string": "He 2010:\n\nTitle: Polymorphisms in genes involved in oxidative stress and their interactions with lifestyle factors on skin cancer risk.\n\nAbstract: Ultraviolet (UV)-induced oxidative stress has been implicated in skin carcinogenesis [1]. Several antioxidant enzymes, such as glutathione peroxidase (GPX) and catalase (CAT), counteract oxidative damage and constitute a primary defense against oxidative stress. GPX is a soluble selenoprotein that reduces H2O2 and organic hydroperoxides to H2O, and GPX1 is the most abundant and ubiquitous intracellular isoform [1]. GPX1 activity is not strongly affected by UV and is considered to be the most important antioxidant enzyme defense mechanism in the skin [2]. CAT is a heme enzyme that neutralizes reactive oxygen species by converting H2O2 to H2O and O2. CAT activity in the skin is significantly reduced after exposure to UV [2], which suggests its effect may be prone to effect modification by environmental factors. \n \nInherited variants in the encoding genes that affect the activity or expression of these antioxidant enzymes are hypothesized to modulate oxidative stress and thus influence skin cancer risk. A polymorphism in the GPX1 gene (Pro198Leu, rs1050450) and a polymorphism in the promoter region of the CAT gene (C-262T, rs1001179) have been shown to be associated with lower enzyme activities of their encoded enzymes [3, 4]. To test our main hypothesis that these two genetic polymorphisms are associated with skin cancer risk, we conducted a nested case-control study of Caucasians (218 melanoma, 285 squamous cell carcinoma (SCC), and 300 basal cell carcinoma (BCC) cases, and 870 age-matched controls) within the Nurses\u2019 Health Study. We further investigated potential gene-environment interactions between these polymorphisms and lifestyle factors such as dietary antioxidant intake and sun exposure related risk factors. A detailed description of the characteristics of cases and controls was published previously [5]. Information on dietary intake was collected prospectively by food-frequency questionnaires, and total-energy-adjusted cumulative average of dietary intake was used to reduce within-person variation and represent long-term dietary intake [6]. \n \nWe genotyped the two single nuclear polymorphisms (SNPs) (rs1050450 and rs1001179) by the 5\u2032 nuclease assay (TaqMan\u00ae) in 384-well format, using the ABI PRISM 7900HT Sequence Detection System (Applied Biosystems, Foster City, CA). The distributions of genotypes for the two SNPs were in Hardy-Weinberg equilibrium among controls (p=0.94, 0.83, respectively). We compared the cases of each type of skin cancer to the common control series. We used unconditional multivariate logistic regression to model the association between genetic polymorphisms and skin cancer risk and to estimate multivariate Odds Ratios (ORs) and 95% Confidence Intervals (CIs). To test statistical significance of gene-environmental interactions, we used dominant model for genotypes and dichotomized environmental exposures as low versus high based on median values among controls. We tested the statistical significance of a single multiplicative interaction term. \n \nIn the main effect analysis (Table 1), we observed that the GPX1 198 Leu/Leu genotype was significantly associated with a two-fold increased risk of melanoma (OR, 2.14; 95% CI, 1.22\u20133.72), after adjustment for age and other covariates. No association was found between this polymorphism and SCC or BCC risk, which was consistent with one previous study [7]. This polymorphism has been shown to be associated with lung cancer [8] and breast cancer [3] previously. We did not observe significant association between the CAT C-262T polymorphism and the risk of any type of skin cancer. \n \n \n \nTable 1 \n \nAssociation between GPX Pro198Leu and CAT C-262T genetic polymorphisms and skin cancer risk a \n \n \n \nAs exploratory analyses, we further tested gene-environment interactions between the genetic variants and lifestyle factors that modulate oxidative stress. We found the association between the CAT C-262T polymorphism and melanoma risk was significantly modified by history of severe sunburns (p for interaction, 0.008, Table 2), a variable combining exposure intensity and biological response to sun exposure. The positive association between history of severe sunburns and melanoma risk was restricted to T carriers (OR, 1.73; 95% CI, 1.02\u20132.92), compared to women with CC genotype (OR, 1.03; 95% CI, 0.63\u20131.69). We also observed a significant gene-diet interaction between the CAT C-262T polymorphism and total carotenoid intake on melanoma risk (p for interaction, 0.01). The inverse association of total carotenoid intake with melanoma risk was limited among women with CC genotype (OR, 0.63; 95% CI, 0.41\u20130.97), whereas no association was observed among T carriers (OR, 1.23; 95% CI, 0.77\u20131.97). Inconsistent results were reported on the relationship between dietary carotenoid intake and melanoma risk in several previous case-control studies. An inverse association between the intake and the risk of melanoma was observed in some studies [9], but not in other studies [10]. Our results suggest that the inconsistency in the literature may reflect a potential gene-diet interaction. As we tested different genetic polymorphisms, multiple environmental exposures and dietary factors, and three types of skin cancer, multiple testing in our study may lead to false positive results. Replications in independent studies are needed to validate these results. No significant interactions were observed between the GPX1 polymorphism and these lifestyle factors on melanoma risk. We did not observe any significant interaction between these genetic variants and environmental exposures on the risk of SCC or BCC. \n \n \n \nTable 2 \n \nInteraction between the CAT C-262T genetic polymorphism and history of severe sunburns and total carotenoid intake on melanoma risk \n \n \n \nIn summary, we first observed the GPX1 198 Leu/Leu genotype was significantly associated with a two-fold increased risk of melanoma, and the association between the CAT C-262T polymorphism and melanoma risk was significantly modified by history of severe sunburns and total carotenoid intake. Further research is needed to confirm these possible associations and illustrate the underlying molecular mechanisms.\n\n==\n\nHeinen 2007:\n\nTitle: Intake of antioxidant nutrients and the risk of skin cancer.\n\nAbstract: To investigate the associations between intake of antioxidant nutrients and risk of basal cell (BCC) and squamous cell carcinomas (SCC) of the skin, we carried out a prospective study among 1001 randomly selected adults living in an Australian community. Intake of antioxidants was estimated in 1996. Incident, histologically-confirmed BCC and SCC were recorded between 1996 and 2004. High dietary intake of lutein and zeaxanthin was associated with a reduced incidence of SCC in persons who had a history of skin cancer at baseline (highest versus lowest tertile, multivariable adjusted relative risk (RR)=0.47, 95% confidence interval (CI): 0.25-0.89; P for trend=0.02). In persons without a history of skin cancer at baseline, development of BCC was positively associated with intake of vitamins C and E from foods plus supplements (RR=3.1, 95% CI: 1.1-8.6; P for trend=0.03 and RR=2.6, 95% CI: 1.1-6.3; P for trend=0.02, respectively). In those with a skin cancer history at baseline, dietary intake in the second tertile for beta-carotene (multivariable adjusted RR=2.2, 95% CI: 1.2-4.1) and for vitamin E (multivariable adjusted RR=2.1, 95% CI: 1.1-3.9) was associated with increased BCC risk, with no trend, and similar results were seen in those with a specific history of BCC. These data suggest quite different associations between antioxidant intake and SCC compared with BCC, consistent with other evidence of their different causal pathways.\n\n==\n\nReiners 1991:\n\nTitle: Assessment of the antioxidant/prooxidant status of murine skin following topical treatment with 12-O-tetradecanoylphorbol-13-acetate and throughout the ontogeny of skin cancer. Part I: Quantitation of superoxide dismutase, catalase, glutathione peroxidase and xanthine oxidase.\n\nAbstract: The activities of several enzymes involved in reactive oxygen production and detoxification were quantified in murine skin during the ontogeny of chemically induced skin cancer. Relative to solvent-treated controls, the specific activities of epidermal superoxide dismutase (SOD), catalase (CAT) and glutathione peroxidase (GPX) were reduced approximately 45, approximately 60 and approximately 24% respectively, 24 h after the fourth or tenth topical application of 1 microgram of 12-O-tetradecanoylphorbol-13-acetate (TPA) to the dorsal skin of SENCAR mice. The specific activity of epidermal xanthine oxidase (XO) increased approximately 350% during the same period. SOD and CAT specific activities in papillomas and carcinomas generated in an initiation-promotion protocol were approximately 15 and approximately 40% respectively of the activities measured in age-matched, non-treated mice. CAT and SOD activities were also significantly suppressed in the skin adjacent to the papillomas for several weeks following the cessation of TPA promotion, but eventually recovered to the levels measured in age-matched controls. XO specific activities in papillomas and squamous cell carcinomas (SCC) were approximately 85-350% greater than the activities determined in skin adjacent to the tumors. The increases in XO and the decreases in SOD and CAT activities measured in the tumors were independent of continued treatment with TPA, and thus characteristic of the tumor phenotype. GPX activities in papillomas were comparable to normal, untreated skin, but reduced approximately 22-41% in SCC. Collectively, these studies demonstrate that TPA orchestrates changes in the activities of several enzymes involved in reactive oxygen metabolism that are characteristic of the papilloma and SCC phenotype.\n\n==\n\nFuchs 1989:\n\nTitle: Impairment of enzymic and nonenzymic antioxidants in skin by UVB irradiation.\n\nAbstract: Antioxidants may play a significant role in ameliorating or preventing photobiologic damage in skin that could lead to cutaneous disorders such as cancer and premature aging. The objective of this study was to assess the acute cutaneous enzymic and nonenzymic antioxidant response to a single exposure of large fluence (300 mJ/cm2) ultraviolet radiation (greater than 280 nm) in hairless mice. This treatment caused an immediate and statistically significant inhibition of glutathione reductase and catalase activity. Glutathione peroxidase and superoxide dismutase were not affected. Glutathione levels decreased and, conversely glutathione disulfide concentrations increased. A slight depletion of the total glutathione was observed, while the content of total ascorbic acid did not change. The lipophilic antioxidants alpha-tocopherol, ubiquinol 9 and ubiquinone 9 also decreased significantly, and the concentration of malondialdehyde remained constant. The free radical scavenging activity of epidermis, as assessed by reduction of the stable, cationic nitroxide radical [2,2,6,6-tetramethyl-1-piperidinoxy-4-(2',4',6'-trimethyl) methylpyridinium perchlorate] was considerably inhibited. The study indicates that immediately after exposure to a large fluence of ultraviolet radiation the enzymic and nonenzymic antioxidant capacity of skin decreases significantly.\n\n==\n\nPrabasheela 2011:\n\nTitle: Association between Antioxidant Enzymes and Breast Cancer\n\nAbstract: The exact antioxidant status in breast cancer patient is still not clear. So present study was focused on enzymic antioxidants such as Superoxide dismutase, Catalase, Glutathione-S-transferase, Glutathione peroxidase and Glutathione reductase in the serum of 25 histopathologically proven breast cancer patients. When the data were analyzed with age matched control the antioxidant \u00c2\u00a0 levels were found to decrease indicating enhanced \u00c2\u00a0 free radical activity in breast cancer patients while the antioxidant defense mechanism is weakened. However further elaborate clinical studies are required to evaluate the role of such antioxidant enzymes in breast cancer management.\n\n==\n\nTd 1997:\n\nTitle: Antioxidant enzyme levels in cancer.\n\nAbstract: : Normal cells are protected by antioxidant enzymes from the toxic effects of high concentrations of reactive oxygen species generated during cellular metabolism. Even though cancer cells generate reactive oxygen species, it has been demonstrated biochemically that antioxidant enzyme levels are low in most animal and human cancers. However, a few cancer types have been found to have elevated levels of antioxidant enzymes, particularly manganese superoxide dismutase. Morphologic studies of animal and human cancer have confirmed that although the majority of tumor cell types from several organ systems have low antioxidant enzymes, adenocarcinomas may have elevated manganese superoxide dismutase and catalase levels. However, all cancers examined to date have some imbalance in antioxidant enzyme levels compared with the cell of origin. Antioxidant enzyme importance in cancer genesis has been difficult to evaluate in early cancerous lesions using biochemical techniques because such lesions are small and therefore below the level of detection. Using immunohistochemical techniques, early lesions of human and animal cancers were demonstrated to have low antioxidant enzymes, thus suggesting a role for these enzymes both in the genesis of cancer and the malignant phenotype. All but one human cancer cell type (the granular cell variant of human renal adenocarcinoma) examined showed both low catalase and glutathione peroxidase levels, suggesting that most cancer cell types cannot detoxify hydrogen peroxide. Our results to date are used to propose new cancer therapies based on modulation of cellular redox state.\n\n==\n\nOberley 1997:\n\nTitle: Antioxidant enzyme levels in cancer.\n\nAbstract: Normal cells are protected by antioxidant enzymes from the toxic effects of high concentrations of reactive oxygen species generated during cellular metabolism. Even though cancer cells generate reactive oxygen species, it has been demonstrated biochemically that antioxidant enzyme levels are low in most animal and human cancers. However, a few cancer types have been found to have elevated levels of antioxidant enzymes, particularly manganese superoxide dismutase. Morphologic studies of animal and human cancer have confirmed that although the majority of tumor cell types from several organ systems have low antioxidant enzymes, adenocarcinomas may have elevated manganese superoxide dismutase and catalase levels. However, all cancers examined to date have some imbalance in antioxidant enzyme levels compared with the cell of origin. Antioxidant enzyme importance in cancer genesis has been difficult to evaluate in early cancerous lesions using biochemical techniques because such lesions are small and therefore below the level of detection. Using immunohistochemical techniques, early lesions of human and animal cancers were demonstrated to have low antioxidant enzymes, thus suggesting a role for these enzymes both in the genesis of cancer and the malignant phenotype. All but one human cancer cell type (the granular cell variant of human renal adenocarcinoma) examined showed both low catalase and glutathione peroxidase levels, suggesting that most cancer cell types cannot detoxify hydrogen peroxide. Our results to date are used to propose new cancer therapies based on modulation of cellular redox state.\n\n==\n\nGodic 2014:\n\nTitle: The Role of Antioxidants in Skin Cancer Prevention and Treatment\n\nAbstract: Skin cells are constantly exposed to reactive oxygen species (ROS) and oxidative stress from exogenous and endogenous sources. UV radiation is the most important environmental factor in the development of skin cancer and skin aging. The primary products caused by UV exposure are generally direct DNA oxidation or generation of free radicals which form and decompose extremely quickly but can produce effects that can last for hours, days, or even years. UV-induced generation of ROS in the skin develops oxidative stress when their formation exceeds the antioxidant defense ability. The reduction of oxidative stress can be achieved on two levels: by lowering exposure to UVR and/or by increasing levels of antioxidant defense in order to scavenge ROS. The only endogenous protection of our skin is melanin and enzymatic antioxidants. Melanin, the pigment deposited by melanocytes, is the first line of defense against DNA damage at the surface of the skin, but it cannot totally prevent skin damage. A second category of defense is repair processes, which remove the damaged biomolecules before they can accumulate and before their presence results in altered cell metabolism. Additional UV protection includes avoidance of sun exposure, usage of sunscreens, protective clothes, and antioxidant supplements.", "final_summary": "The relationship between antioxidant enzymes and skin cancer risk is a complex one, as evidenced by the findings from several studies. \n\nHe (2010) found that the GPX1 198 Leu/Leu genotype was associated with a two-fold increased risk of melanoma, and the association between the CAT C-262T polymorphism and melanoma risk was significantly modified by history of severe sunburns and total carotenoid intake. However, no significant interactions were observed between the GPX1 polymorphism and these lifestyle factors on melanoma risk.\n\nHeinen (2007) found that high dietary intake of lutein and zeaxanthin was associated with a reduced incidence of SCC in persons who had a history of skin cancer at baseline. However, development of BCC was positively associated with intake of vitamins C and E from foods plus supplements in persons without a history of skin cancer at baseline.\n\nReiners (1991) found that the activities of several enzymes involved in reactive oxygen production and detoxification were altered in murine skin during the ontogeny of chemically induced skin cancer. The specific activities of epidermal superoxide dismutase (SOD), catalase (CAT) and glutathione peroxidase (GPX) were reduced after topical application of 12-O-tetradecanoylphorbol-13-acetate (TPA) to the dorsal skin of SENCAR mice.\n\nFuchs (1989) found that immediately after exposure to a large fluence of ultraviolet radiation the enzymic and nonenzymic antioxidant capacity of skin decreases significantly. \n\nPrabasheela (2011) found that the antioxidant levels were found to decrease indicating enhanced free radical activity in breast cancer patients while the antioxidant defense mechanism is weakened.\n\nTd (1997) and Oberley (1997) found that antioxidant enzyme levels are low in most animal and human cancers. However, a few cancer types have been found to have elevated levels of antioxidant enzymes, particularly manganese superoxide dismutase.\n\nGodic (2014) found that UV-induced generation of ROS in the skin develops oxidative stress when their formation exceeds the antioxidant defense ability. The reduction of oxidative stress can be achieved on two levels: by lowering exposure to UVR and/or by increasing levels of antioxidant defense in order to scavenge ROS.\n\nIn conclusion, the relationship between antioxidant enzymes and skin cancer risk is complex and multifaceted. While some antioxidant enzymes may increase skin cancer risk, others may decrease it. Further research is needed to fully understand these relationships and their implications for skin cancer prevention and treatment."}, {"query": "high throughput genetic screen are a powerful tool in identifying individual gene function and interaction", "paper_list_string": "Shalem 2015:\n\nTitle: High-throughput functional genomics using CRISPR\u2013Cas9\n\nAbstract: Forward genetic screens are powerful tools for the discovery and functional annotation of genetic elements. Recently, the RNA-guided CRISPR (clustered regularly interspaced short palindromic repeat)-associated Cas9 nuclease has been combined with genome-scale guide RNA libraries for unbiased, phenotypic screening. In this Review, we describe recent advances using Cas9 for genome-scale screens, including knockout approaches that inactivate genomic loci and strategies that modulate transcriptional activity. We discuss practical aspects of screen design, provide comparisons with RNA interference (RNAi) screening, and outline future applications and challenges.\n\n==\n\nKweon 2018:\n\nTitle: High-throughput genetic screens using CRISPR\u2013Cas9 system\n\nAbstract: The CRISPR\u2013Cas9 system is a powerful tool for genome engineering, and its programmability and simplicity have enabled various types of gene manipulation such as gene disruption and transcriptional and epigenetic perturbation. Particularly, CRISPR-based pooled libraries facilitate high-throughput screening for functional regulatory elements in the human genome. In this review, we describe recent advances in CRISPR\u2013Cas9 technology and its use in high-throughput genetic screening. We also discuss its potential for drug target discovery and current challenges of this technique in biomedical research.\n\n==\n\nKelley 2005:\n\nTitle: Systematic interpretation of genetic interactions using protein networks\n\nAbstract: Genetic interaction analysis,in which two mutations have a combined effect not exhibited by either mutation alone, is a powerful and widespread tool for establishing functional linkages between genes. In the yeast Saccharomyces cerevisiae, ongoing screens have generated >4,800 such genetic interaction data. We demonstrate that by combining these data with information on protein-protein, prote in-DNA or metabolic networks, it is possible to uncover physical mechanisms behind many of the observed genetic effects. Using a probabilistic model, we found that 1,922 genetic interactions are significantly associated with either between- or within-pathway explanations encoded in the physical networks, covering \u223c40% of known genetic interactions. These models predict new functions for 343 proteins and suggest that between-pathway explanations are better than within-pathway explanations at interpreting genetic interactions identified in systematic screens. This study provides a road map for how genetic and physical interactions can be integrated to reveal pathway organization and function.\n\n==\n\nMohr 2010:\n\nTitle: Genomic screening with RNAi: results and challenges.\n\nAbstract: RNA interference (RNAi) is an effective tool for genome-scale, high-throughput analysis of gene function. In the past five years, a number of genome-scale RNAi high-throughput screens (HTSs) have been done in both Drosophila and mammalian cultured cells to study diverse biological processes, including signal transduction, cancer biology, and host cell responses to infection. Results from these screens have led to the identification of new components of these processes and, importantly, have also provided insights into the complexity of biological systems, forcing new and innovative approaches to understanding functional networks in cells. Here, we review the main findings that have emerged from RNAi HTS and discuss technical issues that remain to be improved, in particular the verification of RNAi results and validation of their biological relevance. Furthermore, we discuss the importance of multiplexed and integrated experimental data analysis pipelines to RNAi HTS.\n\n==\n\nFriedman 2004:\n\nTitle: Genome-wide high-throughput screens in functional genomics.\n\nAbstract: The availability of complete genome sequences from many organisms has yielded the ability to perform high-throughput, genome-wide screens of gene function. Within the past year, rapid advances have been made towards this goal in many major model systems, including yeast, worms, flies, and mammals. Yeast genome-wide screens have taken advantage of libraries of deletion strains, but RNA-interference has been used in other organisms to knockdown gene function. Examples of recent large-scale functional genetic screens include drug-target identification in yeast, regulators of fat accumulation in worms, growth and viability in flies, and proteasome-mediated degradation in mammalian cells. Within the next five years, such screens are likely to lead to annotation of function of most genes across multiple organisms. Integration of such data with other genomic approaches will extend our understanding of cellular networks.\n\n==\n\nCarpenter 2004:\n\nTitle: Systematic genome-wide screens of gene function\n\nAbstract: By using genome information to create tools for perturbing gene function, it is now possible to undertake systematic genome-wide functional screens that examine the contribution of every gene to a biological process. The directed nature of these experiments contrasts with traditional methods, in which random mutations are induced and the resulting mutants are screened for various phenotypes. The first genome-wide functional screens in Caenorhabditis elegans and Drosophila melanogaster have recently been published, and screens in human cells will soon follow. These high-throughput techniques promise the rapid annotation of genomes with high-quality information about the biological function of each gene.\n\n==\n\nTo 2000:\n\nTitle: Identification of differential gene expression by high throughput analysis.\n\nAbstract: High throughput analysis of differential gene expression is a powerful tool that can be applied to many areas in molecular cell biology, including differentiation, development, physiology, and pharmacology. In recent years, a variety of techniques have been developed to analyze differential gene expression, including comparative expressed sequence tag sequencing, differential display, representational difference analysis, cDNA or oligonucleotide arrays, and serial analysis of gene expression. This review explains the technologies, their scopes, impact on science, as well as their costs and possible limitations. The application of differential display is presented as a tool to identify genes induced by darkness or yellowing process in rice leaves.\n\n==\n\nCipriani 2011:\n\nTitle: RNAi methods and screening: RNAi based high-throughput genetic interaction screening.\n\nAbstract: Expanding on decades of mutational analyses, numerous genome-scale RNAi screens have now been performed in C. elegans, leading to estimates that the majority of genes with essential functions that can be revealed by single-gene perturbations have already been identified in this organism. To build on this basic foundation and uncover condition-dependent or combinatorial effects of non-essential genes will require even higher-scale screening. Here we describe a method for performing high-throughput RNAi-based screens in C. elegans in liquid in 96-well plates, and we explain how to systematically test for enhancement and suppression of temperature-sensitive mutations. This chapter covers our entire set of protocols, from setting up the experiment and screening schedule, to scoring the results. The rapid acquisition of high-quality images of each experiment allows the management of a large number of samples per screening cycle and opens up new possibilities for quantitative scoring, computerized image analysis, and the ability to review results independent of the time constraints that are associated with large-scale screening.", "final_summary": "High-throughput genetic screens are a powerful tool for identifying individual gene function and interaction, as evidenced by a collection of research papers. Shalem (2015) and Kweon (2018) both highlight the use of the CRISPR-Cas9 system in high-throughput functional genomics, emphasizing its ability to facilitate unbiased, phenotypic screening and gene manipulation. \n\nKelley (2005) underscores the value of genetic interaction analysis, demonstrating how combining these data with protein networks can uncover the physical mechanisms behind observed genetic effects. This approach has been instrumental in predicting new functions for proteins and interpreting genetic interactions identified in systematic screens.\n\nMohr (2010) and Friedman (2004) both discuss the use of RNA interference (RNAi) in high-throughput screens. Mohr (2010) emphasizes the effectiveness of RNAi in genome-scale, high-throughput analysis of gene function, while Friedman (2004) predicts that such screens will lead to the annotation of function of most genes across multiple organisms in the near future.\n\nCarpenter (2004) and To (2000) both discuss the potential of systematic genome-wide functional screens. Carpenter (2004) highlights the promise of these high-throughput techniques in rapidly annotating genomes with high-quality information about the biological function of each gene. To (2000) emphasizes the power of high throughput analysis of differential gene expression in various areas of molecular cell biology.\n\nFinally, Cipriani (2011) describes a method for performing high-throughput RNAi-based screens in C. elegans in liquid in 96-well plates, demonstrating the feasibility of large-scale screening.\n\nIn conclusion, high-throughput genetic screens, whether through the use of CRISPR-Cas9, RNAi, or other methods, are indeed powerful tools in identifying individual gene function and interaction. They offer the potential for rapid, genome-wide annotation of gene function, and the ability to uncover the physical mechanisms behind genetic effects. However, as with any tool, they come with their own set of challenges that need to be addressed for optimal utilization."}, {"query": "competition public transport accessibility inequality poverty", "paper_list_string": "Sun 2021:\n\nTitle: Public transport availability inequalities and transport poverty risk across England\n\nAbstract: The general transit feed specification is becoming a popular data format for the publication of public transport schedules, making possible the collection of a nation-wide public transport schedule dataset, which enables monitoring of transit supply at an up-to-date and more precise level across a country than previously possible. In this paper, we use general transit feed specification data to measure local-scale public transport availability across England based on service frequency and spatial proximity to public transport stops/stations. Moreover, to demonstrate the usefulness of public transport availability measures, we examine inequalities of public transport provision and identify areas at risk of transport poverty across England. Furthermore, we estimate population (number of households) who are likely to suffer from transport poverty, accounting for public transport availability, time-based job accessibility by public transport or walking, household income and car ownership levels. Based on the criteria, we have used to identify public transport risk, we find that investment in the development of public transport services should prioritise West Midlands, East of England, South East and South West as those regions have more households who are likely to suffer from transport poverty. This paper contributes by (1) defining more comprehensive transit availability measures than existing measures at a variety of geography levels and (2) integrating fours aspects (i.e. public transport availability, job accessibility by public transport or walking, household income and car availability) to analyse transport poverty comprehensively.\n\n==\n\nTiznado-Aitken 2021:\n\nTitle: Public transport accessibility accounting for level of service and competition for urban opportunities: An equity analysis for education in Santiago de Chile\n\nAbstract: Abstract Several cities around the world have changed their transportation planning paradigm, understanding that the prime goal is to provide access to opportunities for everyone. To address this goal, public transport plays a fundamental role and, therefore, it is key for developing a sustainable and equitable city. This paper proposes a methodology to analyze access to opportunities through public transport incorporating the user's valuation of attributes that impact the level of service on his/her trip and the competitiveness for urban opportunities. Using data from Santiago, Chile, we applied the proposed methodology to analyze accessibility to higher-quality public primary schools. We compare total travel time (TTT) with a proposed measure of total generalized travel time (TGTT) using simple potential and competitive accessibility indicators, accounting for the subjective valuation of walking time, travel time, waiting time, comfort and transfers, and translating them into in-vehicle time units. We find that the inclusion of competition has a more substantial impact than including the subjective valuation of the level of service in the accessibility to educational opportunities. Using competitive measures with TGTT, we found that around 20% of the zones in Santiago have at least a 50% deficit of higher-quality public education, and 71% of them are in peripheral areas. Furthermore, these zones, where medium and low-income population usually lives, can experience, on average, 1\u20132 transfers, 4\u20135 passengers per square meter, and 15-min waiting. We conclude that the proposed methodology provides a more comprehensive way to understand accessibility by incorporating the traveling experience, allowing to determine how and where to intervene to effectively improve accessibility, with a focus on urban equity.\n\n==\n\nPathak 2017:\n\nTitle: Public transit access and the changing spatial distribution of poverty\n\nAbstract: Abstract This article examines whether access to public transportation plays a significant role in determining the spatial distribution of poverty in a metropolitan area. Our empirical strategy relies on long-term changes in poverty and access to bus transit at the neighborhood level in the Atlanta metropolitan area. We estimate the effect of bus transit access on poverty using fixed effects models to control for time-invariant unobservable characteristics. Furthermore, we undertake several robustness checks using a combination of instrumental variable regression, subsample analysis, and propensity score matching. Our results indicate that, on average, after controlling for neighborhood characteristics, census tracts with better access to public bus transportation have a higher proportion of low-income households \u2013 in both the central city and the suburbs. Thus, policies that improve access to transit in underserved areas can plausibly expand residential opportunities for the poor and reduce spatial inequities in urban centers.\n\n==\n\nMart\u00ednez 2017:\n\nTitle: Creating inequality in accessibility: The relationships between public transport and social housing policy in deprived areas of Santiago de Chile\n\nAbstract: This paper identifies the very limited connectivity provided by the current public transport system to the most deprived groups of Santiago de Chile, and explores the territorial aspects of transport and social housing policies that have contributed to the creation of unequal public transport schemes. To achieve those aims, we present a review of public policies in Chile and the results of an original quantitative analysis that measures the travel times required to access the opportunities and activities located in the city. The results show that housing policies put people at a disadvantage by increasing the distance between them and the opportunities of the city. Three decades after the implementation of housing policies, transport still fails to mitigate these distances and instead of alleviating the patterns of segregation, it may have reinforced them. The travel times required increase towards the periphery (even though densities do not decrease) and are higher than the averages of the city in social housing estates.\n\n==\n\nKelobonye 2020:\n\nTitle: Measuring the accessibility and spatial equity of urban services under competition using the cumulative opportunities measure\n\nAbstract: Abstract As accessibility becomes an increasingly relevant concept in the analysis of sustainable transport and urban development, the accuracy of accessibility measures becomes increasingly vital. While more complex measures are gradually gaining popularity with increasing data and computational resources, policy makers and planners are still prone to opt for less complex methods that are easy to use and interpret. The cumulative opportunities measure is the most widely applied accessibility measure in planning practice, but it is also among the least accurate due to its lack of consideration of the impact of competition for those opportunities. This study seeks to highlight the impact of addressing competition for different urban services in the cumulative opportunities measure. A competition component is added to the measure, which is applied to a case study of three types of urban services in the Perth metropolitan area; jobs, primary/secondary education and shopping. The results show that considering competition changes the spatial patterns of accessibility and its equity. Since this approach reveals demand-supply imbalances, it can more accurately determine spatial inequalities in accessibility, and hence increases the utility of the cumulative opportunities measure. We also find that the three services had varying levels and spatial patterns of accessibility and spatial equity, thus relying on any single one of them for assessing spatial structural performance can be misleading.\n\n==\n\nScheurer 2017:\n\nTitle: Spatial accessibility of public transport in Australian cities: Does it relieve or entrench social and economic inequality?\n\nAbstract: City planning in Australian cities has seen a gradual shift in approach, away from planning to facilitate mobility by car in the post-war period toward planning for land-use/public transport integration. By assessing the supply of public transport for city accessibility, a considerable variation within each city can be seen. Of interest is the extent to which there is a relationship between the quality of public transport accessibility and the spatial distribution of socioeconomic advantage and disadvantage. This paper examines this issue by mapping spatial data on socioeconomic disadvantage and advantage against indicators of public transport accessibility. The findings show that Australian cities are characterized by a significant level of spatially manifested socioeconomic inequality exacerbated by transport disadvantage. It is argued that a coincidence of public transport infrastructure and service improvements as well as urban intensification and housing affordability policies are required to counteract these trends.\n\n==\n\nHern\u00e1ndez 2017:\n\nTitle: Uneven mobilities, uneven opportunities: Social distribution of public transport accessibility to jobs and education in Montevideo\n\nAbstract: Even though mobility is a requirement for participation in \u201cmodern life\u201d, the truth is that mobility is unevenly distributed and it constitutes a field of contestation and dispute among social classes. Mobility does not derive from individual decisions or free choices. On the contrary, it is the result of the interaction between individual attributes and social structure. To grasp this interaction, it is necessary to go well beyond observed mobility. This paper explores the unequal access to urban opportunities among different social classes in Montevideo. It does so by computing potential public transport accessibility to two types of crucial opportunities: jobs and education. The paper findings show an unequal distribution of potential mobility, especially for jobs and upper level public education. Primary public schools are an exception, revealing the spatial footprint of the mature Uruguayan social protection network at this level. This paper's approach allows to identify and describe various fields of contestation such as urban form, transit network, the state provision of public goods (in the case of education), through examining the effects of uneven mobility on social goods access and, as a result, on social equity.\n\n==\n\nMohammadi 2018:\n\nTitle: Optimising public transport for reducing employment barriers and fighting poverty\n\nAbstract: Alleviating poverty in low-income and developing nations is integral to social stability, attracting investments and generating employment opportunities which in turn elevate people\u2019s well-being. Employment could be encouraged through a combination of direct (travel time and training) and indirect measures (social institutions like childcare and elder care). Other factors such as legal reform and infrastructure services could help as well. Transit is perhaps the first element (but not the only one) required to provide better access to the labor market, to health and educational facilities and to social institutions. Transit should be cheap, fast, safe, and secure to reach to most travelers within the influence area. The main objective of this research is to propose a method to fight poverty through better access to employment by a proactive cost-effective planning of investments in existing and future public transit systems. A decision-making system is developed to assess the current employment situation in different geographical regions considering unemployment rate, access to jobs and public transportation systems. Real data from a case study of the Costa Rica metropolitan area is used to illustrate the applicability of the proposed approach. The results show that the proposed model can lead governments to a cost-effective solution that decreases the employment barrier index by more than 50% during the first 5 years. The proposed model will be beneficial for transit agencies in charge of BRT, Tramway, and suburban trains.", "final_summary": "The intricate interplay between public transport accessibility, competition, inequality, and poverty is a multifaceted issue. The papers collectively suggest that public transport accessibility significantly influences socioeconomic inequalities and poverty levels.\n\nSun (2021) underscores that insufficient public transport availability can intensify transport poverty, particularly in regions like West Midlands, East of England, South East, and South West. Tiznado-Aitken (2021) further emphasizes the crucial role of public transport in providing equitable access to urban opportunities, such as education, in Santiago de Chile.\n\nPathak (2017) highlights the impact of public transport access on the spatial distribution of poverty, with areas having better access to public bus transportation tending to house a higher proportion of low-income households. This suggests that enhancing public transport access in underserved areas could potentially mitigate spatial inequities.\n\nMart\u00ednez (2017) and Hern\u00e1ndez (2017) both discuss the role of public transport in creating or reinforcing spatial and social inequalities. In Santiago de Chile, housing policies that increase the distance between residents and city opportunities, coupled with inadequate public transport, have reinforced patterns of segregation (Mart\u00ednez, 2017). In Montevideo, uneven access to public transport has resulted in unequal access to jobs and education, particularly for different social classes (Hern\u00e1ndez, 2017).\n\nKelobonye (2020) emphasizes the importance of considering competition in measuring the accessibility of urban services. The study found that considering competition can reveal demand-supply imbalances and more accurately determine spatial inequalities in accessibility. Scheurer (2017), while not directly addressing competition, found that Australian cities are characterized by significant spatially manifested socioeconomic inequality exacerbated by transport disadvantage.\n\nLastly, Mohammadi (2018) proposes a proactive, cost-effective planning of investments in public transit systems as a method to fight poverty through better access to employment. The study found that such an approach could significantly decrease the employment barrier index.\n\nIn conclusion, these papers collectively suggest that public transport accessibility plays a significant role in shaping socioeconomic inequalities and poverty levels. Improving public transport accessibility and considering competition for urban services, as suggested by Kelobonye (2020), could potentially alleviate these inequalities and reduce poverty levels. However, this requires a comprehensive and proactive approach that considers the complex interplay between public transport, competition, inequality, and poverty."}, {"query": "which classifications are suitable for support vector machines?", "paper_list_string": "Mayoraz 1999:\n\nTitle: Support Vector Machines for Multi-class Classification\n\nAbstract: Support vector machines (SVMs) are primarily designed for 2-class classification problems. Although in several papers it is mentioned that the combination of K SVMs can be used to solve a K-class classification problem, such a procedure requires some care. In this paper, the scaling problem of different SVMs is highlighted. Various normalization methods are proposed to cope with this problem and their efficiencies are measured empirically. This simple way of ssing SVMs to learn a K-class classification problem consists in choosing the maximum applied to the outputs of K SVMs solving a one-per-class decomposition of the general problem. In the second part of this paper, more sophisticated techniques are suggested. On the one hand, a stacking of the K SVMs with other classification techniques is proposed. On the other end, the one-per-class decomposition scheme is replaced by more elaborated schemes based on error-correcting codes. An incremental algorithm for the elaboration of pertinent decomposition schemes is mentioned, which exploits the properties of SVMs for an efficient computation.\n\n==\n\nYing 2001:\n\nTitle: The Mechanism of Classification for Support Vector Machines\n\nAbstract: The support vector machine is a novel type of learning technique, based on statistical learning theory, which uses Mercer kernels for efficiently performing computations in high dimensional spaces. In pattern recognition, the support vector algorithm constructs nonlinear decision functions by training a classifier to perform a linear separation in some high dimensional space which is nonlinearly related to input space.\n\n==\n\nAwad 2015:\n\nTitle: Support Vector Machines for Classification\n\nAbstract: This chapter covers details of the support vector machine (SVM) technique, a sparse kernel decision machine that avoids computing posterior probabilities when building its learning model. SVM offers a principled approach to problems because of its mathematical foundation in statistical learning theory. SVM constructs its solution in terms of a subset of the training input. SVM has been extensively used for classification, regression, novelty detection tasks, and feature reduction. This chapter focuses on SVM for supervised classification tasks only, providing SVM formulations for when the input space is linearly separable or linearly nonseparable and when the data are unbalanced, along with examples. The chapter also presents recent improvements to and extensions of the original SVM formulation. A case study concludes the chapter.\n\n==\n\nJunli 2000:\n\nTitle: Classification mechanism of support vector machines\n\nAbstract: The purpose of this paper is to provide an introductory tutorial on the basic ideas behind support vector machines (SVM). The paper starts with an overview of structural risk minimization (SRM) principle, and describes the mechanism of how to construct SVM. For a two-class pattern recognition problem, we discuss in detail the classification mechanism of SVM in three cases of linearly separable, linearly nonseparable and nonlinear. Finally, for nonlinear case, we give a new function mapping technique: By choosing an appropriate kernel function, the SVM can map the low-dimensional input space into the high dimensional feature space, and construct an optimal separating hyperplane with maximum margin in the feature space.\n\n==\n\nDeisenroth 2020:\n\nTitle: Classification with Support Vector Machines\n\nAbstract: Support Vector Machines (SVMs) perform pattern recognition between two points classes by finding a decision surface determined by certain points the training set, termed Support Vectors (SV).\n\n==\n\nAbe 2010:\n\nTitle: Support Vector Machines for Pattern Classification\n\nAbstract: A guide on the use of SVMs in pattern classification, including a rigorous performance comparison of classifiers and regressors. The book presents architectures for multiclass classification and function approximation problems, as well as evaluation criteria for classifiers and regressors. Features: Clarifies the characteristics of two-class SVMs; Discusses kernel methods for improving the generalization ability of neural networks and fuzzy systems; Contains ample illustrations and examples; Includes performance evaluation using publicly available data sets; Examines Mahalanobis kernels, empirical feature space, and the effect of model selection by cross-validation; Covers sparse SVMs, learning using privileged information, semi-supervised learning, multiple classifier systems, and multiple kernel learning; Explores incremental training based batch training and active-set training methods, and decomposition techniques for linear programming SVMs; Discusses variable selection for support vector regressors.\n\n==\n\nGunn 1998:\n\nTitle: Support Vector Machines for Classification and Regression\n\nAbstract: The foundations of Support Vector Machines (SVM) have been developed by Vapnik and are gaining popularity due to many attractive features, and promising empirical performance. The formulation embodies the Structural Risk Minimisation (SRM) principle, which in our work has been shown to be superior to traditional Empirical Risk Minimisation (ERM) principle employed by conventional neural networks. SRM minimises an upper bound on the VC dimension (generalisation error), as opposed to ERM which minimises the error on the training data. It is this difference which equips SVMs with a greater ability to generalise, which is our goal in statistical learning. SVMs were developed to solve the classification problem, but recently they have been extended to the domain of regression problems.\n\n==\n\nHermes 2000:\n\nTitle: Feature selection for support vector machines\n\nAbstract: In the context of support vector machines (SVM), high dimensional input vectors often reduce the computational efficiency and significantly slow down the classification process. In this paper, we propose a strategy to rank individual components according to their influence on the class assignments. This ranking is used to select an appropriate subset of the features. It replaces the original feature set without significant loss in classification accuracy. Often, the generalization ability of the classifier even increases due to the implicit regularization achieved by feature pruning.", "final_summary": "Support Vector Machines (SVMs) are a versatile tool in the realm of machine learning, capable of handling a variety of classification tasks. The papers collectively provide insights into the suitable classifications for SVMs.\n\nSVMs were originally designed for binary or 2-class classification problems. They can be extended to multi-class classification problems through the combination of multiple SVMs, but this requires careful consideration and the use of various normalization methods and more sophisticated techniques (Mayoraz 1999).\n\nSVMs are effective in pattern recognition tasks, constructing nonlinear decision functions (Ying 2001). They are capable of operating in high dimensional spaces, a feature that is particularly useful in pattern recognition tasks (Junli 2000).\n\nAwad (2015) and Deisenroth (2020) highlight the SVM's proficiency in supervised classification tasks, constructing solutions based on a subset of the training input. SVMs can handle both linearly separable and non-separable input spaces, making them a flexible tool for classification tasks (Awad 2015, Junli 2000).\n\nAbe (2010) discusses the use of SVMs in pattern classification, providing insights into their use in this area. SVMs can also be used for regression problems, as discussed by Gunn (1998), who emphasizes the SVM's superior ability to generalize, a key goal in statistical learning.\n\nLastly, Hermes (2000) discusses the use of SVMs in feature selection, proposing a strategy to rank individual components according to their influence on class assignments. This approach can improve the computational efficiency of SVMs and potentially increase their generalization ability.\n\nIn conclusion, SVMs are suitable for a wide range of classifications, including binary and multi-class classification, pattern recognition, supervised classification tasks, regression problems, and feature selection. Their ability to operate in high dimensional spaces and their superior generalization ability make them a versatile tool in machine learning."}, {"query": "tramsport fare prices environmental impacts climate change", "paper_list_string": "Cano 2021:\n\nTitle: Sustainable tramway, techno-economic analysis and environmental effects in an urban public transport. A comparative study\n\nAbstract: Abstract The problem of climate change because greenhouse gas emissions is aggravating, especially in public transport, which encourages the development of new technologies and clean energy control methods for the propulsion of vehicles such as tramways. A new energy control for a real tramway has been proposed in this paper, combining renewable sources, supercapacitors and lithium ion batteries, both components will absorb the energy from the regenerative braking of the tramway. The system has been modeled in Matlab considering certain restrictions in each component in order to supply the load on the round trip. Finally, a techno-economic and environmental analysis has been done identifying new patterns with respect to existing tramway systems. The annual energy required by the tramway is 867.62 MWh/year. The power variations are mainly supplied by the supercapacitor and the lithium ion battery functions as a backup. In this regard, the proposed system saves $ 2205,724 by supplying energy to the tramway and selling the excess energy to the grid for 20 years. Finally, the renewable system will have avoided 8,445.4 tCO2/MWh.\n\n==\n\nWang 2019:\n\nTitle: Green Tramp Shipping Routing and Scheduling: Effects of Market-Based Measures on CO2 Reduction\n\nAbstract: In this chapter we examine, from a tramp ship operator\u2019s point of view, how potential CO2 emission reduction measures impact the operational decisions and their economic and environmental consequences. Two market-based measures (MBMs) are discussed, the bunker levy scheme and the emission trading scheme, and we show that both can be incorporated in a similar way into a typical tramp ship routing and scheduling model. We also demonstrate with a computational study the environmental benefits of these CO2 reduction schemes.\n\n==\n\nChoia 2022:\n\nTitle: Calculating the Environmental Benefits of Trams\n\nAbstract: The Net-Zero standard was proposed by Science Based Target (SBTi) to bring the net greenhouse gas emissions to zero as global warming intensifies, and this policy is spreading worldwide. Public transportation plays an important role in eco-friendly transportation and establishing a railway-oriented public transportation system is important. Among modes of railway traffic, trams are easy to access compared with subways (a representative modes of railway transportation) and are economical because of their low construction and operation costs. If a priority signal is given to the tram operation, the scheduled speed increases; The efficiency can be further improved. The purpose of this study was to analyse how the conversion of modes to public transportation caused by tram construction can affect the atmosphere and to study how much the increase in physical activity caused by the increase in public transportation affects the reduction of disease. Dongtan New Town in Korea, where trams are scheduled to be introduced, was set as the study area, and the effect of the conversion of modes of transportation resulting from tram construction was analysed through the modal split process of the four-stage transportation demand prediction model. The analysis shows that trams will generate a 54,700 trips/d conversion to public transportation within the affected area. The benefit from air pollution reduction is 25.13\n\n==\n\nProost 2009:\n\nTitle: Will a radical transport pricing reform jeopardize the ambitious EU climate change objectives\n\nAbstract: This paper examines the effects of replacing current fuel taxes by a system of taxes that account better for all the different external costs of the different transport modes. One of the important implications of this reform is that current fuel taxes are decreased to a level of 80 euro/ton of CO2 but that the mileage related taxes on car and truck use increase. Using the TREMOVE model for the transport sector of 31 European countries, one finds that the volume of transport will decrease because current taxes on transport are too low compared to overall external costs. Overall CO2 emissions will decrease slightly. Using the MARKAL-TIMES model for the Belgian energy sector, putting all sectors and technologies on equal footing shows that a fuel tax reform makes that it is not cost efficient to require large CO2 emission reductions in the transport sector and that traditional car technologies will continue to dominate the car market in 2020-2030.\n\n==\n\nZilka 2021:\n\nTitle: Sustainability evaluation of the use of cargo-trams for mixed municipal waste transport in Prague.\n\nAbstract: The use of trams for the transport of waste from urban areas to waste incineration facilities - does it make sense, would it be technically feasible, economically sustainable and environmentally beneficial? These are the questions which we attempted to answer in this comprehensive study. The analysis is performed for a specific potential implementation of this system in Prague and adjacent municipalities. In this work, we compare the current state, where mixed municipal waste is transported to an incineration plant directly by garbage trucks, with variants if mixed municipal waste were taken to transfer stations and from there transported over a longer distance in a large volume by means of tram or truck. Our results show that use of trams results in an overall cost level of \u20ac16.41 per ton of waste transported, which represents a slight saving against the existing system at a cost of \u20ac17.19 per ton. From the purely economic perspective, however, this does not compete with transportation by truck at \u20ac12.28 per ton - above all due to high initial investment into new cargo trams. From the environmental viewpoint, deploying trams brings benefits largely on the local level, where emissions would be reduced by about 50% against the current state. In the global view, assessed through the Life Cycle Assessment method, it appears however only to transfer emissions to the point of energy production, and from this perspective the use of trams is beneficial only if linked with a notable shift from fossil fuel energy to nuclear or renewable sources.\n\n==\n\nKoetse 2009:\n\nTitle: The impact of climate change and weather on transport: An overview of empirical findings\n\nAbstract: This paper presents a survey of the empirical literature on the effects of climate change and weather conditions on the transport sector. Despite mixed evidence on many issues, several patterns can be observed. On a global scale especially shifts in tourism and agricultural production due to increased temperatures may lead to shifts in passenger and freight transport. The predicted rise in sea levels and the associated increase in frequency and intensity of storm surges and flooding incidences may furthermore be some of the most worrying consequences of climate change, especially for coastal areas. Climate change related shifts in weather patterns might also cause infrastructure disruptions. Clear patterns are that precipitation affects road safety by increasing accident frequency but decreasing severity. Precipitation also increases congestion, especially during peak hours. Furthermore, an increased frequency of low water levels may considerably increase costs of inland waterway transport. Despite these insights, the net impact of climate change on generalised costs of the various transport modes are uncertain and ambiguous, with a possible exception for inland waterway transport.\n\n==\n\nNanaki 2017:\n\nTitle: Environmental assessment of 9 European public bus transportation systems\n\nAbstract: The transportation sector is one of the largest sources of EU\u2019s greenhouse gas emissions. In 2011, transportation represented approximately 25 percent of total EU\u2019s greenhouse gas emissions. Urban mobility accounts for 40% of all CO2 emissions of road transport and up to 70% of other pollutants from transport. As, transportation and mobility play a crucial part both in urban economics and the quality of life, it is of great significance to ensure a low carbon transportation sector, so as to deal with the threat that climate change poses to urban areas. This study examines the factors that affect the production of carbon dioxide (CO2) as well as of air pollutants, in 9 major European cities, aiming to provide a comprehensive overview of the actual knowledge on the atmospheric pollution from public transportation systems. CO2 emissions as well as air pollutants, such as CO, HC, PM, NOx are calculated for the diesel and CNG bus fleets of the European cities under study. Finally the environmental benefits, in terms of CO2 and CO, HC, PM, NOx emissions reductions, from the penetration of different biodiesel blends (from B10 to B100) to the bus fleets are estimated.\n\n==\n\nBorken-Kleefeld 2010:\n\nTitle: Specific climate impact of passenger and freight transport.\n\nAbstract: Emissions of short-lived species contribute significantly to the climate impact of transportation. The magnitude of the effects varies over time for each transport mode. This paper compares first the absolute climate impacts of current passenger and freight transportation. Second, the impacts are normalized with the transport work performed and modes are compared. Calculations are performed for the integrated radiative forcing and mean temperature change, for different time horizons and various measures of transport work. An unambiguous ranking of the specific climate impact can be established for freight transportation, with shipping and rail having lowest and light trucks and air transport having highest specific impact for all cases calculated. Passenger travel with rail, coach or two- and three-wheelers has on average the lowest specific climate impact also on short time horizons. Air travel has the highest specific impact on short-term warming, while on long-term warming car travel has an equal or higher impact per passenger-kilometer.", "final_summary": "The collective body of research presents a nuanced picture of the environmental impacts of transport fare prices and climate change. \n\nCano (2021) highlights the potential of sustainable tramways, which combine renewable energy sources and energy-saving technologies, to significantly reduce greenhouse gas emissions and save costs. Similarly, Wang (2019) discusses the potential of market-based measures to reduce CO2 emissions in tramp shipping, suggesting that operational changes can have both economic and environmental benefits.\n\nChoia (2022) emphasizes the environmental benefits of trams, particularly in terms of air pollution reduction, and suggests that increased use of public transportation can contribute to global efforts to reduce greenhouse gas emissions. Proost (2009) also discusses the potential of transport pricing reform to reduce CO2 emissions, although the study suggests that significant reductions in the transport sector may not be cost-efficient.\n\nZilka (2021) presents a case study of using trams for waste transport in Prague, suggesting that while this approach may not be economically competitive with truck transport, it could significantly reduce local emissions. However, the study also notes that the environmental benefits of using trams are contingent on a shift from fossil fuel energy to nuclear or renewable sources.\n\nKoetse (2009) provides an overview of the potential impacts of climate change on transport, suggesting that shifts in tourism and agricultural production, as well as increased frequency of extreme weather events, could disrupt infrastructure and increase transport costs. Nanaki (2017) discusses the significant contribution of the transportation sector to EU greenhouse gas emissions, and suggests that low-carbon public transportation systems could help mitigate these impacts.\n\nFinally, Borken-Kleefeld (2010) compares the climate impacts of different modes of passenger and freight transport, suggesting that air travel has the highest specific impact on short-term warming, while car travel has an equal or higher impact per passenger-kilometer on long-term warming.\n\nIn conclusion, the research suggests that while transport fare prices and operational changes can have significant environmental impacts, these effects are complex and depend on a range of factors, including the specific mode of transport, the energy sources used, and broader shifts in climate patterns and policy."}, {"query": "How does inflammation increase the risk of cardiovascular disease in rheumatoid arthritis?", "paper_list_string": "Troelsen 2006:\n\nTitle: [Chronic inflammation increases the risk of cardiovascular disease in patients with rheumatoid arthritis].\n\nAbstract: Rheumatoid arthritis is associated with increased cardiovascular morbidity and mortality due to atherosclerosis. This cannot be explained by an increased presence of traditional risk factors but seems to depend on inflammatory mechanisms. The association of inflammatory pathways with atherosclerosis is complex, and more research is required to optimise preventative measures against cardiovascular complications in inflammatory rheumatic diseases.\n\n==\n\nChoy 2014:\n\nTitle: Cardiovascular risk in rheumatoid arthritis: recent advances in the understanding of the pivotal role of inflammation, risk predictors and the impact of treatment\n\nAbstract: Risk of cardiovascular (CV) disease is increased among RA patients. High inflammatory burden associated with RA appears to be a key driver of the increased cardiovascular risk. Inflammation is linked with accelerated atherosclerosis and associated with a paradoxical inversion of the relationship between CV risk and lipid levels in patients with untreated RA, recently coined the lipid paradox. Furthermore, the inflammatory burden is also associated with qualitative as well as quantitative changes in lipoproteins, with the anti-inflammatory and atheroprotective roles associated with high-density lipoprotein cholesterol significantly altered. RA therapies can increase lipid levels, which may reflect the normalization of lipids due to their inflammatory-dampening effects. However, these confounding influences of inflammation and RA therapies on lipid profiles pose challenges for assessing CV risk in RA patients and interpretation of traditional CV risk scores. In this review we examine the relationship between the increased inflammatory burden in RA and CV risk, exploring how inflammation influences lipid profiles, the impact of RA therapies and strategies for identifying and monitoring CV risk in RA patients aimed at improving CV outcomes.\n\n==\n\nW\u00e5llberg-Jonsson 1999:\n\nTitle: Extent of inflammation predicts cardiovascular disease and overall mortality in seropositive rheumatoid arthritis. A retrospective cohort study from disease onset.\n\nAbstract: OBJECTIVE\nTo identify predictors for cardiovascular disease (CVD) and for overall survival in patients with rheumatoid arthritis (RA) followed from disease onset.\n\n\nMETHODS\nA retrospective cohort of patients with seropositive RA and disease onset between 1974 and 1978 (n = 211) was followed up at the end of 1995. Potential predictors for CVD, as measured by \"the first cardiovascular event,\" and for overall survival were registered. The predictors were identified by extended Cox regression models.\n\n\nRESULTS\nIn simple Cox regression analysis, male sex, higher age at disease onset, HLA-B27, high disease activity, corticosteroid treatment early in disease, and hypertension significantly increased risk of cardiovascular event. Higher educational level, extensive disease modifying antirheumatic drug (DMARD) treatment, and corticosteroids > or =1 yr before event decreased the risk. In multiple Cox regression analysis, male sex, high age at disease onset, hypertension, higher haptoglobin level at disease onset, and corticosteroid treatment early in disease increased risk of CVD. In a multiple model comprising only patients with CVD, corticosteroids delayed the event. A high last registered erythrocyte sedimentation rate (ESR) value before event increased CVD risk, in particular when early in disease progression. Decreased life span was predicted by higher age at disease onset, male sex, low education level, high disease activity, hypertension, and CVD. HLA-B27 was associated with decreased life span, as was early, but not extensive corticosteroid treatment. DMARD treatment was associated with decreased mortality risk, as was the presence of joint prosthesis. In multiple regression, male sex, higher age at disease onset, atlantoaxial subluxation early in disease, hypertension, and cardiovascular event increased mortality. A high last registered ESR value before event or death added to that risk.\n\n\nCONCLUSION\nThe study emphasizes the importance of inflammation as an important risk indicator for CVD and mortality in RA. The positive impact of disease activity reducing treatment on CVD risk and survival is suggested.\n\n==\n\nSattar 2003:\n\nTitle: Explaining how \"high-grade\" systemic inflammation accelerates vascular risk in rheumatoid arthritis.\n\nAbstract: There is intense interest in mechanisms whereby low-grade inflammation could interact with conventional and novel vascular risk factors to promote the atheromatous lesion. Patients with rheumatoid arthritis (RA), who by definition manifest persistent high levels of inflammation, are at greater risk of developing cardiovascular disease. Mechanisms mediating this enhanced risk are ill defined. On the basis of available evidence, we argue here that the systemic inflammatory response in RA is critical to accelerated atherogenesis operating via accentuation of established and novel risk factor pathways. By implication, long-term suppression of the systemic inflammatory response in RA should be effective in reducing risk of coronary heart disease. Early epidemiological observational and clinical studies are commensurate with this hypothesis. By contrast, risk factor modulation with conventional agents, such as statins, may provide unpredictable clinical benefit in the context of uncontrolled systemic inflammatory parameters. Unraveling such complex relationships in which exaggerated inflammation-risk factor interactions are prevalent may elicit novel insights to effector mechanisms in vascular disease generally.\n\n==\n\nStevens 2005:\n\nTitle: Inflammation and atherosclerosis in rheumatoid arthritis\n\nAbstract: Rheumatoid arthritis (RA) associates with increased cardiovascular mortality. This appears to be predominantly due to ischaemic causes, such as myocardial infarction and congestive heart failure. The higher prevalence of cardiac ischaemia in RA is thought to be due to the accelerated development of atherosclerosis. There are two main reasons for this, which might be inter-related: the systemic inflammatory load, characteristic of RA; and the accumulation in RA of classical risk factors for coronary heart disease, which is reminiscent of the metabolic syndrome. We describe and discuss in the context of RA the involvement of local and systemic inflammatory processes in the development and rupture of atherosclerotic plaques, as well as the role of individual risk factors for coronary heart disease. We also present the challenges facing the clinical and scientific communities addressing this problem, which is receiving increasing attention.\n\n==\n\nKitas 2010:\n\nTitle: Cardiovascular disease in rheumatoid arthritis: state of the art and future perspectives\n\nAbstract: Rheumatoid arthritis is associated with an increased risk for cardiovascular events, such as myocardial infarction and stroke. Epidemiological evidence suggests that classic cardiovascular risk factors, such as hypertension, dyslipidaemia, insulin resistance and body composition alterations are important but not sufficient to explain all of the excess risk. High-grade systemic inflammation and its interplay with classic risk factors may also contribute. Some associations between classic risk factors and cardiovascular risk in people with rheumatoid arthritis appear counterintuitive but may be explained on the basis of biological alterations. More research is necessary to uncover the exact mechanisms responsible for this phenomenon, develop accurate systems used to identify patients at high risk, design and assess prevention strategies specific to this population of patients.\n\n==\n\nMyasoedova 2011:\n\nTitle: Lipid paradox in rheumatoid arthritis: the impact of serum lipid measures and systemic inflammation on the risk of cardiovascular disease\n\nAbstract: Objective To examine the impact of systemic inflammation and serum lipids on cardiovascular disease (CVD) in rheumatoid arthritis (RA). Methods In a population-based RA incident cohort (1987 American College of Rheumatology criteria first met between 1988 and 2007), details were collected of serum lipid measures, erythrocyte sedimentation rates (ESRs), C-reactive protein (CRP) measures and cardiovascular events, including ischaemic heart disease and heart failure. Cox models were used to examine the association of lipids and inflammation with the risk of CVD and mortality, adjusting for age, sex and year of RA incidence. Results The study included 651 patients with RA (mean age 55.8 years, 69% female); 67% were rheumatoid factor positive. ESR was associated with the risk of CVD (HR=1.2 per 10 mm/h increase, 95% CI 1.1 to 1.3). Similar findings, although not statistically significant, were seen with CRP (p=0.07). A significant non-linear association for total cholesterol (TCh) with risk of CVD was found, with 3.3-fold increased risk for TCh <4 mmol/l (95% CI 1.5 to 7.2) and no increased risk of CVD for TCh \u22654 mmol/l (p=0.57). Low low-density lipoprotein cholesterol (LDL <2 mmol/l) was associated with marginally increased risk of CVD (p=0.10); there was no increased risk for LDL \u22652 mmol/l (p=0.76). Conclusion Inflammatory measures (particularly, ESR) are significantly associated with the risk of CVD in RA. Lipids may have paradoxical associations with the risk of CVD in RA, whereby lower TCh and LDL levels are associated with increased cardiovascular risk.\n\n==\n\nGarc\u00eda-G\u00f3mez 2014:\n\nTitle: Inflammation, lipid metabolism and cardiovascular risk in rheumatoid arthritis: A qualitative relationship?\n\nAbstract: Life expectancy in patients with rheumatoid arthritis (RA) is reduced compared to the general population owing to an increase in cardiovascular diseases (CVD) not fully explained by traditional cardiovascular risk factors. In recent years, interest has been focused on the alterations in lipid metabolism in relation to chronic inflammation as one of the possible mechanisms involved in the pathogenesis of atherosclerosis of RA patients. Research regarding this issue has revealed quantitative alterations in lipoproteins during the acute-phase reaction, and has also demonstrated structural alterations in these lipoproteins which affect their functional abilities. Although many alterations in lipid metabolism have been described in this regard, these structural changes associated with inflammation are particularly important in high-density lipoproteins as they affect their cardioprotective functions. In this respect, excessive oxidation in low-density lipoprotein (LDL) and increased lipoprotein(a) with a predominance of smaller apolipoprotein(a) isoforms has also been reported. This article will discuss proinflammatory high-density lipoproteins (piHDL), oxidized LDL and lipoprotein(a). Elevated concentrations of these lipoproteins with marked pro-atherogenic properties have been observed in RA patients, which could help to explain the increased cardiovascular risk of these patients.", "final_summary": "Inflammation, a hallmark of rheumatoid arthritis (RA), has been identified as a key player in increasing the risk of cardiovascular disease (CVD) in RA patients. Troelsen (2006) and Choy (2014) both highlight the role of chronic inflammation in accelerating atherosclerosis, a major contributor to CVD. This inflammatory burden not only speeds up the atherosclerosis process but also influences lipid profiles, leading to a paradoxical relationship between cardiovascular risk and lipid levels in untreated RA patients, a phenomenon termed the \"lipid paradox\" (Choy, 2014; Myasoedova, 2011).\n\nThe extent of inflammation has been directly linked to CVD and overall mortality in RA patients (W\u00e5llberg-Jonsson, 1999). Sattar (2003) and Stevens (2005) further elaborate on this, suggesting that high-grade systemic inflammation in RA interacts with conventional and novel vascular risk factors to promote atherogenesis. This implies that suppressing the systemic inflammatory response could potentially reduce the risk of coronary heart disease in RA patients (Sattar, 2003).\n\nHowever, the relationship between inflammation and CVD in RA is complex. Kitas (2010) and Garc\u00eda-G\u00f3mez (2014) note that while traditional cardiovascular risk factors like hypertension and dyslipidemia are important, they are not sufficient to explain the increased cardiovascular risk in RA. The interplay between high-grade systemic inflammation and these risk factors may contribute to the increased risk. Furthermore, Garc\u00eda-G\u00f3mez (2014) points out that inflammation-related alterations in lipid metabolism and lipoprotein structure could affect their functional abilities, further increasing cardiovascular risk.\n\nIn conclusion, the papers collectively suggest that inflammation, a characteristic feature of RA, plays a pivotal role in increasing the risk of CVD in RA patients. This is achieved through accelerating atherosclerosis, influencing lipid profiles, and interacting with traditional cardiovascular risk factors. However, the exact mechanisms underlying this relationship are complex and require further research."}, {"query": "In project management research, time horizon is to the duration of a study's data collection period or the length of time over which data is analyzed (Wong et al., 2021). Different time horizons can be employed when studying employee engagement and organizational performance.", "paper_list_string": "Ancona 2001:\n\nTitle: Time: A New Research Lens\n\nAbstract: The article discusses the issue of time as it pertains to organizational research. The author believes that looking at research in terms of time is a powerful tool in assessing organizational phenomena. According to the author, temporal research allows researchers to gain more perspective when looking at organizational issues such as decision making, group performance and organizational transformation. The author notes that the field of temporal research is translated into concepts including pacing, timing and sequencing.\n\n==\n\nSonnentag 2012:\n\nTitle: Time in organizational research: Catching up on a long neglected topic in order to improve theory\n\nAbstract: Time should be an important aspect of organizational theory, but it has been neglected for decades. In this essay, I describe four different ways in which time can be conceptualized in organizational research. I give examples from recent theoretical work that include a temporal perspective. I also specify some directions about how future theory development and refinement should incorporate time.\n\n==\n\nBasu 2019:\n\nTitle: Choosing a Time Horizon in Cost and Cost-effectiveness Analyses.\n\nAbstract: When designing a comparative outcomes or a cost-effectiveness analysis, the time horizon defining the duration of time for outcomes assessment must be carefully considered. The time horizon must be long enough to capture the intended and unintended benefits and harms of the intervention(s).1,2 In some instances, the time horizon should extend beyond the duration of a clinical trial when a specific end point is measured, whereas in other instances modeling outcomes over a longer period is unnecessary. Using a longer time horizon than is necessary may add unnecessary cost and complexity to the cost-effectiveness analysis model. In the May 2017 issue of JAMA Ophthalmology, Wittenborn et al3 examined costs and effectiveness of home-based macular degeneration monitoring systems using a lifetime horizon in a cost-effectiveness analysis and a 10-year horizon in a budget impact analysis. The rationale for selection of time horizons and their implications for interpreting the research is reviewed in this JAMA Guide to Statistics and Methods article.\n\n==\n\nBergh 1993:\n\nTitle: Watch the Time Carefully: The Use and Misuse of Time Effects in Management Research\n\nAbstract: This article assesses how management researchers have used, and in some cases misused, time effects in their research designs, data analyses, and interpretations. Drawing from a content analysis of studies appearing in the Academy of Management Journal and Administrative Science Quarterly over the years 1988-1992, five different approaches for defining and operationalizing time effects are identified. Evaluation within and across these five approaches reveals three recurring patterns of \u201ctime series errors. \u201d The implications of these \u201cerrors \u201cfor studies with, and on time are provided.\n\n==\n\nRuneson 2014:\n\nTitle: Get the cogs in synch: time horizon aspects of industry--academia collaboration\n\nAbstract: In industry--academia collaboration projects, there are many issues related to different time horizons in industry and academia. If not adressed upfront, they may hinder collaboration in such projects. We analyze our experiences from a 10 year industry--academia collaboration program, the EASE Industrial Excellence Center in Sweden, and identify issues and feasible practices to overcome the hurdles of different time horizons. Specifically, we identify issues related to contracts, goals, results, organization (in)stability, and work practices. We identify several areas where the time horizon is different, and conclude that mutual awareness of these differences and management commitment to the collaboration are the key means to overcome the differences. The launch of a mediating institute may also be part of the solution.\n\n==\n\nLee 1999:\n\nTitle: Time in Organizational Studies: Towards a New Research Direction\n\nAbstract: While there is much `time-related research', there is little `research on time'. This is striking since time is a key point in understanding organizations, their actions, culture, efficacy, etc. Most studies of time in management and organizational theory take time for granted. While there are numerous studies that address temporal issues, they are widely dispersed and unsystematic. This paper provides a classification of temporal studies of organizations and management. The scheme is built around two criteria: concepts of time and the role of time in research design. In the former, there are two contrasting concepts of time: clock time and social time. In the latter, time plays the roles of independent or dependent variables. By intersecting the two criteria, four notions of temporality (`deciding', `working', `varying' and `changing' times) are introduced to account for a variety of studies of time. The resulting classification not only reveals the current situation of studies about time, but it also indicates a direction which further research effort should take. We conclude by showing that temporally sensitive approaches will benefit research on organizations.\n\n==\n\nReilly 2016:\n\nTitle: Time Horizon of Investments in the Resource Allocation Process\n\nAbstract: Corporate investment inherently relies on time horizon, as profits result from acquiring assets or developing capabilities that yield future benefits that exceed upfront costs. Despite the importance of time horizon to understanding resource allocation, knowledge about the subject has accumulated slowly. Our review therefore encompasses insights from several research streams that partially address the subject even though time horizon is not the central construct in any of them. We aim to clarify key constructs related to time horizon, organize prior research about the antecedents of time horizon, explain the implications of several theoretical traditions for time horizon, and detail the range of measures that have been used to capture time horizon empirically. By focusing narrowly on this topic but searching broadly for references, we provide integrative summaries of existing research and identify opportunities for new and unique research.\n\n==\n\nStritch 2017:\n\nTitle: Minding the Time: A Critical Look at Longitudinal Design and Data Analysis in Quantitative Public Management Research\n\nAbstract: Public management scholars are looking to longitudinal research designs and data to help overcome the many limitations associated with cross-sectional research. However, far less attention has been given to time itself as a research lens for scholars to consider. This article seeks to integrate time as a construct of theoretical importance into a discussion of longitudinal design, data, and public management research. First, I discuss the relative advantages of longitudinal design and data, but also the challenges, limitations, and issues researchers need to consider. Second, I consider the importance of time as a theoretical construct of interest in the pursuit of longitudinal public management research. Third, I offer a brief look at the use of longitudinal design and panel data analyses in the current public management literature. The overview demonstrates a notable absence of public management research considering the attitudes, motives, perceptions, and experiences of individual public employees and managers. Finally, I consider why there are so few longitudinal studies of public employees and point out the issues public management researchers interested in individual employee-level phenomena need to consider when advancing their own longitudinal research designs.", "final_summary": "The concept of time horizon in project management research is a multifaceted and complex topic that has been explored by numerous scholars. It pertains to the duration of a study's data collection period or the length of time over which data is analyzed (Ancona, 2001; Sonnentag, 2012; Basu, 2019; Bergh, 1993; Runeson, 2014; Lee, 1999; Reilly, 2016; Stritch, 2017).\n\nAncona (2001) and Sonnentag (2012) both emphasize the importance of time as a research lens in organizational studies, suggesting that it can provide valuable insights into various organizational phenomena. Basu (2019) further elaborates on the need for careful consideration when choosing a time horizon, as it must be long enough to capture all intended and unintended benefits and harms of the intervention(s) being studied.\n\nBergh (1993) warns of the potential misuse of time effects in research designs, data analyses, and interpretations, highlighting the need for careful operationalization of time effects. Runeson (2014) discusses the challenges posed by different time horizons in industry-academia collaboration projects, suggesting that mutual awareness and management commitment can help overcome these hurdles.\n\nLee (1999) provides a classification of temporal studies of organizations and management, emphasizing the need for temporally sensitive approaches in organizational research. Reilly (2016) discusses the importance of time horizon in understanding resource allocation, suggesting that it can provide valuable insights into corporate investment strategies.\n\nFinally, Stritch (2017) integrates time as a construct of theoretical importance into a discussion of longitudinal design and data in public management research. However, the specific focus of the paper is not solely on the advantages and challenges of longitudinal research designs, but also on the importance of time as a theoretical construct in the pursuit of longitudinal public management research.\n\nIn conclusion, the time horizon is a critical aspect of project management research that can provide valuable insights into various organizational phenomena. However, it must be carefully considered and appropriately operationalized to avoid potential misuse and to ensure the validity and reliability of the research findings (Ancona, 2001; Sonnentag, 2012; Basu, 2019; Bergh, 1993; Runeson, 2014; Lee, 1999; Reilly, 2016; Stritch, 2017)."}, {"query": "What is the codex alimentarius recommendation for second-age infant flour?", "paper_list_string": "Koletzko 2012:\n\nTitle: Compositional Requirements of Follow-Up Formula for Use in Infancy: Recommendations of an International Expert Group Coordinated by the Early Nutrition Academy\n\nAbstract: The follow-up formula (FUF) standard of Codex Alimentarius adopted in 1987 does not correspond to the recently updated Codex infant formula (IF) standard and current scientific knowledge. New Zealand proposed a revision of the FUF Codex standard and asked the non-profit Early Nutrition Academy, in collaboration with the Federation of International Societies for Paediatric Gastroenterology, Hepatology, and Nutrition (FISPGHAN), for a consultation with paediatric nutrition experts to provide scientific guidance. This global expert group strongly supports breastfeeding. FUF are considered dispensable because IF can substitute for breastfeeding throughout infancy, but FUF are widely used and thus the outdated current FUF standard should be revised. Like IF, FUF serve as breast milk substitutes; hence their marketing should respect appropriate standards. The compositional requirements for FUF for infants from 6 months onwards presented here were unanimously agreed upon. For some nutrients, the compositional requirements for FUF differ from those of IF due to differing needs with infant maturation as well as a rising contribution of an increasingly diversified diet with advancing age. FUF should be fed with adequate complementary feeding that is also appropriate for partially breastfed infants. FUF could be fed also after the age of 1 year without safety concerns, but different compositional requirements should be applied for optimal, age-adapted milk-based formulations for young children used only after the age of 1 year. This has not been considered as part of this review and should be the subject of further consideration.\n\n==\n\nKoletzko 2010:\n\nTitle: Global standard for the composition of infant formula: recommendations of an ESPGHAN coordinated international expert group.\n\nAbstract: The Codex Alimentarius Commission of the Food and Agriculture Organization of the United Nations (FAO) and the World Health Organization (WHO) develops food standards, guidelines and related texts for protecting consumer health and ensuring fair trade practices globally. The major part of the world's population lives in more than 160 countries that are members of the Codex Alimentarius. The Codex Standard on Infant Formula was adopted in 1981 based on scientific knowledge available in the 1970s and is currently being revised. As part of this process, the Codex Committee on Nutrition and Foods for Special Dietary Uses asked the ESPGHAN Committee on Nutrition to initiate a consultation process with the international scientific community to provide a proposal on nutrient levels in infant formulae, based on scientific analysis and taking into account existing scientific reports on the subject. ESPGHAN accepted the request and, in collaboration with its sister societies in the Federation of International Societies on Pediatric Gastroenterology, Hepatology and Nutrition, invited highly qualified experts in the area of infant nutrition to form an International Expert Group (IEG) to review the issues raised. The group arrived at recommendations on the compositional requirements for a global infant formula standard which are reported here.\n\n==\n\nZlotkin 2010:\n\nTitle: The Role of the Codex Alimentarius Process in Support of New Products to Enhance the Nutritional Health of Infants and Young Children\n\nAbstract: The Codex Alimentarius is a collection of internationally recognized standards, codes of practice, guidelines, and other recommendations relating to foods, food production, and food safety. Among other functions, it is responsible for setting international standards for safety and hygiene. Codex food standards and guidelines directed at foods produced primarily for young infants and children have important implications for maintaining nutritional status and health, especially given the positioning of these products as components of established World Health Organization (WHO)/UNICEF-recommended feeding strategies. Recently, new products targeted at this age group (e.g., lipid-based nutrient supplements and micronutrient powders) have been produced and used, but these are not totally covered under existing Codex guidelines or standards. The objective of this paper is to review the role of the Codex process and specifically to suggest revisions to existing Codex guidelines on formulated complementary foods (Guidelines for Formulated Supplementary Foods for Older Infants and Young Children, CAC/GL 08\u20131991) to encompass this new category of fortified complementary foods and home fortificants. In reviewing the existing guidelines, potential areas for revision included the sections on the recommended nutrients in these foods and their intended use. Updating the Codex guidelines provides the opportunity to encourage production and use of new products for children and help ensure that such foods, when used as directed, do not interfere with breastfeeding. The revised guidelines would help governments develop national regulations covering all forms of formulated complementary foods. They would also lessen impediments to international trade by providing clear guidance for foods used in feeding programs and for young children, particularly in developing countries.\n\n==\n\nDossa 2011:\n\nTitle: Evaluation of the suitability and acceptability of a newly designed infant flour for infant feeding in the district of Bopa in south of Benin\n\nAbstract: Infant feeding practices do not always fit with quantity and quality requirements,\u00a0leading to low expression of growth potential. In Benin, 43.1% of children less than6\u00a0months old are exclusively breastfed with 68% of children aged 6-8 months receiving\u00a0complementary food. The study aimed to produce infant flour from raw food\u00a0ingredients available in Bopa district and to test its acceptability by 6-12 months old children. In a first step of the study, formulation and determination of nutritional\u00a0characteristics of the infant flour occurred. A second step concerned acceptability\u00a0tests of gruel made from formulated infant flour. The study sample was composed of\u00a0sixty five mothers and their children. Children\ufffds acceptability test took place in the\u00a0morning for three consecutive days. The gruel was consumed ad libitum. Mothers\ufffd acceptability test consisted of appreciation of organoleptic characteristics of the gruel\u00a0and the infant flour processing.The infant flour was made of maize (65 %), bean (20\u00a0%) and peanut (15 %) and was manually processed. Chemical analysesshowed that it\u00a0contains 4.3% of moisture, 69.3% of carbohydrates, 15.1% of proteins, 10.7% of\u00a0lipids, less than 5% of crude fibres and 1.9% of ash. Its energy density (433.9 kcal/100g) was significantly greater than Codex Alimentarius standards (p<0.05).The\u00a0infant flour contained microbial germs up to 4.8log CFU/g which was closed to\u00a0maximum standard values. Total coliforms (1.7log CFU/g) were significantly lower\u00a0than standard values. The flour was yeast, mould and pathogen (Escherichia coli)\u00a0free. Hundred grams gruel was made from 40g of infant flour, 6g of malted maize and\u00a0250ml of stock of boiled greens leaves (Solanummacrocarpum). Dry matter content of\u00a0gruel was 19.2% and its energy density was 81.5 kcal/100g. Basedon the ratio of\u00a0intake and amount served, 83.3% of children accepted the gruel. However based on\u00a0the ratio of the amount of porridge consumed during the testto the amount usually\u00a0consumed by the children, 65.2% of the childrenaccepted the gruel. Mothers\ufffd appreciation of the gruel ranged from unpleasant to very pleasant with 40% as\u00a0pleasant. Sixty percent of mothers judged the infant flour processing as easy and\u00a0feasible. All mothers expressed their desire to feed their children with the gruel.\u00a0Improving nutritional status of their children motivated their decision. It is concluded\u00a0that integrating this infant flour in nutrition and counselling package targeted to\u00a0mothers may be of a great benefit to the children.\n\n==\n\nMacLean 2010:\n\nTitle: Upper levels of nutrients in infant formulas: Comparison of analytical data with the revised Codex infant formula standard\n\nAbstract: Abstract The Codex Alimentarius Commission adopted a revised standard for infant formula in 2007. This standard provides a regulatory framework for infant formula, including provisions for its essential composition. The recommendations for the essential composition specify minimum levels and either maximum values (MVs) or guidance upper levels (GULs) for 31 nutrients. As part of the revision process, the first cooperative survey of levels of nutrients in infant formulas was conducted by several global manufacturers. Whereas formulas met proposed minimum levels of all nutrients, 15 nutrients were identified whose levels were likely to exceed the proposed MV or GUL: vitamins A and K, thiamine, riboflavin, niacin, vitamin B 6 , folic acid, vitamin B 12 , vitamin C, iron, copper, manganese, potassium and iodine. Analytical data were collected for those nutrients from 21,385 batches of milk-based infant formula and 9070 batches of soy-based infant formula, whose total volumes were sufficient to feed more than 33 million infants for periods of three months. The number of batches analyzed ranged from 440 (vitamin K) to 27,920 (vitamin C). Of nutrients with an MV, only levels of vitamin A in some batches exceeded the maximum; no batch contained levels previously reported in the literature to be associated with adverse effects. There were several nutrients with GULs for which there were batches that exceeded the suggested upper limit. Data for some nutrients showed considerable variability, which related to form (liquid vs. powder), inherent levels of nutrients in formula ingredients, protein source, nutrient stability, analytical variability and effects of process, package and container size.\n\n==\n\nFao 2003:\n\nTitle: Codex alimentarius: food hygiene basic texts.\n\nAbstract: This publication contains guidance on the development and application of international food hygiene standards, which covers practices from primary production through to final consumption, highlighting key hygiene controls at each stage. It also contains guidance on the use and application of the Hazard Analysis and Critical Control Point (HACCP) system to promote food safety, as well as principles for the establishment and application of microbiological criteria for foods and the conduct of microbiological assessment.\n\n==\n\nMiz\u00e9houn-Adissoda 2022:\n\nTitle: Household production and energy content of infant flours for children aged 6 to 11 months in two rural settings in southern Benin\n\nAbstract: Background: Homemade complementary foods which are prepared from staples by the mothers or caregivers are mostly used in Africa including Benin. These foods are not adequately enriched and hygienic conditions are sometimes poor. Aims: The aim was to describe household production methods of infant flours and to estimate their macronutrient content in Benin. Methods: From August to October 2020, 20 mothers of children aged 6 to 11 months were selected in the municipalities of Cov\u00e8 and Djakotomey to examine the process of domestic production of infant flours and porridges. Data were collected using a touch screen questionnaire and the energy content of the flours produced by the mothers was calculated and evaluated using the FAO/INFOODS food composition table for West Africa. Results: The main process units for the production of the infant flours were milling:100%, sorting: cereals 60% and legumes: 33%, roasting: cereals 70%, and legumes: 91.7%. Mothers had poor hygienic practices regarding Material: 20%, Manipulator: 20%, and Milieu: 35% during the preparation of infant porridges. Four of the 20 formulas produced had an acceptable energy content, two had a standard fat content (10 - 25 g/100 g flour) and 12 had a standard protein content (15 g/100 g flour). Conclusions: This study showed that the main production methods of infant flours need to be improved. More, the overall energy content from these flours was lower than the recommended macronutrients contents by the standard FAO/WHO/UNICEF and the standards references. It is therefore important to maintain and strengthen nutritional interventions in order to improve the nutritional quality of homemade infant flours.\nKeywords: Infant flour, household production, nutrition, hygiene.\n\n==\n\nMikiel-Kostyra 2000:\n\nTitle: [Foodstuffs intended for special dietary uses for infants and young children in the light of the directives of the European Union and standards of the Codex Alimentarius FAO/WHO].\n\nAbstract: Food products for infants and young children are included in the group of foodstuffs intended for special dietary uses. The classification of these products comprises the following groups: newborns infant formulas, infant formulas, processed cereal foods and other baby foods for infants and young children and gluten-free products. In the European Union countries the detailed requirements concerning their qualitative and quantitative composition, labeling, advertising and release on the market as well as the health quality of these products have been laid down in the Council Directive of 3 May 1989 on the approximation of the laws of the Member States relating to foodstuffs intended for particular nutritional uses (89/398/EEC with later amendments) and in the detailed directives. Presently, work is being continued on preparing standards of the Codex Alimentarius FAO/WHO for infant formulas for processed cereal foods for infants and young children and for gluten-free foods. In the successive sessions of the Codex Committee on Nutrition and Foods for Special Dietary Uses drafts of these standards are discussed. In the last session of the Committee in Berlin (19-23.06.2000) it was not possible to reach an agreement of views of the various delegations on the standards or their formulation. The work on them and trials at establishing international recommendations has been deferred to the next meeting.", "final_summary": "The Codex Alimentarius recommendations for second-age infant flour, also known as follow-up formula (FUF), are a topic of interest in the field of infant nutrition. The Codex Alimentarius, a collection of internationally recognized food standards, guidelines, and recommendations, plays a crucial role in ensuring the safety and nutritional adequacy of infant foods (Zlotkin 2010, Fao 2003).\n\nKoletzko (2012) highlights that the FUF standard of Codex Alimentarius, adopted in 1987, is outdated and does not align with the recently updated Codex infant formula standard and current scientific knowledge. The paper suggests that FUF should be revised to meet the differing nutritional needs of infants as they mature and start consuming a diversified diet. \n\nSimilarly, Koletzko (2010) emphasizes the need for a global infant formula standard that aligns with scientific analysis and existing scientific reports. The paper suggests that the Codex Alimentarius Commission should consider the nutrient levels in infant formulae in their revision process.\n\nZlotkin (2010) discusses the role of the Codex Alimentarius in supporting new products to enhance the nutritional health of infants and young children. The paper suggests revisions to existing Codex guidelines on formulated complementary foods to encompass new categories of fortified complementary foods and home fortificants.\n\nOn the other hand, Dossa (2011) and Miz\u00e9houn-Adissoda (2022) focus on the development and acceptability of infant flours in specific regions. They highlight the importance of integrating these infant flours into nutrition and counselling packages targeted at mothers for the benefit of the children. \n\nMacLean (2010) provides a comparison of the nutrient levels in infant formulas with the revised Codex infant formula standard. The paper identifies several nutrients whose levels are likely to exceed the proposed maximum value or guidance upper levels.\n\nIn conclusion, the Codex Alimentarius recommendations for second-age infant flour are a crucial aspect of infant nutrition. The current consensus among researchers is that these standards need to be updated to reflect current scientific knowledge and to accommodate new categories of fortified complementary foods. Furthermore, the integration of these standards into local food production and mother counselling packages is essential for the nutritional well-being of infants."}, {"query": "what are the effects of resistance exercise on the skeletal muscle proteome?", "paper_list_string": "Son 2012:\n\nTitle: THE EFFECT OF RESISTANCE AND ENDURANCE EXERCISE TRAINING ON MUSCLE PROTEOME EXPRESSION IN HUMAN SKELETAL MUSCLE\n\nAbstract: To investigate the effect of resistance and endurance training on muscle proteome expression, samples of vastus lateralis from 10 physically active young men were analysed by 2-dimensional electrophoresis (2-DE) and matrix-assisted laser desorption ionization time-of-flight mass spectrometry (MALDI-TOF MS). Differential patterns of protein expression were determined after 4 weeks of endurance or resistance exercise training. Following endurance exercise training, carbonic anhydrase III immunoglobulin heavy chain, myosin heavy chain 1, titin, chromosome 12, and fructose-1,6-bisphosphatase 2 were up-regulated while pyruvate kinase 3 isoform, ubiquitin carboxyl-terminal hydrolase, and phosphoglucomutase were down-regulated. After the 4 weeks of resistance exercise training, five proteins, apolipoprotein A-IV precursor, microtubule-actin cross linking factor 1, myosin light chain, growth hormone inducible transmembrane protein, and an unknown protein were up-regulated and pyruvate kinase 3 isoform, human albumin, and enolase 3 were down-regulated. We conclude that endurance and resistance exercise training differently alter the expression of individual muscle proteins, and that the response of muscle protein expression may be associated with specific myofibre adaptations to exercise training. Proteomic studies represent one of the developing techniques of metabolism which may substantially contribute to new insights into muscle and exercise physiology.\n\n==\n\nPetriz 2017:\n\nTitle: The Effects of Acute and Chronic Exercise on Skeletal Muscle Proteome\n\nAbstract: Skeletal muscle plasticity and its adaptation to exercise is a topic that is widely discussed and investigated due to its primary role in the field of exercise performance and health promotion. Repetitive muscle contraction through exercise stimuli leads to improved cardiovascular output and the regulation of endothelial dysfunction and metabolic disorders such as insulin resistance and obesity. Considerable improvements in proteomic tools and data analysis have broth some new perspectives in the study of the molecular mechanisms underlying skeletal muscle adaptation in response to physical activity. In this sense, this review updates the main relevant studies concerning muscle proteome adaptation to acute and chronic exercise, from aerobic to resistance training, as well as the proteomic profile of natural inbred high running capacity animal models. Also, some promising prospects in the muscle secretome field are presented, in order to better understand the role of physical activity in the release of extracellular microvesicles and myokines activity. Thus, the present review aims to update the fast\u2010growing exercise\u2010proteomic scenario, leading to some new perspectives about the molecular events under skeletal muscle plasticity in response to physical activity. J. Cell. Physiol. 232: 257\u2013269, 2017. \u00a9 2016 Wiley Periodicals, Inc.\n\n==\n\nPadr\u00e3o 2016:\n\nTitle: Uncovering the exercise\u2010related proteome signature in skeletal muscle\n\nAbstract: Exercise training has been recommended as a nonpharmacological strategy for the prevention and attenuation of skeletal muscle atrophy in distinct pathophysiological conditions. Despite the well\u2010established phenotypic alterations, the molecular mechanisms underlying exercise\u2010induced skeletal muscle remodeling are poorly characterized. Proteomics based on mass spectrometry have been successfully applied for the characterization of skeletal muscle proteome, representing a pivotal approach for the wide characterization of the molecular networks that lead to skeletal muscle remodeling. Nevertheless, few studies were performed to characterize the exercise\u2010induced proteome remodeling of skeletal muscle, with only six research papers focused on the cross\u2010talk between exercise and pathophysiological conditions. In order to add new insights on the impact of distinct exercise programs on skeletal muscle proteome, molecular network analysis was performed with bioinformatics tools. This analysis highlighted an exercise\u2010related proteome signature characterized by the up\u2010regulation of the capacity for ATP generation, oxygen delivery, antioxidant capacity and regulation of mitochondrial protein synthesis. Chronic endurance training up\u2010regulates the tricarboxylic acid cycle and oxidative phosphorylation system, whereas the release of calcium ion into cytosol and amino acid metabolism are the biological processes up\u2010regulated by a single bout of exercise. Other issues as exercise intensity, load, mode and regimen as well as muscle type also influence the exercise\u2010induced proteome signature. The comprehensive analysis of the molecular networks modulated by exercise training in health and disease, taking in consideration all these variables, might not only support the therapeutic effect of exercise but also highlight novel targets for the development of enhanced pharmacological strategies.\n\n==\n\nBuford 2009:\n\nTitle: Resistance exercise-induced changes of inflammatory gene expression within human skeletal muscle\n\nAbstract: Aberrant local inflammatory signaling within skeletal muscle is now considered a contributing factor to the development of sarcopenia. Recent evidence indicates that chronic resistance training contributes to the control of locally derived inflammation via adaptations to repeated, acute increases in pro-inflammatory mRNA within muscle. However, only a limited number of gene transcripts related to the inflammatory process have been examined in the literature. The present study utilized an acute bout to examine the effects of resistance exercise on several inflammatory-related genes in 24 physically active, post-menopausal women not currently undergoing hormone replacement therapy. Following a standard warm-up, participants completed a lower-body resistance exercise bout consisting of 3 sets of 10 repetitions on machine squat, leg press, and leg extension exercises (80% intensity). Muscle biopsies were obtained from the vastus lateralis of the dominant leg at baseline and 3\u00a0h following exercise. Significant (p\u00a0<\u00a00.05) up-regulation in mRNA content was observed for TNF\u03b1, IL1\u03b2, IL6, IL8, SOCS2, COX2, SAA1, SAA2, IKKB, cfos, and junB. Muscle mRNA content was not significantly altered at the 0.05 level for IL2, IL5, IL10, or IL12 (p35). Venous blood samples were also obtained at baseline as well as at 3, 24, and 48\u00a0h post-exercise. Serum was analyzed for circulating TNF\u03b1, IL1\u03b2, IL6, IL8, COX2, and SAA with no significant changes observed. These results indicate that resistance exercise is capable of up-regulating transcription of numerous inflammatory mediators within skeletal muscle, and these appear to be worthy of future examination in chronic studies.\n\n==\n\nYuan 2013:\n\nTitle: Proteomic Analysis of Skeletal Muscle in Insulin-Resistant Mice: Response to 6-Week Aerobic Exercise\n\nAbstract: Aerobic exercise has beneficial effects on both weight control and skeletal muscle insulin sensitivity through a number of specific signaling proteins. To investigate the targets by which exercise exerts its effects on insulin resistance, an approach of proteomic screen was applied to detect the potential different protein expressions from skeletal muscle of insulin-resistant mice after prolonged aerobic exercise training and their sedentary controls. Eighteen C57BL/6 mice were divided into two groups: 6 mice were fed normal chow (NC) and 12 mice were fed high-fat diet (HFD) for 10 weeks to produce an IR model. The model group was then subdivided into HFD sedentary control (HC, n\u200a=\u200a6) and HFD exercise groups (HE, n\u200a=\u200a6). Mice in HE group underwent 6 weeks of treadmill running. After 6 weeks, mice were sacrificed and skeletal muscle was dissected. Total protein (n\u200a=\u200a6, each group) was extracted and followed by citrate synthase, 2D proteome profile analysis and immunoblot. Fifteen protein spots were altered between the NC and HC groups and 23 protein spots were changed between the HC and HE groups significantly. The results provided an array of changes in protein abundance in exercise-trained skeletal muscle and also provided the basis for a new hypothesis regarding the mechanism of exercise ameliorating insulin resistance.\n\n==\n\nBurniston 2008:\n\nTitle: Changes in the rat skeletal muscle proteome induced by moderate-intensity endurance exercise.\n\nAbstract: The adaptation of skeletal muscle to endurance exercise has not previously been investigated using proteomic techniques. Such work could improve our understanding and generate novel information regarding the effects of exercise. Plantaris muscles were investigated from rats exercised on treadmills at 70-75% peak oxygen uptake (V O(2)peak) for 30 min, 4 days per week for 5 weeks or sedentary controls. Analysis of 2-D gels matched 187 spots across control and exercised muscles and 80 proteins corresponding to 40 gene products were identified by MALDI-ToF MS. Exercise increased the animals' V O(2)peak by 14% and altered the expression of 15 spots consistent with a shift from glycolysis toward greater fatty-acid oxidation. The majority of differentially expressed gene products were present as multi-spot series of similar M(r) but different pI. Mitochondrial aconitase focused to 5 spots, 2 spots (pI 7.6 and 7.7) decreased (57%) whereas the pI 8.0 spot increased (51%) and was found to contain protein carbonyls. This adaptation may be related to exercise-induced oxidative stress and translocation of aconitase to mitochondrial DNA. In conclusion, proteomic techniques simultaneously demonstrated well-established effects, and identified novel changes not previously associated with the adaptation of muscle to exercise.\n\n==\n\nDeshmukh 2016:\n\nTitle: Proteomics of Skeletal Muscle: Focus on Insulin Resistance and Exercise Biology\n\nAbstract: Skeletal muscle is the largest tissue in the human body and plays an important role in locomotion and whole body metabolism. It accounts for ~80% of insulin stimulated glucose disposal. Skeletal muscle insulin resistance, a primary feature of Type 2 diabetes, is caused by a decreased ability of muscle to respond to circulating insulin. Physical exercise improves insulin sensitivity and whole body metabolism and remains one of the most promising interventions for the prevention of Type 2 diabetes. Insulin resistance and exercise adaptations in skeletal muscle might be a cause, or consequence, of altered protein expressions profiles and/or their posttranslational modifications (PTMs). Mass spectrometry (MS)-based proteomics offer enormous promise for investigating the molecular mechanisms underlying skeletal muscle insulin resistance and exercise-induced adaptation; however, skeletal muscle proteomics are challenging. This review describes the technical limitations of skeletal muscle proteomics as well as emerging developments in proteomics workflow with respect to samples preparation, liquid chromatography (LC), MS and computational analysis. These technologies have not yet been fully exploited in the field of skeletal muscle proteomics. Future studies that involve state-of-the-art proteomics technology will broaden our understanding of exercise-induced adaptations as well as molecular pathogenesis of insulin resistance. This could lead to the identification of new therapeutic targets.\n\n==\n\nPsilander 2003:\n\nTitle: Resistance exercise alters MRF and IGF-I mRNA content in human skeletal muscle.\n\nAbstract: Increasing evidence suggests that the myogenic regulatory factors (MRFs) and IGF-I have important roles in the hypertrophy response observed after mechanical loading. We, therefore, hypothesized that a bout of heavy-resistance training would affect the MRF and IGF-I mRNA levels in human skeletal muscle. Six male subjects completed four sets of 6-12 repetitions on a leg press and knee extensor machine separated by 3 min. Myogenin, MRF4, MyoD, IGF-IEabc (isoforms a, b, and c) and IGF-IEbc (isoform b and c) mRNA levels were determined in the vastus lateralis muscle by RT-PCR before exercise, immediately after, and 1, 2, 6, 24, and 48 h postexercise. Myogenin, MyoD, and MRF4 mRNA levels were elevated (P < 0.005) by 100-400% 0-24 h postexercise. IGF-IEabc mRNA content decreased (P < 0.005) by approximately 44% after 1 and 6 h of recovery. The IGF-IEbc mRNA level was unaffected. The present study shows that myogenin, MyoD, and MRF4 mRNA levels are transiently elevated in human skeletal muscle after a single bout of heavy-resistance training, supporting the idea that the MRFs may be involved in regulating hypertrophy and/or fiber-type transitions. The results also suggest that IGF-IEa expression may be downregulated at the mRNA level during the initial part of recovery from resistance exercise.", "final_summary": "Resistance exercise significantly impacts the skeletal muscle proteome, leading to substantial alterations in protein expression and function. Son (2012) discovered that both endurance and resistance exercise training resulted in differential expression of individual muscle proteins, suggesting that the type of exercise may influence the specific adaptations in the muscle proteome. Petriz (2017) also emphasized the role of both acute and chronic exercise in muscle proteome adaptation, with aerobic and resistance training having distinct effects.\n\nPadr\u00e3o (2016) further highlighted the role of exercise in modulating the skeletal muscle proteome. The study found that chronic endurance training up-regulates the tricarboxylic acid cycle and oxidative phosphorylation system, while a single bout of exercise enhances the release of calcium ion into the cytosol and amino acid metabolism. This suggests that the duration and intensity of exercise can significantly influence the proteomic response.\n\nBuford (2009) specifically highlighted the role of resistance exercise in up-regulating the transcription of numerous inflammatory mediators within skeletal muscle, suggesting a potential mechanism through which exercise may influence muscle adaptation. On the other hand, Yuan (2013) focused on the effects of aerobic exercise and found similar up-regulation of inflammatory mediators.\n\nBurniston (2008) also found that endurance exercise altered the expression of several proteins, indicating a shift from glycolysis towards greater fatty-acid oxidation.\n\nDeshmukh (2016) and Psilander (2003) both emphasized the role of myogenic regulatory factors (MRFs) and IGF-I in muscle adaptation to resistance exercise. Psilander (2003) found that a single bout of heavy-resistance training elevated the mRNA levels of myogenin, MyoD, and MRF4, supporting the idea that these factors may regulate muscle hypertrophy and/or fiber-type transitions.\n\nIn conclusion, resistance exercise has a significant impact on the skeletal muscle proteome, leading to alterations in protein expression and function that may underlie the adaptations seen in muscle in response to exercise. The specific changes observed can vary depending on the type, duration, and intensity of exercise, highlighting the complexity of the proteomic response to exercise (Son, 2012; Petriz, 2017; Padr\u00e3o, 2016; Buford, 2009; Yuan, 2013; Burniston, 2008; Deshmukh, 2016; Psilander, 2003)."}, {"query": "What is the relationship between mirror neurons and autism in children?", "paper_list_string": "Ruysschaert 2014:\n\nTitle: Exploring the Role of Neural Mirroring in Children with Autism Spectrum Disorder\n\nAbstract: Investigating the underlying neural mechanisms of autism spectrum disorder (ASD) has recently been influenced by the discovery of mirror neurons. These neurons, active during both observation and execution of actions, are thought to play a crucial role in imitation and other social\u2010communicative skills that are often impaired in ASD. In the current electroencephalographic study, we investigated mu suppression, indicating neural mirroring in children with ASD between the ages of 24 and 48 months and age\u2010matched typically developing children, during observation of goal\u2010directed actions and non\u2010goal\u2010directed mimicked hand movements, as well as during action execution. Results revealed no significant group differences with significant central mu suppression in the ASD children and control children during both execution and observation of goal\u2010directed actions and during observation of hand movements. Furthermore, no significant correlations between mu suppression on one hand and quality of imitation, age, and social communication questionnaire scores on the other hand were found. These findings challenge the \u201cbroken mirror\u201d hypothesis of ASD, suggesting that impaired neural mirroring is not a distinctive feature of ASD. Autism Res 2014, 7: 197\u2013 206. \u00a9 2014 International Society for Autism Research, Wiley Periodicals, Inc.\n\n==\n\nRaymaekers 2009:\n\nTitle: EEG study of the mirror neuron system in children with high functioning autism\n\nAbstract: Individuals with Autism Spectrum Disorder (ASD) are characterised by an impaired imitation, thought to be critical for early affective, social and communicative development. One neurological system proposed to underlie this function is the mirror neuron system (MNS) and previous research has suggested a dysfunctional MNS in ASD. The EEG mu frequency, more precisely the reduction of the mu power, is considered to be an index for mirror neuron functioning. In this work, EEG registrations are used to evaluate the mirror neuron functioning of twenty children with high functioning autism (HFA) between 8 and 13 years. Their mu suppression to self-executed and observed movement is compared to typically developing peers and related to age, intelligence and symptom severity. Both groups show significant mu suppression to both self and observed hand movements. No group differences are found in either condition. These results do not support the hypothesis that HFA is associated with a dysfunctional MNS. The discrepancy with previous research is discussed in light of the heterogeneity of the ASD population.\n\n==\n\nDapretto 2006:\n\nTitle: Understanding emotions in others: mirror neuron dysfunction in children with autism spectrum disorders\n\nAbstract: To examine mirror neuron abnormalities in autism, high-functioning children with autism and matched controls underwent fMRI while imitating and observing emotional expressions. Although both groups performed the tasks equally well, children with autism showed no mirror neuron activity in the inferior frontal gyrus (pars opercularis). Notably, activity in this area was inversely related to symptom severity in the social domain, suggesting that a dysfunctional 'mirror neuron system' may underlie the social deficits observed in autism.\n\n==\n\nPerkins 2010:\n\nTitle: Mirror neuron dysfunction in autism spectrum disorders\n\nAbstract: Autism spectrum disorders (ASDs) are developmental conditions characterized by deficits in social interaction, verbal and nonverbal communication and obsessive/stereotyped patterns of behaviour. Although there is no reliable neurophysiological marker associated with ASDs, dysfunction of the parieto-frontal mirror neuron system has been suggested as a disturbance linked to the disorder. Mirror neurons (MNs) are visuomotor neurons which discharge both when performing and observing a goal directed action. Research suggests MNs may have a role in imitation, empathy, theory of mind and language. Although the research base is small, evidence from functional MRI, transcranial magnetic stimulation, and an electroencephalographic component called the mu rhythm suggests MNs are dysfunctional in subjects with ASD. These deficits are more pronounced when ASD subjects complete tasks with social relevance, or that are emotional in nature. Promising research has identified that interventions targeting MN related functions such as imitation can improve social functioning in ASDs. Boosting the function of MNs may improve the prognosis of ASDs, and contribute to diagnostic clarity.\n\n==\n\nEnticott 2012:\n\nTitle: Mirror Neuron Activity Associated with Social Impairments but not Age in Autism Spectrum Disorder\n\nAbstract: BACKGROUND\nThe neurobiology of autism spectrum disorder (ASD) is not particularly well understood, and biomedical treatment approaches are therefore extremely limited. A prominent explanatory model suggests that social-relating symptoms may arise from dysfunction within the mirror neuron system, while a recent neuroimaging study suggests that these impairments in ASD might reduce with age.\n\n\nMETHODS\nParticipants with autism spectrum disorder (i.e., DSM-IV autistic disorder or Asperger's disorder) (n = 34) and matched control subjects (n = 36) completed a transcranial magnetic stimulation study in which corticospinal excitability was assessed during the observation of hand gestures.\n\n\nRESULTS\nRegression analyses revealed that the ASD group presented with significantly reduced corticospinal excitability during the observation of a transitive hand gesture (relative to observation of a static hand) (p < .05), which indicates reduced putative mirror neuron system activity within ventral premotor cortex/inferior frontal gyrus. Among the ASD group, there was also a negative association between putative mirror neuron activity and self-reported social-relating impairments, but there was no indication that mirror neuron impairments in ASD decrease with age.\n\n\nCONCLUSIONS\nThese data provide general support for the mirror neuron hypothesis of autism; researchers now must clarify the precise functional significance of mirror neurons to truly understand their role in the neuropathophysiology of ASD and to determine whether they should be used as targets for the treatment of ASD.\n\n==\n\nWilliams 2001:\n\nTitle: Imitation, mirror neurons and autism\n\nAbstract: Various deficits in the cognitive functioning of people with autism have been documented in recent years but these provide only partial explanations for the condition. We focus instead on an imitative disturbance involving difficulties both in copying actions and in inhibiting more stereotyped mimicking, such as echolalia. A candidate for the neural basis of this disturbance may be found in a recently discovered class of neurons in frontal cortex, 'mirror neurons' (MNs). These neurons show activity in relation both to specific actions performed by self and matching actions performed by others, providing a potential bridge between minds. MN systems exist in primates without imitative and 'theory of mind' abilities and we suggest that in order for them to have become utilized to perform social cognitive functions, sophisticated cortical neuronal systems have evolved in which MNs function as key elements. Early developmental failures of MN systems are likely to result in a consequent cascade of developmental impairments characterised by the clinical syndrome of autism.\n\n==\n\nHamilton 2013:\n\nTitle: Reflecting on the mirror neuron system in autism: A systematic review of current theories\n\nAbstract: There is much interest in the claim that dysfunction of the mirror neuron system in individuals with autism spectrum condition causes difficulties in social interaction and communication. This paper systematically reviews all published studies using neuroscience methods (EEG/MEG/TMS/eyetracking/EMG/fMRI) to examine the integrity of the mirror system in autism. 25 suitable papers are reviewed. The review shows that current data are very mixed and that studies using weakly localised measures of the integrity of the mirror system are hard to interpret. The only well localised measure of mirror system function is fMRI. In fMRI studies, those using emotional stimuli have reported group differences, but studies using non-emotional hand action stimuli do not. Overall, there is little evidence for a global dysfunction of the mirror system in autism. Current data can be better understood under an alternative model in which social top-down response modulation is abnormal in autism. The implications of this model and future research directions are discussed.\n\n==\n\nFan 2010:\n\nTitle: Unbroken mirror neurons in autism spectrum disorders.\n\nAbstract: BACKGROUND\nThe 'broken mirror' theory of autism, which proposes that a dysfunction of the human mirror neuron system (MNS) is responsible for the core social and cognitive deficits in individuals with autism spectrum disorders (ASD), has received considerable attention despite weak empirical evidence.\n\n\nMETHODS\nIn this electroencephalographic study, we examined mu suppression, as an indicator of sensorimotor resonance, concurrent with oculomotor performance while individuals (n = 20) with ASD and control participants (n = 20) either executed hand actions or observed hand actions or a moving dot. No difference in visual attention between groups was found as indicated by fixation duration and normalized fixation number on the presented stimuli.\n\n\nRESULTS\nThe mu suppression over the sensorimotor cortex was significantly affected by experimental conditions, but not by group membership, nor by the interaction between groups and conditions. Individuals with ASD, similar to the controls, exhibited stronger mu suppression when watching hand actions relative to a moving dot. Notably, participants with ASD failed to imitate the observed actions while their mu suppression indicating the MNS activity was intact. In addition, the mu suppression during the observation of hand actions was positively associated with the communication competence of individuals with ASD.\n\n\nCONCLUSION\nOur study clearly challenges the broken mirror theory of autism. The functioning of the mirror neuron system might be preserved in individuals with ASD to a certain degree. Less mu suppression to action observation coupled with more communicational severity can reflect the symptom heterogeneity of ASD. Additional research needs to be done, and more caution should be used when reaching out to the media.", "final_summary": "The relationship between mirror neurons and autism in children is a topic of considerable debate, with research yielding mixed results. Mirror neurons, which activate during both the execution and observation of actions, have been proposed as a potential key to understanding autism spectrum disorder (ASD) (Williams, 2001).\n\nSeveral studies have challenged the \"broken mirror\" hypothesis, which suggests that a dysfunction in the mirror neuron system underlies the social and communicative deficits in ASD. Ruysschaert (2014) found no significant differences in neural mirroring between children with ASD and typically developing children. Similarly, Raymaekers (2009) found no evidence of a dysfunctional mirror neuron system in children with high functioning autism.\n\nHowever, other studies have found evidence supporting the mirror neuron dysfunction theory. Dapretto (2006) found that children with ASD showed no mirror neuron activity in the inferior frontal gyrus during the imitation and observation of emotional expressions. Perkins (2010) also suggested that mirror neuron dysfunction could be linked to ASD, particularly when tasks are social or emotional in nature.\n\nEnticott (2012) found reduced mirror neuron activity in participants with ASD, which was associated with social impairments. However, this study found no evidence that mirror neuron impairments decrease with age. Hamilton (2013) suggested that the data could be better understood under an alternative model where top-down response modulation is abnormal in ASD.\n\nFan (2010) also challenged the \"broken mirror\" theory, finding that individuals with ASD exhibited similar mu suppression (an indicator of mirror neuron activity) to controls when observing hand actions. However, these individuals failed to imitate the observed actions, suggesting a potential disconnect between mirror neuron activity and action imitation.\n\nIn conclusion, the relationship between mirror neurons and autism in children remains unclear, with studies providing both support for and against the \"broken mirror\" hypothesis. Further research is needed to fully understand the role of mirror neurons in ASD."}, {"query": "summary the current advance of elastography in obgyn field in the past 3 months", "paper_list_string": "Shao 2021:\n\nTitle: Advancements in the Application of Ultrasound Elastography in the Cervix.\n\nAbstract: Ultrasound elastography is a modern imaging technique that has developed rapidly in recent years. It enables objective measurement of tissue stiffness, a physical property intuitive to the human sense of touch. This novel technology has become a hotspot and plays a major role in scientific research and academic practice. Presently, ultrasound elastography has been used in the identification of benign and malignant tumors in superficial organs, such as breast and thyroid, providing clinically accurate diagnosis and treatment. The method has also been widely used for the liver, kidney, prostate, lymph nodes, blood vessels, skin and muscle system. In the application of cervical lesions, ultrasound elastography can distinguish normal cervix from abnormal cervix and differentiate benign from malignant lesions. It can significantly improve the diagnostic specificity for cervical cancer and is also useful for assessing infiltration depth and stage of cervical cancer, as well as predicting chemoradiotherapy treatment response. For cervical evaluation during pregnancy, ultrasound elastography is useful for assessing cervical softening and predicting premature delivery and outcome of induced labor. This article reviews the principles of ultrasound elastography as well as the current status and limitations in its application for cervical lesions and the cervix during pregnancy.\n\n==\n\nCui 2015:\n\nTitle: Endoscopic ultrasound elastography: Current status and future perspectives.\n\nAbstract: Elastography is a new ultrasound modality that provides images and measurements related to tissue stiffness. Endoscopic ultrasound (EUS) has played an important role in the diagnosis and management of numerous abdominal and mediastinal diseases. Elastography by means of EUS examination can assess the elasticity of tumors in the proximity of the digestive tract that are hard to reach with conventional transcutaneous ultrasound probes, such as pancreatic masses and mediastinal or abdominal lymph nodes, thus improving the diagnostic yield of the procedure. Results from previous studies have promised benefits for EUS elastography in the differential diagnosis of lymph nodes, as well as for assessing masses with pancreatic or gastrointestinal (GI) tract locations. It is important to mention that EUS elastography is not considered a modality that can replace biopsy. However, it may be a useful adjunct, improving the accuracy of EUS-fine needle aspiration biopsy (EUS-FNAB) by selecting the most suspicious area to be targeted. Even more, it may be useful for guiding further clinical management when EUS-FNAB is negative or inconclusive. In the present paper we will discuss the current knowledge of EUS elastography, including the technical aspects, along with its applications in the differential diagnosis between benign and malignant solid pancreatic masses and lymph nodes, as well as its aid in the differentiation between normal pancreatic tissues and chronic pancreatitis. Moreover, the emergent indication and future perspectives are summarized, such as the benefit of EUS elastography in EUS-guided fine needle aspiration biopsy, and its uses for characterization of lesions in liver, biliary tract, adrenal glands and GI tract.\n\n==\n\nSigrist 2017:\n\nTitle: Ultrasound Elastography: Review of Techniques and Clinical Applications\n\nAbstract: Elastography-based imaging techniques have received substantial attention in recent years for non-invasive assessment of tissue mechanical properties. These techniques take advantage of changed soft tissue elasticity in various pathologies to yield qualitative and quantitative information that can be used for diagnostic purposes. Measurements are acquired in specialized imaging modes that can detect tissue stiffness in response to an applied mechanical force (compression or shear wave). Ultrasound-based methods are of particular interest due to its many inherent advantages, such as wide availability including at the bedside and relatively low cost. Several ultrasound elastography techniques using different excitation methods have been developed. In general, these can be classified into strain imaging methods that use internal or external compression stimuli, and shear wave imaging that use ultrasound-generated traveling shear wave stimuli. While ultrasound elastography has shown promising results for non-invasive assessment of liver fibrosis, new applications in breast, thyroid, prostate, kidney and lymph node imaging are emerging. Here, we review the basic principles, foundation physics, and limitations of ultrasound elastography and summarize its current clinical use and ongoing developments in various clinical applications.\n\n==\n\nSarvazyan 2011:\n\nTitle: AN OVERVIEW OF ELASTOGRAPHY - AN EMERGING BRANCH OF MEDICAL IMAGING.\n\nAbstract: From times immemorial manual palpation served as a source of information on the state of soft tissues and allowed detection of various diseases accompanied by changes in tissue elasticity. During the last two decades, the ancient art of palpation gained new life due to numerous emerging elasticity imaging (EI) methods. Areas of applications of EI in medical diagnostics and treatment monitoring are steadily expanding. Elasticity imaging methods are emerging as commercial applications, a true testament to the progress and importance of the field.In this paper we present a brief history and theoretical basis of EI, describe various techniques of EI and, analyze their advantages and limitations, and overview main clinical applications. We present a classification of elasticity measurement and imaging techniques based on the methods used for generating a stress in the tissue (external mechanical force, internal ultrasound radiation force, or an internal endogenous force), and measurement of the tissue response. The measurement method can be performed using differing physical principles including magnetic resonance imaging (MRI), ultrasound imaging, X-ray imaging, optical and acoustic signals.Until recently, EI was largely a research method used by a few select institutions having the special equipment needed to perform the studies. Since 2005 however, increasing numbers of mainstream manufacturers have added EI to their ultrasound systems so that today the majority of manufacturers offer some sort of Elastography or tissue stiffness imaging on their clinical systems. Now it is safe to say that some sort of elasticity imaging may be performed on virtually all types of focal and diffuse disease. Most of the new applications are still in the early stages of research, but a few are becoming common applications in clinical practice.\n\n==\n\nDudea-Simon 2020:\n\nTitle: Elastography of the uterine cervix in gynecology: normal appearance, cervical intraepithelial neoplasia and cancer. A systematic review.\n\nAbstract: AIMS\nTo revise the current literature about the usefulness of elastography in cervical cancer (CC) and cervical intraepithelial neoplasia (CIN), from methods and technical limitations, to diagnosis, staging and the ability of predicting the response to oncologic treatment.\n\n\nMETHODS\nAn electronic database search was performed (PubMed, EMBASE, Web of Science) with the data range from January 2000 until May 2020. All studies, fully-available in English, assessing elastography of the uterine cervix in CC and CIN were selected. Studies were reviewed and discussed according to the elastographic technique and to the purpose of the research.\n\n\nRESULTS\nTwenty-three articles were found: 11 articles regarding strain elastography, 4 articles assessing shear wave elastography and 8 papers with matter-related information. Elastography was used in the study of normal variants of the uterine cervix as well as: the positive diagnosis of CC and CIN, clinical staging and the prediction of therapeutic response in CC. Comparison of the elastographic techniques was also performed.\n\n\nCONCLUSIONS\nElastography has multiple applications in the gynecological pathology of the cervix. The methods used to assess the cervix are diverse, and none have become universally accepted. With regard to CC and CIN, elastography is still an ongoing research field.\n\n==\n\nKim 2017:\n\nTitle: Elastographic measurement of the cervix during pregnancy: Current status and future challenges\n\nAbstract: The cervix is a cylindrical structure that is proximally connected to the uterus and distally to the vaginal cavity. The Bishop score has been used to evaluate the cervix during pregnancy. However, alternatives have been evaluated because the Bishop score is uncomfortable for patients, relies on a subjective examination, and lacks internal os data. Elastography has been used to assess the cervix, as it can estimate tissue stiffness. Recent articles on elastography for cervical assessment during pregnancy have focused on its usefulness for prediction of preterm birth and successful labor induction. There is a clinical need for cervical elastography, as an evaluation of biomechanical factors, because cervical length only assesses morphological changes. However, until now, cervical elastography has been studied in the limited field, and not shown a uniformed methodological technique. In this review, the current status, limitations, and future possibility of cervical elastography were discussed. Future studies should focus on overcoming the limitations of cervical elastography. Although the cervical elastography is presently an incompletely defined technique, it needs to be improved and evaluated as a method for use in combination with cervical length.\n\n==\n\nKozma 2021:\n\nTitle: Application of ultrasound elastography in obstetrics and gynecology\n\nAbstract: \u00d6sszefoglal\u00f3. Az ultrahang-elasztogr\u00e1fia az elm\u00falt \u00e9vek sor\u00e1n egyre n\u00f6vekv\u0151 figyelmet kapott a l\u00e1gysz\u00f6vetek elaszticit\u00e1s\u00e1nak vizsg\u00e1lat\u00e1ban. A m\u00f3dszer haszn\u00e1lat\u00e1t az teszi sz\u00fcks\u00e9gess\u00e9, hogy egyes, a mechanikai tulajdons\u00e1gaikban k\u00fcl\u00f6nb\u00f6z\u0151 sz\u00f6vetek hasonl\u00f3 echogenit\u00e1s\u00faak lehetnek, valamint hogy egy adott sz\u00f6vet megv\u00e1ltozott strukt\u00far\u00e1ja vagy mechanikai tulajdons\u00e1ga nem minden esetben j\u00e1r egy\u00fctt a sz\u00f6vet hagyom\u00e1nyos ultrahangk\u00e9p\u00e9nek megv\u00e1ltoz\u00e1s\u00e1val. Az elm\u00falt \u00e9vtizedben a deform\u00e1ci\u00f3s \u00e9s a ny\u00edr\u00e1si ultrahang-elasztogr\u00e1fia v\u00e1lt sz\u00e9les k\u00f6rben el\u00e9rhet\u0151v\u00e9. Ezen \u00faj k\u00e9palkot\u00e1si technika egyre nagyobb szerepet t\u00f6lt be a sz\u00fcl\u00e9szeti-n\u0151gy\u00f3gy\u00e1szati ultrahang-diagnosztik\u00e1ban is. A n\u0151gy\u00f3gy\u00e1szatban szerephez juthat az endometriosis \u00e9s az adenomyosis kimutat\u00e1s\u00e1ban, valamint a benignus \u00e9s a malignus cervicalis \u00e9s ovarialis k\u00e9pletek elk\u00fcl\u00f6n\u00edt\u00e9s\u00e9ben. A n\u0151gy\u00f3gy\u00e1szathoz hasonl\u00f3an a sz\u00fcl\u00e9szetben is jelent\u0151s v\u00e1ltoz\u00e1st hozhat az ultrahang-elasztogr\u00e1fia: alkalmas lehet a sz\u00fcl\u00e9sindukci\u00f3 sikeress\u00e9g\u00e9nek, a korasz\u00fcl\u00e9s bek\u00f6vetkez\u00e9s\u00e9nek \u00e9s a praeeclampsia kialakul\u00e1s\u00e1nak el\u0151rejelz\u00e9s\u00e9re. Orv Hetil. 2021; 162(18): 690-695. Summary. Ultrasound elastography has received significant attention for the assessment and measurement of soft tissue elastic properties in recent years. The advantage of ultrasound elastography lies in the fact that two different tissues can share similar echogenicities but may have other mechanical properties or, on the contrary, mechanical abnormalities of a designated tissue do not necessarily go hand in hand with an altered appearance on a conventional ultrasound image. In the last decade, strain and shear-wave elasticity imaging methods have become the most widely available among commercial ultrasound equipments. The importance of this new method expands rapidly also in the field of obstetrics and gynecology. Ultrasound elastography has a promising role in the diagnosis of endometriosis and adenomyosis and helps to differentiate benign and malignant cervical and ovarian lesions. The use in the prediction of the outcome of labor induction and preterm birth, and in the evaluation of preeclampsia are emerging. Orv Hetil. 2021; 162(18): 690-695.\n\n==\n\nOphir 1991:\n\nTitle: Elastography: A Quantitative Method for Imaging the Elasticity of Biological Tissues\n\nAbstract: We describe a new method for quantitative imaging of strain and elastic modulus distributions in soft tissues. The method is based on external tissue compression, with subsequent computation of the strain profile along the transducer axis, which is derived from cross-correlation analysis of pre- and post-compression A-line pairs. The strain profile can then be converted to an elastic modulus profile by measuring the stresses applied by the compressing device and applying certain corrections for the nonuniform stress field. We report initial results of several phantom and excised animal tissue experiments which demonstrate the ability of this technique to quantitatively image strain and elastic modulus distributions with good resolution, sensitivity and with diminished speckle. We discuss several potential clinical uses of this technique.", "final_summary": "Elastography, a modern imaging technique that measures tissue stiffness, has seen significant advancements in the field of obstetrics and gynecology (OBGYN) in recent months. Shao (2021) highlights the use of ultrasound elastography in identifying cervical abnormalities and differentiating between benign and malignant lesions. This technology has shown promise in improving diagnostic specificity for cervical cancer and predicting premature delivery outcomes.\n\nCui (2015) and Sigrist (2017) discuss the role of elastography in diagnosing abdominal and mediastinal diseases, including pancreatic masses and lymph nodes. They suggest that elastography can enhance the diagnostic yield of endoscopic ultrasound examinations, aiding in the differentiation between benign and malignant solid pancreatic masses and lymph nodes.\n\nDudea-Simon (2020) focuses on the application of elastography in gynecology, particularly in the study of cervical cancer and cervical intraepithelial neoplasia (CIN), among other applications in gynecological pathology. Kim (2017), on the other hand, discusses the use of elastographic measurement of the cervix during pregnancy, rather than specifically on CC and CIN.\n\nKozma (2021) emphasizes the promising role of ultrasound elastography in diagnosing endometriosis and adenomyosis, differentiating benign and malignant cervical and ovarian lesions, and predicting the outcome of labor induction and preterm birth. \n\nIn conclusion, the advancements in elastography in the OBGYN field have shown significant potential in improving diagnostic accuracy and predicting treatment outcomes. However, the diversity in methods used and the need for universal acceptance of these methods remain challenges to be addressed in future research."}, {"query": "Write about authenticity and credibility when conducting qualitative research.", "paper_list_string": "Brink 1993:\n\nTitle: Validity and reliability in qualitative research.\n\nAbstract: Validity and reliability are key aspects of all research. Meticulous attention to these two aspects can make the difference between good research and poor research and can help to assure that fellow scientists accept findings as credible and trustworthy. This is particularly vital in qualitative work, where the researcher\u2019s subjectivity can so readily cloud the interpretation of the data, and where research findings are often questioned or viewed with scepticism by the scientific community.\n\n==\n\nShenton 2004:\n\nTitle: Strategies for ensuring trustworthiness in qualitative research projects\n\nAbstract: Although many critics are reluctant to accept the trustworthiness of qualitative research, frameworks for ensuring rigour in this form of work have been in existence for many years. Guba\u2019s constructs, in particular, have won considerable favour and form the focus of this paper. Here researchers seek to satisfy four criteria. In addressing credibility, investigators attempt to demonstrate that a true picture of the phenomenon under scrutiny is being presented. To allow transferability, they provide sufficient detail of the context of the fieldwork for a reader to be able to decide whether the prevailing environment is similar to another situation with which he or she is familiar and whether the findings can justifiably be applied to the other setting. The meeting of the dependability criterion is difficult in qualitative work, although researchers should at least strive to enable a future investigator to repeat the study. Finally, to achieve confirmability, researchers must take steps to demonstrate that findings emerge from the data and not their own predispositions. The paper concludes by suggesting that it is the responsibility of research methods teachers to ensure that this or a comparable model for ensuring trustworthiness is followed by students undertaking a qualitative inquiry.\n\n==\n\nAmin 2020:\n\nTitle: Establishing trustworthiness and authenticity in qualitative pharmacy research.\n\nAbstract: Spurred by the value it can add, the use of qualitative research methods has been steadily growing by social pharmacy researchers around the globe, either separately or as part of mixed methods research projects. Given this increase, it is important to provide guidance to assist researchers in ensuring quality when employing such methods. This commentary addresses both theoretical fundamentals as well as practical aspects of establishing quality in qualitative social pharmacy research. More specifically, it provides an explanation of each of the criteria of trustworthiness proposed by Lincoln and Guba (credibility, transferability, dependability and confirmability) and different techniques used in establishing them. It also provides a brief overview of authenticity, a more recent and less widely used set of criteria that involve demonstrating fairness, ontological authenticity, educative authenticity, catalytic authenticity, and tactical authenticity. For each of these terms, the commentary provides a definition, how it applies to social pharmacy research, and guidance on when and how to use them. These are accompanied by examples from the pharmacy literature where the criteria have been used. The commentary ends by providing a summary of competing viewpoints of establishing quality in the published literature while inviting the reader to reflect on how the presented criteria would apply to different qualitative research projects.\n\n==\n\nPatton 1999:\n\nTitle: Enhancing the quality and credibility of qualitative analysis.\n\nAbstract: Varying philosophical and theoretical orientations to qualitative inquiry remind us that issues of quality and credibility intersect with audience and intended research purposes. This overview examines ways of enhancing the quality and credibility of qualitative analysis by dealing with three distinct but related inquiry concerns: rigorous techniques and methods for gathering and analyzing qualitative data, including attention to validity, reliability, and triangulation; the credibility, competence, and perceived trustworthiness of the qualitative researcher; and the philosophical beliefs of evaluation users about such paradigm-based preferences as objectivity versus subjectivity, truth versus perspective, and generalizations versus extrapolations. Although this overview examines some general approaches to issues of credibility and data quality in qualitative analysis, it is important to acknowledge that particular philosophical underpinnings, specific paradigms, and special purposes for qualitative inquiry will typically include additional or substitute criteria for assuring and judging quality, validity, and credibility. Moreover, the context for these considerations has evolved. In early literature on evaluation methods the debate between qualitative and quantitative methodologists was often strident. In recent years the debate has softened. A consensus has gradually emerged that the important challenge is to match appropriately the methods to empirical questions and issues, and not to universally advocate any single methodological approach for all problems.\n\n==\n\nJones 2013:\n\nTitle: Authenticity and scientific integrity in qualitative research.\n\nAbstract: Jacqueline Jones, PhD, RN, FRCNA, is an associate professor in the Division of Informatics, Health Systems & Leadership, College of Nursing University of Colorado, Aurora, CO. Scholarship for the care of women, childbearing families, and newborns demands attention to detail, creativity, and purpose in inquiry. A perplexing question is often raised by nurse scientists: Is qualitative research relevant today? A brief survey of a funding search engine, RePORTer, provided a resounding answer in the affirmative (1,470 hits with the term qualitative). Qualitative research makes a contribution to the contemporary development of health-related knowledge for individuals, families, communities, and populations. Qualitative research can help identify facilitators and barriers to intervention and can assist in the development of health care policy (Sandelowski & Leeman, 2012). It helps bring richness, context, and dimension to the study of human beings and their environments.\n\n==\n\nMorrow 2005:\n\nTitle: Quality and trustworthiness in qualitative research in counseling psychology.\n\nAbstract: This article examines concepts of the trustworthiness, or credibility, of qualitative research. Following a \u201cresearcher-as-instrument,\u201d or self-reflective, statement, the paradigmatic underpinnings of various criteria for judging the quality of qualitative research are explored, setting the stage for a discussion of more transcendent standards (those not associated with specific paradigms) for conducting quality research: social validity, subjectivity and reflexivity, adequacy of data, and adequacy of interpretation. Finally, current guidelines for writing and publishing qualitative research are reviewed, and strategies for conducting and writing qualitative research reports are suggested. Qualitative research, ensuing from a variety of disciplines, paradigms, and epistemologies, embraces multiple standards of quality, known variously as validity, credibility, rigor ,o rtrustworthiness. In addition to some standards that may be thought of as somewhat universal across disciplines and paradigms, the \u201cgoodness\u201d (Morrow & Smith, 2000) of qualitative inquiry is assessed on the basis of the paradigmatic underpinnings of the research and the standards of the discipline. Thus, a grounded theory study or a consensual qualitative research investigation in counseling psychology that is rooted in a postpositivist or constructivist/interpretivist paradigm will look quite different from a critical ethnography in education; and the standards appropriate for evaluating these studies will vary accordingly. I begin this article by addressing the paradigmatic underpinnings of trustworthiness or rigor in qualitative research. Next, I discuss central topics related to trustworthiness or validity that span paradigms and may be thought of as relevant across most research designs. I then provide an overview of guidelines that have been suggested for evaluating qualitative research, particularly in psychology. Finally, I offer recommendations for enhancing the quality of qualitative research in counseling psychology and suggest strategies for writing and publishing. First, however, in keeping with the standard of reflexivity as a way for researchers to inform their audiences about their perspectives as well as to manage their subjectivities, I describe my own assumptions about qualitative research methodology and quality.\n\n==\n\nNoble 2015:\n\nTitle: Issues of validity and reliability in qualitative research\n\nAbstract: Evaluating the quality of research is essential if findings are to be utilised in practice and incorporated into care delivery. In a previous article we explored \u2018bias\u2019 across research designs and outlined strategies to minimise bias.1 The aim of this article is to further outline rigour, or the integrity in which a study is conducted, and ensure the credibility of findings in relation to qualitative research. Concepts such as reliability, validity and generalisability typically associated with quantitative research and alternative terminology will be compared in relation to their application to qualitative research. In addition, some of the strategies adopted by qualitative researchers to enhance the credibility of their research are outlined.\n\nAssessing the reliability of study findings requires researchers and health professionals to make judgements about the \u2018soundness\u2019 of the research in relation to \u2026\n\n==\n\nBeck 1993:\n\nTitle: Qualitative Research: The Evaluation of Its Credibility, Fittingness, and Auditability\n\nAbstract: The three criteria of credibility, fittingness, and auditability have been focused on in the hope of facilitating the critique of qualitative research. Once criteria that are appropriate to qualitative methodologies are developed, the scientific merit of these research studies can truly be appreciated. If traditional scientific criteria relevant to quantitative studies are used to critique qualitative methods, the development and acceptance of this paradigm-transcending research will be hindered.", "final_summary": "Credibility and reliability are fundamental aspects of conducting qualitative research, ensuring that the findings are trustworthy (Brink, 1993). These aspects are particularly crucial in qualitative research, where the researcher's subjectivity can potentially cloud data interpretation (Brink, 1993).\n\nStrategies for ensuring trustworthiness in qualitative research include demonstrating a true picture of the phenomenon under scrutiny (credibility), providing sufficient context for the research (transferability), enabling future investigators to repeat the study (dependability), and demonstrating that findings emerge from the data and not from the researcher's predispositions (confirmability) (Shenton, 2004; Amin, 2020).\n\nAuthenticity, a less widely used set of criteria, involves demonstrating fairness, ontological authenticity, educative authenticity, catalytic authenticity, and tactical authenticity (Amin, 2020). \n\nThe quality and credibility of qualitative analysis can be enhanced by rigorous techniques and methods for gathering and analyzing data, including attention to validity, reliability, and triangulation (Patton, 1999). The credibility of the researcher and the philosophical beliefs of evaluation users also play a significant role in enhancing the quality of qualitative research (Patton, 1999).\n\nTrustworthiness in qualitative research can be achieved through social validity, subjectivity and reflexivity, adequacy of data, and adequacy of interpretation (Morrow, 2005). Evaluating the quality of research is essential for the findings to be utilized in practice and incorporated into care delivery (Noble, 2015).\n\nIn conclusion, credibility and reliability in qualitative research are achieved through a combination of rigorous methods, researcher credibility, and the application of appropriate criteria for trustworthiness. These aspects ensure that the findings of qualitative research are trustworthy (Beck, 1993)."}, {"query": "Genome mining for bioactive secondary metabolites from microbes", "paper_list_string": "Arulprakasam 2021:\n\nTitle: Genome mining of biosynthetic gene clusters intended for secondary metabolites conservation in actinobacteria.\n\nAbstract: Evolution of genome sequencing technology, on the one hand, and advancement of computational genome mining tools, on the other hand, paves way for improvement in predicting secondary metabolites. In past, numerous efforts were made concerning genome mining for recognizing secondary metabolites within the genus, but only a negligible quantity of comparative genomic reports had carried out among species of different genera. In this study, we explored potential of 24 actinobacteria species belonging to the genera, including Streptomyces, Nocardia, Micromonospora, and Saccharomonospora, to traverse diversity and distribution of Biosynthetic Gene Clusters (BGCs). Investigating results obtained from antiSMASH (Antibiotics and Secondary Metabolites Analysis Shell), NaPDoS (Natural Product Domain Seeker), and NP.searcher revealed conservation of genus-specific gene clusters among various species. E.g., NAGGN (n-acetyl glutaminyl glutamine amide) is present in Micromonospora, furan in Nocardia, melanin, and lassopeptide occur in Streptomyces. Bioactive compounds like alkyl-O-dihydro geranyl methoxy hydroquinone, SapB, desferrioxamine E, 2-Methylisoborneol, mayamycin, cyclodipeptide synthase, diisonitrile, salinichelin, hopene, ectoine and isorenieratene are highly conserved among diverse genera. Furthermore, pharmacological activity of actinobacterial derived metabolites against bacterial and fungal pathogens were illustrated. We need to accomplish large-scale analysis of natural products, including various genera of actinobacteria to deliver comprehensive intuition to overcome antibiotic resistance.\n\n==\n\nBlin 2019:\n\nTitle: antiSMASH 5.0: updates to the secondary metabolite genome mining pipeline\n\nAbstract: Abstract Secondary metabolites produced by bacteria and fungi are an important source of antimicrobials and other bioactive compounds. In recent years, genome mining has seen broad applications in identifying and characterizing new compounds as well as in metabolic engineering. Since 2011, the \u2018antibiotics and secondary metabolite analysis shell\u2014antiSMASH\u2019 (https://antismash.secondarymetabolites.org) has assisted researchers in this, both as a web server and a standalone tool. It has established itself as the most widely used tool for identifying and analysing biosynthetic gene clusters (BGCs) in bacterial and fungal genome sequences. Here, we present an entirely redesigned and extended version 5 of antiSMASH. antiSMASH 5 adds detection rules for clusters encoding the biosynthesis of acyl-amino acids, \u03b2-lactones, fungal RiPPs, RaS-RiPPs, polybrominated diphenyl ethers, C-nucleosides, PPY-like ketones and lipolanthines. For type II polyketide synthase-encoding gene clusters, antiSMASH 5 now offers more detailed predictions. The HTML output visualization has been redesigned to improve the navigation and visual representation of annotations. We have again improved the runtime of analysis steps, making it possible to deliver comprehensive annotations for bacterial genomes within a few minutes. A new output file in the standard JavaScript object notation (JSON) format is aimed at downstream tools that process antiSMASH results programmatically.\n\n==\n\nBelknap 2020:\n\nTitle: Genome mining of biosynthetic and chemotherapeutic gene clusters in Streptomyces bacteria\n\nAbstract: Streptomyces bacteria are known for their prolific production of secondary metabolites, many of which have been widely used in human medicine, agriculture and animal health. To guide the effective prioritization of specific biosynthetic gene clusters (BGCs) for drug development and targeting the most prolific producer strains, knowledge about phylogenetic relationships of Streptomyces species, genome-wide diversity and distribution patterns of BGCs is critical. We used genomic and phylogenetic methods to elucidate the diversity of major classes of BGCs in 1,110 publicly available Streptomyces genomes. Genome mining of Streptomyces reveals high diversity of BGCs and variable distribution patterns in the Streptomyces phylogeny, even among very closely related strains. The most common BGCs are non-ribosomal peptide synthetases, type 1 polyketide synthases, terpenes, and lantipeptides. We also found that numerous Streptomyces species harbor BGCs known to encode antitumor compounds. We observed that strains that are considered the same species can vary tremendously in the BGCs they carry, suggesting that strain-level genome sequencing can uncover high levels of BGC diversity and potentially useful derivatives of any one compound. These findings suggest that a strain-level strategy for exploring secondary metabolites for clinical use provides an alternative or complementary approach to discovering novel pharmaceutical compounds from microbes.\n\n==\n\nKhoshakhlagh 2022:\n\nTitle: Investigation of diverse biosynthetic secondary metabolites gene clusters using genome mining of indigenous Streptomyces strains isolated from saline soils in Iran\n\nAbstract: Background and Objectives: Bioactive secondary metabolites are the products of microbial communities adapting to environmental challenges, which have yet remained anonymous. As a result of demands in the pharmaceutical, agricultural, and food industries, microbial metabolites should be investigated. The most substantial sources of secondary metabolites are Streptomyces strains and are potential candidates for bioactive compound production. So, we used genome mining and bioinformatics to predict the isolates secondary metabolites, biosynthesis, and potential pharmaceuticals. Materials and Methods: This is a bioinformatics part of our previous experimental research. Here, we aimed to inspect the underlying secondary metabolite properties of 20 phylogenetically diverse Streptomyces species of saline soil by a rationalized computational workflow by several software tools. We examined the Metabolites\u2019 cytotoxicity and antibacterial effects using the MTT assay and plate count technique, respectively. Results: Among Streptomyces species, three were selected for genome mining and predicted novel secondary metabolites and potential drug abilities. All 11 metabolites were cytotoxic to A549, but ectoine (p\u22640.5) and geosmin (p\u22640.001) significantly operated as an anti-cancer drug. Metabolites of oxytetracycline and phosphinothricin (p\u22640.001), 4Z-annimycin and geosmin (p\u22640.01), and ectoine (p\u22640.5) revealed significant antibacterial activity. Conclusion: Of all the 11 compounds investigated, annimycin, geosmin, phosphinothricin, and ectoine had antimicrobial properties, but geosmin also showed very significant anti-cancer properties.\n\n==\n\nZiemert 2016:\n\nTitle: The evolution of genome mining in microbes - a review.\n\nAbstract: Covering: 2006 to 2016The computational mining of genomes has become an important part in the discovery of novel natural products as drug leads. Thousands of bacterial genome sequences are publically available these days containing an even larger number and diversity of secondary metabolite gene clusters that await linkage to their encoded natural products. With the development of high-throughput sequencing methods and the wealth of DNA data available, a variety of genome mining methods and tools have been developed to guide discovery and characterisation of these compounds. This article reviews the development of these computational approaches during the last decade and shows how the revolution of next generation sequencing methods has led to an evolution of various genome mining approaches, techniques and tools. After a short introduction and brief overview of important milestones, this article will focus on the different approaches of mining genomes for secondary metabolites, from detecting biosynthetic genes to resistance based methods and \"evo-mining\" strategies including a short evaluation of the impact of the development of genome mining methods and tools on the field of natural products and microbial ecology.\n\n==\n\nMicallef 2015:\n\nTitle: Genome mining for natural product biosynthetic gene clusters in the Subsection V cyanobacteria\n\nAbstract: BackgroundCyanobacteria are well known for the production of a range of secondary metabolites. Whilst recent genome sequencing projects has led to an increase in the number of publically available cyanobacterial genomes, the secondary metabolite potential of many of these organisms remains elusive. Our study focused on the 11 publically available Subsection V cyanobacterial genomes, together with the draft genomes of Westiella intricata UH strain HT-29-1 and Hapalosiphon welwitschii UH strain IC-52-3, for their genetic potential to produce secondary metabolites. The Subsection V cyanobacterial genomes analysed in this study are reported to produce a diverse range of natural products, including the hapalindole-family of compounds, microcystin, hapalosin, mycosporine-like amino acids and hydrocarbons.ResultsA putative gene cluster for the cyclic depsipeptide hapalosin, known to reverse P-glycoprotein multiple drug resistance, was identified within three Subsection V cyanobacterial genomes, including the producing cyanobacterium H. welwitschii UH strain IC-52-3. A number of orphan NRPS/PKS gene clusters and ribosomally-synthesised and post translationally-modified peptide gene clusters (including cyanobactin, microviridin and bacteriocin gene clusters) were identified. Furthermore, gene clusters encoding the biosynthesis of mycosporine-like amino acids, scytonemin, hydrocarbons and terpenes were also identified and compared.ConclusionsGenome mining has revealed the diversity, abundance and complex nature of the secondary metabolite potential of the Subsection V cyanobacteria. This bioinformatic study has identified novel biosynthetic enzymes which have not been associated with gene clusters of known classes of natural products, suggesting that these cyanobacteria potentially produce structurally novel secondary metabolites.\n\n==\n\nRomsdahl 2019:\n\nTitle: Recent advances in the genome mining of Aspergillus secondary metabolites (covering 2012-2018).\n\nAbstract: Secondary metabolites (SMs) produced by filamentous fungi possess diverse bioactivities that make them excellent drug candidates. Whole genome sequencing has revealed that fungi have the capacity to produce a far greater number of SMs than have been isolated, since many of the genes involved in SM biosynthesis are either silent or expressed at very low levels in standard laboratory conditions. There has been significant effort to activate SM biosynthetic genes and link them to their downstream products, as the SMs produced by these \"cryptic\" pathways offer a promising source for new drug discovery. Further, an understanding of the genes involved in SM biosynthesis facilitates product yield optimization of first-generation molecules and genetic engineering of second-generation analogs. This review covers advances made in genome mining SMs produced by Aspergillus nidulans, Aspergillus fumigatus, Aspergillus niger, and Aspergillus terreus in the past six years (2012-2018). Genetic identification and molecular characterization of SM biosynthetic gene clusters, along with proposed biosynthetic pathways, will be discussed in depth.\n\n==\n\nBok 2006:\n\nTitle: Genomic mining for Aspergillus natural products.\n\nAbstract: The genus Aspergillus is renowned for its ability to produce a myriad of bioactive secondary metabolites. Although the propensity of biosynthetic genes to form contiguous clusters greatly facilitates assignment of putative secondary metabolite genes in the completed Aspergillus genomes, such analysis cannot predict gene expression and, ultimately, product formation. To circumvent this deficiency, we have examined Aspergillus nidulans microarrays for expressed secondary metabolite gene clusters by using the transcriptional regulator LaeA. Deletion or overexpression of laeA clearly identified numerous secondary metabolite clusters. A gene deletion in one of the clusters eliminated the production of the antitumor compound terrequinone A, a metabolite not described, from A. nidulans. In this paper, we highlight that LaeA-based genome mining helps decipher the secondary metabolome of Aspergilli and provides an unparalleled view to assess secondary metabolism gene regulation.", "final_summary": "Genome mining has emerged as a powerful tool for the discovery of bioactive secondary metabolites from microbes, particularly in the field of drug discovery. This technique leverages advancements in genome sequencing technology and computational tools to predict and characterize secondary metabolites (Arulprakasam 2021; Blin 2019; Ziemert 2016).\n\nArulprakasam (2021) explored the potential of 24 actinobacteria species to traverse the diversity and distribution of Biosynthetic Gene Clusters (BGCs). The study revealed the conservation of genus-specific gene clusters among various species, suggesting the potential for large-scale analysis of natural products to overcome antibiotic resistance.\n\nBlin (2019) discussed the use of antiSMASH, a widely used tool for identifying and analyzing BGCs in bacterial and fungal genome sequences. The tool has been updated to detect clusters encoding the biosynthesis of various compounds, improving the navigation and visual representation of annotations.\n\nBelknap (2020) emphasized the importance of understanding the phylogenetic relationships of Streptomyces species and the genome-wide diversity and distribution patterns of BGCs for effective drug development. The study revealed high diversity of BGCs and variable distribution patterns in the Streptomyces phylogeny.\n\nKhoshakhlagh (2022) used genome mining and bioinformatics to predict the secondary metabolites, biosynthesis, and potential pharmaceuticals of 20 Streptomyces species. The study found that several compounds had antimicrobial properties, and one also showed significant anti-cancer properties.\n\nZiemert (2016) reviewed the evolution of genome mining in microbes over the last decade, highlighting how the revolution of next-generation sequencing methods has led to the evolution of various genome mining approaches, techniques, and tools.\n\nMicallef (2015) revealed the diversity, abundance, and complex nature of the secondary metabolite potential of the Subsection V cyanobacteria through genome mining. The study identified novel biosynthetic enzymes not previously associated with known classes of natural products.\n\nRomsdahl (2019) discussed advances made in genome mining of Aspergillus secondary metabolites, highlighting the genetic identification and molecular characterization of SM biosynthetic gene clusters and proposed biosynthetic pathways.\n\nBok (2006) used the transcriptional regulator LaeA to examine Aspergillus nidulans microarrays for expressed secondary metabolite gene clusters. The study found that LaeA-based genome mining helps decipher the secondary metabolome of Aspergilli.\n\nIn conclusion, genome mining has proven to be a valuable tool in the discovery and characterization of bioactive secondary metabolites from microbes. The studies collectively highlight the potential of this approach in identifying novel compounds for drug discovery and understanding the regulation of secondary metabolism genes (Arulprakasam 2021; Blin 2019; Belknap 2020; Khoshakhlagh 2022; Ziemert 2016; Micallef 2015; Romsdahl 2019; Bok 2006)."}, {"query": "Are emotions innate or constructed?", "paper_list_string": "Kurth 2019:\n\nTitle: Are Emotions Psychological Constructions?\n\nAbstract: According to psychological constructivism, emotions result from projecting folk emotion concepts onto felt affective episodes. While constructivists acknowledge there is a biological dimension to emotion, they deny that emotions are (or involve) affect programs. So they also deny emotions are natural kinds. However, the essential role that constructivism gives to felt experience and folk concepts leads to an account that is extensionally inadequate and functionally inaccurate. Moreover, biologically oriented proposals that reject these commitments are not similarly encumbered. Recognizing this has two implications: biological mechanisms are more central to emotion than constructivism allows, and the conclusion that emotions are not natural kinds is premature.\n\n==\n\nLindquist 2013:\n\nTitle: The hundred-year emotion war: are emotions natural kinds or psychological constructions? Comment on Lench, Flores, and Bench (2011).\n\nAbstract: For the last century, there has been a continuing debate about the nature of emotion. In the most recent offering in this scientific dialogue, Lench, Flores, and Bench (2011) reported a meta-analysis of emotion induction research and claimed support for the natural kind hypothesis that discrete emotions (e.g., happiness, sadness, anger, and anxiety) elicit specific changes in cognition, judgment, behavior, experience, and physiology. In this article, we point out that Lench et al. (2011) is not the final word on the emotion debate. First, we point out that Lench et al.'s findings do not support their claim that discrete emotions organize cognition, judgment, experience, and physiology because they did not demonstrate emotion-consistent and emotion-specific directional changes in these measurement domains. Second, we point out that Lench et al.'s findings are in fact consistent with the alternative (a psychological constructionist approach to emotion). We close by appealing for a construct validity approach to emotion research, which we hope will lead to greater consensus on the operationalization of the natural kind and psychological construction approaches, as well as the criteria required to finally resolve the emotion debate.\n\n==\n\nAverill 1980:\n\nTitle: A CONSTRUCTIVIST VIEW OF EMOTION\n\nAbstract: ABSTRACT Traditionally, the emotions have been viewed from a biological perspective; that is, the emotions have been seen as genetically determined and relatively invariable responses. The present chapter, by contrast, views the emotions as social constructions. More precisely, the emotions are here defined as socially constituted syndromes or transitory social roles. A role-conception does not deny the contribution of biological systems to emotional syndromes; it does, however, imply that the functional significance of emotional responses is to be found largely within the sociocultural system. With regard to subjective experience, a person interprets his own behavior as emotional in much the same way that an actor interprets a role\u201cwith feeling.\u201d This involves not only the monitoring of behavior (including feedback from physiological arousal, facial expressions, etc.), but also an understanding of how the emotional role fits into a larger\u201cdrama\u201d written by society. Some of the biological, personal, and situational factors that influence emotional behavior are also discussed.\n\n==\n\nBarrett 2016:\n\nTitle: The theory of constructed emotion: an active inference account of interoception and categorization\n\nAbstract: Abstract The science of emotion has been using folk psychology categories derived from philosophy to search for the brain basis of emotion. The last two decades of neuroscience research have brought us to the brink of a paradigm shift in understanding the workings of the brain, however, setting the stage to revolutionize our understanding of what emotions are and how they work. In this article, we begin with the structure and function of the brain, and from there deduce what the biological basis of emotions might be. The answer is a brain-based, computational account called the theory of constructed emotion.\n\n==\n\nOno 2014:\n\nTitle: How are innate emotions evoked?\n\nAbstract: We read with great interest the book and its precis by Rolls (2014a,b). He defines emotions as states in organisms that are elicited by reinforcements (rewards and punishments) and motivation as a state in which organisms are working for a goal to acquire or avoid reinforcements. Based on Darwin\u2019s theory of evolution (1859), Rolls proposes that emotions have an important evolutionary role in specifying a goal (rewards) (e.g., rewards that elicit positive emotions are necessary for the survival of organisms). According to Rolls\u2019 theory, neural mechanisms are hierarchically implemented in cortical systems. Stimuli (objects) are consciously represented in the higher association areas, such as the inferotemporal cortex, and this information is then sent to the amygdala and orbitofrontal cortex that code emotions and the emotional values of the stimuli. Finally, the value information is transferred to other areas, including the anterior cingulate cortex and basal ganglia, in order to select optimal actions required to acquire rewards. These theories elegantly explain human conscious emotions and behaviors. Furthermore, the theories and hypotheses for human higher cognitive and conscious processes proposed in the book can be testable by computer simulation. This method could uncover the neural mechanisms of emotions in humans at the cellular and system levels. We completely agree with the Darwinian account of emotion (Ono & Nishijo, 1992), as Rolls has proposed. In addition to the cortical systems that Rolls says is important for emotion processing, we would like to stress a role of subcortical sensory systems, such as subcortical fear modules (LeDoux, 2012), in unconscious innate emotions, which might also affect behavior and decision making in primates. Rolls has suggested that early stages in sensory processing may evoke emotional responses in non-primates, such as rodents. We similarly suggest that the phylogenetically old subcortical\n\n==\n\nBarrett 2006:\n\nTitle: Are Emotions Natural Kinds?\n\nAbstract: Laypeople and scientists alike believe that they know anger, or sadness, or fear, when they see it. These emotions and a few others are presumed to have specific causal mechanisms in the brain and properties that are observable (on the face, in the voice, in the body, or in experience)\u2014that is, they are assumed to be natural kinds. If a given emotion is a natural kind and can be identified objectively, then it is possible to make discoveries about that emotion. Indeed, the scientific study of emotion is founded on this assumption. In this article, I review the accumulating empirical evidence that is inconsistent with the view that there are kinds of emotion with boundaries that are carved in nature. I then consider what moving beyond a natural-kind view might mean for the scientific understanding of emotion.\n\n==\n\nScherer 2009:\n\nTitle: Emotions are emergent processes: they require a dynamic computational architecture\n\nAbstract: Emotion is a cultural and psychobiological adaptation mechanism which allows each individual to react flexibly and dynamically to environmental contingencies. From this claim flows a description of the elements theoretically needed to construct a virtual agent with the ability to display human-like emotions and to respond appropriately to human emotional expression. This article offers a brief survey of the desirable features of emotion theories that make them ideal blueprints for agent models. In particular, the component process model of emotion is described, a theory which postulates emotion-antecedent appraisal on different levels of processing that drive response system patterning predictions. In conclusion, investing seriously in emergent computational modelling of emotion using a nonlinear dynamic systems approach is suggested.\n\n==\n\nGriffiths 1997:\n\nTitle: What emotions really are : the problem of psychological categories\n\nAbstract: This study argues that much research of the emotions has been misguided. It attempts to show that \"emotion\" encompasses psychological states of very different, and thus not comparable, kinds. Some emotions, such as a brief flaring up of anger in response to some experience, are evolutionary ancient, reflex-like responses which appear insensitive to culture. Others, like moral guilt, differ importantly across cultures, despite their long history in humans, and affinity to behaviour seen in other species. Yet other emotions appear to be the acting-out of today's psychological myths, as ghost possession acted out the metaphysical myths of past centuries. These three kinds of responses have different evolutionary origins, different adaptive functions, different biological bases, and different roles in human psychology. The concept that binds them together, emotion, plays no useful role, since there is no object of scientific knowledge that corresponds to it. A detailed overview of the relevant theoretical approaches is provided in this text, assessing the relative merits of three main theoretical approaches: affect programme theory, evolutionary psychology, and social constructionism.", "final_summary": "The debate on whether emotions are innate or constructed is a complex and multifaceted one, with scholars presenting compelling arguments on both sides. \n\nOn one hand, some researchers argue that emotions are psychological constructions. Lindquist (2013) and Averill (1980) both propose that emotions are not natural kinds, but rather social constructs that are influenced by cultural and societal factors. Similarly, Barrett (2016) and Barrett (2006) propose a theory of constructed emotion, suggesting that emotions are not biologically predetermined but are instead constructed based on individual experiences and perceptions.\n\nOn the other hand, some researchers argue for the innate nature of emotions. Kurth (2019) criticizes the constructivist view, arguing that biological mechanisms play a more central role in emotions than constructivism allows. Ono (2014) also leans towards the innate perspective, suggesting that emotions have an evolutionary role and are elicited by reinforcements.\n\nHowever, some researchers propose a more nuanced view that integrates both perspectives. Scherer (2009) suggests that emotions are emergent processes, implying a dynamic interplay between innate biological mechanisms and constructed elements. Similarly, Griffiths (1997) argues that the concept of emotion encompasses psychological states of different kinds, some of which are evolutionary ancient and reflex-like, while others differ across cultures or are the acting-out of psychological myths.\n\nIn conclusion, the debate on whether emotions are innate or constructed is far from settled. The evidence suggests that emotions may be both biologically rooted and socially constructed, reflecting the complex interplay between our biological makeup and our individual and collective experiences."}, {"query": "police violence and child victimization OR adverse childhood experinces", "paper_list_string": "Finkelhor 2010:\n\nTitle: Trends in childhood violence and abuse exposure: evidence from 2 national surveys.\n\nAbstract: OBJECTIVE\nTo assess trends in children's exposure to abuse, violence, and crime victimizations.\n\n\nDESIGN\nAn analysis based on a comparison of 2 cross-sectional national telephone surveys using identical questions conducted in 2003 and 2008.\n\n\nSETTING\nTelephone interview.\n\n\nPARTICIPANTS\nExperiences of children aged 2 to 17 years (2030 children in 2003 and 4046 children in 2008) were assessed through interviews with their caretakers and the children themselves. Outcome Measure Responses to the Juvenile Victimization Questionnaire.\n\n\nRESULTS\nSeveral types of child victimization were reported significantly less often in 2008 than in 2003: physical assaults, sexual assaults, and peer and sibling victimizations, including physical bullying. There were also significant declines in psychological and emotional abuse by caregivers, exposure to community violence, and the crime of theft. Physical abuse and neglect by caregivers did not decline, and witnessing the abuse of a sibling increased.\n\n\nCONCLUSION\nThe declines apparent in this analysis parallel evidence from other sources, including police data, child welfare data, and the National Crime Victimization Survey, suggesting reductions in various types of childhood victimization in recent years.\n\n==\n\nEdje 2021:\n\nTitle: The Case for Expanding Adverse Childhood Experiences to Include Police Violence\n\nAbstract: \u201cAll our phrasing race relations, racial chasm, racial justice, racial profiling, white privilege, even white supremacy serves to obscure that racism is a visceral experience, that it dislodges brains, blocks airway, rips muscle, extracts organs, cracks bones, breaks teeth. You must never look away from this. You must always remember that the sociology, the history, the economics, the graphs, the charts, the regressions all land, with great violence on the body.\u201d Ta-Nehisi Coates\n\n==\n\nDuke 2010:\n\nTitle: Adolescent Violence Perpetration: Associations With Multiple Types of Adverse Childhood Experiences\n\nAbstract: OBJECTIVE: Adverse childhood experiences are associated with significant functional impairment and life lost in adolescence and adulthood. This study identified relationships between multiple types of adverse events and distinct categories of adolescent violence perpetration. METHODS: Data are from 136 549 students in the 6th, 9th, and 12th grades who responded to the 2007 Minnesota Student Survey, an anonymous, self-report survey examining youth health behaviors and perceptions, characteristics of primary socializing domains, and youth engagement. Linear and logistic regression models were used to determine if 6 types of adverse experiences including physical abuse, sexual abuse by family and/or other persons, witnessing abuse, and household dysfunction caused by family alcohol and/or drug use were significantly associated with risk of adolescent violence perpetration after adjustment for demographic covariates. An adverse-events score was entered into regression models to test for a dose-response relationship between the event score and violence outcomes. All analyses were stratified according to gender. RESULTS: More than 1 in 4 youth (28.9%) reported at least 1 adverse childhood experience. The most commonly reported adverse experience was alcohol abuse by a household family member that caused problems. Each type of adverse childhood experience was significantly associated with adolescent interpersonal violence perpetration (delinquency, bullying, physical fighting, dating violence, weapon-carrying on school property) and self-directed violence (self-mutilatory behavior, suicidal ideation, and suicide attempt). For each additional type of adverse event reported by youth, the risk of violence perpetration increased 35% to 144%. CONCLUSIONS: Multiple types of adverse childhood experiences should be considered as risk factors for a spectrum of violence-related outcomes during adolescence. Providers and advocates should be aware of the interrelatedness and cumulative impact of adverse-event types. Study findings support broadening the current discourse on types of adverse events when considering pathways from child maltreatment to adolescent perpetration of delinquent and violent outcomes.\n\n==\n\nRivera 1990:\n\nTitle: Childhood Victimization and Violent Offending\n\nAbstract: The relationship between childhood victimization and violent offending is examined using a prospective cohorts design. Official criminal histories for a large sample of substantiated and validated cases of physical and sexual abuse and neglect (N = 908) from the years 1967 through 1971 were compared to those of a matched control group (N = 667) of individuals with no official record of abuse or neglect. Sex-specific and race-specific effects of childhood victimization and other characteristics of violent offending (chronicity, age of onset, temporal patterns, and continuity) are assessed. Childhood victimization increased overall risk for violent offending and particularly increased risk for males and blacks. In comparison to controls, abused and neglected children began delinquent careers earlier. Temporal patterns of violent offending were examined and childhood victims did not differ in age of arrest for first violent offense, nor were they more likely to continue offending. The findings and their limitations are discussed, as well as directions for future research.\n\n==\n\nFinkelhor 2009:\n\nTitle: Violence, Abuse, and Crime Exposure in a National Sample of Children and Youth\n\nAbstract: OBJECTIVE: The objective of this research was to obtain national estimates of exposure to the full spectrum of the childhood violence, abuse, and crime victimizations relevant to both clinical practice and public-policy approaches to the problem. METHODS: The study was based on a cross-sectional national telephone survey that involved a target sample of 4549 children aged 0 to 17 years. RESULTS: A clear majority (60.6%) of the children and youth in this nationally representative sample had experienced at least 1 direct or witnessed victimization in the previous year. Almost half (46.3%) had experienced a physical assault in the study year, 1 in 4 (24.6%) had experienced a property offense, 1 in 10 (10.2%) had experienced a form of child maltreatment, 6.1% had experienced a sexual victimization, and more than 1 in 4 (25.3%) had been a witness to violence or experienced another form of indirect victimization in the year, including 9.8% who had witnessed an intrafamily assault. One in 10 (10.2%) had experienced a victimization-related injury. More than one third (38.7%) had been exposed to 2 or more direct victimizations, 10.9% had 5 or more, and 2.4% had 10 or more during the study year. CONCLUSIONS: The scope and diversity of child exposure to victimization is not well recognized. Clinicians and researchers need to inquire about a larger spectrum of victimization types to identify multiply victimized children and tailor prevention and interventions to the full range of threats that children face.\n\n==\n\nFinkelhor 2013:\n\nTitle: Violence, crime, and abuse exposure in a national sample of children and youth: an update.\n\nAbstract: IMPORTANCE\nBecause exposure to violence, crime, and abuse has been shown to have serious consequences on child development, physicians and policymakers need to know the kinds of exposure that occur at various developmental stages.\n\n\nOBJECTIVES\nTo provide updated estimates of and trends for childhood exposure to a broad range of violence, crime, and abuse victimizations.\n\n\nDESIGN\nThe National Survey of Children's Exposure to Violence was based on a cross-sectional, US national telephone survey conducted in 2011.\n\n\nSETTING\nInterviews by telephone.\n\n\nPARTICIPANTS\nThe experiences of 4503 children and youth aged 1 month to 17 years were assessed by interviews with caregivers and with youth in the case of those aged 10 to 17 years.\n\n\nRESULTS\nTwo-fifths (41.2%) of children and youth experienced a physical assault in the last year, and 1 in 10 (10.1%) experienced an assault-related injury. Two percent experienced sexual assault or sexual abuse in the last year, but the rate was 10.7% for girls aged 14 to 17 years. More than 1 in 10 (13.7%) experienced maltreatment by a caregiver, including 3.7% who experienced physical abuse. Few significant changes could be detected in rates since an equivalent survey in 2008, but declines were documented in peer flashing, school bomb threats, juvenile sibling assault, and robbery and total property victimization.\n\n\nCONCLUSIONS AND RELEVANCE\nThe variety and scope of children's exposure to violence, crime, and abuse suggest the need for better and more comprehensive tools in clinical and research settings for identifying these experiences and their effects.\n\n==\n\nFinkelhor 2015:\n\nTitle: Prevalence of Childhood Exposure to Violence, Crime, and Abuse: Results From the National Survey of Children's Exposure to Violence.\n\nAbstract: IMPORTANCE\nIt is important to estimate the burden of and trends for violence, crime, and abuse in the lives of children.\n\n\nOBJECTIVE\nTo provide health care professionals, policy makers, and parents with current estimates of exposure to violence, crime, and abuse across childhood and at different developmental stages.\n\n\nDESIGN, SETTING, AND PARTICIPANTS\nThe National Survey of Children's Exposure to Violence (NatSCEV) includes a representative sample of US telephone numbers from August 28, 2013, to April 30, 2014. Via telephone interviews, information was obtained on 4000 children 0 to 17 years old, with information about exposure to violence, crime, and abuse provided by youth 10 to 17 years old and by caregivers for children 0 to 9 years old.\n\n\nMAIN OUTCOME AND MEASURE\nExposure to violence, crime, and abuse using the Juvenile Victimization Questionnaire.\n\n\nRESULTS\nIn total, 37.3% of youth experienced a physical assault in the study year, and 9.3% of youth experienced an assault-related injury. Two percent of girls experienced sexual assault or sexual abuse in the study year, while the rate was 4.6% for girls 14 to 17 years old. Overall, 15.2% of children and youth experienced maltreatment by a caregiver, including 5.0% who experienced physical abuse. In total, 5.8% witnessed an assault between parents. Only 2 significant rate changes could be detected compared with the last survey in 2011, namely, declines in past-year exposure to dating violence and lifetime exposure to household theft.\n\n\nCONCLUSIONS AND RELEVANCE\nChildren and youth are exposed to violence, abuse, and crime in varied and extensive ways, which justifies continued monitoring and prevention efforts.\n\n==\n\nFinkelhor 2005:\n\nTitle: The Victimization of Children and Youth: A Comprehensive, National Survey\n\nAbstract: This study examined a large spectrum of violence, crime, and victimization experiences in a nationally representative sample of children and youth ages 2 to 17 years. More than one half (530 per 1,000) of the children and youth had experienced a physical assault in the study year, more than 1 in 4 (273 per 1,000) a property offense, more than 1 in 8 (136 per 1,000) a form of child maltreatment, 1 in 12 (82 per 1,000) a sexual victimization, and more than 1 in 3 (357 per 1,000) had been a witness to violence or experienced another form of indirect victimization. Only a minority (29%) had no direct or indirect victimization. The mean number of victimizations for a child or youth with any victimization was 3.0, and a child or youth with one victimization had a 69% chance of experiencing another during a single year.", "final_summary": "The research question delves into the intricate web of police violence, child victimization, and adverse childhood experiences. The papers assembled offer a comprehensive understanding of these intertwined issues.\n\nFinkelhor (2010) offers a macroscopic view on the trends in children's exposure to violence, abuse, and crime, noting a significant decline in several types of child victimization between 2003 and 2008. However, the study also points out that physical abuse and neglect by caregivers did not see a similar decline, and witnessing the abuse of a sibling even increased. This suggests that while some forms of violence are on the decline, others persist or are even on the rise.\n\nDuke (2010) underscores the impact of multiple types of adverse events on adolescent violence perpetration. The study found that each additional type of adverse event reported by youth increased the risk of violence perpetration by 35% to 144%.\n\nRivera (1990) provides a historical perspective, examining the relationship between childhood victimization and violent offending. The study found that childhood victimization increased the overall risk for violent offending, particularly for males and blacks.\n\nFinkelhor (2009, 2013, 2015) provides a series of national estimates of exposure to violence, crime, and abuse among children and youth. The studies consistently found that a significant proportion of children and youth had experienced at least one direct or witnessed victimization in the previous year.\n\nIn conclusion, the collected papers collectively highlight the pervasive and multifaceted nature of child victimization and adverse childhood experiences. They underscore the need for comprehensive and multi-pronged approaches to address these issues."}, {"query": "cucurbitins", "paper_list_string": "Kupchan 1978:\n\nTitle: New cucurbitacins from Phormium tenax and Marah oreganus\n\nAbstract: Abstract Cucurbitacins I and D and two new cucurbitacins, isocucurbitacin D and 3-epi-isocucurbitacin D, were isolated from Phormium tenax . A new cucurbitacin, dihydroisocucurbitacin B, was isolated from Marah oreganus . The acid sensitivity of the 2\u03b2-hydroxy-3-keto system found in cucurbitacin D was demonstrated.\n\n==\n\nMir\u00f3 1995:\n\nTitle: Cucurbitacins and their pharmacological effects\n\nAbstract: The cucurbitacins are triterpenic plant principles with varied pharmacological activities, found in several botanical families. This review includes the structures described in the literature, the plant\u2010containing cucurbitacins and their main pharmacological effects.\n\n==\n\nChen 2012:\n\nTitle: Biological activities and potential molecular targets of cucurbitacins: a focus on cancer\n\nAbstract: Cucurbitacin and its derivatives (cucurbitacins) are a class of highly oxidized tetracyclic triterpenoids. They are widely distributed in the plant kingdom, where they act as heterologous chemical pheromones that protect plants from external biological insults. Their bioactivities first attracted attention in the 1960s. Documented data demonstrate that cucurbitacins possess strong pharmacological properties, such as antitumor, anti-inflammatory, and hepatoprotective effects, etc. Several molecular targets for cucurbitacins have been discovered, such as fibrous-actin, signal transducer and activator of transcription 3, cyclooxygenase-2, etc. The present study summarizes the achievements of the 50 years of research on cucurbitacins. The aim was to systematically analyze their bioactivities with an emphasis on their anticancer effects. Research and development has shed new insight into the beneficial properties of these compounds.\n\n==\n\nLavie 1971:\n\nTitle: The cucurbitanes, a group of tetracyclic triterpenes.\n\nAbstract: The potent physiological activity of plants belonging to the Cucurbitaceae family has been known since antiquity. They were feared on account of their high toxicity (Elisha\u2019s Miracle)*, and vet valued because of the medicinal properties ascribed to them (40). Greeks and Romans used them, the doctors of the Middle Ages praised their virtues and some were still described in the British Pharmacopoeia of 1914.\n\n==\n\nChen 2005:\n\nTitle: Cucurbitacins and cucurbitane glycosides: structures and biological activities.\n\nAbstract: The natural cucurbitacins constitute a group of triterpenoid substances which are well-known for their bitterness and toxicity. Structurally, they are characterized by the tetracyclic cucurbitane nucleus skeleton, namely, 19-(10-->9beta)-abeo-10alpha-lanost-5-ene (also known as 9beta-methyl-19-norlanosta-5-ene), with a variety of oxygen substitutions at different positions. According to the characteristics of their structures, cucurbitacins are divided into twelve categories. The biological effects of the cucurbitacins are also covered.\n\n==\n\nWalker 2011:\n\nTitle: The potential of cucurbit[n]urils in drug delivery\n\nAbstract: In this paper we review cucurbit[n]urils (CB[n]), a relatively new family of macrocycles that has shown potential in improving drug delivery. Encapsulation of drugs within the homologues CB[6], CB[7], or CB[8] can impart enhanced chemical and physical stability, improve drug solubility, and control drug release. The formulation of CB[n] into a dosage form suitable for clinical use is a non-trivial task, because the free macrocycle and its host-drug complex generally exhibit pseudo-polymorphism in the solid state. Despite this, cucurbiturils have been included in tablets for oral delivery and inserts for nasal delivery. Here we examine the potential use of cucurbiturils in drug delivery in the context of getting a new drug into clinical trials and discuss what further research is needed in this area.\n\n==\n\nR\u00edos 2005:\n\nTitle: New insights into the bioactivity of cucurbitacins\n\nAbstract: The cucurbitacins are a group of tetracyclic triterpenoids derived from the cucurbitane skeleton and found primarily in the Cucurbitaceae family. These triterpenoids, present in free or glycosidic form, are generally responsible for the bitter taste of the plants that contain them and are probably the principal cause of the antifeedant effects observed for such plants. Several plants used in traditional medicine to treat both inflammatory diseases as well as various types of tumors are rich in cucurbitacins, a fact which has given rise to several studies concerning their potential use as anti-inflammatory and anticancer agents. Nevertheless, since many cucurbitacins are extremely toxic, relatively few papers have dealt with their pharmacological activity. Recently, however, the relationship between the toxicity of a compound and its chemical pattern of substitution has been established, thus allowing for a more in-depth understanding of this class of triterpenes. In the present review, we provide a compilation of all the studies published in the last ten years on the pharmacological and biological effects of cucurbitacins, focusing principally on their pharmacological properties, especially their anti-inflammatory and anticancer effects.\n\n==\n\nMetcalf 1980:\n\nTitle: Cucurbitacins as kairomones for diabroticite beetles.\n\nAbstract: The characteristic bitter substances of the Cucurbitaceae act as kairomones for a large group of diabroticite beetles (Chrysomelidae, Galerucinae, Luperini), promoting host selection and compulsive feeding behavior. These beetles (e.g., Diabrotica undecimpunctata howardi) respond to as little as 1 ng of cucurbitacin (Cuc) B on thin-layer plates by arrest and compulsive feeding. Six species of diabroticite beetles were about 10 times more responsive to Cuc B than to Cuc E and less responsive to Cuc D, I, and L. Chloroform extracts of 18 species of Cucurbita were developed on thin-layer chromatograms and exposed to diabroticite beetles. The feeding patterns showed pronounced beetle responses to three general Cuc distribution patterns: Cuc B and D as in Cucurbita andreana and C. ecuadorensis; Cuc E and I as in C. okeechobeensis and C. martinezii; and Cuc E glycoside in C. texana. All the diabroticites responded in exactly the same feeding patterns. The results demonstrate a coevolutionary association between the Cucurbitaceae and the Luperini, during which the intensely bitter and toxic Cucs that arose to repel herbivores and protect the plants from attack became specific kairomone feeding stimulants for the beetles.", "final_summary": "Cucurbitacins, a class of highly oxidized tetracyclic triterpenoids, have been the subject of extensive research due to their varied pharmacological activities. These compounds, characterized by their tetracyclic cucurbitane nucleus skeleton, are known for their bitterness and toxicity (Chen, 2005; Mir\u00f3, 1995).\n\nKupchan (1978) isolated new cucurbitacins from Phormium tenax and Marah oreganus, demonstrating the wide distribution of these compounds in the plant kingdom. Mir\u00f3 (1995) and Chen (2005) further highlighted the diverse pharmacological activities of cucurbitacins.\n\nChen (2012) focused on the bioactivities of cucurbitacins, suggesting that these compounds could be potential therapeutic agents. This was supported by R\u00edos (2005), who emphasized the potential use of cucurbitacins in pharmacology, despite their known toxicity.\n\nWalker (2011) explored the potential of cucurbit[n]urils in drug delivery, suggesting that these compounds could enhance drug stability, improve solubility, and control drug release. However, the formulation of these compounds into a dosage form suitable for clinical use remains a challenge.\n\nMetcalf (1980) provided an interesting perspective on the role of cucurbitacins as kairomones for diabroticite beetles, demonstrating a coevolutionary association between certain plants and beetles.\n\nIn conclusion, cucurbitacins are a group of triterpenoids with diverse pharmacological activities. However, their high toxicity poses a challenge for their use in clinical settings. Further research is needed to overcome this challenge and fully harness the therapeutic potential of these compounds."}, {"query": "what is flexible work policy", "paper_list_string": "Rubery 2016:\n\nTitle: Flexibility bites back: the multiple and hidden costs of flexible employment policies\n\nAbstract: Flexible labour markets are increasingly regarded as the answer to a wide spectrum of labour market and societal challenges from creating jobs to reducing segmentation and welfare dependency, improving public finances and supporting workforce diversity and innovation. The contention is that, contrary to these claims, flexible labour markets generate fundamental contradictions and unsustainable long-term trends. The jobs miracle is exaggerated and based on low productivity jobs, outsiders often lose most from competition, claimants must work flexibly but still secure a full-time wage, low-wage employment is shrinking the fiscal base, jobs are not being adjusted to accommodate workers' changing needs and capacities and the disposable labour model is undermining long-term productivity.\n\n==\n\nOorschot 2004:\n\nTitle: Flexible work and flexicurity policies in the Netherlands. Trends and experiences\n\nAbstract: Increasing the flexibility of work and working life has been high on the agenda of Dutch public debate for at least 15 years. Resulting policies have been guided by the aspiration of combining flexibility and security, or of achieving adequate \u2018flexicurity\u2019, as the combination of these goals has come to be known. This article describes and analyses Dutch flexicurity policies of recent years, as they have been adopted in the fields of part-time work, social security, labour law and the work-care combination. It shows that the government has made it easier for employers and employees to choose part-time work as a strategy for increasing flexibility. In the field of social security there are numerous problems, especially for \u2018flex\u2019 -workers (not for part-time workers as such), but little substantial improvements have been implemented. In labour law important flexibility and security measures have been adopted, but here government has been rather slow in taking the lead. As regards the work-care combination, new policies have improved conditions, but the Netherlands still lags behind other European countries. L\u2019 augmentation de la flexibilit\u00e9 du travail et de la vie active figure parmi les priorit\u00e9s \u00e0 l\u2019 ordre du jour des d\u00e9bats publics aux Pays-Bas depuis au moins 15 ans. Les politiques d\u00e9velopp\u00e9es se sont efforc\u00e9es de combiner flexibilit\u00e9 et s\u00e9curit\u00e9, ou de mettre en oeuvre une \u00abflexicurit\u00e9\u00bb ad\u00e9quate, terme choisi pour d\u00e9signer cette combinaison. Cet article d\u00e9crit et analyse les politiques de flexicurit\u00e9 aux Pays-Bas ces derni\u00e8res ann\u00e9es, qui ont \u00e9t\u00e9 adopt\u00e9es dans le domaine du travail \u00e0 temps partiel, de la s\u00e9curit\u00e9 sociale, de la l\u00e9gislation du travail et de la conciliation de la vie professionnelle et familiale. Il montre que le gouvernement a permis aux employeurs et aux travailleurs de choisir le travail \u00e0 temps partiel comme strat\u00e9gie pour accro\u00eetre la flexibilit\u00e9. Dans le domaine de la s\u00e9curit\u00e9 sociale, il y a de nombreux probl\u00e8mes, particuli\u00e8rement pour les \u00ab flex \u00bb-travailleurs (mais pas pour les travailleurs \u00e0 temps partiel en tant que tels), mais de petites am\u00e9liorations substantielles ont \u00e9t\u00e9 mises en oeuvre. Dans la l\u00e9gislation du travail, d\u2019 importantes mesures de flexibilit\u00e9 et de s\u00e9curit\u00e9 ont \u00e9t\u00e9 adopt\u00e9es, m\u011bme si ici le gouvernement a \u00e9t\u00e9 plut\u00f4t lent \u00e0 lancer l'initiative. En ce qui concerne la conciliation de la vie professionnelle et familiale, les nouvelles politiques ont am\u00e9lior\u00e9 les conditions, mais les Pays-Bas sont toujours \u00e0 la tra\u00eene des autres pays europ\u00e9ens. Seit mindestens 15 Jahren steht die Erh\u00f6hung der Flexibilit\u00e4t der Arbeit und des Arbeitslebens in den Niederlanden im Mittelpunkt der \u00f6ffentlichen Debatte. Die daraus resultierenden Ma\u03b2nahmen wurden von dem Streben geleitet, Flexibilit\u00e4t und Sicherheit miteinander zu verkn\u00fcpfen, oder zu einer angemessenen \u201cFlexicurity\u201d, wie diese Verkn\u00fcpfung der beiden Ziele bezeichnet wird, zu gelangen. Dieser Beitrag beschreibt und analysiert die niederl\u00e4ndischen Ma\u03b2nahmen, die in den letzten Jahren in den Bereichen Teilzeitarbeit, soziale Sicherheit, Arbeitsrecht und Vereinbarkeit von Arbeit und Betreuungspflichten ergriffen wurden, um das Ziel der Flexicurity zu erreichen. Der Beitrag zeigt, dass die Regierung es aus strategischen Gr\u00fcnden Arbeitgebern und Arbeitnehmern leichter macht, sich f\u00fcr Teilzeitarbeit zu entscheiden, um die Flexibilit\u00e4t zu erh\u00f6hen. Im Bereich der sozialen Sicherheit gibt es zahlreiche Probleme, besonders f\u00fcr \u201cFlex\u201c-Arbeitnehmer (nicht f\u00fcr Teilzeitbesch\u00e4ftigte an sich), aber hier wurden nur geringe substanzielle Verbesserungen erzielt. Im Arbeitsrecht wurden zwar bedeutende Ma\u03b2nahmen zur F\u00f6rderung der Flexibilit\u00e4t und der Sicherheit ergriffen, aber die Regierung hat in diesem Bereich nur mit Verz\u00f6gerung die Initiative ergriffen. Was die Vereinbarkeit von Arbeit und Betreuungspflichten angeht, haben sich die Bedingungen zwar aufgrund neuer Ma\u03b2nahmen verbessert, aber die Niederlande bleiben dennoch nach wie vor hinter anderen europ\u00e4ischen L\u00e4ndern zur\u00fcck.\n\n==\n\nBeatson 1994:\n\nTitle: Labour market flexibility\n\nAbstract: The overarching labour market policy of the British government is to aim for a competitive, efficient and flexible labour market. This study aims to assess whether the British labour market has indeed become more flexible.\n\n==\n\nLiechty 2007:\n\nTitle: Flexible Workplace Policies: Lessons From the Federal Alternative Work Schedules Act\n\nAbstract: Abstract: This case study uses a feminist framework to examine the 7-year process by which the Federal Alternative Work Schedules Act (1978 \u2013 1985) became law and the reasons for reenergized implementation in the 1990s. We analyze the legislative discourse for rationale in support of and opposition to this policy, connect findings to current flexible work research, and identify criteria for evaluating the impact of flextime policies with attention to intersections of gender and socioeconomic status. Unexpectedly, traditional views on hierarchical manager-worker relations figured more prominently than gender in the Congressional debates.\n\n==\n\nHalpern 2005:\n\nTitle: How time\u2010flexible work policies can reduce stress, improve health, and save money\n\nAbstract: Data from the US National Study of the Changing Workforce (a nationally representative sample of working adults) were used to test the hypothesis that employees with time-flexible work policies reported less stress, higher levels of commitment to their employer, and reduced costs to the organization because of fewer absences, fewer days late, and fewer missed deadlines. The model provides persuasive findings for the hypothesized relationship and offers important suggestions to employers who can translate reduced illness into savings and increased commitment into better employees. Contrary to expectations, there were no gender differences in how employees responded to flexible work policies, showing that gender-neutral work policies make financial sense. By showing that time-flexible work policies provide employer benefits, we can hasten the change to a new worker model\u2014one that is family and employer friendly. The business case for family-friendly work policies may prove to be the best tool we have in changing how we live and work. Copyright \u00a9 2005 John Wiley & Sons, Ltd.\n\n==\n\nBrewster 1997:\n\nTitle: Flexible Working in Europe: A Review of the Evidence\n\nAbstract: Introduction(1) The concept of flexible working practices(2), the extent of such practices and the implications for practitioners and policy makers in the area, have been much discussed. These are critical issues for employers, trade unions and governments. Recent opinions from the European Court of Justice have raised the political profile of the subject and the European Commission is committed to further action on this issue in 1997. This paper addresses these issues in a comparative European context. The paper presents evidence on developments in flexible working from organisations across Europe. Flexible working here covers only working time and contractual variations (temporary contracts, outsourcing etc.).(3) The paper summarise the debates on the topic; briefly outlines the research that was done; shows the extent and growth of flexibility; explores the reasons for the massive growth in flexible working in Europe and considers whether this is evidence of convergence of European labour markets. This is followed by a wide-ranging examination of the implications of the findings. Theories of Labour Flexibility The concept of \"labour flexibility\" remains, both in theoretical and practical terms, highly problematic. Despite the huge volume of literature devoted to the so-called \"flexibility debate\" (see bibliography in Brewster et al. 1996a), relatively little progress has been made in resolving many of the problems associated with the concept. In the literature, the term \"flexibility\" is applied to a series of quite distinct (if related) theories. There are those which have been labelled \"post-Fordist\": a category which covers a range of variants, but is characterised by the work of Piore and Sabel (1984), Mathews (1989a, 1989b, 1990, 1992), Lash and Urry (1987), Katz (1985), Kern and Schumann (1987), Tolliday and Zeitlin (1986), and Streeck (1987). For these writers, who generally concentrate on manufacturing industry, new technology is the key to a more flexible form of production, more responsive to increasingly rapid changes in the market. Whilst such developments may depend for their success upon a more skilled, motivated, and flexible workforce, the focus of this stream of writing is on production systems rather than employment. A more critical, \"neo-Marxian\" (Clegg 1990) or \"neo-Fordist\" (Wood 1989b, p. 21) group of writers is also concerned with flexible production, though taking a more negative view of its likely effect on individuals and including discussion of the impact on labour markets (Bramble 1988; Bramble/Fieldes 1989, 1992; Harvey 1989, 1991). An alternative conception of flexibility is provided by researchers in the operational management area. There is also an important set of literature labelled by some as \"managerialist\" (Bagguley 1991, p. 164) or \"neomanagerialist\" (Clegg 1990, p. 210) and typified by the work of Atkinson (1984, 1985a, 1985b, 1986, 1987; Atkinson/Gregory 1986; Atkinson/Meager 1986). His work has been subjected to critiques which have attempted to demonstrate the limited utility and lack of theoretical robustness of his work or even, in the early debates, to deny that the labour market had changed substantially, rather than attempting to build upon the insights which it provides or to develop a more comprehensive theoretical framework based on it (see for example: Pollert 1988a, 1988b). Nonetheless, Atkinson's work has been extremely influential. His vision of flexibility has influenced policy debates internationally (OECD 1986a, 1989). These changes and the development of a more flexible labour market have been controversial. They are seen by some to have taken our societies back towards the early years of the industrial revolution, with the creation of a significant underclass of underprivileged and \"vulnerable\" workers. They are seen by others as evidence of an uncaring and irresponsible approach to employees by employers -- sometimes associated with a desire to \"de-unionise\" the workforce. \u2026\n\n==\n\nGolden 2001:\n\nTitle: Flexible Work Schedules\n\nAbstract: More than 27% of the U.S. workforce now reports having an ability to alter their daily starting and ending times of work. Yet, provision of flexibility in the timing of work is not keeping pace with demand. Moreover, there is much disparity in access to schedule flexibility by workers' demographic, work, and job characteristics. Probit estimation finds that the probability that a worker has such flexibility is reduced by being female, non-White, and less educated. The likelihood is increased by being self-employed, in college, married, part-time, in certain occupations and industries, and working 50 or more hours per week. Flexibility is reduced for those working a standard day shift or 40-hour workweek. Workers thus sacrifice either leisure time or income to gain better access to flexibility in the scheduling of work, or they endure the costs of job mobility. Public policy should focus on delivering more flexible schedules to the excluded 73%.\n\n==\n\nBrewster 1997:\n\nTitle: Flexible working in Europe\n\nAbstract: This research paper analyzes and reports upon the current practice of flexible working amongst organizations in Europe: focussing on current developments in the use, by employing organizations, of parttime workers and a range of contractual variations (temporary work; fixed-term contracts etc). The analysis draws heavily, though not exclusively, upon a substantial, independent database of organizational level questionnaire responses covering all sectors of the economy in 14 European countries. We provide evidence that across Europe there is a substantial amount of flexible working and that there has been a continuing increase in its use. It is argued that these developments have considerable, as yet little understood, implications for policy makers, individuals and employers.", "final_summary": "Flexible work policy, a term that encompasses a variety of arrangements allowing employees to alter their working hours or location, has been the subject of extensive research. \n\nRubery (2016) critically examines the notion that flexible labor markets are a panacea for various labor market and societal challenges, arguing that they may generate contradictions and unsustainable long-term trends. Oorschot (2004) discusses the Dutch approach to flexible work, known as 'flexicurity', which aims to balance flexibility and security. However, the author notes that while part-time work has been promoted as a strategy for increasing flexibility, substantial improvements in social security for 'flex' workers have been limited.\n\nBeatson (1994) assesses the British government's overarching labor market policy, which aims for a competitive, efficient, and flexible labor market. Liechty (2007) provides a feminist perspective on the Federal Alternative Work Schedules Act, analyzing the legislative discourse for rationale in support of and opposition to this policy.\n\nHalpern (2005) presents data from the US National Study of the Changing Workforce, suggesting that time-flexible work policies can reduce stress, improve health, and save money for organizations. Brewster (1997) provides a comparative European context on flexible working practices, noting the massive growth in flexible working in Europe.\n\nGolden (2001) reports that more than 27% of the U.S. workforce has the ability to alter their daily starting and ending times of work, but notes that provision of flexibility in work timing is not keeping pace with demand. Brewster (1997) provides further evidence of the substantial amount of flexible working across Europe and the continuing increase in its use.\n\nIn conclusion, flexible work policies are a complex and multifaceted issue with significant implications for employees, employers, and society at large. While they offer potential benefits such as reduced stress and increased efficiency, they also present challenges and may contribute to disparities in the workforce. Further research and policy development are needed to maximize the benefits and mitigate the drawbacks of these policies."}, {"query": "organizational attention diversion", "paper_list_string": "Yaniv 2011:\n\nTitle: ORGANIZATIONAL ATTENTION: A METAPHOR FOR A CORE COGNITIVE PROCESS\n\nAbstract: Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen\u0219s domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.\n\n==\n\nYaniv 2011:\n\nTitle: Organizational Attention\n\nAbstract: INTRODUCTION Attention is a term commonly used in education, psychiatry, and psychology. Attention can be defined as an internal cognitive process by which one actively selects environmental information (i.e., sensation) or actively processes information from internal sources (i.e., stored memories and thoughts; Sternberg, 1996). In more general terms, attention can be defined as an ability to focus and maintain interest in a given task or idea, including managing distractions. Attention is selective by its nature. According to Pashler (1998, p. 37), \" The process of selecting from among the many potentially available stimuli is the clearest manifestation of selective attention. \" Why do firms respond to certain events or stimuli in their environment while neglecting others? It seems that organizations, just like individuals , have limited attention capacity. Hence, they must select from among the many potentially available stimuli and respond to these selected stimuli only. Organizational attention is defined as the socially structured pattern of attention by decision makers within the organization (Ocasio, 1997). Organizational attention, like human attention , is a limited resource: \" Attentional limits filter or screen incoming information such that a great deal of data pertinent to strategic decision may never get processed \" (2003) show that the extent to which CEOs (chief executive officers) are selective in their attention to sectors of the environment is a significant predictor of performance. Knowledge management (KM) models and process theories, almost without exception, incorporate a stage or phase in which a given knowledge item is brought to bear on a current decision or action. This stage, referred to alternatively as, is of crucial importance in any knowledge-management cycle. The flow of knowledge in and out of an awareness stage is not merely a function of the universe of available organizational memory or the technological tools available to filter and identify such knowledge. It is influenced to a large degree by organizational attention. The second area in which organizational attention is key is knowledge acquisition and creation as discussed by Ocasio (1997), and Yaniv and Elizur (2003). Successful knowledge management requires attention. Davenport and Volpel (2001) argues that attention is the currency of the information age. Knowledge consumers must pay attention to knowledge and become actively involved in the knowledge-transfer processes. This is particularly important when the knowledge to be received is tacit (Nonaka, 1994). Knowledge can be part of the organization's repository, however, if it does not \u2026\n\n==\n\nMay 2007:\n\nTitle: Organizing Attention: Responses of the Bureaucracy to Agenda Disruption\n\nAbstract: Federal agencies are routinely confronted with requests from policymakers that they must address in some manner. These range from routine directives to cut through red tape to exceptional demands to alter policy priorities. We theorize that how attention is organized by public bureaucracies affects their responses. We draw on a variety of scholarship about public bureaucracies to develop a theory about the bureaucratic organization of attention and its consequences. In illustrating these notions, we trace federal agency attention to the threat of terrorism as it gained prominence on the national policy agenda over the 1980s to 1990s and became a prominent issue after the terrorist attacks of 2001. The consequences of the Department of Homeland Security's centralized attention to the terrorism threat suggest a paradox of issue attention. Though concentration of authority at the top of the organization holds the prospect of control over the substance and speed of policymaking, this control is highly circumscribed by the limits of attention faced by all organizations.\n\n==\n\nG\u00f3mez 2018:\n\nTitle: Just Paying Attention: Communication for Organizational Attention\n\nAbstract: The main premises in this article are that organizational attention is inherently communicative, and can be nurtured through communication interventions. Two communication practices that reflect organizational attention\u2014information allocation and dialogue\u2014can be nurtured through organizational structures and interventions. Increasing opportunities for dialogue across organizational functions is critical to improve collective attention. Prior research and empirical data are presented to assert that a long-term orientation is also imperative to develop attention through communication practices such as information allocation and dialogue.\n\n==\n\nRamos 2019:\n\nTitle: The Impact of using IT artefacts on Organizational Attention: the Case of a City Hall\n\nAbstract: Using the concept of organizational attention proposed by William Ocasio (1997), which states that formal, informal and unofficial communication engages collective attention in issues and action alternatives relevant for decision, our study investigates the role played by IT artefacts in shaping attentional engagement. Based on a case study of the City Planning Division of the Porto City Council in Portugal, our findings reveal that IT artefacts channel attentional engagement, expanding or hindering the ability to understand situations. When shifts in attention are promoted by a changing strategy (the dominant pattern of attention) without the corresponding change in the IT infrastructure, serious difficulties may arise in sharing perceptions that strongly bind collective cognition and action to the achievement of organizational goals (collective attentional engagement). In order to circumvent arising problems, unofficial IT-supported communicative practices tend to emerge. Our exploratory research extends the literature on organizational attention and information systems by providing insights on the impact of IT use on organizational attention.\n\n==\n\nFerreira 2010:\n\nTitle: Attention Mosaics: Studies of Organizational Attention\n\nAbstract: textabstractOrganizational studies emphasizing the role of attention in organizational behavior depart from the idea that organizations, like individuals, have limited capacity to attend to environmental stimuli. The bounded capacity of the organizations to respond to stimuli is conditioned by the limited cognitions of individuals and by the limited capability of organizations to distribute, coordinate and integrate those cognitions. The cross-level nature of organizational attention, its dual character as both a process and an output, means that theories of attention afford interesting insights to explain organizational behavior.\nThis dissertation presents one conceptual and two empirical studies about organizational attention. In the conceptual study entitled \u201cAttention span: expanding the attention-based view to team, organizational and social movements levels\u201d, it is argued that attentional processes have functional equivalence at the team, organizational and social movements level. The study entitled \u201cWhen a thousand words are (not) enough: an empirical study of the relationship between firm performance and attention to shareholders\u201d, tests the power of the attention-based view combined with resource dependence theory to explain the relationship between financial performance and attention to shareholders. Finally, the study \u201cSense and sensibility: testing the effects of attention structures and organizational attention on financial performance\u201d tests the process model of situated attention by examining the effects of attention structures and the allocation of attention on organizational social responses and performance/\nTogether, these studies deepen and expand attentional perspectives on organizational behavior. Moreover, they renew scholars\u2019 interest in organizational attention, indicating some of the strengths and limitations of theories of attention and also revealing a prolific research stream.\n\n==\n\nDessein 2013:\n\nTitle: Rational Inattention and Organizational Focus\n\nAbstract: This paper studies optimal communication flows in organizations. A production process can be coordinated ex ante, by letting agents stick to a prespecified plan of action. Alternatively, agents may adapt to task-specific shocks, in which case tasks must be coordinated ex post, using communication. When attention is scarce, an optimal organization coordinates only a few tasks ex post. Those tasks are higher performing, more adaptive to the environment, and influential. Hence, scarce attention requires setting priorities, not just local optimization. Our results provide microfoundations for a central idea in the management literature that firms should focus on a limited set of core competencies.\n\n==\n\nForster 2016:\n\nTitle: Establishing the Attention-Distractibility Trait\n\nAbstract: Failures to focus attention will affect any task engagement (e.g., at work, in the classroom, when driving). At the clinical end, distractibility is a diagnostic criterion of attention-deficit/hyperactivity disorder (ADHD). In this study, we examined whether the inability to maintain attentional focus varies in the overall population in the form of an attention-distractibility trait. To test this idea, we administered an ADHD diagnostic tool to a sample of healthy participants and assessed the relationship between ADHD symptoms and task distraction. ADHD symptom summary scores were significantly positively associated with distractor interference in letter-search and name-classification tasks (as measured by reaction time), as long as the distractors were irrelevant (cartoon images) rather than relevant (i.e., compatible or incompatible with target names). Higher perceptual load during a task eliminated distraction irrespective of ADHD score. These findings suggest the existence of an attention-distractibility trait that confers vulnerability to irrelevant distraction, which can be remedied by increasing the level of perceptual load during the task.", "final_summary": "Organizational attention, as explored by Yaniv (2011) and Ferreira (2010), refers to how organizations selectively focus on certain stimuli while ignoring others due to their limited cognitive capacity. This concept is likened to the cognitive process of attention in individuals, where a limited amount of information is selected and processed from a vast array of internal and environmental inputs (Yaniv, 2011).\n\nYaniv (2011) further suggests that organizational attention is a critical factor in managing knowledge within an organization. It plays a significant role in knowledge acquisition, creation, and transfer processes, particularly when dealing with tacit knowledge. May (2007) adds that the way attention is organized within bureaucracies can influence their responses to policy directives, ranging from routine to exceptional demands.\n\nCommunication is also a key aspect of organizational attention. G\u00f3mez (2018) posits that organizational attention is inherently communicative and can be nurtured through communication interventions. Similarly, Ramos (2019) found that IT artefacts can shape attentional engagement, either expanding or hindering the ability to understand situations.\n\nDessein (2013) extends the concept of organizational attention to the idea of focus on core competencies. He suggests that optimal communication flows within organizations require setting priorities, particularly when attention is scarce. This aligns with the management literature's idea that firms should concentrate on a limited set of core competencies.\n\nLastly, Forster (2016) introduces the concept of an attention-distractibility trait, suggesting that the inability to maintain attentional focus varies in the overall population and can be remedied by increasing the level of perceptual load during a task.\n\nIn conclusion, organizational attention is a multifaceted concept that plays a crucial role in knowledge management, policy response, communication, and focus on core competencies. It is influenced by various factors, including the organization's structure, communication practices, IT infrastructure, and the inherent attention-distractibility trait of individuals within the organization."}, {"query": "organizational attention definition Ocasio", "paper_list_string": "Yaniv 2011:\n\nTitle: Organizational Attention\n\nAbstract: INTRODUCTION Attention is a term commonly used in education, psychiatry, and psychology. Attention can be defined as an internal cognitive process by which one actively selects environmental information (i.e., sensation) or actively processes information from internal sources (i.e., stored memories and thoughts; Sternberg, 1996). In more general terms, attention can be defined as an ability to focus and maintain interest in a given task or idea, including managing distractions. Attention is selective by its nature. According to Pashler (1998, p. 37), \" The process of selecting from among the many potentially available stimuli is the clearest manifestation of selective attention. \" Why do firms respond to certain events or stimuli in their environment while neglecting others? It seems that organizations, just like individuals , have limited attention capacity. Hence, they must select from among the many potentially available stimuli and respond to these selected stimuli only. Organizational attention is defined as the socially structured pattern of attention by decision makers within the organization (Ocasio, 1997). Organizational attention, like human attention , is a limited resource: \" Attentional limits filter or screen incoming information such that a great deal of data pertinent to strategic decision may never get processed \" (2003) show that the extent to which CEOs (chief executive officers) are selective in their attention to sectors of the environment is a significant predictor of performance. Knowledge management (KM) models and process theories, almost without exception, incorporate a stage or phase in which a given knowledge item is brought to bear on a current decision or action. This stage, referred to alternatively as, is of crucial importance in any knowledge-management cycle. The flow of knowledge in and out of an awareness stage is not merely a function of the universe of available organizational memory or the technological tools available to filter and identify such knowledge. It is influenced to a large degree by organizational attention. The second area in which organizational attention is key is knowledge acquisition and creation as discussed by Ocasio (1997), and Yaniv and Elizur (2003). Successful knowledge management requires attention. Davenport and Volpel (2001) argues that attention is the currency of the information age. Knowledge consumers must pay attention to knowledge and become actively involved in the knowledge-transfer processes. This is particularly important when the knowledge to be received is tacit (Nonaka, 1994). Knowledge can be part of the organization's repository, however, if it does not \u2026\n\n==\n\nRamos 2019:\n\nTitle: The Impact of using IT artefacts on Organizational Attention: the Case of a City Hall\n\nAbstract: Using the concept of organizational attention proposed by William Ocasio (1997), which states that formal, informal and unofficial communication engages collective attention in issues and action alternatives relevant for decision, our study investigates the role played by IT artefacts in shaping attentional engagement. Based on a case study of the City Planning Division of the Porto City Council in Portugal, our findings reveal that IT artefacts channel attentional engagement, expanding or hindering the ability to understand situations. When shifts in attention are promoted by a changing strategy (the dominant pattern of attention) without the corresponding change in the IT infrastructure, serious difficulties may arise in sharing perceptions that strongly bind collective cognition and action to the achievement of organizational goals (collective attentional engagement). In order to circumvent arising problems, unofficial IT-supported communicative practices tend to emerge. Our exploratory research extends the literature on organizational attention and information systems by providing insights on the impact of IT use on organizational attention.\n\n==\n\nYaniv 2011:\n\nTitle: ORGANIZATIONAL ATTENTION: A METAPHOR FOR A CORE COGNITIVE PROCESS\n\nAbstract: Organizational attention is an underdeveloped construct that can account for a variety of organizational phenomena. Attention is the means by which individuals select and process a limited amount of input from the enormous amount of internal and environmental inputs bombarding the senses, memories and other cognitive processes. This article develops a coherent theory of organizational attention, drawing on Cornelissen\u0219s domain-interactive metaphor model. Topics that form the building blocks of individual attention research, including selective and divided attention, automatic versus controlled processes, attention and memory, attention and learning, are examined in terms of their applicability to the organizational context.\n\n==\n\nEberhard 2013:\n\nTitle: Unpacking the Conceptual Linkages Between Organizational Attention and Sensemaking\n\nAbstract: \u201cFurther examining the relationship between sensemaking and attention may provide additional insights because sensemaking is particularly consequential when rare or critical events make top-down attention insufficient.\u201d (Ocasio, 2011) The purpose of the symposium is to openly debate the relatively unexplored but critical relationship between organizational attention and sensemaking (Ocasio, 1997; Weick, 1995) and to describe contexts and approaches for empirical work most likely to yield further insights. Challenges to this conversation include bridging the multiplicity of perspectives and language in the field of attention (Ocasio, 2011), crossing levels of analysis between attention and sensemaking approaches (Maitlis & Sonenshein, 2010; Weber & Glynn, 2006), and defining where attention ends and sensemaking begins (or vice versa) and whether the two constructs intersect, interact, or operate in parallel. We present three case studies of rare events that cross levels of analysis and temporal contexts, a...\n\n==\n\nG\u00f3mez 2018:\n\nTitle: Just Paying Attention: Communication for Organizational Attention\n\nAbstract: The main premises in this article are that organizational attention is inherently communicative, and can be nurtured through communication interventions. Two communication practices that reflect organizational attention\u2014information allocation and dialogue\u2014can be nurtured through organizational structures and interventions. Increasing opportunities for dialogue across organizational functions is critical to improve collective attention. Prior research and empirical data are presented to assert that a long-term orientation is also imperative to develop attention through communication practices such as information allocation and dialogue.\n\n==\n\nFerreira 2010:\n\nTitle: Attention Mosaics: Studies of Organizational Attention\n\nAbstract: textabstractOrganizational studies emphasizing the role of attention in organizational behavior depart from the idea that organizations, like individuals, have limited capacity to attend to environmental stimuli. The bounded capacity of the organizations to respond to stimuli is conditioned by the limited cognitions of individuals and by the limited capability of organizations to distribute, coordinate and integrate those cognitions. The cross-level nature of organizational attention, its dual character as both a process and an output, means that theories of attention afford interesting insights to explain organizational behavior.\nThis dissertation presents one conceptual and two empirical studies about organizational attention. In the conceptual study entitled \u201cAttention span: expanding the attention-based view to team, organizational and social movements levels\u201d, it is argued that attentional processes have functional equivalence at the team, organizational and social movements level. The study entitled \u201cWhen a thousand words are (not) enough: an empirical study of the relationship between firm performance and attention to shareholders\u201d, tests the power of the attention-based view combined with resource dependence theory to explain the relationship between financial performance and attention to shareholders. Finally, the study \u201cSense and sensibility: testing the effects of attention structures and organizational attention on financial performance\u201d tests the process model of situated attention by examining the effects of attention structures and the allocation of attention on organizational social responses and performance/\nTogether, these studies deepen and expand attentional perspectives on organizational behavior. Moreover, they renew scholars\u2019 interest in organizational attention, indicating some of the strengths and limitations of theories of attention and also revealing a prolific research stream.\n\n==\n\nMarkovska-Simoska 2013:\n\nTitle: \u041erganizational Attention Deficit as Sustainability Indicator: Assessment and Management\n\nAbstract: The aim of this paper is to contribute towards dealing with the at- tention indicator in a standard Pressure-State-Response (PSR) framework of sustainable development. Hence, the daily information overflow is identified as a pressure variable adversely affecting the attention. This is manifested by an increased likelihood of missing key information when making decisions, diminished time for attentive reflection, difficulty in holding others' attention, as well as by decreased ability to focus when necessary, which are symptoms of the so-called organizational attention deficit disorder - OADD. As to the state variable there were combined techniques from cognitive neuroscience and applied sciences in order to objectively measure the incidence of OADD. In particular, the use of neuropsychological procedures for objective assessment of OADD will be summarized. Namely, few attention diagnostic tests of the Amsterdam Neuropsychological Tasks Program (ANT) will be considered for determining the focused, shifted and sustained attention. As intervention measure attention strengthening techniques is applied. In particular, in re- search were proposed the neurofeedback technique as response variable and have shown its effectiveness in coping with the OADD. This approach repre- sents a new perspective in social psychophysiology and expands the potential applications of ANT and neurofeedback methodology. 1 Macedonian Academy of Sciences and Arts, Skopje, Republic of Macedonia,\n\n==\n\nOrgan 1997:\n\nTitle: Organizational Citizenship Behavior: It's Construct Clean-Up Time\n\nAbstract: Accumulated empirical evidence, some telling criticisms, and even the most cursory glance at the business press compel us to rethink the defining character of organizational citizenship behavior (OCB). It no longer seems fruitful to regard OCB as extra-role, beyond the job, or unrewarded by the formal system. A more tenable position is one that defines OCB much along the lines of what Borman and Motowidlo (1993) called contextual performance. Some preliminary suggestions are offered for the repositioning and articulation of the OCB construct as redefined; due attention is given to the problems that nonetheless remain, and will occupy us for some time to come as we reckon with root changes in the very character of organizations.", "final_summary": "Organizational attention, a concept that has been explored by various researchers including Yaniv (2011), Ferreira (2010), and Ocasio (1997), is a cognitive process that determines how organizations select and respond to environmental stimuli. \n\nOcasio's work, as cited by multiple researchers, suggests that organizational attention is engaged through formal, informal, and unofficial communication, emphasizing the role of collective attention in decision-making processes (Yaniv, 2011; Ramos, 2019). This attention is influenced not only by the available organizational memory or technological tools but also by the organization's ability to manage and distribute knowledge (Yaniv, 2011).\n\nIT artefacts have been found to shape attentional engagement, either expanding or hindering the ability to understand situations (Ramos, 2019). A shift in attention due to a changing strategy without a corresponding change in IT infrastructure can lead to difficulties in sharing perceptions that bind collective cognition and action to organizational goals (Ramos, 2019).\n\nThe relationship between organizational attention and sensemaking is also explored, with attention being particularly crucial when rare or critical events make top-down attention insufficient (Eberhard, 2013). This relationship is complex and can be influenced by multiple perspectives and levels of analysis (Eberhard, 2013).\n\nCommunication practices such as information allocation and dialogue are critical for nurturing organizational attention, and a long-term orientation is necessary for developing attention through these practices (G\u00f3mez, 2018). \n\nIn conclusion, organizational attention is a complex, multi-faceted concept that plays a crucial role in decision-making, knowledge management, and sensemaking within organizations. It is influenced by various factors, including communication practices, IT infrastructure, and the organization's ability to manage and distribute knowledge. Further research is needed to fully understand the intricacies of this concept and its impact on organizational behavior."}, {"query": "Amylase-Producing Bacteria", "paper_list_string": "Ekka 2018:\n\nTitle: Screening , Isolation and Characterization of Amylase Producing Bacteria and optimization for Production of Amylase\n\nAbstract: Amylase is (E.C.3.2.1.1-1,4-alpha D-glucanohydrolase) an extracellular enzyme, which is involved in the starch processing industries where it breaks starch into simple sugar constituents.Amylase has extensive application in starch processing, brewing and sugar production, in textile industries and in detergent manufacturing processes. Interestingly, the first enzyme produced industrially was an amylase.In the present study, amylase producing bacteria were isolated from rice field, sugarcane field and sugarcane dump area and characterized for their morphological and biochemical properties. Then amylase activity of isolated bacterial cultures were determined and it was concluded that 3 (NN1, NN2, NN5)out of 6 bacterial colonies(NN1, NN2, NN3, NN4, NN5, NN6) were potent and their enzyme activity was more than other colonies. The potent colonies were also optimized for enzyme activity under certain conditions like different carbon sources, nitrogen sources, pH, incubation time and chlorides.Agro-industrial wastes were used as substrate for amylase production by Solid-State FermentationSSF) and we have found that wheat bran was the suitable substrate for amylase production.\n\n==\n\nGarba 2021:\n\nTitle: Preliminary Investigation of Amylase Producing-Bacteria from Soil in Gombe Metropolis\n\nAbstract: Amylases are enzymes that are able to hydrolyse starch or glycogen molecules into polymers of glucose units. They have great potential applications in various industrial processes like in pharmaceutical, fermentation and food industries. Research on starch degrading enzymes has resulted into increased applications of amylases in different industrial processes. These enzymes occupy a greater space in the current biotechnological processes such as detergent, starch degradation, pharmaceutical, foodstuff, textile, and paper manufacturing. In fact, amylases constitute nearly 25% of the total sale of global enzymes. Amylases have been screened and identified from various sources, both eukaryotic and prokaryotic organisms such as animals, plants, fungi and bacteria, respectively. To further isolate novel amylases with enhanced desirable properties for such diverse industrial application, more organisms need to be screened. In this study, a total of 27 bacterial isolates were isolated from soil samples in Gombe metropolis. The bacteria were screened for amylase production using plate screening method. Each isolate was streaked onto a 1% starch agar plate and incubated for 24h at 37 \u00c2\u00b0C. The plates were covered with iodine solution and observed for positive amylase isolates based on the formation of clearing zones against the blue black background. The results confirmed eight (8) isolates of amylase-producing bacteria which include Bacillus subtilis, Escherichia coli, Streptococcus spp., Salmonella spp., Pseudomonas spp., Serratia spp., Proteus vulgaris, and Klebsiella spp. In conclusion, bacterial isolates capable of amylase production have been successfully screened and identified. This research may serve as a stepping stone to isolating functional amylase enzymes from these bacteria for promising industrial applications.\n\n==\n\nSamanta 2013:\n\nTitle: Characterization and Optimization of Amylase Producing Bacteria Isolated from Solid Waste\n\nAbstract: Municipal waste is one of the most hazardous components of developing countries. However, enzymes do provide an eco-friendly solution in this case. Amylase is an important enzyme in food, textile and pharmaceutical industry and can be used for bioconversion of waste. From the municipal solid waste we have isolated an amylase producing bacteria that can grow in the irritant municipal waste and help in their bio conversation. The bacteria were identified as Cronobacter sakazakii Jor52 (C2). The optimized media for maximum amylase production after 24 h of incubation, contains 2% starch, 0.6% peptone, 0.01% CaCl2, 0.05% KCl, 0.05% MgSO4 and 0.05% K2HPO4. The crude enzyme activity and stability study revealed that the amylase is stable within the pH 6 - 8 and temperature 30\u00b0C - 40\u00b0C and give maximum activity at 37\u00b0C at pH-8.\n\n==\n\nSyed 2013:\n\nTitle: Isolation of Amylase Producing Bacteria from Solar Salterns of Nellore District, Andhra Pradesh, India\n\nAbstract: \u03b1-Amylases are a class of starch degrading enzymes catalyzing the hydrolysis of internal \u03b1-1,4-O-glycosidic bonds in polysaccharides. The following investigation were carried out to isolate haloalkaliphilic bacteria, a group of organisms with twin extremities of pH and salinity, capable of producing \u03b1-amylases from an artificial solar saltern. A total of 25 discrete colonies were isolated, 21 isolates showed amylase production. Among these 7 isolates produced amylase at extreme conditions such as salt, alkalinity and temperature. The isolates were characterized biochemically and also for other enzymes. From the results it is imperative that these isolates can be further studied to exploit them up to industrial scale.\n\n==\n\nShah 2012:\n\nTitle: Characterization of Amylase Producing Bacterial Isolates\n\nAbstract: Amylases are among the most important industrial enzymes and also have great significance in Biotechnological studies. In this study cultural, morphological, and metabolic characteristics of the bacterial isolates were studied. Total 18 bacterial cultures were isolated from collected soil samples. Among 18 bacterial isolates, 14 isolates showed the amylolytic activity. These 18 isolate was identified according to Bergey\u2019s manual of systemic Bacteriology .These isolates related to Bacillus sp. The optimum pH for the growth of all the cultures was observed at pH 7. Submerged fermentation was carried out for the production of amylase was observed in the range of 0.045-1.35 U/min/mL. The maximum activity of amylase was 1.35 (U/min/mL) after 48 hours was recorded, have great significance.\n\n==\n\nSingh 2017:\n\nTitle: Isolation and Characterization of Amylase producing Bacteria from Diverse Environmental samples\n\nAbstract: Amylases are starch hydrolyzing enzymes. These proteins are very important to various industrial processes. Microorganisms are considered as a best source of amylase production. Most of the microbial processes are time consuming and thus the present study is focused on isolation and characterization of a microbial spp. which can produce amylase in early phase of growth with minimum time of incubation. Diverse environmental samples were collected from various locations and subjected for isolation on defined medium with a selection pressure. The isolate was identified and characterized for enzyme production and nutritional requirements. Key words: Starch, Amylase, Identification, Optimization etc.\n\n==\n\nLuang-In 2019:\n\nTitle: Isolation and Identification of Amylase-producing Bacteria from Soil in Nasinuan Community Forest, Maha Sarakham, Thailand\n\nAbstract: This study aimed to isolate and identify bacteria that can produce amylase enzyme from the unexplored Nasinuan Forest, Kantarawichai District, Mahasarakham Province, Thailand. Thirteen bacterial isolates with amylase-producing capacity on 1% starch agar were identified using 16S rRNA sequencing. Twelve bacteria were gram-positive, rod shaped and identified as Bacillus spp. and one bacterium with gram-negative and rod shaped character was Enterobacter cloacae. Their closest relatives were found in India, China, Korea, Indonesia, Argentina, Italy, Israel, USA, Argentina and South Africa. These bacteria were tested for specific amylase activity after 1-3 days enzyme induction with 1% starch at 37\u00b0C. The results showed the highest specific activity at day 2 incubation in the order: Bacillus cereus 3.5AL2 > 3.4AL1 > 1.4AL3 and thus 2-day enzyme induction was chosen for further analysis. Bacillus sp. 3.5AL2 was found to exhibit the highest specific amylase enzyme activity of 1.97 \u00b1 0.41 U/mg protein at the optimal conditions of 60\u00b0C and pH 7.0 after 30 min incubation with 1% starch in 0.05 M PBS buffer. This amylase\u2013producing bacterial strain offers great potential for applications in food and agricultural industries in Thailand.\n\n==\n\nIndriati 2018:\n\nTitle: Potency of Amylase-producing Bacteria and Optimization Amylase Activities\n\nAbstract: Enzymes are capable to act as biocatalyst for a wide variety of chemical reactions. Amylase have potential biotechnological applications in a wide range of industrial processes and account for nearly 30% of the world\u2019s enzyme market. Amylase are extracellular enzymes that catalyze the hydrolysis of internal \u03b1-1,4-glycosidic linkages in starch to dextrin, and other small carbohydrate molecules constituted of glucose units. Although enzymes are produced from animal and plant sources, the microbial sources are generally the most suitable for commercial applications. Bacteria from hot springs is widely used as a source of various enzymes, such as amylase. But the amount of amylase-producing bacteria is still very limited. Therefore it is necessary to search sources of amylase-producing bacteria new, such as from hot springs Pariangan. The purpose of this study was to isolation of amylase-producing bacteria from Pariangan hot spring, West Sumatera and amylase activity optimization. The results were obtained 12 isolates of thermophilic bacteria and 5 isolates of amyalse-producing bacteria with the largest amylolytic index of 3.38 mm. The highest amylase activity was obtained at 50\u00b0C and pH 7.5.", "final_summary": "Amylase-producing bacteria are a captivating subject of study due to their extensive industrial applications. These bacteria, which produce amylase, an enzyme that simplifies starch into sugars, are found in a variety of environments globally (Ekka, 2018; Garba, 2021; Samanta, 2013; Syed, 2013; Shah, 2012; Singh, 2017; Luang-In, 2019; Indriati, 2018).\n\nEkka (2018) isolated amylase-producing bacteria from rice and sugarcane fields, discovering that three out of six colonies exhibited potent enzyme activity. Similarly, Garba (2021) isolated 27 bacterial strains from soil samples, eight of which were confirmed as amylase-producers, including common bacteria such as Escherichia coli and Bacillus subtilis. Samanta (2013) isolated an amylase-producing strain, Cronobacter sakazakii, from municipal solid waste, suggesting potential for waste bioconversion.\n\nSyed (2013) focused on isolating haloalkaliphilic bacteria, which thrive in high salt and alkaline conditions, from solar salterns. Out of 25 colonies, 21 showed amylase production. Shah (2012) isolated 18 bacterial cultures from soil samples, with 14 showing amylolytic activity, all related to the Bacillus species. Singh (2017) isolated amylase-producing bacteria from various environmental samples, but the abstract does not specify the growth phase in which these bacteria produce amylase.\n\nLuang-In (2019) isolated 13 amylase-producing bacteria from the Nasinuan Forest in Thailand, with Bacillus cereus showing the highest specific amylase activity. Lastly, Indriati (2018) isolated 12 thermophilic bacteria from hot springs, five of which were amylase-producers, with the highest amylase activity at 50\u00b0C and pH 7.5.\n\nIn conclusion, these studies collectively demonstrate the ubiquity and diversity of amylase-producing bacteria in various environments. Their ability to produce amylase, an enzyme with significant industrial applications, underscores their potential for biotechnological exploitation. Further research is needed to fully harness this potential and optimize amylase production for industrial use."}, {"query": "monetary policy, bank assets", "paper_list_string": "Dang 2020:\n\nTitle: Monetary policy, bank leverage and liquidity\n\nAbstract: PurposeThe study explores how banks design their financial structure and asset portfolio in response to monetary policy changes.Design/methodology/approachThe authors conduct the research design for the Vietnamese banking market during 2007\u20132018. To ensure robust findings, the authors employ two econometric models of static and dynamic panels, multiple monetary policy indicators and alternative measures of bank leverage and liquidity.FindingsBanks respond to monetary expansion by raising their financial leverage on the liability side and cutting their liquidity positions on the asset side. Further analysis suggests that larger banks' financial leverage is more responsive to monetary policy changes, while smaller banks strengthen the potency of monetary policy transmission toward bank liquidity. Additionally, the authors document that lower interest rates induce a beneficial effect on the net stable funding ratio (NSFR) under Basel III guidelines, implying that banks appear to modify the composition of liabilities to improve the stability of funding sources.Originality/valueThe study is the first attempt to simultaneously examine the impacts of monetary policy on both sides of bank balance sheets, across various banks of different sizes under a multiple-tool monetary regime. Besides, understanding how banks organize their stable funding sources and illiquid assets amid monetary shocks is an innovation of this study.\n\n==\n\nAikman 2006:\n\nTitle: Bank Capital, Asset Prices and Monetary Policy\n\nAbstract: We study a general equilibrium model in which informational frictions impede entrepreneurs' ability to borrow and banks' ability to intermediate funds. These financial market frictions are embedded in an otherwise-standard dynamic New Keynesian model. We find that exogenous shocks have an amplified and more persistent effect on output and investment, relative to the case of perfect capital markets. The chief contribution of the paper is to analyse how these financial sector imperfections - in particular, those relating to the banking sector - modify our understanding of optimal monetary policy. Our main finding is that optimal monetary policy tolerates only a very small amount of inflation volatility. Given that similar results have been reported for models that abstract from banks, we conclude that assigning a non-trivial role for banks need not materially affect the properties of optimal monetary policy.\n\n==\n\nGambacorta 2018:\n\nTitle: Why Bank Capital Matters for Monetary Policy\n\nAbstract: One aim of post-crisis monetary policy has been to ease credit conditions for borrowers by unlocking bank lending. We find that bank equity is an important determinant of both the bank's funding cost and its lending growth. In a cross-country bank-level study, we find that a 1 percentage point increase in the equity-to-total assets ratio is associated with a 4 basis point reduction in debt financing and with a 0.6 percentage point increase in annual loan growth.\n\n==\n\nMcMillin 1996:\n\nTitle: Monetary policy and bank portfolios\n\nAbstract: Abstract This paper examines the existence of the bank lending channel for monetary policy over the period 1973:1\u20131994:11. The results are consistent with a bank lending channel when the Bernanke-Blinder model is extended to include commercial paper and the spread between the loan and commercial paper rates. The results are robust to alternative monetary policy measures. However, stability tests indicate instability over the nonborrowed reserves operating regime. When the estimates excluded data for this period, there was little evidence of systematic movement in bank loans in the direction predicted by the bank lending channel.\n\n==\n\nHeuvel 2006:\n\nTitle: The Bank Capital Channel of Monetary Policy\n\nAbstract: This paper examines the role of bank lending in the transmission of monetary policy in the presence of capital adequacy regulations. I develop a dynamic model of bank asset and liability management that incorporates risk-based capital requirements and an imperfect market for bank equity. These conditions imply a failure of the Modigliani-Miller theorem for the bank: its lending will depend on the bank\u00e2\u20ac\u2122s financial structure, as well as on lending opportunities and market interest rates. Combined with a maturity mismatch on the bank\u00e2\u20ac\u2122s balance sheet, this gives rise to a \u00e2\u20ac\u02dcbank capital channel\u00e2\u20ac\u2122 by which monetary policy affects bank lending through its impact on bank equity capital. This mechanism does not rely on any particular role of bank reserves and thus falls outside the conventional \u00e2\u20ac\u02dcbank lending channel\u00e2\u20ac\u2122. I analyze the dynamics of the new channel. An important result is that monetary policy effects on bank lending depend on the capital adequacy of the banking sector; lending by banks with low capital has a delayed and then amplified reaction to interest rate shocks, relative to well-capitalized banks. Other implications are that bank capital affects lending even when the regulatory constraint is not momentarily binding, and that shocks to bank profits, such as loan defaults, can have a persistent impact on lending\n\n==\n\nBernanke 2000:\n\nTitle: Monetary Policy and Asset Price Volatility\n\nAbstract: We explore the implications of asset price volatility for the management of monetary policy. We show that it is desirable for central banks to focus on underlying inflationary pressures. Asset prices become relevant only to the extent they may signal potential inflationary or deflationary forces. Rules that directly target asset prices appear to have undesirable side effects. We base our conclusions on (i) simulation of different policy rules in a small scale macro model and (ii) a comparative analysis of recent U.S. and Japanese monetary policy.\n\n==\n\nBianchi 2014:\n\nTitle: Banks, Liquidity Management and Monetary Policy\n\nAbstract: We develop a new framework for studying the implementation of monetary policy through the banking sector. Banks are subject to a maturity mismatch problem leading to precautionary holdings of reserves. Through various instruments, monetary policy alters tradeos banks face between lending, holding reserves, holding deposits and paying dividends. This translates into the real economy via eects on real interests and lending. We study how these instruments interact with shocks to the volatility in the payments system, bank losses, the demand for loans and with capital requirements. We use a calibrated version of the model to answer, quantitatively, why have banks held onto a substantial increase in reserves while not increasing lending since 2008.\n\n==\n\nC\u00fardia 2010:\n\nTitle: The Central-Bank Balance Sheet as an Instrument of Monetary Policy\n\nAbstract: While many analyses of monetary policy consider only the adjustment of a central bank\u2019s target for a short-term nominal interest rate, other dimensions of policy have recently been of greater importance: changes in the supply of bank reserves beyond those required to achieve an interest-rate target, changes in the assets acquired by central banks, and changes in the interest rate paid on reserves. We extend a standard New Keynesian model to allow a role for the central bank\u2019s balance sheet in equilibrium determination, and consider the connections between these alternative dimensions of policy and traditional interest-rate policy. We distinguish between \\quantitative easing\" in the strict sense and targeted asset purchases by a central bank, and argue that while the former is likely be inefiective at all times, the latter dimension of policy can be efiective when flnancial markets are su\u2010ciently disrupted. Neither is a perfect substitute for conventional interest-rate policy, but purchases of illiquid assets are particularly likely to improve welfare when the zero lower bound on the policy rate is reached. We also consider optimal policy with regard to the payment of interest on reserves, and argue that the interest rate on reserves should be kept near the central bank\u2019s target for the policy rate at all times.", "final_summary": "Monetary policy plays a significant role in shaping bank assets and their financial structure. Dang (2020) found that banks respond to monetary expansion by increasing their financial leverage and reducing their liquidity positions. This response varies with bank size, with larger banks being more responsive in terms of financial leverage and smaller banks in terms of liquidity.\n\nBank capital also plays a crucial role in the interaction between monetary policy and bank assets. Aikman (2006) found that financial sector imperfections, particularly those related to the banking sector, amplify and prolong the effects of exogenous shocks on output and investment. Gambacorta (2018) further emphasized the importance of bank equity, showing that it significantly influences both the bank's funding cost and its lending growth.\n\nThe bank lending channel of monetary policy is another aspect that has been explored. McMillin (1996) found evidence of this channel when considering commercial paper and the spread between loan and commercial paper rates. Heuvel (2006) introduced the concept of a 'bank capital channel', where monetary policy affects bank lending through its impact on bank equity capital.\n\nAsset price volatility and its implications for monetary policy were explored by Bernanke (2000), who suggested that asset prices become relevant for central banks only when they signal potential inflationary or deflationary forces. Bianchi (2014) developed a framework for studying the implementation of monetary policy through the banking sector, highlighting the role of monetary policy in altering the trade-offs banks face between lending, holding reserves, holding deposits, and paying dividends.\n\nFinally, C\u00fardia (2010) emphasized the role of the central bank's balance sheet as an instrument of monetary policy, distinguishing between 'quantitative easing' and targeted asset purchases by a central bank.\n\nIn conclusion, the interaction between monetary policy and bank assets is multifaceted, involving aspects such as bank leverage, liquidity, capital, lending channels, and asset price volatility. The effects of monetary policy on bank assets are amplified by financial sector imperfections and are influenced by the size and capital adequacy of banks."}, {"query": "topological mapping with encoded latent vectors from inverse camera projections", "paper_list_string": "Bertrand 2001:\n\nTitle: Topological Map: Minimal Encoding of 3d Segmented Images\n\nAbstract: In this paper we define formally and completely the 3d topological map introduced in BertrandAl00: a model which encodes totally and minimally all the topological information of a 3d image. In particular, we focus on the problem of disconnections induced by the constructive definition based on several levels of maps. This multilevel representation is more or less a graph pyramid in sense that each level can be compute from the previous one in term of merge operations. Furthermore, algorithms extracting these maps from a segmented image have been given in BertrandAl00 and have been implemented and tested in practical applications.\n\n==\n\nKorrapati 2012:\n\nTitle: Topological Mapping with Image Sequence Partitioning\n\nAbstract: Topological maps are vital for fast and accurate localization in large environments. Sparse topological maps can be constructed by partitioning a sequence of images acquired by a robot, according to their appearance. All images in a partition have similar appearance and are represented by a node in a topological map. In this paper, we present a topological mapping framework which makes use of image sequence partitioning (ISP) to produce sparse maps. The framework facilitates coarse loop closure at node level and a finer loop closure at image level. Hierarchical inverted files (HIF) are proposed which are naturally adaptable to our sparse topological mapping framework and enable efficient loop closure. Computational gain attained in loop closure with HIF over sparse topological maps is demonstrated. Experiments are performed on outdoor environments using an omni-directional camera.\n\n==\n\nMontijano 2011:\n\nTitle: Distributed multi-camera visual mapping using topological maps of planar regions\n\nAbstract: This paper presents a multi-agent solution for cooperative visual mapping using planar regions. Each agent is assumed to be equipped with a conventional camera and has limited communication capabilities. Our approach starts building topological maps from independent image sequences where natural landmarks extracted from conventional images are grouped to create a graph of planes. With this approach the features observed in several images belonging to the same planar region are stored only once, reducing the size of the individual maps. In a distributed scenario this is very important because smaller maps can be transmitted faster, which makes our approach better suited for cooperative mapping. The later fusion of the individual maps is obtained via distributed consensus without any initial information about the relations between the different maps. Experiments with real images in complex scenarios show the good performance of our proposal.\n\n==\n\nLiu 2014:\n\nTitle: Topological Mapping and Scene Recognition With Lightweight Color Descriptors for an Omnidirectional Camera\n\nAbstract: Scene recognition problems for mobile robots have been extensively studied. This is important for tasks such as visual topological mapping. Usually, sophisticated key-point-based descriptors are used, which can be computationally expensive. In this paper, we describe a lightweight novel scene recognition method using an adaptive descriptor, which is based on color features and geometric information that are extracted from an uncalibrated omnidirectional camera. The proposed method enables a mobile robot to perform online registration of new scenes onto a topological representation automatically and solve the localization problem to topological regions simultaneously, all in real time. We adopt a Dirichlet process mixture model (DPMM) to describe the online inference process. It is based on an approximation of conditional probabilities of the new measurements given incrementally estimated reference models. It enables online inference speeds of up to 50 Hz for a normal CPU. We compare it with state-of-the-art key-point descriptors and show the advantage of the proposed algorithm in terms of performance and computational efficiency. A real-world experiment is carried out with a mobile robot equipped with an omnidirectional camera. Finally, we show the results on extended datasets.\n\n==\n\nKorrapati 2011:\n\nTitle: Efficient Topological Mapping with Image Sequence Partitioning\n\nAbstract: Topological maps are vital for fast and accurate localization in large environments. Sparse topological maps can be constructed by partitioning a sequence of images acquired by a robot, according to their appearance. All images in a partition have similar appearance and are represented by a node in a topological map. In this paper, we present a topological mapping framework which makes use of image sequence partitioning (ISP) to produce sparse maps. The framework facilitates coarse loop closure at node level and a finer loop closure at image level. Hierarchical inverted files (HIF) are proposed which are naturally adaptable to our sparse topological mapping framework and enable efficient loop closure. Computational gain attained in loop closure with HIF over sparse topological maps is demonstrated. Experiments are performed on outdoor environments using an omnidirectional camera.\n\n==\n\nStimec 2008:\n\nTitle: Unsupervised Learning of a Hierarchy of Topological Maps Using Omnidirectional Images\n\nAbstract: This paper presents a novel appearance-based method for path-based map learning by a mobile robot equipped with an omnidirectional camera. In particular, we focus on an unsupervised construction of topological maps, which provide an abstraction of the environment in terms of visual aspects. An unsupervised clustering algorithm is used to represent the images in multiple subspaces, forming thus a sensory grounded representation of the environment's appearance. By introducing transitional fields between clusters we are able to obtain a partitioning of the image set into distinctive visual aspects. By abstracting the low-level sensory data we are able to efficiently reconstruct the overall topological layout of the covered path. After the high level topology is estimated, we repeat the procedure on the level of visual aspects to obtain local topological maps. We demonstrate how the resulting representation can be used for modeling indoor and outdoor environments, how it successfully detects previously visited locations and how it can be used for the estimation of the current visual aspect and the retrieval of the relative position within the current visual aspect.\n\n==\n\nBertrand 2000:\n\nTitle: Topological Encoding of 3D Segmented Images\n\nAbstract: In this paper we define the 3d topological map and give an optimal algorithm which computes it from a segmented image. This data structure encodes totally all the information given by the segmentation. More, it allows to continue segmentation either algorithmically or interactively. We propose an original approach which uses several levels of maps. This allows us to propose a reasonable and implementable solution where other approaches don't allow suitable solutions. Moreover our solution has been implemented and the theoretical results translate very well in practical applications.\n\n==\n\nLi 2018:\n\nTitle: Topological Map Extraction From Overhead Images\n\nAbstract: We propose a new approach, named PolyMapper, to circumvent the conventional pixel-wise segmentation of (aerial) images and predict objects in a vector representation directly. PolyMapper directly extracts the topological map of a city from overhead images as collections of building footprints and road networks. In order to unify the shape representation for different types of objects, we also propose a novel sequentialization method that reformulates a graph structure as closed polygons. Experiments are conducted on both existing and self-collected large-scale datasets of several cities. Our empirical results demonstrate that our end-to-end learnable model is capable of drawing polygons of building footprints and road networks that very closely approximate the structure of existing online map services, in a fully automated manner. Quantitative and qualitative comparison to the state-of-the-arts also show that our approach achieves good levels of performance. To the best of our knowledge, the automatic extraction of large-scale topological maps is a novel contribution in the remote sensing community that we believe will help develop models with more informed geometrical constraints.", "final_summary": "The research question \"topological mapping with encoded latent vectors from inverse camera projections\" is addressed by several papers, each contributing unique insights.\n\nBertrand (2001) and Bertrand (2000) focus on the complete and minimal encoding of 3D segmented images into topological maps. They propose a multilevel representation, which is akin to a graph pyramid, where each level is computed from the previous one using merge operations. This approach allows for the continuation of segmentation either algorithmically or interactively.\n\nKorrapati (2012) and Korrapati (2011) propose a framework for topological mapping using image sequence partitioning (ISP) to produce sparse maps. They suggest that all images in a partition, which have a similar appearance, can be represented by a node in a topological map. This approach facilitates coarse loop closure at the node level and a finer loop closure at the image level, demonstrating computational gain in loop closure with hierarchical inverted files (HIF) over sparse topological maps.\n\nMontijano (2011) presents a multi-agent solution for cooperative visual mapping using planar regions. Their approach reduces the size of individual maps by storing features observed in several images belonging to the same planar region only once. This approach is particularly beneficial in a distributed scenario where smaller maps can be transmitted faster, making it better suited for cooperative mapping.\n\nLiu (2014) describes a lightweight scene recognition method using an adaptive descriptor based on color features and geometric information extracted from an uncalibrated omnidirectional camera. This method enables a mobile robot to perform online registration of new scenes onto a topological representation automatically and solve the localization problem to topological regions simultaneously, all in real time.\n\nStimec (2008) presents an unsupervised method for path-based map learning by a mobile robot equipped with an omnidirectional camera. They use an unsupervised clustering algorithm to represent the images in multiple subspaces, forming a sensory grounded representation of the environment's appearance. This approach allows for efficient reconstruction of the overall topological layout of the covered path.\n\nFinally, Li (2018) proposes a new approach, named PolyMapper, to circumvent the conventional pixel-wise segmentation of aerial images and predict objects in a vector representation directly. PolyMapper directly extracts the topological map of a city from overhead images as collections of building footprints and road networks.\n\nIn conclusion, these papers collectively suggest that topological mapping with encoded latent vectors from inverse camera projections can be achieved through various methods, including multilevel representation, image sequence partitioning, cooperative visual mapping using planar regions, lightweight scene recognition methods, unsupervised path-based map learning, and direct extraction of topological maps from overhead images. Each approach has its unique advantages and potential applications, contributing to the advancement of this research area."}, {"query": "History of Nucleic Acid Vaccines", "paper_list_string": "Restifo 2000:\n\nTitle: The promise of nucleic acid vaccines\n\nAbstract: Establishing the effective use of \u2018naked\u2019 nucleic acids as vaccines would undoubtedly be one of the most important advances in the history of vaccinology. While nucleic acids show much promise for use as vaccine vectors in experimental animals, not a single naked nucleic acid vector has been approved for use in humans. Indeed, data from human clinical trials is scant: nucleic acid vaccines have not been clearly demonstrated to have any convincing efficacy in the prevention or treatment of infectious disease or cancer. Here we illustrate possible mechanisms underlying effective nucleic acid vaccination. We focus on progress that has been made in the improvement of their function. Additionally, we identify promising new strategies and try to forecast future developments that could lead to the real success of nucleic acid vaccines in the prevention and treatment of human disease.\n\n==\n\nRando 2022:\n\nTitle: The Coming of Age of Nucleic Acid Vaccines during COVID-19\n\nAbstract: In the 21st century, several emergent viruses have posed a global threat. Each pathogen has emphasized the value of rapid and scalable vaccine development programs. The ongoing SARS-CoV-2 pandemic has made the importance of such efforts especially clear. New biotechnological advances in vaccinology allow for recent advances that provide only the nucleic acid building blocks of an antigen, eliminating many safety concerns. During the COVID-19 pandemic, these DNA and RNA vaccines have facilitated the development and deployment of vaccines at an unprecedented pace. This success was attributable at least in part to broader shifts in scientific research relative to prior epidemics; the genome of SARS-CoV-2 was available as early as January 2020, facilitating global efforts in the development of DNA and RNA vaccines within two weeks of the international community becoming aware of the new viral threat. Additionally, these technologies that were previously only theoretical are not only safe but also highly efficacious. Although historically a slow process, the rapid development of vaccines during the COVID-19 crisis reveals a major shift in vaccine technologies. Here, we provide historical context for the emergence of these paradigm-shifting vaccines. We describe several DNA and RNA vaccines and in terms of their efficacy, safety, and approval status. We also discuss patterns in worldwide distribution. The advances made since early 2020 provide an exceptional illustration of how rapidly vaccine development technology has advanced in the last two decades in particular and suggest a new era in vaccines against emerging pathogens.\n\n==\n\nRando 2023:\n\nTitle: The Coming of Age of Nucleic Acid Vaccines during COVID-19\n\nAbstract: The SARS-CoV-2 pandemic has caused untold damage globally, presenting unusual demands on but also unique opportunities for vaccine development. The development, production, and distribution of vaccines are imperative to saving lives, preventing severe illness, and reducing the economic and social burdens caused by the COVID-19 pandemic. ABSTRACT In the 21st century, several emergent viruses have posed a global threat. Each pathogen has emphasized the value of rapid and scalable vaccine development programs. The ongoing severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic has made the importance of such efforts especially clear. New biotechnological advances in vaccinology allow for recent advances that provide only the nucleic acid building blocks of an antigen, eliminating many safety concerns. During the COVID-19 pandemic, these DNA and RNA vaccines have facilitated the development and deployment of vaccines at an unprecedented pace. This success was attributable at least in part to broader shifts in scientific research relative to prior epidemics: the genome of SARS-CoV-2 was available as early as January 2020, facilitating global efforts in the development of DNA and RNA vaccines within 2 weeks of the international community becoming aware of the new viral threat. Additionally, these technologies that were previously only theoretical are not only safe but also highly efficacious. Although historically a slow process, the rapid development of vaccines during the COVID-19 crisis reveals a major shift in vaccine technologies. Here, we provide historical context for the emergence of these paradigm-shifting vaccines. We describe several DNA and RNA vaccines in terms of their efficacy, safety, and approval status. We also discuss patterns in worldwide distribution. The advances made since early 2020 provide an exceptional illustration of how rapidly vaccine development technology has advanced in the last 2 decades in particular and suggest a new era in vaccines against emerging pathogens. IMPORTANCE The SARS-CoV-2 pandemic has caused untold damage globally, presenting unusual demands on but also unique opportunities for vaccine development. The development, production, and distribution of vaccines are imperative to saving lives, preventing severe illness, and reducing the economic and social burdens caused by the COVID-19 pandemic. Although vaccine technologies that provide the DNA or RNA sequence of an antigen had never previously been approved for use in humans, they have played a major role in the management of SARS-CoV-2. In this review, we discuss the history of these vaccines and how they have been applied to SARS-CoV-2. Additionally, given that the evolution of new SARS-CoV-2 variants continues to present a significant challenge in 2022, these vaccines remain an important and evolving tool in the biomedical response to the pandemic.\n\n==\n\nVogel 1995:\n\nTitle: Nucleic acid vaccines\n\nAbstract: The use of nucleic acid-based vaccines is a novel approach to immunization that elicits immune responses similar to those induced by live, attenuated vaccines. Administration of nucleic acid vaccines results in the endogenous generation of viral proteins with native conformation, glycosylation profiles, and other posttranslational modifications that mimic antigen produced during natural viral infection. Nucleic acid vaccines have been shown to elicit both antibody and cytotoxic T-lymphocyte responses to diverse protein antigens. Advantages of nucleic acid-based vaccines include the simplicity of the vector, the ease of delivery, the duration of expression, and, to date, the lack of evidence of integration. Further studies are needed to assess the feasibility, safety, and efficacy of this new and promising technology.\n\n==\n\nHoffman 1995:\n\nTitle: Nucleic Acid Malaria Vaccines\n\nAbstract: During the twentieth century, the primary approach to malaria prevention has been to interfere with transmission of the parasite between the infected mosquito and the human host using physical barriers, insecticides and prophylactic drugs. Despite these measures, it is estimated that there are 300-500 new Plasmodium infections and 1-2 million deaths annually due to malaria. Thus, there have been major efforts to develop malaria vaccines. This is thought to be a feasible approach because immunization with radiation-attenuated Plasmodium spp. sporozoites induces sterile protective immunity against malaria in rodents, primates and humans(reviewed in Ref. 2). The complex life cycle of the Plasmodium spp. parasites and the extensive variability among strains of the same Plasmodium species dictate, however, that an effective malaria vaccine will probably need to induce protective antibodies as well as effector CD4+ and CD8+ T lymphocytes specific for variants of multiple antigens expressed at different stages of the life cycle. It is our view that nucleic acid vaccines offer an excellent approach to developing a multivalent vaccine that effectively activates all arms of the immune system. Furthermore, such vaccines may overcome the additional problem inherent in subunit vaccine development: major histocompatibility complex (MHC) restriction of the protective immune responses to identified target epitopes. Supporting data and rationale for our view are presented here. The major emphasis of the work thus far has been to use plasmid DNA to induce protective CD8+ cytotoxic T lymphocytes (CTL), but the induction of protective antibody and CD4+ T cell responses is being investigated.\n\n==\n\nHasan 1999:\n\nTitle: Nucleic acid immunization: concepts and techniques associated with third generation vaccines.\n\nAbstract: A radical change in vaccine methodology arrived nine years ago with the advent of nucleic acid immunization. Aspects such as plasmid design, gene selection, the use of immunostimulatory complexes and clinical trials are discussed in this review. Furthermore, concepts and protocols involved in the construction, evaluation and immunization of a DNA vaccine have been examined as new strategies to enhance this technology continues to grow.\n\n==\n\nChavda 2021:\n\nTitle: Nucleic Acid Vaccines for COVID-19: A Paradigm Shift in the Vaccine Development Arena\n\nAbstract: Coronavirus disease, COVID-19, has touched every country globally except five countries (North Korea, Turkmenistan, Tonga, Tuvalu and Nauru). Vaccination is the most effective method to protect against infectious diseases. The objective is to ensure that everyone has access to a COVID-19 vaccine. The conventional vaccine development platforms are complex and time-consuming to obtain desired approved vaccine candidates through rigorous regulatory pathways. These safeguards guarantee that the optimized vaccine product is safe and efficacious for various demographic populations prior to it being approved for general use. Nucleic acid vaccines employ genetic material from a pathogen, such as a virus or bacteria, to induce an immune response against it. Based on the vaccination, the genetic material might be DNA or RNA; as such, it offers instructions for producing a specific pathogen protein that the immune system will perceive as foreign and mount an immune response. Nucleic acid vaccines for multiple antigens might be made in the same facility, lowering costs even more. Most traditional vaccine regimens do not allow for this. Herein, we demonstrate the recent understanding and advances in nucleic acid vaccines (DNA and mRNA based) against COVID-19, specifically those in human clinical trials.\n\n==\n\nBabiuk 2000:\n\nTitle: Nucleic acid vaccines: research tool or commercial reality.\n\nAbstract: Polynucleotide immunization has captured the imagination of numerous researchers and commercial companies around the world as a novel approach for inducing immunity in animals. Clearly, the 'proof-of-principle' has been demonstrated both in rodents and various animal species. However, to date, no commercial veterinary vaccine has been developed, or to our knowledge, is in the licensing phase. The present review summarizes the types of pathogens and host species for which polynucleotide immunization has been tried. We have tried to identify possible barriers to commercialization of this technology and areas that need attention if this promising technology is ever to become a reality in the commercial arena.", "final_summary": "Nucleic acid vaccines, a revolutionary approach to immunization, have been a topic of interest for several decades. These vaccines utilize the genetic material from a pathogen to stimulate an immune response (Hasan, 1999; Chavda, 2021). Despite their promise, no nucleic acid vaccine had been approved for human use until the COVID-19 pandemic (Restifo, 2000; Rando, 2022).\n\nThe concept of nucleic acid vaccines was introduced in the mid-1990s (Vogel, 1995). Hoffman (1995) suggested that nucleic acid vaccines could be an effective approach to developing multivalent vaccines that activate all arms of the immune system. \n\nHowever, the journey from concept to commercial reality has been fraught with challenges. Restifo (2000) noted that while nucleic acid vaccines showed promise in experimental animals, none had been approved for use in humans. Babiuk (2000) echoed this sentiment, stating that despite the proof-of-principle being demonstrated in various animal species, no commercial veterinary vaccine had been developed.\n\nThe COVID-19 pandemic marked a turning point in the history of nucleic acid vaccines. The urgency of the situation and advances in biotechnology facilitated the rapid development and deployment of DNA and RNA vaccines (Rando, 2022; Rando, 2023). The genome of SARS-CoV-2 was available as early as January 2020, enabling global efforts in the development of DNA and RNA vaccines within two weeks of the international community becoming aware of the new viral threat (Rando, 2022). \n\nIn conclusion, the history of nucleic acid vaccines is a testament to the power of scientific innovation, particularly in times of crisis. While the journey has been long and challenging, the successful deployment of nucleic acid vaccines during the COVID-19 pandemic marks a new era in vaccinology (Rando, 2022; Rando, 2023). Future research will undoubtedly continue to explore and refine this promising technology."}, {"query": "\"Environmental uncertainty would moderate the relationship between demand risk and supply chain disruption impact such that the relationship would become stronger when environmental uncertainty is high.\"", "paper_list_string": "Fynes 2004:\n\nTitle: Environmental uncertainty, supply chain relationship quality and performance\n\nAbstract: Abstract Environmental uncertainty is a fact of life in today's supply chains. In this paper we develop a model of environmental uncertainty, supply chain (SC) relationship quality and SC performance. We use data from the electronics sector in Ireland to test our model. Our results provide mixed support for the model, with the moderating role of both demand and supply uncertainty being supported, but technological uncertainty not supported. We reflect on these findings and suggest a research agenda based on our results.\n\n==\n\nSreedevi 2017:\n\nTitle: Uncertainty and supply chain risk: The moderating role of supply chain flexibility in risk mitigation\n\nAbstract: Abstract In order to remain competitive in the market, firms are forced to expand their product offerings and offer high levels of customization, bringing about high uncertainty in their supply chain. Firms that face high environmental uncertainty are increasingly facing higher risks in terms of supply disruptions, production and delivery delays that ultimately result in poor operational performance. This study aims at understanding the antecedents of supply chain operational risk faced by firms and the conditions under which such risks can be mitigated. Using Indian data from the sixth edition of International Manufacturing Strategy Survey (IMSS) and structural equation modeling, we investigate the relationships between environmental uncertainty and supply chain risk and the moderating effect of supply chain flexibility. We identify appropriate types of flexibility to mitigate the three major aspects of supply chain risk: supply risk, manufacturing process risk and delivery risk. Our empirical investigation reveals that uncertainty in the supply chain leads to high supply chain risk; and in uncertain environments, supply and manufacturing flexibility help in reducing the supply and manufacturing process risks respectively. However, our results also indicate that, in emerging markets such as India where logistic infrastructure is less developed, internal capabilities alone may not be sufficient in reducing supply chain delivery risk. Our findings not only contribute towards filling certain gaps in the supply chain risk management literature, but also provide practicing managers and researchers a better understanding of the types of flexibility that can mitigate supply chain risk in different business environments.\n\n==\n\nFord 2015:\n\nTitle: Supply Chain Quality Management and Environmental Uncertainty: A Contingency Perspective\n\nAbstract: This article extends contingency perspectives of quality management to the interorganizational quality context. Supply chain quality management (SCQM) is conceptualized as a mechanism for governing quality of resource flows between interdependent organizations. Higher environmental uncertainty is proposed to intensify SCQM practices among supply chain members. However, SCQM practices are likely to assume different orientations depending on whether volume or technological uncertainty is present. High asset specificities among exchange partners are proposed to moderate the relationship between SCQM and environmental uncertainty by elevating quality practice intensities as environments become more turbulent. Disequilibrium environments should discourage SCQM, as organizations lack bureaucratic resources to manage the quality of interorganizational resource flows via bureaucratic means. Implications of these propositions on research and practice are discussed.\n\n==\n\nSingh 2020:\n\nTitle: Managing environmental uncertainty for improved firm financial performance: the moderating role of supply chain risk management practices on managerial decision making\n\nAbstract: ABSTRACT With increasing global exposure, organisations have started to witness supply chain risks that they traditionally were not exposed to. This article therefore attempts to answer the research question of how environmental uncertainty within a firm\u2019s supply chain, impacts firm financial performance. We further evaluate the role of supply chain risk management practices in mitigating the negative impact of such disruption events. To answer the research question, we conduct a literature review and develop the corresponding hypothesis. We test our hypothesis using both primary and secondary data. The results show that environmental uncertainty has a negative impact on firm financial performance, with entrepreneurial managerial capitalism mediating the impact. However, organisations that adopt macro and network supply chain risk management practices are able to improve managerial decision making frame, and mitigate the negative impact of environmental uncertainty on firm financial performance. The article concludes with our findings, along with managerial and practitioner implications of the research.\n\n==\n\nBoon-itt 2008:\n\nTitle: Moderating Effects of Environmental Uncertainty on Supply Chain Integration and Product Quality: An Empirical Study of Thai Automotive Industry\n\nAbstract: Although effective and efficient supply chain management requires integrated business processes that go beyond purchasing and logistics activities, some doubts are expressed concerning the applicability of supply chain integration practices. Specifically, a careful literature search reveals that no studies have included the effect of environmental uncertainty on the relationship between sup- ply chain integration and product quality. This study, therefore, represents an attempt to provide the contribution in the field by developing a model to explore the relationships and to fulfill the gap between the literature on supply chain integration and environmental uncertainty. The findings indi- cate that the effects of supply chain integration on product quality are moderated by environmental uncertainty as demonstrated by Chow tests. The results also provide managerial insights about envi- ronmental uncertainty-supply chain integration practices connection.\n\n==\n\nGanbold 2017:\n\nTitle: IMPACT OF ENVIRONMENTAL UNCERTAINTY ON SUPPLY CHAIN INTEGRATION\n\nAbstract: Today\u2019s ever-changing business environment is often described to be highly competitive, dynamic and complex. Customers are demanding more variability, better quality, higher reliability and faster delivery. Organizations are being faced with more uncertainties from its task environment than before. In order to respond to the uncertainties, organizations are internalizing fewer resources and capabilities, while increasing their integration with partners in the supply chain. Drawing on the resource-dependence theory, this study aims to examine the impact of environmental uncertainty on supply chain integration initiatives. Environmental uncertainty is considered in terms of three types, namely, supply uncertainty, demand or customer uncertainty, and technology uncertainty, based on its sources. Supply chain integration is comprised of internal integration, customer integration, and supplier integration. Based on the empirical study with 108 Japanese manufacturing firms, this study makes significant contributions to the knowledge base and provides theoretical and practical implications.\n\n==\n\nInman 2021:\n\nTitle: Environmental uncertainty and supply chain performance: the effect of agility\n\nAbstract: PurposeToday's businesses are facing a world that is more complex, turbulent and unpredictable than in the past with increasing levels of environmental complexity. Rather than proposing environmental uncertainty as a mediator/moderator of the relationship between agility and performance as others have done, the authors offer an alternative view where supply chain agility is seen as mediating the relationship between environmental uncertainty and supply chain performance.Design/methodology/approachThe authors propose that supply chain agility is a response to the effects of environmental uncertainty and, as such, environmental uncertainty should be seen as a driver of supply chain agility. Few studies test the direct relationship between uncertainty and supply chain performance, and none simultaneously test for agility's mediation and moderation effect between environmental uncertainty and agility.FindingsThe model was statistically assessed using partial-least-squares structural equation modeling (PLS/SEM) by analyzing survey data from manufacturing managers in 136 US firms. The study results did not indicate a significant relationship between environmental uncertainty and supply chain performance. However, the authors did find a significant positive relationship between agile manufacturing and supply chain performance using measures that were primarily operations-centered rather than financial. Additionally, the authors found that agile manufacturing fully mediates the relationship between environmental uncertainty and supply chain performance.Originality/valueThe authors\u2019 model, though simple, provides a base for future research for them and other researchers who can incorporate other impacting variables into the model. The study results show that uncertainty can be a force for good and that utilizing agile manufacturing can be a new source of opportunity.\n\n==\n\nXu 2010:\n\nTitle: The effect of environmental uncertainty on supply chain integration in Chinese manufacturing industry\n\nAbstract: Supply chain integration nowadays is considered as an important approach to building and sustaining advantages. Many previous empirical researches focus on the effect of supply chain integration on the firms' performance, yet, the antecedence of supply chain integration is still largely unknown. This paper investigates the relationship between environmental uncertainty (including demand uncertainty, supply uncertainty and technology uncertainty) and supply chain integration (including customer integration, supplier integration and internal integration) with 139 samples from Chinese manufacturing industry. The results reveal both supply uncertainty and technology uncertainty significantly influence supply chain integration; the effect of supply uncertainty is negative, while that of technology uncertainty is positive. However, demand uncertainty just has a significant effect on internal integration. The relationship between demand uncertainty and external integration (including customer integration and supplier integration) is mediated by the internal integration.", "final_summary": "The relationship between environmental uncertainty, demand risk, and supply chain disruption impact is a complex interplay that has been explored in various studies. \n\nFynes (2004) found mixed support for the moderating role of both demand and supply uncertainty in the relationship between environmental uncertainty and supply chain performance. Similarly, Sreedevi (2017) found that high environmental uncertainty leads to increased supply chain risk, with supply and manufacturing flexibility serving as mitigating factors. \n\nFord (2015) proposed that higher environmental uncertainty intensifies supply chain quality management practices among supply chain members. However, the orientation of these practices may vary depending on the presence of volume or technological uncertainty. \n\nSingh (2020) found that environmental uncertainty negatively impacts firm financial performance, but this impact can be mitigated by adopting supply chain risk management practices. Boon-itt (2008) also found that the effects of supply chain integration on product quality are moderated by environmental uncertainty. \n\nGanbold (2017) examined the impact of environmental uncertainty on supply chain integration initiatives, considering supply uncertainty, demand or customer uncertainty, and technology uncertainty. Inman (2021) proposed that supply chain agility mediates the relationship between environmental uncertainty and supply chain performance. \n\nLastly, Xu (2010) investigated the relationship between environmental uncertainty and supply chain integration in the Chinese manufacturing industry, finding that both supply uncertainty and technology uncertainty significantly influence supply chain integration.\n\nIn conclusion, these studies collectively suggest that environmental uncertainty does moderate the relationship between demand risk and supply chain disruption impact, making the relationship stronger when environmental uncertainty is high. However, the specific effects can vary depending on factors such as supply chain flexibility, supply chain quality management practices, and supply chain integration initiatives."}, {"query": "phylogeny reconstruction in breast cancer", "paper_list_string": "Pennington 2006:\n\nTitle: Expectation-maximization method for reconstructing tumor phylogenies from single-cell data.\n\nAbstract: Recent studies of gene expression in cancerous tumors have revealed that cancers presenting indistinguishable symptoms in the clinic can represent substantially different entities at the molecular level. The ability to distinguish between these different cancers makes possible more accurate prognoses and more finely targeted therapeutics. Making full use of this knowledge, however, requires characterizing commonly occurring cancer sub-types and the specific molecular abnormalities that produce them. Computational approaches to this problem to date have been hindered by the fact that tumors are highly heterogeneous masses typically containing cells at multiple stages of progression from healthy to aggressively malignant. We present a computational approach for taking advantage of tumor heterogeneity when characterizing tumor progression pathways by inferring those pathways from single-cell assays. Our approach uses phylogenetic algorithms to infer likely evolutionary sequences producing cell populations in single tumors, which are in turn used to create a profile of commonly used pathways across the patient population. This approach is combined with expectation maximization to infer unknown parameters used in the phylogeny construction. We demonstrate the approach on a set of fluorescent in situ hybridization (FISH) data measuring cell-by-cell gene and chromosome copy numbers in a large sample of breast cancers. The results validate the proposed computational methods by showing consistency with several previous findings on these cancers. They also provide novel insights into the mechanisms of tumor progression in these patients.\n\n==\n\nPennington 2007:\n\nTitle: Reconstructing Tumor phylogenies from Heterogeneous Single-Cell Data\n\nAbstract: Studies of gene expression in cancerous tumors have revealed that tumors presenting indistinguishable symptoms in the clinic can be substantially different entities at the molecular level. The ability to distinguish between these genetically distinct cancers will make possible more accurate prognoses and more finely targeted therapeutics, provided we can characterize commonly occurring cancer sub-types and the specific molecular abnormalities that produce them. We develop a new method for identifying these common tumor progression pathways by applying phylogeny inference algorithms to single-cell assays, taking advantage of information on tumor heterogeneity lost to prior microarray-based approaches. We combine this approach with expectation maximization to infer unknown parameters used in the phylogeny construction. We further develop new algorithms to merge inferred trees across different assays. We validate the expectation maximization method on simulated data and demonstrate the combined approach on a set of fluorescent in situ hybridization (FISH) data measuring cell-by-cell gene and chromosome copy numbers in a large sample of breast cancers. The results further validate the proposed computational methods by showing consistency with several previous findings on these cancers and provide novel insights into the mechanisms of tumor progression in these patients.\n\n==\n\nShackney 1995:\n\nTitle: Preferred genetic evolutionary sequences in human breast cancer: a case study.\n\nAbstract: Multiparameter flow cytometry studies were performed on the cells of an aggressive human breast cancer at the time of diagnosis and at relapse. The aneuploid cells that overexpressed large amounts of both HER-2/neu and ras survived intensive chemotherapy and were responsible for tumor relapse. At relapse, these cells were shown to overexpress simultaneously at least five oncogenes: HER-2/neu, ras, EGF receptor, p53 and c-myc. A partial reconstruction of the genetic evolutionary sequence in this tumor indicated that HER-2/neu overexpression was an early step in the sequence. Subsequent HER-2/neu overexpression, EGF receptor overexpression and p53 protein overexpression were each associated with ras overexpression. The data suggest that ploidy and oncogene overexpression cannot be used as independent clinical prognostic factors. The ability to characterize tumors according to the degree of advancement in the genetic evolutionary might serve as a basis for genetic staging for adjuvant therapy.\n\n==\n\nMeaburn 2009:\n\nTitle: Disease-specific gene repositioning in breast cancer\n\nAbstract: The nuclear repositioning of specific genes may be a novel diagnostic strategy to distinguish malignant from normal tissue.\n\n==\n\nTan 2015:\n\nTitle: Genomic landscapes of breast fibroepithelial tumors\n\nAbstract: Breast fibroepithelial tumors comprise a heterogeneous spectrum of pathological entities, from benign fibroadenomas to malignant phyllodes tumors. Although MED12 mutations have been frequently found in fibroadenomas and phyllodes tumors, the landscapes of genetic alterations across the fibroepithelial tumor spectrum remain unclear. Here, by performing exome sequencing of 22 phyllodes tumors followed by targeted sequencing of 100 breast fibroepithelial tumors, we observed three distinct somatic mutation patterns. First, we frequently observed MED12 and RARA mutations in both fibroadenomas and phyllodes tumors, emphasizing the importance of these mutations in fibroepithelial tumorigenesis. Second, phyllodes tumors exhibited mutations in FLNA, SETD2 and KMT2D, suggesting a role in driving phyllodes tumor development. Third, borderline and malignant phyllodes tumors harbored additional mutations in cancer-associated genes. RARA mutations exhibited clustering in the portion of the gene encoding the ligand-binding domain, functionally suppressed RARA-mediated transcriptional activation and enhanced RARA interactions with transcriptional co-repressors. This study provides insights into the molecular pathogenesis of breast fibroepithelial tumors, with potential clinical implications.\n\n==\n\nDing 2010:\n\nTitle: Genome Remodeling in a Basal-like Breast Cancer Metastasis and Xenograft\n\nAbstract: Massively parallel DNA sequencing technologies provide an unprecedented ability to screen entire genomes for genetic changes associated with tumour progression. Here we describe the genomic analyses of four DNA samples from an African-American patient with basal-like breast cancer: peripheral blood, the primary tumour, a brain metastasis and a xenograft derived from the primary tumour. The metastasis contained two de novo mutations and a large deletion not present in the primary tumour, and was significantly enriched for 20 shared mutations. The xenograft retained all primary tumour mutations and displayed a mutation enrichment pattern that resembled the metastasis. Two overlapping large deletions, encompassing CTNNA1, were present in all three tumour samples. The differential mutation frequencies and structural variation patterns in metastasis and xenograft compared with the primary tumour indicate that secondary tumours may arise from a minority of cells within the primary tumour.\n\n==\n\nPrat 2011:\n\nTitle: Deconstructing the molecular portraits of breast cancer\n\nAbstract: Breast cancer is a heterogeneous disease in terms of histology, therapeutic response, dissemination patterns to distant sites, and patient outcomes. Global gene expression analyses using high\u2010throughput technologies have helped to explain much of this heterogeneity and provided important new classifications of cancer patients. In the last decade, genomic studies have established five breast cancer intrinsic subtypes (Luminal A, Luminal B, HER2\u2010enriched, Claudin\u2010low, Basal\u2010like) and a Normal Breast\u2010like group. In this review, we dissect the most recent data on this genomic classification of breast cancer with a special focus on the Claudin\u2010low subtype, which appears enriched for mesenchymal and stem cell features. In addition, we discuss how the combination of standard clinical\u2010pathological markers with the information provided by these genomic entities might help further understand the biological complexity of this disease, increase the efficacy of current and novel therapies, and ultimately improve outcomes for breast cancer patients.\n\n==\n\nRobinson 2011:\n\nTitle: Functionally Recurrent Rearrangements of the MAST Kinase and Notch Gene Families in Breast Cancer\n\nAbstract: Breast cancer is a heterogeneous disease that has a wide range of molecular aberrations and clinical outcomes. Here we used paired-end transcriptome sequencing to explore the landscape of gene fusions in a panel of breast cancer cell lines and tissues. We observed that individual breast cancers have a variety of expressed gene fusions. We identified two classes of recurrent gene rearrangements involving genes encoding microtubule-associated serine-threonine kinase (MAST) and members of the Notch family. Both MAST and Notch-family gene fusions have substantial phenotypic effects in breast epithelial cells. Breast cancer cell lines harboring Notch gene rearrangements are uniquely sensitive to inhibition of Notch signaling, and overexpression of MAST1 or MAST2 gene fusions has a proliferative effect both in vitro and in vivo. These findings show that recurrent gene rearrangements have key roles in subsets of carcinomas and suggest that transcriptome sequencing could identify individuals with rare, targetable gene fusions.", "final_summary": "The reconstruction of phylogeny in breast cancer is a complex and multifaceted topic, as evidenced by the diverse findings in the papers reviewed. \n\nPennington (2006, 2007) developed a computational approach to infer tumor progression pathways from single-cell assays, which takes advantage of tumor heterogeneity. This method was validated on fluorescent in situ hybridization (FISH) data from a large sample of breast cancers, providing insights into the mechanisms of tumor progression.\n\nShackney (1995) conducted a case study on an aggressive human breast cancer, revealing that the overexpression of certain oncogenes was associated with tumor relapse. The study suggested that characterizing tumors according to their genetic evolutionary advancement could serve as a basis for genetic staging for adjuvant therapy.\n\nMeaburn (2009) proposed that the nuclear repositioning of specific genes could be a novel diagnostic strategy to distinguish malignant from normal tissue, although the paper did not provide specific findings related to breast cancer.\n\nTan (2015) conducted exome sequencing of breast fibroepithelial tumors. The study emphasized the importance of MED12 and RARA mutations in fibroepithelial tumorigenesis and suggested a role for FLNA, SETD2, and KMT2D mutations in driving phyllodes tumor development.\n\nDing (2010) used massively parallel DNA sequencing technologies to screen entire genomes for genetic changes associated with tumor progression. The study found that secondary tumors may arise from a minority of cells within the primary tumor.\n\nPrat (2011) reviewed genomic studies that have established five breast cancer intrinsic subtypes and discussed how the combination of standard clinical-pathological markers with genomic entities might help further understand the biological complexity of breast cancer.\n\nRobinson (2011) used paired-end transcriptome sequencing to explore the landscape of gene fusions in a panel of breast cancer cell lines and tissues. The study found that recurrent gene rearrangements involving genes encoding microtubule-associated serine-threonine kinase (MAST) and members of the Notch family have key roles in subsets of carcinomas.\n\nIn conclusion, these papers collectively suggest that the reconstruction of phylogeny in breast cancer involves a variety of methods and approaches, including computational modeling, single-cell assays, exome sequencing, and transcriptome sequencing. These methods have provided valuable insights into the genetic changes associated with tumor progression and the molecular heterogeneity of breast cancer."}, {"query": "Tallinn emerged as a pioneer by introducing free public transport in 2013.", "paper_list_string": "Gabald\u00f3n-Estevan 2016:\n\nTitle: Environmental innovation through transport policy. The implementation of the free fare policy on public transport in Tallinn, Estonia\n\nAbstract: Urban areas are of increasing relevance when it comes to sustainability. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 First, about half of the world\u2019s population now lives in cities (increasing to 60% by 2030). \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 Second, cities are nowadays responsible for levels of resource consumption and waste generation that are higher beyond their share on world population. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 Third, cities are more vulnerable to disruptive events that can lead to restrictions on the provision of resources and to changes on the environment caused by climate change. \u2022\u00a0\u00a0\u00a0\u00a0\u00a0 And fourth, because they concentrate key resources (political, social, cultural\u2026), cities are seen as strategic scenarios where to experiment and develop solutions to cope with the prevailing sustainability challenges driven by the major social and environmental transformations. Urban agglomerations can be seen as complex innovation systems where human activities are shaped in order to transform societies towards sustainable development. For this paper, we focus on the case of an environmental innovation regarding transport policy, the implementation of the fare-free policy on public transport for all inhabitants of Tallinn, Estonia. Tallinn, with 414,000 inhabitants in 2015, is the capital of Estonia and the largest city in the country. Over the last two decades the share of public transport trips decreased dramatically. After a public opinion poll in 2012, in which over 75% of the participants voted for a fare-free public transportation system (FFPTS) in Tallinn, the new policy was implemented on 1st January 2013. From that date on inhabitants of Tallinn could use all public transport services (busses, trams, trolly-busses) operated by city-run operators for free. Later the fare-free system was implemented also on trains within Tallinn. In this paper we analyze the context, in which this policy was implemented, the main characteristics of its implementation and its actual situation. DOI: http://dx.doi.org/10.4995/CIT2016.2016.3532\n\n==\n\nHess 2017:\n\nTitle: Decrypting fare-free public transport in Tallinn, Estonia\n\nAbstract: Abstract Among many possible interventions in public transport finance and policy designed to enhance the attractiveness of riding public transport, one of the most extreme, which is seldom implemented, is the elimination of passenger fares, effectively making public transport \u201cfree\u201d for riders (with operating costs paid from other funding sources). This article describes a fare-free public transport program in Tallinn, Estonia, launched in 2013, which has exhibited lower-than-expected increases in ridership. Evaluations of Tallinn\u2019s fare-free public transport program are presented and synthesized, with a focus on program goals and how goals are met through program performance. Findings suggest certain flaws limit the program\u2019s potential success since the program design is misaligned with its primary stated goals, and several program goals relating to external effects of fare reform cannot be evaluated. Although it would be valuable for transport managers in other cities to learn about this experience, the Tallinn fare-free public transport program provides scant transferable evidence about how such a program can operate outside of a politicized context, which was crucial to its implementation in Estonia.\n\n==\n\nGaley 2014:\n\nTitle: License to Ride: Free Public Transportation for Residents of Tallinn\n\nAbstract: The City of Tallinn, capital of Estonia, with a population of 420,000, recently became the world\u2019s largest municipality offering free public transportation. Tourists still have to pay to ride the city\u2019s bus, trolley, and tram network, but registered residents\u2014including a large population of Russian-speaking non-citizens\u2014only have to tap their municipal transit cards once onboard. This article presents a qualitative account of the world\u2019s largest free public transporta- tion experiment to date. The results challenge and inform the conventional measures and objectives of transportation experts. The analysis is meant to complement the existing literature surveying free public transportation experiments and evaluating transportation pricing schemes.\n\n==\n\nGabald\u00f3n-Estevan 2019:\n\nTitle: Broader impacts of the fare-free public transportation system in Tallinn\n\nAbstract: ABSTRACT In this paper, we focus on the rationale for implementing the fare-free public transportation system (FFPTS) in Tallinn, Estonia, that took place on 1 January 2013. Through a series of interviews with relevant informants, we identify the main enablers and the FFPTS in Tallinn faced and the potential of such a system to contribute to the sustainable city development. Our analysis shows that the interlinking between local and national politics determines not only the type of initiatives implemented and the support they receive but also the degree of their success and their stability. We conclude that to be even more effective, it should be extended to the all potential users, not just to local registered residents as it has been recently applied in state-run bus travels in rural municipalities in Estonia. Finally, more restrictive private car policies should be considered to fuel a sustainable mobility transition and increase cities life quality.\n\n==\n\nK\u0119b\u0142owski 2019:\n\nTitle: Towards an urban political geography of transport: Unpacking the political and scalar dynamics of fare-free public transport in Tallinn, Estonia\n\nAbstract: In this article, we study the largest existing fare-free public transport (FFPT) programme, launched in 2013 in Tallinn, Estonia. Instead of focusing solely on the rationale and impact of fare-free public transport in terms of finances and travel patterns, we propose to analyse FFPT from the perspective of urban political geography, and to inquire into its political and scalar dynamics. We analyse how Tallinn\u2019s fare-free programme was developed, and demonstrate the politics of its conception and implementation. We observe who has access to free travel and we reveal how FFPT is embedded in Estonia\u2019s place-of-residence-based taxation system. Finally, we identify where lies the impact of territorial competition exacerbated by FFPT. Therefore, we argue that transport policies \u2013 of which FFPT is but an example \u2013 should be understood as much more than strategies dealing with transport issues per se. Instead, we propose to approach them as political and spatial projects, whose processual, cross-sectorial and scalar dimensions help to reveal the embeddedness of transport in inherently urban questions of metropolitan governance, electoral strategies, territorial competition and socio-spatial inequalities.\n\n==\n\nCats 2014:\n\nTitle: Public Transport Pricing Policy\n\nAbstract: Cities worldwide are looking for new policies to attract travelers to shift from cars to public transport. Policies focused on reducing public transport fares are aimed at improving social inclusion and leading to a modal shift. The City of Tallinn, the capital of Estonia, has recently introduced a fare-free public transport (FFPT) service in an effort to improve accessibility and mobility for its residents. The case of Tallinn is a full-scale, real-world experiment that provides a unique opportunity for investigating the impacts of FFPT policy. A macrolevel empirical evaluation of FFPT impacts on service performance, passenger demand, and accessibility for various groups of travelers is presented. In contrast to previous studies, the influence of FFPT on passenger demand was estimated while changes in supply were controlled. The results indicate that the FFPT measure accounts for an increase of 1.2% in passenger demand, with the remaining increase attributed to an extended network of public transport priority lanes and increased service frequency. The relatively small effect could be attributed to the previous price level and public transport share as well as analysis of the short-term impact. The evidence-based policy evaluation in this paper is instrumental in supporting policy making and facilitating the design of public transport pricing strategies.\n\n==\n\nCats 2017:\n\nTitle: The prospects of fare-free public transport: evidence from Tallinn\n\nAbstract: The subsidy level of public transport systems varies considerably among systems worldwide. While limited-scale free-fare public transport (FFPT) services such as limited campaigns and fare evasion for special groups or specific services are prevalent, there is only limited evidence on the consequences of introducing a full-fledged FFPT. The case of Tallinn, Estonia offers a full-scale experiment that provides a unique opportunity to investigate the impacts of FFPT. This study examines travel pattern changes based on individual travel habit survey shortly before and almost 1\u00a0year after the introduction of FFPT policy in Tallinn based on interviews and travel diaries of a random sample of 1500 household. We analyse modal shift effects and whether they are driven by trip generation or trip substitution, travel attitudes and satisfactions as well as impacts on equity, employment prospects, and trip destination choices. Almost a year after the introduction of FFPT, public transport usage increased by 14\u00a0% and there is evidence that the mobility of low-income residents has improved. The effect of FFPT on ridership is substantially lower than those reported in previous studies due to the good level of service provision, high public transport usage and low public transport fees that existed already prior to the FFPT.\n\n==\n\nCats 2017:\n\nTitle: Erratum to: The prospects of fare-free public transport: evidence from Tallinn\n\nAbstract: The prospects of fare-free public transport : evidence from Tallinn (vol 44, pg 1083, 2017)", "final_summary": "Tallinn, the capital of Estonia, made a groundbreaking move in 2013 by introducing free public transport for its residents, becoming a pioneer in this field (Gabald\u00f3n-Estevan 2016; Galey 2014). This bold step was taken after a public opinion poll in 2012, where participants voted for a fare-free public transportation system (Gabald\u00f3n-Estevan 2016).\n\nThe implementation of this policy, however, has had mixed results. While Gabald\u00f3n-Estevan (2016) and Galey (2014) highlight the positive impact of the policy, Hess (2017) suggests that the increase in ridership was lower than expected. This could be due to the program's design being misaligned with its primary stated goals (Hess 2017).\n\nThe fare-free public transport system (FFPTS) was not only a transportation policy but also a political and spatial project, with implications for metropolitan governance, electoral strategies, territorial competition, and socio-spatial inequalities (K\u0119b\u0142owski 2019). The policy was also embedded in Estonia\u2019s place-of-residence-based taxation system (K\u0119b\u0142owski 2019).\n\nThe FFPTS policy led to an increase of 1.2% in passenger demand, with the remaining increase attributed to an extended network of public transport priority lanes and increased service frequency (Cats 2014). However, Cats (2017) later found that the effect of FFPTS on ridership was substantially lower than those reported in previous studies due to the good level of service provision, high public transport usage, and low public transport fees that existed already prior to the FFPTS.\n\nIn conclusion, Tallinn's introduction of free public transport in 2013 was a pioneering move that had mixed results. While it led to an increase in ridership and improved mobility for low-income residents, the impact was less than expected due to pre-existing conditions such as good service provision and high public transport usage (Cats 2017). The policy also had broader implications beyond transportation, affecting areas such as metropolitan governance and electoral strategies (K\u0119b\u0142owski 2019)."}, {"query": "application of algebraic analysis for stochastic partial differential equation", "paper_list_string": "Ocone 1988:\n\nTitle: Stochastic calculus of variations for stochastic partial differential equations\n\nAbstract: Abstract This paper develops the stochastic calculus of variations for Hilbert space-valued solutions to stochastic evolution equations whose operators satisfy a coercivity condition. An application is made to the solutions of a class of stochastic pde's which includes the Zakai equation of nonlinear filtering. In particular, a Lie algebraic criterion is presented that implies that all finite-dimensional projections of the solution define random variables which admit a density. This criterion generalizes hypoellipticity-type conditions for existence and regularity of densities for finite-dimensional stochastic differential equations.\n\n==\n\nZhou 1992:\n\nTitle: A duality analysis on stochastic partial differential equations\n\nAbstract: Abstract The duality equations of stochastic partial differential equations are solved in the Sobolev space H m (= W 2 m ( R d )), and the H m -norm estimates of the solutions are obtained. As an application, the H m -norm estimates with negative m for the solutions of stochastic partial differential equations are derived.\n\n==\n\nSerrano 1985:\n\nTitle: Analysis of stochastic groundwater flow problems. Part II: Stochastic partial differential equations in groundwater flow. A functional-analytic approach\n\nAbstract: Abstract Following the scheme and concepts presented in Part I, Part II uses functional-analytic theory to analyze the problem of stochastic partial differential equations of the type appearing in groundwater flow. Equations are treated as abstract stochastic evolution equations for elliptic partial differential operators in an appropriate functional Sobolev space. Explicit forms of solutions are obtained by using a strongly continuous semigroup. The deterministic and the stochastic problem can then be treated under the same theoretical framework. Use of the theory is indicated in an application to the solution of the stochastic analogue of the regional groundwater flow problem studied in Part I. Two cases are solved: The randomly forced and the randomly initiated equation. The solution is obtained by applying the properties of semigroups and expressing the Wiener process as an infinite basis in a Hilbert space composed of independent unidimensional Wiener processes with incremental variance parameters. The first two moments of the solution as well as sample functions for different cases are derived.\n\n==\n\nSee\u03b2elberg 1993:\n\nTitle: Numerical integration of stochastic partial differential equations\n\nAbstract: Abstract The solution of stochastic partial differential equations generally relies on numerical tools. However, conventional numerical procedures are not appropriate to solve such problems. In this paper an algorithm is proposed which allows the numerical treatment of a large class of stochastic partial differential equations. To this end we reduce stochastic partial differential equations to a system of stochastic ordinary differential equations which can be solved numerically by a well-known stochastic Euler-procedure. We apply our algorithm to two stochastic partial differential equations which are special examples because their stationary two-point correlation functions can be determined analytically. Our algorithm proves to work out very well when numerical results are compared with the analytic correlation function.\n\n==\n\nHigham 2001:\n\nTitle: An Algorithmic Introduction to Numerical Simulation of Stochastic Differential Equations\n\nAbstract: A practical and accessible introduction to numerical methods for stochastic differential equations is given. The reader is assumed to be familiar with Euler's method for deterministic differential equations and to have at least an intuitive feel for the concept of a random variable; however, no knowledge of advanced probability theory or stochastic processes is assumed. The article is built around $10$ MATLAB programs, and the topics covered include stochastic integration, the Euler--Maruyama method, Milstein's method, strong and weak convergence, linear stability, and the stochastic chain rule.\n\n==\n\nNunno 2014:\n\nTitle: Approximations of stochastic partial differential equations\n\nAbstract: In this paper we show that solutions of stochastic partial dierential equations driven by Brownian motion can be approximated by stochastic partial dierential equations forced by pure jump noise/random kicks. Applications to stochastic Burgers equations are discussed.\n\n==\n\nSigrist 2012:\n\nTitle: Stochastic partial differential equation based modelling of large space\u2013time data sets\n\nAbstract: Increasingly larger data sets of processes in space and time ask for statistical models and methods that can cope with such data. We show that the solution of a stochastic advection\u2013diffusion partial differential equation provides a flexible model class for spatiotemporal processes which is computationally feasible also for large data sets. The Gaussian process defined through the stochastic partial differential equation has, in general, a non\u2010separable covariance structure. Its parameters can be physically interpreted as explicitly modelling phenomena such as transport and diffusion that occur in many natural processes in diverse fields ranging from environmental sciences to ecology. To obtain computationally efficient statistical algorithms, we use spectral methods to solve the stochastic partial differential equation. This has the advantage that approximation errors do not accumulate over time, and that in the spectral space the computational cost grows linearly with the dimension, the total computational cost of Bayesian or frequentist inference being dominated by the fast Fourier transform. The model proposed is applied to post\u2010processing of precipitation forecasts from a numerical weather prediction model for northern Switzerland. In contrast with the raw forecasts from the numerical model, the post\u2010processed forecasts are calibrated and quantify prediction uncertainty. Moreover, they outperform the raw forecasts, in the sense that they have a lower mean absolute error.\n\n==\n\nLamnabhi-Lagarrigue 1983:\n\nTitle: Algebraic computation of the statistics of the solution of some nonlinear stochastic differential equations\n\nAbstract: This paper presents an algebraic method for computing the statistics of the solution of some stochastic non-linear differential equations by mean of the Volterra functional expansion. The symbolic calculus introduced, based on noncommutative variables and iterated integrals has the advantage of allowing easily the use of symbolic computation systems, like REDUCE or MACSYMA, to perform the manipulations. This becomes necessary as soon as one tries to get high order terms.", "final_summary": "The application of algebraic analysis in stochastic partial differential equations (SPDEs) is a complex and multifaceted topic, as evidenced by the diverse findings in the papers reviewed.\n\nOcone (1988) developed a stochastic calculus of variations for Hilbert space-valued solutions to SPDEs, presenting a Lie algebraic criterion that generalizes hypoellipticity-type conditions for existence and regularity of densities for finite-dimensional stochastic differential equations. Zhou (1992) solved the duality equations of SPDEs in the Sobolev space, obtaining H m -norm estimates of the solutions, which can be applied to derive estimates for solutions of SPDEs.\n\nSerrano (1985) used functional-analytic theory to analyze stochastic partial differential equations in groundwater flow, treating them as abstract stochastic evolution equations for elliptic partial differential operators in a functional Sobolev space. See\u03b2elberg (1993) proposed an algorithm for the numerical treatment of a large class of SPDEs, reducing them to a system of stochastic ordinary differential equations that can be solved numerically.\n\nHigham (2001) provided a practical introduction to numerical methods for SPDEs, built around MATLAB programs, covering topics such as stochastic integration, the Euler--Maruyama method, Milstein's method, strong and weak convergence, linear stability, and the stochastic chain rule. Nunno (2014) showed that solutions of SPDEs driven by Brownian motion can be approximated by SPDEs forced by pure jump noise/random kicks.\n\nSigrist (2012) demonstrated that the solution of a stochastic advection\u2013diffusion partial differential equation provides a flexible model class for spatiotemporal processes, which is computationally feasible for large data sets. Lamnabhi-Lagarrigue (1983) presented an algebraic method for computing the statistics of the solution of some stochastic non-linear differential equations by means of the Volterra functional expansion.\n\nIn conclusion, the application of algebraic analysis in SPDEs is a rich and varied field, with numerous methods and approaches available for solving and approximating these complex equations. The papers reviewed provide a comprehensive overview of the current state of the field, highlighting the potential for further research and development in this area."}, {"query": "Technology Transfer: COP26 emphasized the importance of technology transfer from developed to developing countries to foster climate adaptation and mitigation. This entails sharing and facilitating access to clean and sustainable technologies that can support low-carbon pathways for development. Unilever, as a global player in the consumer goods industry, has made significant contributions through technology transfer. For instance, the company's Sustainable Living Plan includes initiatives such as sharing renewable energy solutions, water-saving technologies, and sustainable packaging innovations with suppliers and partners across its value chain. Unilever's commitment to technology transfer supports the agenda set at COP26, promoting the equitable distribution of climate-friendly technologies for a more sustainable future.", "paper_list_string": "Flamos 2010:\n\nTitle: Technology transfer insights for new climate regime\n\nAbstract: The purpose of technology transfer under the UNFCCC Article 4.5 is to \u201c\u2026promote, facilitate, and finance as appropriate the transfer of, or access to, environmentally sound technologies and know how to other Parties particularly Developing Country parties to enable them to implement the provisions of the Convention.\u201d The key challenge in this respect is that low-carbon sustainable technologies need to be adopted both by developed as well as developing countries. However, this paper focuses on the process of technology transfer to developing countries to allow them to move quickly to environmentally sound and sustainable practices, institutions and technologies. In the above framework, this paper reviews key aspects of technology transfer from a range of perspectives in the literature and discusses insights from this literature for the transfer and innovation process needed to reduce global vulnerability to climate change in the context of current international activities based on the research undertaken by the EU sponsored ENTTRANS project.\n\n==\n\nPopp 2010:\n\nTitle: International Technology Transfer for Climate Policy\n\nAbstract: While the developed world is starting to limit emissions of greenhouse gases, emissions from the developing world are increasing as a result of economic growth. Reducing these emissions while still enabling developing countries to grow requires the use of new technologies. In most cases, these technologies are first created in high-income countries. Thus, the challenge for climate policy is to encourage the transfer of these climate-friendly technologies to the developing world. This policy brief reviews the economic literature on environmental technology transfer. It then discusses the implications of this literature for climate policy, focuing on the Clean Developmenht Mechanism (CDM) ofthe Kyoto Protocol. It concludes by asking whether the current structure of the CDM provides sufficient incentives for technology transfer. Are CDM projects providing real emissions reductions, or are developed countries simply receiving credit for reductions that developing countries could have achieved on their own? What lessons can we learn from recent experience that may guide the development of the CDM (or other similar policy tools) during the next round of international climate policy negotiations?\n\n==\n\nKarakosta 2010:\n\nTitle: Technology transfer through climate change: Setting a sustainable energy pattern\n\nAbstract: Climate change mitigation is considered as a high priority internationally and is placed in the top of the agenda for most politicians and decision makers. The key challenge is that low-carbon sustainable technologies need to be adopted both by developed as well as developing countries, in an effort to avoid past unsustainable practices and being locked into old, less sustainable technologies. Technology transfer (TT), as an important feature of both the United Nations Framework Convention on Climate Change (UNFCCC) and its Kyoto Protocol can play a key role. TT can allow countries to move quickly to environmentally sound and sustainable practices, institutions and technologies. Indeed, the transfer or innovation process must be fast enough, to reduce global vulnerability to climate change. The aim of this paper is to analyse the TT challenges and emerging opportunities under the new climate regime, in terms of the process of innovation into an existing energy system, the related barriers and the supporting diffusion mechanisms. Good practices for renewable energy are also presented and discussed by both the developed and the developing countries in this respect.\n\n==\n\nTawney 2011:\n\nTitle: Innovation and Technology Transfer: Supporting Low Carbon Development with Climate Finance\n\nAbstract: January 2011 OVERVIEW Meeting the ambitious goal of limiting global warming to 2\u00b0 Celsius or less will require significant innovation the improvement of technologies and processes to drive down their cost and improve their performance. Public climate finance is essential to spurring innovation and creating the conditions that attract private investment. Investing in innovation also makes the most efficient use of the limited financial resources available and takes advantage of the developing world's growth to improve technologies. Countries like the UAE have an opportunity to play a pioneering role in this expanded international innovation system.\n\n==\n\nLee 2021:\n\nTitle: Digitalization to Achieve Technology Innovation in Climate Technology Transfer\n\nAbstract: Technology Innovation has the potential to play a strategic role in improving the effectiveness and efficiency of national efforts to address climate change. The United Nations (UN) Climate Technology Centre and Network (CTCN) is mandated to support developing countries\u2019 climate change responses through innovative technologies to achieve the goals of the Paris Agreement. In order to enhance the role of the CTCN as an innovation matchmaker, it is important to explore and leverage the implementation potential of new digital technologies and their transformational impact. Thus, in this research, to engage digitalization as an innovative tool with the environment, we first explored digitalization during the climate technology transfer processes by comprehensively reviewing CTCN Technical Assistance (Digitalization Technical Assistance, D-TA) activities in three climate sectors of risk prediction, policy decision making, and resource optimization. Then, by applying analytical methodologies of in-depth interviews with major digital-climate stakeholders and a staged model for technology innovation, we propose future strategies for enhancing the role of CTCN as an innovation matchmaker in the three digitalization cases of digital collection, digital analysis, and digital diffusion.\n\n==\n\nPopp 2011:\n\nTitle: International Technology Transfer, Climate Change, and the Clean Development Mechanism\n\nAbstract: As the developed world begins efforts to limit its emissions of greenhouse gases, economic growth in developing countries is causing increased emissions from the developing world. Reducing these emissions while still enabling developing countries to grow requires the use of climate-friendly technologies in these countries. In most cases, these technologies are first created in high-income countries. Thus, the challenge for climate policy is to encourage the transfer of these climate-friendly technologies to the developing world. This article reviews the economic literature on environmental technology transfer and discusses the implications of this literature for climate policy, focusing on the Clean Development Mechanism (CDM). A key point is that technology diffusion is gradual. Early adoption of policy by developed countries leads to the development of new technologies that make it easier for developing countries to reduce pollution as well. Since clean technologies are first developed in the world's leading economies, international trade and foreign investments provide access to these technologies. Moreover, evidence suggests that some technologies, such as those enhancing energy efficiency, will diffuse to developing countries even without the aid of policy prescriptions, such as the CDM. This is important for assessing the potential emissions reductions of proposed CDM projects.\n\n==\n\nUeno 2009:\n\nTitle: Technology Transfer to China to Address Climate Change Mitigation\n\nAbstract: This paper analyzes whether and how transfer of climate mitigation technologies to China occurs, by studying cases of seven technologies that are at the stage of deployment or diffusion. Most of these technologies were already transferred to China in terms of both technology adoption and local production. International division of labor of manufacturing and localization policies by the Chinese government facilitated local production by China, which resulted in deep cuts in production costs. Such cost reduction, coupled with technology diffusion policies by the Chinese government, then accelerated deployment and diffusion in China and other emerging economies that import Chinese products.\n\n==\n\nPigato 2020:\n\nTitle: Technology Transfer and Innovation for Low-Carbon Development\n\nAbstract: Technological revolutions have increased the world\u2019s wealth to a level that was once unimaginable. They have done so unevenly, however, and in ways that have accelerated climate change. Technology Transfer and Innovation for Low-Carbon Development argues that most of the emissions reductions required to achieve the Paris Agreement goals can be reached through the global deployment of existing and commercially proven low-carbon technologies (LCTs). Deploying LCTs from high-income countries, which account for 80 percent of all LCT innovations and 70 percent of all LCT exports, to developing countries, where they are needed most, will not be easy; but the evidence is clear that it can be done. Transferring LCTs is not only necessary to meet the climate targets, but it is also an opportunity to achieve development goals. The policies needed to deploy LCTs to developing countries could raise output and employment while yielding welfare benefits, such as reduced air and water pollution. Moreover, adopting LCT offers an opportunity for countries to benefit from participation in global value chains and to become LCT producers and exporters.", "final_summary": "The body of research highlights the importance of technology transfer in promoting climate adaptation and mitigation, particularly from developed to developing countries (Flamos, 2010; Popp, 2010; Karakosta, 2010; Tawney, 2011; Lee, 2021; Popp, 2011; Ueno, 2009; Pigato, 2020). \n\nFlamos (2010) and Karakosta (2010) discuss the necessity for both developed and developing countries to adopt low-carbon sustainable technologies to reduce global vulnerability to climate change. Popp (2010; 2011) further underscores the challenge of encouraging the transfer of these climate-friendly technologies, which are often first created in high-income countries, to the developing world. \n\nTawney (2011) posits that public climate finance is crucial to stimulate innovation and attract private investment, thereby optimizing the use of limited resources. Lee (2021) delves into the potential of digitalization as an innovative tool in climate technology transfer, suggesting it can enhance the effectiveness and efficiency of national efforts to address climate change. \n\nUeno (2009) provides evidence of successful technology transfer to China, facilitated by international division of labor and localization policies. This has led to significant cost reductions and accelerated deployment and diffusion of climate mitigation technologies. \n\nPigato (2020) concludes that the majority of the emissions reductions required to achieve the Paris Agreement goals can be reached through the global deployment of existing and commercially proven low-carbon technologies. The author also suggests that transferring these technologies to developing countries is not only necessary for meeting climate targets but also presents an opportunity to achieve development goals. \n\nIn conclusion, the research collectively underscores the importance of technology transfer in climate change mitigation and adaptation. It highlights the need for widespread adoption of low-carbon technologies, the role of public finance and digitalization in fostering innovation, and the potential benefits of technology transfer for developing countries."}, {"query": "Female attractiveness evolves compared to male attractiveness because men are more interested in attractiveness?", "paper_list_string": "Rozmus-Wrzesinska 2005:\n\nTitle: Men\u2019s ratings of female attractiveness are influenced more by changes in female waist size compared with changes in hip size\n\nAbstract: Women's attractiveness has been found to be negatively correlated with waist-to-hip ratio (WHR) in many studies. Two components of this ratio can, however, carry different signals for a potential mate. Hip size indicates pelvic size and the amount of additional fat storage that can be used as a source of energy. Waist size conveys information such as current reproductive status or health status. To assess which of these two dimensions is more important for men's perception of female attractiveness, we used a series of photographs of a woman with WHR manipulated either by hip or waist changes. Attractiveness was correlated negatively with WHR, when WHR was manipulated by waist size. The relation was inverted-U shape when WHR was changed by hip size. We postulate that in westernized societies with no risk of seasonal lack of food, the waist, conveying information about fecundity and health status, will be more important than hip size for assessing a female's attractiveness.\n\n==\n\nLittle 2001:\n\nTitle: Self-perceived attractiveness influences human female preferences for sexual dimorphism and symmetry in male faces\n\nAbstract: Exaggerated sexual dimorphism and symmetry in human faces have both been linked to potential \u2018good\u2013gene\u2019 benefits and have also been found to influence the attractiveness of male faces. The current study explores how female self\u2013rated attractiveness influences male face preference in females using faces manipulated with computer graphics. The study demonstrates that there is a relatively increased preference for masculinity and an increased preference for symmetry for women who regard themselves as attractive. This finding may reflect a condition\u2013dependent mating strategy analogous to behaviours found in other species. The absence of a preference for proposed markers of good genes may be adaptive in women of low mate value to avoid the costs of decreased parental investment from the owners of such characteristics.\n\n==\n\nBurriss 2011:\n\nTitle: Men\u2019s attractiveness predicts their preference for female facial femininity when judging for short-term, but not long-term, partners.\n\nAbstract: It is well established that women\u2019s preferences for masculinity are contingent on their own market-value and the duration of the sought relationship, but few studies have investigated similar effects in men. Here, we tested whether men\u2019s attractiveness predicts their preferences for feminine face shape in women when judging for long- and short-term relationship partners. We found that attractive men expressed a stronger preference for facial femininity compared to less attractive men. The relationship was evident when men judged women for a short-term, but not for a long-term, relationship. These findings suggest that market-value may influence men\u2019s preferences for feminine characteristics in women\u2019s faces and indicate that men\u2019s preferences may be subject to facultative variation to a greater degree than was previously thought.\n\n==\n\nGottschall 2007:\n\nTitle: Greater Emphasis on Female Attractiveness in Homo Sapiens: A Revised Solution to an Old Evolutionary Riddle\n\nAbstract: Substantial evidence from psychology and cross-cultural anthropology supports a general rule of greater emphasis on female physical attractiveness in Homo sapiens. As sensed by Darwin (1871) and clarified by Trivers (1972), generally higher female parental investment is a key determinant of a common pattern of sexual selection in which male animals are more competitive, more eager sexually and more conspicuous in courtship display, ornamentation, and coloration. Therefore, given the larger minimal and average parental investment of human females, keener physical attractiveness pressure among women has long been considered an evolutionary riddle. This paper briefly surveys previous thinking on the question, before offering a revised explanation for why we should expect humans to sharply depart from general zoological pattern of greater emphasis on male attractiveness. This contribution hinges on the argument that humans have been seen as anomalies mainly because we have been held up to the wrong zoological comparison groups. I argue that humans are a partially sex-role reversed species, and more emphasis on female physical attractiveness is relatively common in such species. This solution to the riddle, like those of other evolutionists, is based on peculiarities in human mating behavior, so this paper is also presented as a refinement of current thinking about the evolution of human mating preferences.\n\n==\n\nScott 2007:\n\nTitle: An evolutionary perspective on male preferences for female body shape\n\nAbstract: Cross-culturally, humans make systematic use of physical attractiveness to discriminate among members of the opposite sex, and physical cues to youth, health, and fertility may be particularly important to men (Buss, 1989). Nevertheless, there is controversy over whether attraction preferences are adaptive, particularly in novel environments, and whether they are universal or flexible depending on cultural circumstances (Singh & Luis, 1995). To date, a good deal of research into somatic (i.e., body) attractiveness has focused on two particular characteristics: waist-to-hip ratio (WHR) and the body mass index (BMI). WHR is calculated as the circumference of the waist divided by circumference of the hips, and provides an index of a woman\u2019s \u2018curvaceousness.\u2019 BMI is calculated as an individual\u2019s weight (kilogrammes) divided by height (metres) squared, and provides an estimate of body fatness.\n\n==\n\nWiederman 1993:\n\nTitle: Evolved gender differences in mate preferences: Evidence from personal advertisements\n\nAbstract: Abstract Evolutionary theorists have posited that contemporary men and women may differ in their specific psychological mechanisms having to do with mate selection because different strategies would have benefitted men versus women in our distant ancestral past. From these theorized gender differences in mating strategies, several hypotheses were generated and subsequently tested in the current study using a large sample of personal advertisements ( N = 1111). The results were generally supportive of evolutionary predictions: men were more likely than women to offer financial resources and honesty/ sincerity, and to seek attractiveness, appealing body shape, and a photograph in selecting a potential mate; women were more likely than men to offer an appealing body shape and to seek financial resources, qualities likely to lead to resource acquisition, and honesty/sincerity in potential mates. Women were also more likely than men to seek male friendship and/or companionship and to offer greater involvement only after the establishment of such friendship, whereas men more frequently than women made explicit requests for a sexual relationship. In general, men sought potential mates who were younger than themselves, a trend which became more pronounced among older advertisers. Women generally sought mates who were older than themselves, a trend which decreased slightly with the age of the advertiser. Results are discussed with regard to implications for hypothesized gender differences in evolved psychological mechanisms.\n\n==\n\nConfer 2010:\n\nTitle: More than just a pretty face: Men's priority shifts toward bodily attractiveness in short-term versus long-term mating contexts\n\nAbstract: Abstract Studies of physical attractiveness have long emphasized the constituent features that make faces and bodies attractive, such as symmetry, skin texture, and waist-to-hip ratio. Few studies, however, have examined the reproductively relevant cues conveyed by faces and bodies as whole units. Based on the premise that fertility cues are more readily assessed from a woman's body than her face, the present study tested the hypothesis that men evaluating a potential short-term mate would give higher priority to information gleaned from her body, relative to her face, than men evaluating a potential long-term mate. Male and female participants ( N =375) were instructed to consider dating an opposite sex individual, whose face was occluded by a \"face box\" and whose body was occluded by a \"body box,\" as a short-term or long-term mate. With the instruction that only one box could be removed to make their decision about their willingness to engage in the designated relationship with the occluded individual, significantly more men assigned to the short-term, compared to the long-term, mating condition removed the body box. Women's face versus body information choice, in contrast, was unaffected by the temporal dimension of the mating condition. These results suggest that men, but not women, have a condition-dependent adaptive proclivity to prioritize facial cues in long-term mating contexts, but shift their priorities toward bodily cues in short-term mating contexts.\n\n==\n\nJohnston 2001:\n\nTitle: Male facial attractiveness: evidence for hormone-mediated adaptive design\n\nAbstract: Abstract Experimenters examining male facial attractiveness have concluded that the attractive male face is (1) an average male face, (2) a masculinized male face, or (3) a feminized male face. Others have proposed that symmetry, hormone markers, and the menstrual phase of the observer are important variables that influence male attractiveness. This study was designed to resolve these issues by examining the facial preferences of 42 female volunteers at two different phases of their menstrual cycle. Preferences were measured using a 40-s QuickTime movie (1200 frames) that was designed to systematically modify a facial image from an extreme male to an extreme female configuration. The results indicate that females exhibit (1) a preference for a male face on the masculine side of average, (2) a shift toward a more masculine male face preference during the high-risk phase of their menstrual cycle, and (3) no shift in other facial preferences. An examination of individual differences revealed that women who scored low on a \"masculinity\" test (1) showed a larger menstrual shift, (2) had lower self-esteem, and (3) differed in their choice of male faces for dominance and short-term mates. The results are interpreted as support for a hormonal theory of facial attractiveness whereby perceived beauty depends on an interaction between displayed hormone markers and the hormonal state of the viewer.", "final_summary": "The research question at hand explores the evolutionary basis of female attractiveness and its perceived importance by males. The collected papers provide a comprehensive insight into this intriguing topic.\n\nRozmus-Wrzesinska (2005) found that men's perception of female attractiveness is significantly influenced by changes in female waist size, suggesting that waist size, which conveys information about fecundity and health status, is more important than hip size in assessing a female's attractiveness. This is echoed by Scott (2007), who highlighted the importance of physical cues to youth, health, and fertility in male attraction preferences.\n\nLittle (2001) and Burriss (2011) both found that attractiveness influences preferences for certain facial features. Little (2001) found that women who regard themselves as attractive have a relatively increased preference for masculinity and symmetry in male faces. Burriss (2011) found that attractive men expressed a stronger preference for facial femininity compared to less attractive men, but only when judging for short-term relationships.\n\nGottschall (2007) proposed that humans are a partially sex-role reversed species, and more emphasis on female physical attractiveness is relatively common in such species. This is supported by Wiederman (1993), who found that men were more likely than women to seek attractiveness and appealing body shape in potential mates.\n\nConfer (2010) found that men prioritized bodily attractiveness over facial attractiveness when considering short-term mates, suggesting that fertility cues are more readily assessed from a woman's body. Johnston (2001) found that women exhibit a preference for a male face on the masculine side of average and that this preference shifts towards a more masculine male face during the high-risk phase of their menstrual cycle.\n\nIn conclusion, the collected papers suggest that female attractiveness, particularly bodily attractiveness, plays a significant role in male mate selection, potentially due to its association with fertility cues. This supports the research question that female attractiveness evolves compared to male attractiveness because men are more interested in attractiveness. However, the papers also highlight the complexity of attractiveness preferences, suggesting that they can be influenced by various factors, including self-perceived attractiveness and relationship context."}, {"query": "laws and policies governing climate change disinformation or misinformation", "paper_list_string": "Treen 2020:\n\nTitle: Online misinformation about climate change\n\nAbstract: Policymakers, scholars, and practitioners have all called attention to the issue of misinformation in the climate change debate. But what is climate change misinformation, who is involved, how does it spread, why does it matter, and what can be done about it? Climate change misinformation is closely linked to climate change skepticism, denial, and contrarianism. A network of actors are involved in financing, producing, and amplifying misinformation. Once in the public domain, characteristics of online social networks, such as homophily, polarization, and echo chambers\u2014characteristics also found in climate change debate\u2014provide fertile ground for misinformation to spread. Underlying belief systems and social norms, as well as psychological heuristics such as confirmation bias, are further factors which contribute to the spread of misinformation. A variety of ways to understand and address misinformation, from a diversity of disciplines, are discussed. These include educational, technological, regulatory, and psychological\u2010based approaches. No single approach addresses all concerns about misinformation, and all have limitations, necessitating an interdisciplinary approach to tackle this multifaceted issue. Key research gaps include understanding the diffusion of climate change misinformation on social media, and examining whether misinformation extends to climate alarmism, as well as climate denial. This article explores the concepts of misinformation and disinformation and defines disinformation to be a subset of misinformation. A diversity of disciplinary and interdisciplinary literature is reviewed to fully interrogate the concept of misinformation\u2014and within this, disinformation\u2014particularly as it pertains to climate change.\n\n==\n\nKa\u1e91ys 2018:\n\nTitle: CLIMATE CHANGE INFORMATION ON INTERNET BY DIFFERENT BALTIC SEA REGION LANGUAGES: RISKS OF DISINFORMATION & MISINTERPRETATION\n\nAbstract: The internet space is the most important and affluent source of climate change related information. Hoverer information content are not always satisfying and threat of fake news and disinformation are very realistic. The analysis included top10 search results of four phrases (Climate change, Global warming, Adaptation to climate change and Climate change policy) using Google search engine. The phrases were searched in 11 Baltic Sea Region (BSR) languages and in the Ukrainian and English languages. The results revealed that climate change disinformation and misinterpretation exists on the internet. Mostly it displayed in indirect forms such as old information, existence of junksites, advertisements, unequal share by main actors (government, mass media, etc.). Moreover, on Eastern BSR languages, internet search results of climate change information are less convenient comparing to western BSR languages. The usage of multilanguage approach in Wikipedia pages could be one of the freshest and most reliable sources of information about climate change.\n\n==\n\nMoxnes 2008:\n\nTitle: Misperceptions of global climate change: information policies\n\nAbstract: Previous experimental studies have found that people generally misperceive the basic dynamics of renewable resources, and in particular the accumulation of greenhouse gases (GHGs) in the atmosphere. The purpose of the present laboratory experiment is to find out why people misperceive the dynamics of CO2 accumulation and how misperceptions could be avoided. Using a simulator, 242 subjects were each asked to control total global emissions of CO2 to reach a given target for the stock of CO2 in the atmosphere. Consistent with previous investigations we find a strong tendency for people to overshoot the stated goal. Furthermore, our results point out that people need help to develop proper mental models of CO2 accumulation and they need motivation to reconsider inappropriate decision heuristics. Based on these results and the literature on conceptual change a new information strategy is designed. To motivate, it imposes cognitive conflict; and to facilitate new understanding, it provides simple analogies. A new test shows promising learning effects. The results have important implications for the Intergovernmental Panel on Climate Change (IPCC), governments, and media covering the climatic change issue as well as for general education.\n\n==\n\nMehling 2013:\n\nTitle: Climate Law in the United States: Facing Structural and Procedural Limitations\n\nAbstract: Just 5 years ago, the official position of the White House on the issue of climate change was that there was no such position. President George W. Bush and his administration declined to address whether climate change was even occurring, much less how to mitigate the causes of a phenomenon that had potentially contributed to billion-dollar disasters, thousands of fatalities during Hurricanes Rita and Katrina, and a significant number of displaced U.S. citizens.\n\n==\n\nLinden 2017:\n\nTitle: Inoculating the Public against Misinformation about Climate Change\n\nAbstract: Effectively addressing climate change requires significant changes in individual and collective human behavior and decision\u2010making. Yet, in light of the increasing politicization of (climate) science, and the attempts of vested\u2010interest groups to undermine the scientific consensus on climate change through organized \u201cdisinformation campaigns,\u201d identifying ways to effectively engage with the public about the issue across the political spectrum has proven difficult. A growing body of research suggests that one promising way to counteract the politicization of science is to convey the high level of normative agreement (\u201cconsensus\u201d) among experts about the reality of human\u2010caused climate change. Yet, much prior research examining public opinion dynamics in the context of climate change has done so under conditions with limited external validity. Moreover, no research to date has examined how to protect the public from the spread of influential misinformation about climate change. The current research bridges this divide by exploring how people evaluate and process consensus cues in a polarized information environment. Furthermore, evidence is provided that it is possible to pre\u2010emptively protect (\u201cinoculate\u201d) public attitudes about climate change against real\u2010world misinformation.\n\n==\n\nFarrell 2019:\n\nTitle: The growth of climate change misinformation in US philanthropy: evidence from natural language processing\n\nAbstract: Two of the most consequential developments affecting US politics are (1) the growing influence of private philanthropy, and (2) the large-scale production and diffusion of misinformation. Despite their importance, the links between these two trends have not been scientifically examined. This study employs a sophisticated research design on a large collection of new data, utilizing natural language processing and approximate string matching to examine the relationship between the large-scale climate misinformation movement and US philanthropy. The study finds that over a twenty year period, networks of actors promulgating scientific misinformation about climate change were increasingly integrated into the institution of US philanthropy. The degree of integration is predicted by funding ties to prominent corporate donors. These findings reveal new knowledge about large-scale efforts to distort public understanding of science and sow polarization. The study also contributes a unique computational approach to be applied at this increasingly important, yet methodologically fraught, area of research.\n\n==\n\nWentz 2022:\n\nTitle: LIABILITY FOR PUBLIC DECEPTION: LINKING FOSSIL FUEL DISINFORMATION TO CLIMATE DAMAGES\n\nAbstract: Over two dozen U.S. states and municipalities have filed lawsuits against fossil fuel companies, seeking abatement orders and compensation for climate damages based on theories such as public nuisance, negligence, and failure to warn, and alleging these companies knew about the dangers of their products, intentionally concealed those dangers, created doubt about climate science, and undermined public support for climate action. This Article examines how tort plaintiffs can establish a causal nexus between public deception and damages, drawing from past litigation, particularly claims filed against manufacturers for misleading the public about the risks of tobacco, lead paint, and opioids. A key finding is that courts may infer public reliance on false and misleading statements using multiple lines of evidence, including information about the scope and magnitude of the deceptive communications, defendants\u2019 internal assessments of the efficacy of their disinformation campaigns, acknowledgements of intended reliance made by defendants, expert testimony on the effects of disinformation, public polling data\n\n==\n\nBarry 2013:\n\nTitle: Climate change ethics, rights, and policies: an introduction\n\nAbstract: Climate change continues to dominate academic work within green/environmental politics. Indeed, there appears to be almost an inverse relationship between the lack of political leadership on tackling climate change and the growth in ever more sophisticated academic analyses of this complex and multifaceted problem. There is an increasing disjunction between the growth in our knowledge and understanding of the ethical, political, economic, sociological, cultural, and psychological aspects of climate change and the lack of political achievement in putting in place clear and binding targets, an agreed decarbonisation roadmap, and associated regulatory and policy instruments with enforcement. This gap might be taken as evidence that we do not need more reports on climate change. To quote that most unlikely of green politicians, Arnold Schwarzenegger, former Governor of California: \u2018The debate is over. We know the science. We see the threat. And we know that the time for action is now\u2019 (California Energy Commission 2007, p. 1). This special issue focuses on a variety of ways in which climate change is conceptualised in normative political and ethical theory, and addressed in policy and regulations.", "final_summary": "Climate change misinformation and disinformation pose significant challenges to the global response to environmental crises. The papers collectively highlight the complexity of this issue, the actors involved, and the potential strategies to address it.\n\nTreen (2020) provides a comprehensive overview of climate change misinformation, its origins, and its spread, particularly in online social networks. The paper suggests that misinformation is often linked to climate change skepticism and denial, and is propagated by a network of actors. The paper also discusses potential strategies to address misinformation, including educational, technological, regulatory, and psychological approaches.\n\nKa\u1e91ys (2018) focuses on the risks of disinformation and misinterpretation of climate change information on the internet, particularly in different languages in the Baltic Sea Region. The paper highlights the threat of outdated information, junk sites, and advertisements, and suggests multilingual approaches on platforms like Wikipedia as a potential solution.\n\nMoxnes (2008) explores why people misperceive the dynamics of CO2 accumulation and suggests an information strategy that imposes cognitive conflict and provides simple analogies to help people develop proper mental models of CO2 accumulation.\n\nMehling (2013) discusses the structural and procedural limitations of climate law in the United States, highlighting the need for more robust policies to address climate change misinformation.\n\nLinden (2017) suggests that inoculating the public against misinformation about climate change can be an effective strategy. The paper argues that conveying the high level of consensus among experts about human-caused climate change can counteract the politicization of science.\n\nFarrell (2019) examines the growth of climate change misinformation in US philanthropy, revealing that networks of actors spreading misinformation have become increasingly integrated into US philanthropy, particularly those with funding ties to prominent corporate donors.\n\nWentz (2022) discusses the potential for legal liability for public deception about climate change, drawing parallels with litigation against manufacturers for misleading the public about the risks of tobacco, lead paint, and opioids.\n\nFinally, Barry (2013) underscores the ethical, political, economic, sociological, cultural, and psychological aspects of climate change and the lack of political achievement in putting in place clear and binding targets and policies.\n\nIn conclusion, these papers collectively highlight the multifaceted nature of climate change misinformation and disinformation, the actors involved, and the potential strategies to address it, including educational, technological, regulatory, psychological, and legal approaches. They underscore the urgent need for interdisciplinary efforts to tackle this complex issue."}, {"query": "Curry (2005) and Seip (2013) highlight that government-owned businesses often prioritize meeting social obligations over commercial objectives, leading to inefficient resource allocation and financial struggles for these enterprises.", "paper_list_string": "Christiansen 2013:\n\nTitle: Balancing Commercial and Non-Commercial Priorities of State-Owned Enterprises\n\nAbstract: The overarching question for the government owners of state-owned enterprises (SOEs) is why these companies need to be owned by the state. The OECD Guidelines on Corporate Governance of State-Owned Enterprises provides a \u201cblueprint\u201d for the corporatisation and commercialisation of such enterprises, but it may be assumed that the reason for continued state ownership is that they are expected to act differently from private companies. A relatively clear case occurs when SOEs are established with the purpose of pursuing mostly non-commercial activities. In many cases, their activities might otherwise be carried out by government institutions; the SOE incorporation has been chosen mostly on efficiency grounds.A number of other rationales for public ownership of enterprises have been offered, including: (i) monopolies in sectors where competition and market regulation is not deemed feasible or efficient; (ii) market incumbency, for instance in sectors where competition has been introduced but a state-owned operator remains responsible for public service obligations; (iii) imperfect contracts, where those public service obligations that SOEs are charged with are too complex or malleable to be laid down in service contracts; (iv) industrial policy or development strategies, where SOEs are being used to overcome obstacles to growth or correct market imperfections...\n\n==\n\nMoizer 2010:\n\nTitle: Strategy making in social enterprise: The role of resource allocation and its effects on organizational sustainability\n\nAbstract: Social enterprises are non-profit organizations that seek to achieve social goals through commercial activity. Because they have both social and commercial objectives, social enterprises are confronted with high levels of complexity in their strategizing and operations; not only do they have to generate sufficient revenue to re-invest in their business operations, they also have to maintain investment in social projects in their community. Under conditions of resource scarcity, this poses severe challenges which can threaten the long-term sustainability of the enterprise. In this paper the tension between allocating resources to commercial activity and social action is explored. A simple causal-loop diagram is constructed which maps out the relationships between resource allocation and a number of other variables thought to influence the sustainability of social enterprises. By tracing through the diagram, a range of generic strategic alternatives available to social enterprises is evaluated and discussed. Copyright \u00a9 2010 John Wiley & Sons, Ltd.\n\n==\n\nJanda 2011:\n\nTitle: Inefficient Credit Rationing and Public Support of Commercial Credit Provision\n\nAbstract: Credit contracting between a lender with market power and a small start-up entrepreneur may lead to the rejection of projects whose expected benefits are higher than their total costs when adverse selection is present. This inefficiency may be eliminated by government support in the form of credit guarantees or interest-rate subsidies. The principal-agent model of this paper compares different forms of government support and concludes that credit guarantees and interest-rate subsidies have an unambiguous positive effect on social efficiency, since they enable the funding of socially efficient projects that would not be financed otherwise.\n\n==\n\nMiko\u0142ajczak 2021:\n\nTitle: How do barriers to the activities of social enterprises affect their financial situation? Evidence based on data from Poland and resource mobilization theory\n\nAbstract: \nPurpose\nThe purpose of the research is to identify the degree of intensity of barriers to the activities of social enterprises (SEs) and to examine the significance of such barriers regarding the financial situation of SE\u2019s in emerging economies.\n\n\nDesign/methodology/approach\nThe data relates to 200 SEs selected from a national survey of 1,300 Polish non-governmental organizations (NGOs). An analysis of barriers to SEs according to the frequency of their occurrence was conducted. An indicator of the intensity of barriers to the activities of social enterprises and an indicator on these enterprises\u2019 overall financial condition were determined. Spearman rank correlation analysis was used to assess the relationship between the indices.\n\n\nFindings\nThe results of the study indicate that in addition to excessive bureaucracy in public administration and the complex formalities related to the use of private and public funds, SEs have difficulties in maintaining good staff and volunteers, whereas people in key positions reference burnout, not only among their own employees but also in themselves. These have a significant impact on the financial situation of SEs.\n\n\nOriginality/value\nThis study contributes to the field of social entrepreneurship in two ways. One is at the macro level in that it provides suggestions for public authorities in emerging economies interested in maintaining SEs in good financial condition so that they can effectively fulfil their social functions. The second contribution \u2013 the micro approach \u2013 is recognizing the extent of the impact of barriers on the financial condition of SEs and also determining the intensity of such barriers with regard to the mobilization of resources by managers, especially in the field of human resources.\n\n\n==\n\nSappington 2004:\n\nTitle: Anticompetitive Behavior by State-Owned Enterprises: Incentives and Capabilities\n\nAbstract: State-owned enterprises (SOEs), also known as public enterprises, are owned by governments rather than private investors and compete directly with private, profit-maximizing enterprises in many important markets. For example, government postal firms typically offer overnight mail and package shipping services in direct competition with private delivery companies. In addition, many public hospitals and educational institutions compete directly with private suppliers of similar services. Production by public enterprises is particularly widespread in developing countries. During the 1980s, for example, public enterprises accounted for approximately 14 percent of gross domestic product in African nations and approximately 11 percent in developing countries as a whole.1 Typically, SOEs are required to pursue goals other than pure profit maximization. One might therefore suspect they would act less aggressively toward their competitors than would private, profit-\n\n==\n\nGroves 1937:\n\nTitle: Government Corporations and Federal Funds\n\nAbstract: The increasing use of the government corporation in the conduct of governmental business enterprises is one of the most significant of recent trends in public administration. Indeed, the corporate device represents one answer to the charge that government is so poorly organized and so beset by red tape that public operation of economic enterprise is inevitably sluggish and inefficient. Most notably during the mobilization crisis of 1917 and the depression years following 1929, when speedy and vigorous action was needed, the federal government has relied heavily upon government corporations which could cut through red tape and \u201cget things done.\u201d Also in more normal times, however, particular enterprises somewhat isolated from regular governmental activities have been carried on in corporate form.\n\n==\n\nVernon-Wortzel 1989:\n\nTitle: Privatization: Not the only answer\n\nAbstract: Abstract Governments in many developing countries created state-owned enterprises (SOEs) to produce goods or provide services the private sector seemed unwilling or unable to offer. In many cases, SOEs have turned out to be problem children\u2014at best, inefficient and, at worst, unable to fulfill their original functions. Now the recommended solution to the problem of state-owned enterprises is privatization. But privatization is no more a solution to the problems of SOEs than SOEs were a solution to the problems they were created to solve. The problem of SOEs is not ownership, but rather a lack of explicit goals and objectives, and an absence of organization cultures and systems that support and encourage fulfillment of those goals and objectives. In some circumstances, privatization may aid in fulfillment of an enterprise's goals and objectives because of the culture and systems it fosters; in other cases not.\n\n==\n\nKodzi 2015:\n\nTitle: The Clash of Missions: Juxtaposing Competing Pressures in South Africa's Social Enterprises\n\nAbstract: Abstract Social enterprises seek resource combinations to provide some assurance of sustainability as they create social value in a defined domain of action. However, this resource-seeking mandate also constitutes a distraction that complicates the operations of any social enterprise. How do social enterprises manage the implicit duality of maintaining a commercial mission in order to achieve their social mission? Using the context of South Africa, this study examines the process trade-offs that enhance or limit social impact under the referenced duality. The study clarifies the nature of these trade-offs, as a basis for appropriating efficiency-enhancing process design in enterprises that create, but do not capture value. The author proposes a unique intersection between the logic of control and the logic of empowerment in the field of social entrepreneurship, and suggests that value chain processes be controlled to the extent that the enterprise acts as a custodian of community empowerment for its target beneficiaries.", "final_summary": "The research question explores the balance between social obligations and commercial objectives in government-owned businesses, and how this balance affects resource allocation and financial stability. \n\nChristiansen (2013) suggests that state-owned enterprises (SOEs) are expected to act differently from private companies, often prioritizing non-commercial activities. This aligns with the findings of Moizer (2010), who highlights the tension in social enterprises between allocating resources to commercial activity and social action, which can threaten the long-term sustainability of the enterprise. \n\nJanda (2011) discusses the inefficiency that can arise from credit contracting between a lender and a small start-up entrepreneur, suggesting that government support in the form of credit guarantees or interest-rate subsidies can improve social efficiency. This implies that government intervention can help balance commercial and social objectives in SOEs.\n\nMiko\u0142ajczak (2021) identifies barriers to the activities of social enterprises, including excessive bureaucracy and complex formalities, which can impact their financial situation. This suggests that the prioritization of social obligations over commercial objectives can lead to financial struggles for these enterprises.\n\nSappington (2004) and Groves (1937) both discuss the role of SOEs in competition with private enterprises, suggesting that SOEs often have different goals than pure profit maximization. This supports the idea that SOEs often prioritize social obligations over commercial objectives.\n\nVernon-Wortzel (1989) argues that the problem with SOEs is not ownership, but a lack of explicit goals and objectives, and an absence of organization cultures and systems that support and encourage fulfillment of those goals and objectives. This suggests that the balance between social obligations and commercial objectives in SOEs can be improved with clearer goals and better organizational systems.\n\nFinally, Kodzi (2015) discusses the trade-offs that social enterprises must make between maintaining a commercial mission and achieving their social mission. This suggests that the balance between social obligations and commercial objectives in SOEs can be complex and challenging.\n\nIn conclusion, the papers collectively suggest that government-owned businesses often prioritize social obligations over commercial objectives, which can lead to inefficient resource allocation and financial struggles. However, they also suggest that this balance can be improved with clearer goals, better organizational systems, and government support."}, {"query": "how does deliberate ignorance affect punishment in the dictator game?", "paper_list_string": "Bartling 2013:\n\nTitle: Does Willful Ignorance Deflect Punishment? - An Experimental Study\n\nAbstract: This paper studies whether people can avoid punishment by remaining willfully ignorant about possible negative consequences of their actions for others. We employ a laboratory experiment, using modified dictator games in which a dictator can remain willfully ignorant about the payoff consequences of his decision for a receiver. A third party can punish the dictator after observing the dictator\u2019s decision and the resulting payoffs. On the one hand, willfully ignorant dictators are punished less if their actions lead to unfair outcomes than dictators who reveal the consequences before implementing the same outcome. On the other hand, willfully ignorant dictators are punished more than revealing dictators if their actions do not lead to unfair outcomes. We conclude that willful ignorance can circumvent blame when unfair outcomes result, but that the act of remaining willfully ignorant is itself punished, regardless of the outcome.\n\n==\n\nRuffle 1998:\n\nTitle: More Is Better, But Fair Is Fair: Tipping in Dictator and Ultimatum Games\n\nAbstract: Abstract This paper examines Allocators' willingness to reward and punish their paired Recipients. Recipients only compete in a skill-testing contest, the outcome of which determines the size of the surplus. In the dictator game, Allocators reward skillful Recipients, but punish unskillful ones only modestly. The punishment effect is mitigated by the belief held by some Allocators that effort is the appropriate measure of deservingness. The ultimatum game extension reveals offerers' ability to adapt to the strategic environment. Offers to skillful Recipients in the ultimatum game, however, are shown to be motivated by a taste for fairness, and not strategic considerations. Journal of Economic Literature Classification Numbers: C70, C91, D63.\n\n==\n\nCason 1998:\n\nTitle: Social Influence in the Sequential Dictator Game.\n\nAbstract: This paper introduces the sequential dictator game to study how social influence may affect subjects' choices when making dictator allocations. Subjects made dictator allocations of $40 before and after learning the allocation made by one other subject in the Relevant Information treatment, or the birthday of one other subject in the Irrelevant Information treatment. Subjects on average become more self-regarding in the Irrelevant Information treatment, but observing relevant information constrains some subjects from moving toward more self-regarding choices. We also find that subjects who exhibit more self-regarding behavior on their first decisions are less likely to change choices between their first and second decisions, and the use of the Strategy Method in this setting does not significantly alter choices. The relationships between our findings and the economic and psychological literature regarding how social influence operates are also explored. Copyright 1998 Academic Press.\n\n==\n\nSchulz 2014:\n\nTitle: Affect and fairness: Dictator games under cognitive load\n\nAbstract: We investigate the impact of affect and deliberation on other-regarding decisions. In our laboratory experiment subjects decide on a series of mini-Dictator games while under varying degrees of cognitive load. Cognitive load is intended to decrease deliberation and therefore enhance the influence of affect on behavior. In each game subjects have two options: they can decide between a fair and an unfair allocation. We find that subjects in a high-load condition are more generous \u2013 they more often choose the fair allocation than subjects in a low-load condition. The series of mini-Dictator games also allows us to investigate how subjects react to the games\u2019 varying levels of advantageous inequality. Low-load subjects react considerably more to the degree of advantageous inequality. Our results underscore the importance of affect for basic altruistic behavior and deliberation in adjusting decisions to a given situation.\n\n==\n\nAchtziger 2015:\n\nTitle: Money, depletion, and prosociality in the dictator game\n\nAbstract: We study the effects of ego depletion, a manipulation which consumes self-control resources, on social preferences in a dictator game. Depleted dictators give considerably less than non-depleted dictators and hence exhibit strong preferences for selfish allocation. In contrast to earlier studies, participants were explicitly paid for completing the egodepletion task (with either a flat rate or strictly performance-based payment). We studied the dynamics of decisions by repeating the dictator game 12 times (anonymously). Depleted dictators start with much lower offers than non-depleted ones, but, strikingly, offers decrease in time for both groups, and more rapidly so for non-depleted dictators. We conclude that, while depleted dictators neglect fairness motives from the very first decision on, non-depleted dictators initially resist the tendency to act selfishly, but eventually become depleted or learn to act selfishly. Hence, prosocial behavior may be short-lived, and ego depletion uncovers the default tendencies for selfishness earlier.\n\n==\n\nDana 2006:\n\nTitle: What you don't know won't hurt me: Costly (but quiet) exit in dictator games\n\nAbstract: We used simple economic games to examine pro-social behavior and the lengths that people will take in order to avoid engaging in it. Over two studies, we found that about one third of participants were willing to exit a $10 dictator game and take $9 instead. The exit option left the receiver nothing, but also ensured that the receiver never knew that a dictator game was to be played. Because most social utility models are defined over monetary outcomes, they cannot explain choosing the ($9, $0) exit outcome over the dominating $10 dictator game, since the game includes outcomes of ($10, $0) and ($9, $1). We also studied exiting using a private dictator game. In the private game, the receiver never knew about the game or from where any money was received. Gifts in this game were added innocuously to a payment for a separate task. Almost no dictators exited from the private game, indicating that receivers' beliefs are the key factor in the decision to exit. When, as in the private game, the receivers' beliefs and expectations cannot be manipulated by exit, exit is seldom taken. We conclude that giving often reflects a desire not to violate others' expectations rather than a concern for others' welfare per se. We discuss the implications of our results for understanding ethical decisions and for testing and modeling social preferences. An adequate specification of social preferences should include \"psychological\" payoffs that directly incorporate beliefs about actions into the utility function.\n\n==\n\nEdele 2013:\n\nTitle: Explaining altruistic sharing in the dictator game : The role of affective empathy, cognitive empathy, and justice sensitivity\n\nAbstract: Abstract Experimental games like the dictator game have proven of great value for the study of altruism and sharing behavior. It has been shown that individuals differ substantially in the amount of money they offer to an anonymous receiver. Yet, to date little is known about how personality dispositions shape differences in altruistic sharing. The current study aimed at elucidating the effects of affective empathy in comparison to cognitive empathy and the disposition of justice sensitivity on altruistic sharing in the dictator game. Different aspects of these constructs were assessed multi-methodologically in young adults and related to their offers in the dictator game. Analyses revealed that affective empathy and justice sensitivity explain altruistic sharing behavior, but different dimensions of cognitive empathy do not. The results support the assumption that affective reaction tendencies explain differences in altruistic sharing.\n\n==\n\nG\u0105siorowska 2012:\n\nTitle: Psychological consequences of money and money attitudes in dictator game\n\nAbstract: According to the research conducted by Vohs, Mead, and Goode (2006, 2008), reminders of money cause people to behave self-sufficiently, and especially to reveal a reduced tendency to charitable behaviour. In this study, we wanted to establish if this tendency would be present in the dictator game, and if so, whether money activation would just change behaviour, or whether it would also change people\u2019s evaluation of their own decisions. We assumed that people who revealed symbolic and instrumental money attitudes would react differently to money activation. As expected, money priming caused smaller money transfers compared to the control condition, and this effect was significant among those with symbolic attitudes toward money. Moreover, participants who transferred at least half of the sum after money activation declared stronger negative emotions and lower satisfaction with the decision than those who made similar transfers in the control condition.", "final_summary": "Deliberate ignorance, or willful ignorance, plays a complex role in the dynamics of the dictator game, a popular experimental tool in economics and psychology. The dictator game involves one player, the \"dictator,\" who is given a sum of money to split between themselves and another player, the \"receiver.\" The dictator's decision is unilateral and the receiver must accept whatever is offered.\n\nBartling (2013) found that dictators who chose to remain willfully ignorant about the consequences of their actions for the receiver were punished less if their actions led to unfair outcomes. However, the act of remaining willfully ignorant was itself punished, regardless of the outcome. This suggests that while deliberate ignorance can shield dictators from punishment when outcomes are unfair, it can also invite punishment in and of itself.\n\nSchulz (2014) explored the impact of cognitive load on other-regarding decisions in the dictator game. The study found that subjects under high cognitive load were more generous, suggesting that affect plays a significant role in decision-making. Achtziger (2015) studied the effects of ego depletion on social preferences in the dictator game and found that depleted dictators gave considerably less than non-depleted dictators, indicating that self-control resources may influence prosocial behavior.\n\nDana (2006) found that about one third of participants were willing to exit a dictator game and take less money, leaving the receiver with nothing but also ensuring the receiver never knew a game was to be played. This suggests that some dictators may use deliberate ignorance as a strategy to avoid engaging in prosocial behavior.\n\nEdele (2013) explored the role of empathy in the dictator game and found that affective empathy and justice sensitivity explain altruistic sharing behavior. G\u0105siorowska (2012) explored the role of money attitudes in the dictator game and found that reminders of money caused smaller money transfers, especially among those with symbolic attitudes toward money.\n\nIn conclusion, the role of deliberate ignorance in the dictator game is complex and multifaceted. While it can serve as a shield against punishment in some cases (Bartling, 2013), it can also invite punishment (Bartling, 2013) and be used as a strategy to avoid prosocial behavior (Dana, 2006). Other factors such as cognitive load (Schulz, 2014), ego depletion (Achtziger, 2015), empathy (Edele, 2013), and money attitudes (G\u0105siorowska, 2012) also influence decision-making and behavior in the dictator game. However, the specific impact of these factors on the dynamics of punishment in the dictator game requires further investigation."}, {"query": "wheat stem sugar reserve mobilization", "paper_list_string": "Ram 2018:\n\nTitle: Stem Reserve Mobilization in Relation to Yield under Different Drought and High Temperature Stress Conditions in Wheat (Triticum aestivum L.) Genotypes\n\nAbstract: Carbon requirements for grain filling in wheat are mainly from current assimilation by photosynthesis and remobilization of reserves from the stems (Yang et al., 2000). Remobilization of assimilates is an active process that involves translocation of stored reserves from stems and sheaths to grains (Gupta et al., 2015). Stem reserves contribute 20 to 40% weight of the grain in non-stressed condition (Vignjevic et al., 2015) and this can be up to 70% under stressed conditions during grain filling (Rebetzke et al., 2008). Drought and high temperature induced earlier mobilization of non-structural reserve carbohydrates from stem and leaf sheaths, which provided a greater proportion of grain International Journal of Current Microbiology and Applied Sciences ISSN: 2319-7706 Volume 7 Number 04 (2018) Journal homepage: http://www.ijcmas.com\n\n==\n\nBlum 1997:\n\nTitle: Improving wheat grain filling under stress by stem reserve mobilisation\n\nAbstract: Stem reserves from pre-anthesis plant assimilation are being increasingly recognised as an important source of carbon for grain filling when current photosynthesis is inhibited by drought, heat or disease stress during this stage. Genotypic and environmental factors affecting reserve accumulation and utilisation for grain filling are reviewed. The genetic improvement of stem reserve storage and utilisation as a potent mechanism for grain filling under stress is discussed, and practical guidelines for selection work are provided.\n\n==\n\nBlum 2004:\n\nTitle: Improving wheat grain filling under stress by stem reserve mobilisation\n\nAbstract: Stem reserves from pre-anthesis plant assimilation are being increasingly recognised as an important source of carbon for grain filling when current photosynthesis is inhibited by drought, heat or disease stress during this stage. Genotypic and environmental factors affecting reserve accumulation and utilisation for grain filling are reviewed. The genetic improvement of stem reserve storage and utilisation as a potent mechanism for grain filling under stress is discussed, and practical guidelines for selection work are provided.\n\n==\n\nDavidson 1992:\n\nTitle: Storage and remobilization of water-soluble carbohydrates in stems of spring wheat\n\nAbstract: Grain yield of wheat (Triticum aestivum L.) depends, in part, on carbohydrate reserves available in the stem. This study was conducted to determine the effects of water deficit during the post-jointing period on quantitative changes in water soluble carbohydrates (WSC; including simple sugars, starch, and fructans) in the stems of spring wheat. Cultivars Edwall and Waverly were planted in 1983 and 1984 at Spillman Agronomy Farm near Pullman, WA, at rates of 94 and 168 kg per ha in rows 30 and 15 cm apart, respectively, in both irrigated and nonirrigated treatments. Beginning at jointing, plants were harvested weekly. Stem material was dried, milled, digested with amyloglucosidase, and analyzed for WSC by iodometric titration. Results were similar for both varieties and both years. Anthesis and peak stem carbohydrate concentration occurred 4 to 7 d earlier in nonirrigated than irrigated plants; and physiological maturity of the grain occurred 6 to 14 days earlier. The concentration of WSC in stems increased to between 250 and 380 mg per g dry wt. at approximately equal to 10 to 14 d after anthesis and then declined to less than 50 mg per g dry wt. by physiological maturity of the grain. From the time of peak WSC content until physiological maturity in 1984, 959 to 1235 mg WSC were lost from the stems of irrigated plants but only 619 to 662 mg WSC were lost from stems of nonirrigated plants. The data indicate that stems are an important temporary storage site for reserve carbohydrates in both irrigated and nonirrigated plants.\n\n==\n\nBlum 1994:\n\nTitle: Stem Reserve Mobilisation Supports Wheat-grain Filling Under Heat Stress\n\nAbstract: The grain filling of wheat (Triticum aestivum L.) is seriously impaired by heat stress due to reductions in current leaf and ear photosynthesis at high temperatures. An alternative source of carbon for grain filling is stored stem reserves. Two spring wheat cultivars (V5 and V2183) of very similar phenology and plant stature, which had previously been found to differ in grain shrivelling under drought and heat stress conditions in the field, were used to evaluate the hypothesis that the mobilisation of stored stem reserves into the growing grain is an important source of carbon for supporting grain filling under heat stress. In two experiments in Israel (1990 and 1991), the rates of stem dry matter (DM) and stem total non-structural carbohydrates (TNC) loss, grain growth and leaf senescence were monitored under optimal (control) and high (stressed) temperatures in the glasshouse (1990) and the growth chamber (1991). Cultivar V5 always sustained a smaller reduction in grain dry weight under heat stress, than V2183. Irrespective of temperature, V5 had a higher stem DM and TNC content at the onset of grain filling, greater depletion of stem dry matter (or TNC) during grain filling, and longer duration of grain filling, than V2183. During grain filling V5 generally exported about two to three times more DM from the stems than V2183, under both non-stressed and stressed conditions. On the other hand, V5 was more heat-susceptible than V2183 in terms of leaf longevity, in vivo chlorophyll stability and grain abortion under heat stress. In a third experiment (1992) five cultivars (including V5 and V2183) were subjected to chemical desiccation (0.3% potassium iodide) of the canopy in the field in order to destroy the photosynthetic source ofthe plant after anthesis. The same cultivars were subjected to heat stress (35/25oC) or non-stressed (25/15oC) conditions after anthesis in the growth chamber. It was found that grain dry weight reduction by chemical desiccation was highly correlated with grain dry weight reduction by heat stress (r2 = 0.89). Therefore, the superior capacity of V5 for grain filling from mobilised stem reserves is a consti- tutive trait which supports grain filling under heat stress which can be tested for by chemical desiccation of plants under non-stressed conditions.\n\n==\n\nSrivastava 2017:\n\nTitle: Effect of stem reserve mobilization on grain filling under drought stress conditions in recombinant inbred population of wheat\n\nAbstract: Pre-anthesis carbon assimilation of stem reserves is considered as an important source for grain filling during post anthesis drought stresses that inhibit photosynthesis. 175 RILs from cross (C518/2*PBW343) along with check cultivars were evaluated for stem reserve mobilization under irrigated and rainfed conditions. These two cultivars belonging to distinct adaptation mechanism, offer several morpho -physiological and biochemical con-trasts. C 518 is tall and adapted to low input rainfed conditions whereas PBW 343 is semi -dwarf and input re-sponsive. Further C 518 is known for better stem reserves on account of larger peduncle and strong commitment to grain filling due to effective stem reserve mobilization. The parents and the RIL population was tested for stem reserve mobilization by defoliation of flag leaf and second leaf at anthesis under irrigated and rainfed environments. Evaluated entries differed significantly (pl0.001) for reduction in 1000 grain weight under defoliation (TGWL). Percent reduction in 1000 grain weight ranged from 4.4 % to 39.6 % under irrigated environment and 3.2 % to 35.0 under rainfed condition. A significant positive correlation (r = +0.357) between stem reserve mobilization and peduncle length was observed under rainfed condition. Tested RILs vary individually for stem reserve mobilization when subjected to removal of flag leaf and second leaf inhibiting the photosynthesis. The genotypes with better stem reserve mobilization based on 1000 grain weight in the absence of photosynthesis may also provide relative tolerance to drought.\n\n==\n\nSharbatkhari 2016:\n\nTitle: Wheat stem reserves and salinity tolerance: molecular dissection of fructan biosynthesis and remobilization to grains\n\nAbstract: AbstractMain conclusionFructan accumulation and remobilization to grains under salinity can decrease dependency of the wheat tolerant cultivar on current photosynthesis and protect it from severe yield loss under salt stress.\n Tolerance of plants to abiotic stresses can be enhanced by accumulation of soluble sugars, such as fructan. The current research sheds light on the role of stem fructan remobilization on yield of bread wheat under salt stress conditions. Fructan accumulation and remobilization as well as relative expression of the major genes of fructan metabolism were investigated in the penultimate internodes of \u2018Bam\u2019 as the salt-tolerant and \u2018Ghods\u2019 as the salt-sensitive wheat cultivars under salt-stressed and controlled conditions and their correlations were analyzed. More fructan production and higher efficiency of fructan remobilization was detected in Bam cultivar under salinity. Up-regulation of sucrose: sucrose 1-fructosyltransferase (1-SST) and sucrose: fructan 6-fructosyltransferase (6-SFT) (fructan biosynthesis genes) at anthesis and up-regulation of fructan exohydrolase (1-FEH) and vacuolar invertase (IVR) genes (contributed to fructan metabolism) during grain filling stage and higher expression of sucrose transporter gene (SUT1) in Bam was in accordance with its induced fructan accumulation and remobilization under salt stress. A significant correlation was observed between weight density, WSCs and gene expression changes under salt stress. Based on the these results, increased fructan production and induced stem reserves remobilization under salinity can decrease dependency of the wheat tolerant cultivar on current photosynthesis and protect it from severe yield loss under salt stress conditions.\n\n==\n\nGupta 2011:\n\nTitle: Stem Reserve Mobilization and Sink Activity in Wheat under Drought Conditions\n\nAbstract: The effect of water deficit on stem reserve mobilization and sink activity in wheat (Triticum aestivum L.) cultivars, viz., C306 (drought tolerant) and PBW343 (drought sensitive) was studied. Drought was maintained in pot raised plants by withholding irrigation at 95 days after sowing (DAS), i.e. just five days before the initiation of anthesis. Drought induced a significant reduction in mean biomass of all the internodes of sensitive cultivar as compared to those of tolerant one. Mobilized dry matter and mobilization efficiency were observed to be higher in the internodes of tolerant cultivar, both under control and stress conditions, which resulted in enhanced translocation of stem reserves to the grains. Water soluble carbohydrates (WSC), which mainly occur as fructans, were observed to be higher in the internodes of tolerant cultivar than those of sensitive one. When drought was applied, fructans were mobilized more effectively from the internodes of tolerant cultivar. A significantly higher sucrose synthase activity in the grains of tolerant cultivar, under drought conditions, increased the sink strength by unloading the assimilates in the sink, thereby increasing further mobilization of assimilates to the grains. Grains of sensitive cultivar attained maturity much earlier as compared to the tolerant one, both under control and stress conditions. The longer duration of grain maturation in tolerant cultivar supported enhanced mobilization of stem reserves, thus restricting heavy decrease in grain yield, under stress conditions, as compared to the sensitive cultivar. It may, therefore, be concluded that certain characteristics viz., enhanced capability of fructan storage, higher mobilization efficiency, stronger sink activity and longer duration of grain maturation might help the drought tolerant cultivar in coping the stress conditions", "final_summary": "The mobilization of stem sugar reserves in wheat, particularly under stress conditions, is a critical factor in grain filling and overall yield. Ram (2018) found that stem reserves can contribute up to 70% of grain weight under stress conditions, such as drought and high temperatures. This is echoed by Blum (1997, 2004), who highlighted the importance of stem reserves for grain filling when photosynthesis is inhibited by stressors like drought, heat, or disease. \n\nDavidson (1992) further emphasized the role of stem reserves, noting that they serve as a significant temporary storage site for carbohydrates in both irrigated and non-irrigated plants. Blum (1994) suggested that the mobilization of stem reserves can support grain filling under heat stress, providing an alternative source of carbon when photosynthesis is impaired. \n\nSrivastava (2017) found a positive correlation between stem reserve mobilization and peduncle length under rainfed conditions, suggesting that genotypes with better stem reserve mobilization may exhibit relative tolerance to drought. Sharbatkhari (2016) found that increased fructan production and induced stem reserves remobilization under salinity can decrease dependency on current photosynthesis and protect wheat from severe yield loss under salt stress conditions. \n\nLastly, Gupta (2011) concluded that characteristics such as enhanced capability of fructan storage, higher mobilization efficiency, stronger sink activity, and longer duration of grain maturation might help drought-tolerant cultivars cope with stress conditions. \n\nIn conclusion, the mobilization of stem sugar reserves in wheat is a crucial factor in grain filling, particularly under stress conditions. This process can contribute significantly to grain weight and yield, providing an alternative source of carbon when photosynthesis is impaired. Further research is needed to fully understand the mechanisms and genetic factors influencing stem reserve mobilization and its impact on wheat yield under various stress conditions."}, {"query": "Health economic benefits of reducing NHS waiting times", "paper_list_string": "Cooper 2009:\n\nTitle: Equity, waiting times, and NHS reforms: retrospective study\n\nAbstract: Objective To determine whether observable changes in waiting times occurred for certain key elective procedures between 1997 and 2007 in the English National Health Service and to analyse the distribution of those changes between socioeconomic groups as an indicator of equity. Design Retrospective study of population-wide, patient level data using ordinary least squares regression to investigate the statistical relation between waiting times and patients\u2019 socioeconomic status. Setting English NHS from 1997 to 2007. Participants 427\u2009277 patients who had elective knee replacement, 406\u2009253 who had elective hip replacement, and 2\u2009568\u2009318 who had elective cataract repair. Main outcome measures Days waited from referral for surgery to surgery itself; socioeconomic status based on Carstairs index of deprivation. Results Mean and median waiting times rose initially and then fell steadily over time. By 2007 variation in waiting times across the population tended to be lower. In 1997 waiting times and deprivation tended to be positively related. By 2007 the relation between deprivation and waiting time was less pronounced, and, in some cases, patients from the most deprived fifth were waiting less time than patients from the most advantaged fifth. Conclusions Between 1997 and 2007 waiting times for patients having elective hip replacement, knee replacement, and cataract repair in England went down and the variation in waiting times for those procedures across socioeconomic groups was reduced. Many people feared that the government\u2019s NHS reforms would lead to inequity, but inequity with respect to waiting times did not increase; if anything, it decreased. Although proving that the later stages of those reforms, which included patient choice, provider competition, and expanded capacity, was a catalyst for improvements in equity is impossible, the data show that these reforms, at a minimum, did not harm equity.\n\n==\n\nGiuntella 2015:\n\nTitle: The Effects of Immigration on NHS Waiting Times\n\nAbstract: This paper analyzes the effects of immigration on waiting times for the National Health Service (NHS) in England. Linking administrative records from Hospital Episode Statistics (2003-2012) with immigration data drawn from the UK Labour Force Survey, we find that immigration reduced waiting times for outpatient referrals and did not have significant effects on waiting times in accident and emergency departments (A&E) and elective care. The reduction in outpatient waiting times can be explained by the fact that immigration increases natives' internal mobility and that immigrants tend to be healthier than natives who move to different areas. Finally, we find evidence that immigration increased waiting times for outpatient referrals in more deprived areas outside of London. The increase in average waiting times in more deprived areas is concentrated in the years immediately following the 2004 EU enlargement and disappears in the medium term (e.g., 3-4 years).\n\n==\n\nHarrison 2009:\n\nTitle: Reducing Waiting Times for Hospital Treatment: Lessons from the English NHS\n\nAbstract: In recent years, the English NHS has achieved substantial reductions in waiting times for hospital treatment. This paper considers first whether the data used by the Government provide an accurate description of changes in waiting times and identifies some of the limitations of the measures used. It then attempts to identify how reductions have been achieved. It argues that some features of central government policy have been important - such as the use of targets - others, such as the introduction of new private sector capacity have not. It also shows that changes at local level have been critical to achieving the recorded improvements, but the precise impact of these is hard to identify.\n\n==\n\nMarques 2014:\n\nTitle: Disclosing total waiting times for joint replacement: evidence from the English NHS using linked HES data.\n\nAbstract: For the last decade, stringent monitoring of waiting time performance targets provided English hospitals with incentives to reduce official waiting times for elective surgery. It is less clear whether the total amount of time patients waited in secondary care, from first referral to outpatient clinic until treatment, has also fallen. We used Hospital Episode Statistics inpatient data for patients undergoing total joint replacement during a period of active monitoring of targets (between 2006/7 and 2008/9) and linked it to outpatient data to reconstruct patients' pathway in the 3\u2009years before surgery and provide alternative measurements of waiting times. Our findings suggest that although official waiting times decreased drastically in our study period, total waiting time in secondary care has not declined. Patients with shorter official waits spent a longer time in a 'work-up' period prior to inclusion in the official waiting list, and socio-economic inequities persisted in waiting times for joint replacement. We found no evidence that target policies achieved efficiency gains during our study period.\n\n==\n\nHamilton 1999:\n\nTitle: The Impact of the NHS Reforms on Queues and Surgical Outcomes in England: Evidence From Hip Fracture Patients\n\nAbstract: National Health Service (NHS) reform introduced incentives for efficiency and cost effectiveness, yet little is known about their effectiveness in reducing waiting times for surgery or improving postsurgical outcomes. This paper finds that waiting times for hip fracture surgery declined after the NHS reforms and patients were more likely to be discharged to another provider. However, hospitals have not simply shifted the burden of care to other providers, since lengths of stay ending in a discharge to home fell. The effect of wait time on surgical outcomes is small in magnitude and cannot explain the postreform improvements in outcomes.\n\n==\n\nPropper 2002:\n\nTitle: Waiting times for hospital admissions: the impact of GP fundholding.\n\nAbstract: Waiting times for hospital care are a significant issue in the UK National Health Service (NHS). The reforms of the health service in 1990 gave a subset of family doctors (GP fundholders) both the ability to choose the hospital where their patients were treated and the means to pay for some services. One of the key factors influencing family doctors' choice of hospital was patient waiting time. However, without cash inducements, hospitals would get no direct reward from giving shorter waiting times to a subset of patients. Using a unique dataset, we investigate whether GP fundholders were able to secure shorter waiting times for their patients, whether they were able to do so in cases where they had no financial rewards to offer hospitals, and whether the impact of fundholding spilled over into shorter waiting times for all patients.\n\n==\n\nSiciliani 2005:\n\nTitle: Tackling excessive waiting times for elective surgery: a comparative analysis of policies in 12 OECD countries.\n\nAbstract: This paper compares policies to tackle excessive waiting times for elective surgery in 12 OECD countries. It is found that waiting times may be reduced by acting on the supply of or on the demand for surgery (or both). On the supply side, evidence suggests that both capacity and financial incentives towards productivity can play an important role. On the demand side, inducing a raising of clinical thresholds may reduce waiting times but may also provoke tension between clinicians and policy makers. Preliminary evidence also suggests that an increase in private health insurance coverage may reduce waiting times.\n\n==\n\nJones 2010:\n\nTitle: The four hour target to reduce emergency department \u2018waiting time\u2019: A systematic review of clinical outcomes\n\nAbstract: Governments in Australasia are introducing emergency department length of stay (EDLOS) time targets similar to the UK \u2018four hour rule\u2019. There is debate about whether this rule had beneficial effects on health\u2010care outcomes. We sought to determine what effects the four hour time target for EDLOS had on clinically relevant outcomes in the UK by conducting a systematic search for evidence. Articles were screened by both authors independently and assessed for quality using standard tools. Differences in outcomes measured and how they were measured precluded meta\u2010analysis. There were inconsistencies between target achievement reported by Trusts and that reported in the studies, and empirical evidence that the target might be unattainable. National Health Service spending on ED increased \u00a3820\u2003000\u2003000.00 (1998\u20132007) and emergency admissions rose overall by 35% (2002\u20132006), but not in all hospitals. Time to see a treating clinician and hospital mortality was unchanged. One hospital demonstrated a small reduction in return rate. The impact of the introduction of an ED time target and the associated massive financial investment has not resulted in a consistent improvement in care with markedly varying effects being reported between hospitals. Countries seeking to emulate the UK experience should proceed with caution.", "final_summary": "The economic benefits of reducing waiting times in the National Health Service (NHS) have been a topic of extensive research. The findings, however, are varied and complex.\n\nCooper (2009) found that between 1997 and 2007, waiting times for elective procedures in the NHS decreased, and the variation in waiting times across socioeconomic groups was reduced. This suggests that reducing waiting times can lead to more equitable healthcare access. Similarly, Harrison (2009) noted that the English NHS achieved substantial reductions in waiting times for hospital treatment, attributing this to certain government policies such as the use of targets.\n\nHowever, Marques (2014) found that while official waiting times decreased, the total waiting time in secondary care did not decline. This suggests that while reducing official waiting times may appear beneficial, it may not necessarily lead to overall efficiency gains in the healthcare system. \n\nGiuntella (2015) found that immigration reduced waiting times for outpatient referrals and did not significantly affect waiting times in accident and emergency departments. This suggests that factors outside of healthcare policy, such as population dynamics, can also influence waiting times.\n\nHamilton (1999) found that waiting times for hip fracture surgery declined after the NHS reforms and patients were more likely to be discharged to another provider. However, the effect of wait time on surgical outcomes was small and could not explain the post-reform improvements in outcomes.\n\nPropper (2002) investigated whether GP fundholders were able to secure shorter waiting times for their patients, and found that one of the key factors influencing family doctors' choice of hospital was patient waiting time. \n\nSiciliani (2005) compared policies to tackle excessive waiting times for elective surgery in 12 OECD countries and found that both capacity and financial incentives towards productivity can play an important role in reducing waiting times.\n\nJones (2010), however, cautioned that the introduction of an emergency department time target and the associated massive financial investment has not resulted in a consistent improvement in care with markedly varying effects being reported between hospitals.\n\nIn conclusion, while reducing NHS waiting times can lead to more equitable healthcare access and potential efficiency gains, the overall impact on healthcare outcomes and costs is complex and influenced by a variety of factors. Further research is needed to fully understand the economic benefits of reducing NHS waiting times."}, {"query": "Online university versus on campus and academic performance differences", "paper_list_string": "Xu 2014:\n\nTitle: Performance Gaps Between Online and Face-to-Face Courses: Differences Across Types of Students and Academic Subject Areas\n\nAbstract: Using a dataset containing nearly 500,000 courses taken by over 40,000 community and technical college students in Washington State, this study examines the performance gap between online and face-to-face courses and how the size of that gap differs across student subgroups and academic subject areas. While all types of students in the study suffered decrements in performance in online courses, those with the strongest declines were males, younger students, Black students, and students with lower grade point averages. Online performance gaps were also wider in some academic subject areas than others. After controlling for individual and peer effects, the social sciences and the applied professions (e.g., business, law, and nursing) showed the strongest online performance gaps.\n\n==\n\nXu 2014:\n\nTitle: Performance Gaps between Online and Face-to-Face Courses: Differences across Types of Students and Academic Subject Areas\n\nAbstract: Using a dataset containing nearly 500,000 courses taken by over 40,000 community and technical college students in Washington State, this study examines the performance gap between online and face-to-face courses and how the size of that gap differs across student subgroups and academic subject areas. While all types of students in the study suffered decrements in performance in online courses, those with the strongest declines were males, younger students, Black students, and students with lower grade point averages. Online performance gaps were also wider in some academic subject areas than others. After controlling for individual and peer effects, the social sciences and the applied professions (e.g., business, law, and nursing) showed the strongest online performance gaps.\n\n==\n\nMcPhee 2010:\n\nTitle: Comparison of equated learning for online and on campus postgraduate students on academic achievement\n\nAbstract: This study assesses the effects of study mode on student achievement in two modes of study: on-campus learning and online learning. The University of the West of Scotland has been offering flexible postgraduate programmes in Alcohol and Drugs Studies online since 1999 and uses Blackboard, the Virtual Learning Environment (VLE), to support equated learning. The explicit focus of this continuing longitudinal study (dating originally from 2002) is on student achievement. In this continuing evaluation comparing on-campus and online student grade performance, online study groups have exactly the same module syllabus as their on-campus counterparts. There is equivalence of support in that students on both modes of study are taught on the same traditional 15-week trimesters as students on the on-campus version, have the same learning materials, live interactive lectures using the VLE as a central hub, and the same assessment methods including assignments, projects, and class tests. Most importantly, the online and oncampus modes of study had the same learning outcomes, the same academic module moderator and also the same external examiner to ensure that assessed work by students on each mode of study was marked to the same standard. Statistical analysis of academic outcomes revealed no significant differences in grades (summative marks) between online and on-campus groups. This finding indicates that students are not disadvantaged by selecting to study via online learning and that equated learning is indeed occurring in practice.\n\n==\n\nCampbell 2011:\n\nTitle: Assessment Of Student Performance And Attitudes For Courses Taught Online Versus Onsite\n\nAbstract: This paper assesses the differences in performance and attitudes of students taught online versus onsite. Students completed a course evaluation designed to determine student satisfaction in specific areas. Student performance was measured by means of a comprehensive exam that tested all material covered in the course. Results support the contention that students in online courses learn as much or more than students in traditional onsite courses and are as satisfied with the course and the instruction as their onsite counterparts.\n\n==\n\nChung 2022:\n\nTitle: Correlates of Academic Performance in Online Higher Education: A Systematic Review\n\nAbstract: The existing steady and continual rise of online learning in higher education has been accelerated by COVID-19 and resulted in a move away from solely on-campus teaching. Prior to the pandemic, online education was providing higher education to students who were returning to study to up-skill, are employed full-time, caring for family members, living rurally or remotely and/or for whom otherwise face-to-face campus learning was not a preference or option. To understand how we can better support online students in their unique circumstances and create an optimal learning environment, we must understand the factors associated with academic achievement within an online setting. This systematic review involved a search of relevant databases published between January 2009 and May 2021 examining factors and constructs related to academic performance in online higher education settings. Across 34 papers, 23 (67.6%) explored factors and constructs related to student characteristics including cognitive and psychological, demographic, university enrolment, and prior academic performance. Twenty-one (61.8%) papers explored learning environment factors including engagement, student experience, course design, and instructor. Our overall synthesis of findings indicates that academic performance in online learning is most strongly associated with motivation (including self-efficacy), and self-regulation. We propose three main implications of our review for online learning stakeholders such as educators and designers. Firstly, we argue that the wellbeing of online learners is important to understand, and future research should explore its impact on students\u2019 experience and success in online higher education. Secondly, we emphasise the importance of developing and designing online courses utilising relevant frameworks and evidence-based principles. Finally, we propose an approach to promoting improved student cognitive and psychosocial experiences (such as self-efficacy, self-regulation, and perceived support) could be achieved by creating and incorporating an online learning orientation module at the commencement of enrolment. Systematic Review Registration: (website), identifier (registration number).\n\n==\n\nDendir 2018:\n\nTitle: Performance differences between face-to-face and online students in economics\n\nAbstract: Abstract The author analyzes differences in student performance in an economics course offered face to face (F2F) and online over a period of four semesters at a comprehensive public university. Apart from mode of delivery, the characteristics of the course stayed nearly identical throughout. Exam and homework assignment scores are used as measures of performance. Although mean comparisons and basic regressions indicate that online students outperform F2F students on exams, once self-selection is addressed, online students no more outperform F2F students. Furthermore, according to assessments that gauge higher levels of learning, there is evidence that online students underperform compared with F2F students.\n\n==\n\nXu 2013:\n\nTitle: The impact of online learning on students\u2019 course outcomes: Evidence from a large community and technical college system\n\nAbstract: Using a large administrative dataset from a statewide system including 34 community and technical colleges, the authors employed an instrumental variable technique to estimate the impact of online versus face-to-face course delivery on student course performance. The travel distance between each student's home and college campus served as an instrument for the likelihood of enrolling in an online section of a given course. In addition, college-by-course fixed effects controlled for within- and between-course selection bias. Analyses yield robust negative estimates for online learning in terms of both course persistence and course grade, contradicting the notion that there is no significant difference between online and face-to-face student outcomes\u2014at least within the community college setting. Accordingly, both two-year and four-year colleges may wish to focus on evaluating and improving the quality of online coursework before engaging in further expansions of online learning.\n\n==\n\nUry 2004:\n\nTitle: A comparison of undergraduate student performance in online and traditional courses\n\nAbstract: As online instruction becomes more prevalent at the college and university level, researchers are attempting to measure the success of these programs using a variety of methodologies, instruments, and sample sizes. There is a need for continued exploration and study to assure quality instruction. The purpose of this study was to compare course performance over time between online and traditional classroom students enrolled in a required management information systems course included in the business school's common professional component. The online delivery method was found to be effective, but performance, as measured by final course grades, showed a significantly lower mean score than students enrolled in traditional sections of the course. Other findings indicated that traditional course designs can be adequately adapted to the online model and that collaborative course development can lead to a level of consistency in student performance within high demand courses.", "final_summary": "The research question at hand explores the academic performance differences between online and on-campus university education. The findings from the collected papers present a complex and nuanced picture.\n\nXu (2014) found that all types of students experienced a decrease in performance in online courses, with the most significant declines observed among males, younger students, Black students, and students with lower grade point averages. Similarly, Xu (2013) found that online learning had a negative impact on both course persistence and course grade. Dendir (2018) also found that once self-selection was addressed, online students did not outperform face-to-face students, and there was evidence that online students underperformed in assessments that gauge higher levels of learning.\n\nHowever, other studies found no significant difference in performance between online and on-campus students. McPhee (2010) found no significant differences in grades between online and on-campus groups, indicating that students are not disadvantaged by selecting to study via online learning. Similarly, Campbell (2011) found that students in online courses learn as much or more than students in traditional onsite courses and are as satisfied with the course and the instruction as their onsite counterparts.\n\nChung (2022) conducted a systematic review and found that academic performance in online learning is most strongly associated with motivation (including self-efficacy), and self-regulation. The study also emphasized the importance of developing and designing online courses utilizing relevant frameworks and evidence-based principles.\n\nUry (2004) found that while the online delivery method was effective, performance, as measured by final course grades, showed a significantly lower mean score than students enrolled in traditional sections of the course.\n\nIn conclusion, the findings suggest that while online education can be as effective as on-campus education in some contexts, certain student groups may experience decreased performance in online courses. Factors such as motivation, self-efficacy, and self-regulation play a significant role in academic performance in online learning. Therefore, it is crucial to consider these factors when designing and implementing online courses."}, {"query": "ClO/ClO2 data in water analysis", "paper_list_string": "Zhang 2021:\n\nTitle: Rapid in situ determination of ClO2 in drinking water by improved solid DPD spectrophotometry\n\nAbstract: This research aims to realize the rapid detection of ClO2 content in drinking water by adopting improved solid DPD. This method is fast and convenient with low cost and less waste liquid. The results show that this method has good precision and sensitivity. The linear correlation coefficients of the cubic regression equation were all greater than 0.999. The detection limit of the method was 0.002mg/L ClO2. The relative standard deviations (RSD) of seven parallel tests were between 1.37% and 8.87%, and the relative errors were small. The recovery rate was 96.67~110%. The method could be used for the direct determination of water samples with a mass concentration of 0.02mg/L~2.00mg/L in drinking water after ClO2 disinfection.\n\n==\n\nJiang 2006:\n\nTitle: Resonance scattering effect of rhodamine dye association nanoparticles and its application to respective determination of trace ClO2 and Cl2.\n\nAbstract: A new resonance scattering method, based on resonance scattering (RS) effect, for the respective determination of ClO2 and Cl2 in water samples was developed. In HCl-NaAc buffer solutions with the pH value of 1.42, chlorine dioxide, or chlorine, oxidizes I- to form 12, which then reacts with the excess I- to form I3-. The resulting 13- would combine, respectively, with four rhodamine(Rh) dyes, including rhodamine B (RhB), butyl rhodamine B (b-RhB), rhodamine 6G (RhG), and rhodamine S (RhS), to form association particles which exhibit a stronger resonance scattering (RS) effect at 420 nm. For four systems of RhB, bRhB, RhG, and RhS, chlorine dioxide was, respectively, determined in the concentration range of 0.0056 to approximately 0.787 mg/L, 0.0034 to approximately 0.396 mg/L, 0.0057 to approximately 0.795 mg/L, and 0.0052 to approximately 0.313 mg/L, with the detection limits of 0.0011 mg/L, 0.006 mg/L, 0.0054 mg/ L, and 0.0023 mg/L ClO2, respectively. At the same experimental conditions as those for the determination of ClO2, chlorine was, respectively, determined in the concentration range of 0.013 to approximately 0.784 mg/L, 0.0136 to approximately 0.522 mg/ L, 0.014 to approximately 0.81 mg/L, and 0.014 to approximately 0.42 mg/L, with the detection limits of 0.0016 mg/L, 0.0104 mg/L, 0.0079 mg/L, and 0.0037 mg/L Cl2, respectively. The total RS value originally from ClO2 and Cl2 was recorded in the buffer solution, while the RS value from ClO2 was obtained by using dimethyl sulfoxide to mask chlorine. Thus the RS value of chlorine was calculated by deducting the RS value of chlorine dioxide from the total RS value. The RhB RS method was chosen for the determination of ClO2 and Cl2 in drinking water, with advantages of high sensitivity, good selectivity, simplicity, rapidity, and convenience.\n\n==\n\nWheeler 1978:\n\nTitle: A rapid microdetermination of chlorine dioxide in the presence of active chlorine compounds\n\nAbstract: Abstract A titrimetric and spectrophotometric procedure has been developed for the determination of ClO2 in water samples. The procedure is rapid, accurate, and free of normal interferences present in water. It is based upon the reaction of ClO2 with substituted halophenol indicators.\n\n==\n\nGan 2020:\n\nTitle: The reactions of chlorine dioxide with inorganic and organic compounds in water treatment: kinetics and mechanisms\n\nAbstract: Chlorine dioxide (ClO2), as an alternative to chlorine, has been widely applied in water treatment. In order to better understand the performance of ClO2 in water treatment, the kinetics and mechanisms of ClO2 reactions with inorganic and organic compounds found in waters are critically reviewed. In the case of inorganic compounds, ClO2 reacts with I\u2212, CN\u2212, NO2\u2212, SO32\u2212, Fe(II) and Mn(II) rapidly at apparent second-order reaction rate constants (kapp) of 102\u2013106 M\u22121 s\u22121 at pH 7.0 and barely reacts with NH4+ and Br\u2212. In the case of organic compounds, ClO2 selectively reacts with compounds with electron-rich moieties, such as phenols (kapp = 103\u2013109 M\u22121 s\u22121), anilines (kapp = 105\u2013108 M\u22121 s\u22121), and thiols (kapp > 108 M\u22121 s\u22121). ClO2 also shows high reactivity towards aliphatic tertiary amines and heterocyclic nitrogenous compounds (i.e., indoles and piperidines) with kapp of 101\u2013106 M\u22121 s\u22121 at pH 7.0, but low reactivity with unsaturated structures (i.e., olefins and aldehydes). The kapp values at pH 7.0 in ClO2 oxidation vary over 14 orders of magnitude. Electron transfer is the dominant pathway for ClO2 reactions. Quantitative structure\u2013activity relationships (QSARs) can be used to predict the species-specific secondary reaction rate constants for ClO2 oxidation of compounds containing phenolic and amine structures. Little modifications are expected on the structure of the parent compounds upon the primary attack of ClO2, but further oxidation generally leads to the formation of quinones, aldehydes and carboxylic acids. Furthermore, the transformation kinetics of inorganic compounds, typical organic compounds and emerging micropollutants are compared and their half-life times under typical water treatment conditions during ClO2 oxidation are calculated.\n\n==\n\nHuber 2005:\n\nTitle: Oxidation of pharmaceuticals during water treatment with chlorine dioxide.\n\nAbstract: The potential of chlorine dioxide (ClO2) for the oxidation of pharmaceuticals during water treatment was assessed by determining second-order rate constants for the reaction with selected environmentally relevant pharmaceuticals. Out of 9 pharmaceuticals only the 4 following compounds showed an appreciable reactivity with ClO2 (in brackets apparent second-order rate constants at pH 7 and T = 20 degrees C): the sulfonamide antibiotic sulfamethoxazole (6.7 x 10(3) M(-1) s(-1)), the macrolide antibiotic roxithromycin (2.2 x 10(2) M(-1) s(-1)), the estrogen 17alpha-ethinylestradiol (approximately 2 x 10(5) M(-1) s(-1)), and the antiphlogistic diclofenac (1.05 x 10(4) M(-1) s(-1)). Experiments performed using natural water showed that ClO2 also reacted fast with other sulfonamides and macrolides, the natural hormones estrone and 17beta-estradiol as well as 3 pyrazolone derivatives (phenazone, propylphenazone, and dimethylaminophenazone). However, many compounds in the study were ClO2 refractive. Experiments with lake water and groundwater that were partly performed at microgram/L to nanogram/L levels proved that the rate constants determined in pure water could be applied to predict the oxidation of pharmaceuticals in natural waters. Compared to ozone, ClO2 reacted more slowly and with fewer compounds. However, it reacted faster with the investigated compounds than chlorine. Overall, the results indicate that ClO2 will only be effective to oxidize certain compound classes such as the investigated classes of sulfonamide and macrolide antibiotics, and estrogens.\n\n==\n\nQuentel 1994:\n\nTitle: Electrochemical determination of low levels of residual chlorine dioxide in tap water\n\nAbstract: The reaction between 1,2-dihydroxyanthraquinone-3-sulphonic acid and chlorine dioxide, in phosphate buffer medium, was studied spectrophotometrically and electrochemically. An electroanalytical method is proposed for the determination of traces of ClO2 with a detection limit of 2 \u03bcg l\u22121. The stoichiometry of the reaction, the effects of various operational parameters and the influence of foreign ions are reported.\n\n==\n\nHoehn 2003:\n\nTitle: ClO2 and by\u2010product persistence in a Drinking Water System\n\nAbstract: This research evaluated a chlorine gas\u2013solid sodium chlorite chlorine dioxide (ClO2) generation system at the Roanoke County (Va.) Spring Hollow Water Treatment Facility and monitored concentrations of the oxidant and its by\u2010products within the facility and distribution system. Project objectives were to (1) document changes in ClO2, chlorite ion (ClO2\u2013), and chlorate ion (ClO3\u2013) concentrations throughout the facility and distribution system following raw water ClO2 pretreatment; (2) evaluate ClO2\u2013 removal by the postfilter granular activated carbon (GAC) contactors; and (3) determine distribution system ClO2 concentrations associated with odor complaints. The average raw water ClO2 dose was 0.4 mg/L, and overall ClO2\u2013 removals by GAC averaged 63% but declined markedly with increasing throughput volume. Distribution system ClO2\u2013 concentrations were generally <0.2 mg/L and decreased with increasing distance from the treatment facility; ClO3\u2013 never exceeded 0.1 mg/L. ClO2 was measured at low concentrations (0.017\u20130.17 mg/L) in the distribution system, and a measured concentration of 0.03 mg/L was found at the home of one customer who complained of odors.\n\n==\n\nKang 2006:\n\nTitle: [A new fluorescence quenching method for the determination of trace ClO2 in water using silver nanoparticles].\n\nAbstract: In pH 9.1 NH4Cl-NH3 x H2O buffer solution, there is a fluorescence peak at 470 nm for silver nanoparticles. A fluorescence quenching takes place when it was oxidized by ClO2. The intensity of fluorescence quenching is linear with the concentration of ClO2 in the range of 0.0011-0.185 microg x mL(-1). The detection limit is 0.004 7 microg x mL(-1). A new fluorescence method has been proposed for the determination of ClO2 in water samples with satisfactory results.", "final_summary": "The collective body of research provides a comprehensive understanding of ClO/ClO2 data in water analysis. \n\nZhang (2021) developed a rapid detection method for ClO2 content in drinking water using improved solid DPD spectrophotometry. This method was found to be fast, convenient, and cost-effective, with a detection limit of 0.002mg/L ClO2. \n\nJiang (2006) introduced a new resonance scattering method for the determination of ClO2 in water samples. This method was found to be highly sensitive, selective, and convenient. \n\nWheeler (1978) developed a titrimetric and spectrophotometric procedure for the determination of ClO2 in water samples, which was found to be rapid, accurate, and free of normal interferences present in water. \n\nGan (2020) provided a critical review of the kinetics and mechanisms of ClO2 reactions with inorganic and organic compounds found in waters. The study found that ClO2 reacts rapidly with certain compounds and barely with others. \n\nHuber (2005) assessed the potential of ClO2 for the oxidation of pharmaceuticals during water treatment. The study found that ClO2 reacted fast with certain compounds but was refractive to many others. \n\nQuentel (1994) proposed an electroanalytical method for the determination of traces of ClO2 with a detection limit of 2 \u03bcg l\u22121. \n\nHoehn (2003) evaluated a chlorine gas\u2013solid sodium chlorite chlorine dioxide (ClO2) generation system and monitored concentrations of the oxidant and its by-products within the facility and distribution system. \n\nFinally, Kang (2006) proposed a new fluorescence method for the determination of ClO2 in water samples with satisfactory results. \n\nIn conclusion, the research collectively suggests that various methods can be used for the determination of ClO2 in water, each with its own advantages and limitations. Further research is needed to optimize these methods and explore new ones for more accurate and efficient water analysis."}, {"query": "polar vortex stretching stratosphere", "paper_list_string": "Charlton 2005:\n\nTitle: The Splitting of the Stratospheric Polar Vortex in the Southern Hemisphere, September 2002: Dynamical Evolution\n\nAbstract: Abstract The polar vortex of the Southern Hemisphere (SH) split dramatically during September 2002. The large-scale dynamical effects were manifest throughout the stratosphere and upper troposphere, corresponding to two distinct cyclonic centers in the upper troposphere\u2013stratosphere system. High-resolution (T511) ECMWF analyses, supplemented by analyses from the Met Office, are used to present a detailed dynamical analysis of the event. First, the anomalous evolution of the SH polar vortex is placed in the context of the evolution that is usually witnessed during spring. Then high-resolution fields of potential vorticity (PV) from ECMWF are used to reveal several dynamical features of the split. Vortex fragments are rapidly sheared out into sheets of high (modulus) PV, which subsequently roll up into distinct synoptic-scale vortices. It is proposed that the stratospheric circulation becomes hydrodynamically unstable through a significant depth of the troposphere\u2013stratosphere system as the polar vortex elo...\n\n==\n\nVargin 2015:\n\nTitle: Stratospheric Polar Vortex Splitting in December 2009\n\nAbstract: Abstract The 2009\u201310 Arctic stratospheric winter, in comparison with other recent winters, is mainly characterized by a major Sudden Stratospheric Warming (SSW) in late January associated with planetary wavenumber 1. This event led to a large increase in the temperature of the polar stratosphere and to the reversal of the zonal wind. Unlike other major SSW events in recent winters, after the major SSW in January 2010 the westerlies and polar vortex did not recover to their pre-SSW strength until the springtime transition. As a result, the depletion of the ozone layer inside the polar vortex over the entire winter was relatively small over the past 20 years. The other distinguishing feature of the 2010 winter was the splitting of the stratospheric polar vortex into two lobes in December. The vortex splitting was accompanied by an increase in the temperature of the polar stratosphere and a weakening of the westerlies but with no reversal. The splitting occurred when, in addition to the high-pressure system over northeastern Eurasia and the northern Pacific Ocean, the tropospheric anticyclone over Europe amplified and extended to the lower stratosphere. Analysis of wave activity in the extratropical troposphere revealed that two Rossby wave trains propagated eastward to the North Atlantic several days prior to the vortex splitting. The first wave train propagated from the subtropics and mid-latitudes of the eastern Pacific Ocean over North America and the second one propagated from the northern Pacific Ocean. These wave trains contributed to an intensification of the tropospheric anticyclone over Europe and to the splitting of the stratospheric polar vortex.\n\n==\n\nPalmer 1959:\n\nTitle: The stratospheric polar vortex in winter\n\nAbstract: In winter the polar stratospheric air within the earth's shadow forms the core of an intense \u2018cold low\u2019 which extends from about 10 km to at least 50 km and possibly to the base of the ionosphere. Compared with the tropospheric general circulation, this vortex seems to be remarkably stable, particularly in the Southern Hemisphere. \n \nRecent research work in Canada and the United States on the characteristics of the vortex is reviewed. The chief conclusions are that the vortex is more stable in the Southern than in the Northern Hemisphere, that \u2018explosive warmings\u2019 in the lower stratosphere of the Northern Hemisphere follow the breakdown of the vortex at high levels close to the pole, and that the breakdown extends from above downward over a period of several days. It is suggested that the high-level breakdown is correlated with solar activity.\n\n==\n\nWaugh 1994:\n\nTitle: Transport out of the lower stratospheric Arctic vortex by Rossby wave breaking\n\nAbstract: The fine-scale structure in lower stratospheric tracer transport during the period of the two Arctic Airborne Stratospheric Expeditions (January and February 1989; December 1991 to March 1992) is investigated using contour advection with surgery calculations. These calculations show that Rossby wave breaking is an ongoing occurrence during these periods and that air is ejected from the polar vortex in the form of long filamentary structures. There is good qualitative agreement between these filaments and measurements of chemical tracers taken aboard the NASA ER-2 aircraft. The ejected air generally remains filamentary and is stretched and mixed with midlatitude air as it is wrapped around the vortex. This process transfers vortex air into midlatitudes and also produces a narrow region of fine-scale filaments surrounding the polar vortex. Among other things, this makes it difficult to define a vortex edge. The calculations also show that strong stirring can occur inside as well as outside the vortex.\n\n==\n\nManney 2000:\n\nTitle: Development of the polar vortex in the 1999\u20132000 Arctic winter stratosphere\n\nAbstract: The 1999\u20132000 Arctic stratospheric vortex was unusually cold, especially in the early winter lower stratosphere, with a larger area near polar stratospheric cloud formation temperatures in Dec and Jan, and much lower temperatures averaged over Nov\u2013Jan, than any previously observed Arctic winter. In Nov and early Dec, there was a double jet in the upper stratosphere, with the anticyclone cutoff in a region of cyclonic material. By late Dec, there was a discontinuous vortex, large in the upper stratosphere, small in the lower stratosphere; evolving to a strong, continuous, relatively upright vortex by mid\u2010Jan. This vortex evolution in 1999\u20132000 is typical of that in other cold early winters. Despite unusually low temperatures, the lower stratospheric vortex developed more slowly than in previous unusually cold early winters, and was weaker than average until late Dec.\n\n==\n\nSeviour 2016:\n\nTitle: Stratospheric polar vortex splits and displacements in the high\u2010top CMIP5 climate models\n\nAbstract: Sudden stratospheric warming (SSW) events can occur as either a split or a displacement of the stratospheric polar vortex. Recent observational studies have come to different conclusions about the relative impacts of these two types of SSW upon surface climate. A clearer understanding of their tropospheric impact would be beneficial for medium\u2010range weather forecasts and could improve understanding of the physical mechanism for stratosphere\u2010troposphere coupling. Here we perform the first multimodel comparison of stratospheric polar vortex splits and displacements, analyzing 13 stratosphere\u2010resolving models from the fifth Coupled Model Intercomparison Project (CMIP5) ensemble. We find a wide range of biases among models in both the mean state of the vortex and the frequency of vortex splits and displacements, although these biases are closely related. Consistent with observational results, almost all models show vortex splits to occur barotropically throughout the depth of the stratosphere, while vortex displacements are more baroclinic. Vortex splits show a slightly stronger North Atlantic surface signal in the month following onset. However, the most significant difference in the surface response is that vortex displacements show stronger negative pressure anomalies over Siberia. This region is shown to be colocated with differences in tropopause height, suggestive of a localized response to lower stratospheric potential vorticity anomalies.\n\n==\n\nWaugh 2013:\n\nTitle: Stratospheric Polar Vortices\n\nAbstract: The Stratosphere: Geophysical Mon Copyright 2010 b 10.1029/2009GM The intense cyclonic vortices that form over the winter pole are one of the most prominent features of the stratospheric circulation. The structure and dynamics of these \u201cpolar vortices\u201d play a dominant role in the winter and spring stratospheric circulation and are key to determining distribution of trace gases, in particular ozone, and the couplings between the stratosphere and troposphere. In this chapter, we review the observed structure, dynamical theories, and modeling of these polar vortices. We consider both the zonal mean and three-dimensional potential vorticity perspective and examine the occurrence of extreme events and long-term trends.\n\n==\n\nMitchell 2011:\n\nTitle: Characterizing the Variability and Extremes of the Stratospheric Polar Vortices Using 2D Moment Analysis\n\nAbstract: AbstractThe mean state, variability, and extreme variability of the stratospheric polar vortices, with an emphasis on the Northern Hemisphere (NH) vortex, are examined using two-dimensional moment analysis and extreme value theory (EVT). The use of moments as an analysis tool gives rise to information about the vortex area, centroid latitude, aspect ratio, and kurtosis. The application of EVT to these moment-derived quantities allows the extreme variability of the vortex to be assessed. The data used for this study are 40-yr ECMWF Re-Analysis (ERA-40) potential vorticity fields on interpolated isentropic surfaces that range from 450 to 1450 K.Analyses show that the most extreme vortex variability occurs most commonly in late January and early February, consistent with when most planetary wave driving from the troposphere is observed. Composites around sudden stratospheric warming (SSW) events reveal that the moment diagnostics evolve in statistically different ways between vortex splitting events and vort...", "final_summary": "The polar vortex, a cyclonic circulation in the stratosphere over the winter poles, undergoes a phenomenon known as \"stretching\" or \"splitting\" under certain conditions. This process has been extensively studied in the context of its impact on stratospheric dynamics and weather patterns.\n\nCharlton (2005) and Vargin (2015) both provide detailed accounts of specific instances of polar vortex splitting. Charlton (2005) describes the dramatic splitting of the Southern Hemisphere's polar vortex in September 2002, resulting in two distinct cyclonic centers in the upper troposphere-stratosphere system. Similarly, Vargin (2015) discusses the splitting of the Arctic stratospheric polar vortex into two lobes in December 2009, which was accompanied by an increase in the temperature of the polar stratosphere and a weakening of the westerlies.\n\nPalmer (1959) provides a historical perspective, noting the stability of the polar vortex, particularly in the Southern Hemisphere, and suggesting a correlation between high-level breakdowns of the vortex and solar activity. Waugh (1994) further explores the dynamics of the vortex, highlighting the role of Rossby wave breaking in the ejection of air from the polar vortex in the form of long filamentary structures.\n\nManney (2000) and Seviour (2016) delve into the development and variability of the polar vortex. Manney (2000) describes the unusually cold 1999-2000 Arctic stratospheric vortex, noting its slow development despite low temperatures. Seviour (2016) compares stratospheric polar vortex splits and displacements across multiple climate models, finding a wide range of biases and highlighting the different surface responses associated with each event.\n\nWaugh (2013) provides a comprehensive review of the structure and dynamics of the polar vortices, emphasizing their role in the winter and spring stratospheric circulation and their impact on the distribution of trace gases. Mitchell (2011) uses two-dimensional moment analysis and extreme value theory to characterize the variability and extremes of the stratospheric polar vortices.\n\nIn conclusion, the stretching or splitting of the polar vortex is a complex and dynamic process with significant implications for stratospheric circulation and surface weather patterns. Further research is needed to fully understand the mechanisms driving these events and their potential impacts on climate variability and change."}, {"query": "I need all research papers which used Shotgun sequencing to study gut microbiome of IBS patients in India", "paper_list_string": "Dhakan 2019:\n\nTitle: The unique composition of Indian gut microbiome, gene catalogue, and associated fecal metabolome deciphered using multi-omics approaches\n\nAbstract: Abstract Background Metagenomic studies carried out in the past decade have led to an enhanced understanding of the gut microbiome in human health; however, the Indian gut microbiome has not been well explored. We analyzed the gut microbiome of 110 healthy individuals from two distinct locations (North-Central and Southern) in India using multi-omics approaches, including 16S rRNA gene amplicon sequencing, whole-genome shotgun metagenomic sequencing, and metabolomic profiling of fecal and serum samples. Results The gene catalogue established in this study emphasizes the uniqueness of the Indian gut microbiome in comparison to other populations. The gut microbiome of the cohort from North-Central India, which was primarily consuming a plant-based diet, was found to be associated with Prevotella and also showed an enrichment of branched chain amino acid (BCAA) and lipopolysaccharide biosynthesis pathways. In contrast, the gut microbiome of the cohort from Southern India, which was consuming an omnivorous diet, showed associations with Bacteroides, Ruminococcus, and Faecalibacterium and had an enrichment of short chain fatty acid biosynthesis pathway and BCAA transporters. This corroborated well with the metabolomics results, which showed higher concentration of BCAAs in the serum metabolome of the North-Central cohort and an association with Prevotella. In contrast, the concentration of BCAAs was found to be higher in the fecal metabolome of the Southern-India cohort and showed a positive correlation with the higher abundance of BCAA transporters. Conclusions The study reveals the unique composition of the Indian gut microbiome, establishes the Indian gut microbial gene catalogue, and compares it with the gut microbiome of other populations. The functional associations revealed using metagenomic and metabolomic approaches provide novel insights on the gut-microbe-metabolic axis, which will be useful for future epidemiological and translational researches.\n\n==\n\nMitra 2013:\n\nTitle: Analysis of the intestinal microbiota using SOLiD 16S rRNA gene sequencing and SOLiD shotgun sequencing\n\nAbstract: BackgroundMetagenomics seeks to understand microbial communities and assemblages by DNA sequencing. Technological advances in next generation sequencing technologies are fuelling a rapid growth in the number and scope of projects aiming to analyze complex microbial environments such as marine, soil or the gut. Recent improvements in longer read lengths and paired-sequencing allow better resolution in profiling microbial communities. While both 454 sequencing and Illumina sequencing have been used in numerous metagenomic studies, SOLiD sequencing is not commonly used in this area, as it is believed to be more suitable in the context of reference-guided projects.ResultsTo investigate the performance of SOLiD sequencing in a metagenomic context, we compared taxonomic profiles of SOLiD mate-pair sequencing reads with Sanger paired reads and 454 single reads. All sequences were obtained from the bacterial 16S rRNA gene, which was amplified from microbial DNA extracted from a human fecal sample. Additionally, from the same fecal sample, complete genomic microbial DNA was extracted and shotgun sequenced using SOLiD sequencing to study the composition of the intestinal microbiota and the existing microbial metabolism. We found that the microbiota composition of 16S rRNA gene sequences obtained using Sanger, 454 and SOLiD sequencing provide results comparable to the result based on shotgun sequencing. Moreover, with SOLiD sequences we obtained more resolution down to the species level. In addition, the shotgun data allowed us to determine a functional profile using the databases SEED and KEGG.ConclusionsThis study shows that SOLiD mate-pair sequencing is a viable and cost-efficient option for analyzing a complex microbiome. To the best of our knowledge, this is the first time that SOLiD sequencing has been used in a human sample.\n\n==\n\nQiu 2017:\n\nTitle: Targeted Metagenome Based Analyses Show Gut Microbial Diversity of Inflammatory Bowel Disease patients\n\nAbstract: Inflammatory bowel disease (IBD) is a multifactorial disease including both genetic and environmental factors. We compared the diversity of intestinal microbesamong a cohort of IBD patients to study the microbial ecological effects on IBD. Fecal samples from patients were sequenced with next generation sequence technology at 16S rDNA region. With statistical tools, microbial community was investigated at different level. The gut microbial diversity of Crohn\u2019s disease (CD) patients and colonic polyp (CP) patients significantly different from each other. However, the character of ulcerative colitis (UC) patients has of both CD and CP features. The microbial community from IBD patients can be very different (CD patient) or somewhat similar (UC patients) to non-IBD patients. Microbial diversity can be an important etiological factor for IBD clinical phenotype.\n\n==\n\nMalinen 2005:\n\nTitle: Analysis of the Fecal Microbiota of Irritable Bowel Syndrome Patients and Healthy Controls with Real-Time PCR\n\nAbstract: OBJECTIVE:The gut microbiota may contribute to the onset and maintenance of irritable bowel syndrome (IBS). In this study, the microbiotas of patients suffering from IBS were compared with a control group devoid of gastrointestinal (GI) symptoms.METHODS:Fecal microbiota of patients (n = 27) fulfilling the Rome II criteria for IBS was compared with age- and gender-matched control subjects (n = 22). Fecal samples were obtained at 3 months intervals. Total bacterial DNA was analyzed by 20 quantitative real-time PCR assays covering approximately 300 bacterial species.RESULTS:Extensive individual variation was observed in the GI microbiota among both the IBS- and control groups. Sorting of the IBS patients according to the symptom subtypes (diarrhea, constipation, and alternating predominant type) revealed that lower amounts of Lactobacillus spp. were present in the samples of diarrhea predominant IBS patients wheras constipation predominant IBS patients carried increased amounts of Veillonella spp. Average results from three fecal samples suggested differences in the Clostridium coccoides subgroup and Bifidobacterium catenulatum group between IBS patients (n = 21) and controls (n = 15). Of the intestinal pathogens earlier associated with IBS, no indications of Helicobacter spp. or Clostridium difficile were found whereas one case of Campylobacter jejuni was identified by sequencing.CONCLUSIONS:With these real-time PCR assays, quantitative alterations in the GI microbiota of IBS patients were found. Increasing microbial DNA sequence information will further allow designing of new real-time PCR assays for a more extensive analysis of intestinal microbes in IBS.\n\n==\n\nAttri 2018:\n\nTitle: High throughput sequence profiling of gut microbiome in Northern Indian infants during the first four months and its global comparison\n\nAbstract: Abstract The present study characterized the colonization and development of gut microbial communities in healthy Indian infants from North-Western Himalayan region in the province Himachal Pradesh. The diversity and transitions of core genera was assessed targeting the 16S rRNA V3-V4 hypervariable region on an Illumina platform. Analysis of more than 17,000 filtered high quality reads indicated that the diversity was lowest in the month 2 followed by gradual increase towards month 4 (1.24 folds increase in Shannon index). The microbial population in month 1 was dominated by Firmicutes and Proteobacteria followed by dominance of Actinobacteria and Firmicutes in the month 4. The analysis of aggregate microbiota at class level indicated relatively higher abundance of Clostridia, Bacteroides and Actinobacteria in month 1, 3 and 4, respectively. The global comparison of dominance of different phyla with the similar subjects indicated that the Indian microbiome is more similar with studies conducted with Swedish infants, although the differences in DNA extraction protocols, geographical location and sequencing platforms as confounding factors cannot be neglected. The findings in this small cohort study could facilitate future studies exploring various aspects of the human gut microbiome in Indian subcontinent.\n\n==\n\nSaulnier 2011:\n\nTitle: Gastrointestinal microbiome signatures of pediatric patients with irritable bowel syndrome.\n\nAbstract: BACKGROUND & AIMS\nThe intestinal microbiomes of healthy children and pediatric patients with irritable bowel syndrome (IBS) are not well defined. Studies in adults have indicated that the gastrointestinal microbiota could be involved in IBS.\n\n\nMETHODS\nWe analyzed 71 samples from 22 children with IBS (pediatric Rome III criteria) and 22 healthy children, ages 7-12 years, by 16S ribosomal RNA gene sequencing, with an average of 54,287 reads/stool sample (average 454 read length = 503 bases). Data were analyzed using phylogenetic-based clustering (Unifrac), or an operational taxonomic unit (OTU) approach using a supervised machine learning tool (randomForest). Most samples were also hybridized to a microarray that can detect 8741 bacterial taxa (16S rRNA PhyloChip).\n\n\nRESULTS\nMicrobiomes associated with pediatric IBS were characterized by a significantly greater percentage of the class \u03b3-proteobacteria (0.07% vs 0.89% of total bacteria, respectively; P < .05); 1 prominent component of this group was Haemophilus parainfluenzae. Differences highlighted by 454 sequencing were confirmed by high-resolution PhyloChip analysis. Using supervised learning techniques, we were able to classify different subtypes of IBS with a success rate of 98.5%, using limited sets of discriminant bacterial species. A novel Ruminococcus-like microbe was associated with IBS, indicating the potential utility of microbe discovery for gastrointestinal disorders. A greater frequency of pain correlated with an increased abundance of several bacterial taxa from the genus Alistipes.\n\n\nCONCLUSIONS\nUsing 16S metagenomics by PhyloChip DNA hybridization and deep 454 pyrosequencing, we associated specific microbiome signatures with pediatric IBS. These findings indicate the important association between gastrointestinal microbes and IBS in children; these approaches might be used in diagnosis of functional bowel disorders in pediatric patients.\n\n==\n\nZyoud 2021:\n\nTitle: Global research trends in the microbiome related to irritable bowel syndrome: A bibliometric and visualized study\n\nAbstract: BACKGROUND Irritable bowel syndrome (IBS) is a common functional gastrointestinal disorder. Dysregulation of the gut\u2013brain axis plays a central role in the pathophysiology of IBS. It is increasingly clear that the microbiome plays a key role in the development and normal functioning of the gut\u2013brain axis. AIM To facilitate the identification of specific areas of focus that may be of relevance to future research. This study represents a bibliometric analysis of the literature pertaining to the microbiome in IBS to understand the development of this field. METHODS The data used in our bibliometric analysis were retrieved from the Scopus database. The terms related to IBS and microbiome were searched in titles or abstracts within the period of 2000\u20132019. VOSviewer software was used for data visualization. RESULTS A total of 13055 documents related to IBS were retrieved at the global level. There were 1872 scientific publications focused on the microbiome in IBS. There was a strong positive correlation between publication productivity related to IBS in all fields and productivity related to the microbiome in IBS (r = 0.951, P < 0.001). The United States was the most prolific country with 449 (24%) publications, followed by the United Kingdom (n = 176, 9.4%), China (n = 154, 8.2%), and Italy (n = 151, 8.1%). The h-index for all retrieved publications related to the microbiome in IBS was 138. The hot topics were stratified into four clusters: (1) The gut\u2013brain axis related to IBS; (2) Clinical trials related to IBS and the microbiome; (3) Drug-mediated manipulation of the gut microbiome; and (4) The role of the altered composition of intestinal microbiota in IBS prevention. CONCLUSION This is the first study to evaluate and quantify global research productivity pertaining to the microbiome in IBS. The number of publications regarding the gut microbiota in IBS has continuously grown since 2013. This finding suggests that the future outlook for interventions targeting the gut microbiota in IBS remains promising.\n\n==\n\nKrogius-Kurikka 2009:\n\nTitle: Microbial community analysis reveals high level phylogenetic alterations in the overall gastrointestinal microbiota of diarrhoea-predominant irritable bowel syndrome sufferers\n\nAbstract: BackgroundA growing amount of scientific evidence suggests that microbes are involved in the aetiology of irritable bowel syndrome (IBS), and the gastrointestinal (GI) microbiota of individuals suffering from diarrhoea-predominant IBS (IBS-D) is distinguishable from other IBS-subtypes. In our study, the GI microbiota of IBS-D patients was evaluated and compared with healthy controls (HC) by using a high-resolution sequencing method. The method allowed microbial community analysis on all levels of microbial genomic guanine plus cytosine (G+C) content, including high G+C bacteria.MethodsThe collective faecal microbiota composition of ten IBS-D patients was analysed by examining sequences obtained using percent G+C (%G+C) -based profiling and fractioning combined with 16S rRNA gene clone library sequencing of 3267 clones. The IBS-D library was compared with an analogous healthy-control library of 23 subjects. Real-time PCR analysis was used to identify phylotypes belonging to the class Gammaproteobacteria and the order Coriobacteriales.ResultsSignificant differences were found between clone libraries of IBS-D patients and controls. The microbial communities of IBS-D patients were enriched in Proteobacteria and Firmicutes, but reduced in the number of Actinobacteria and Bacteroidetes compared to control. In particular, 16S rDNA sequences belonging to the family Lachnospiraceae within the phylum Firmicutes were in greater abundance in the IBS-D clone library.ConclusionsIn the microbiota of IBS-D sufferers, notable differences were detected among the prominent bacterial phyla (Firmicutes, Actinobacteria, Bacteroidetes, and Proteobacteria) localized within the GI tract.", "final_summary": "The exploration of the gut microbiome in Irritable Bowel Syndrome (IBS) patients in India using Shotgun sequencing has been a focal point in several research studies. \n\nDhakan (2019) conducted a comprehensive study on the gut microbiome of healthy individuals in India. However, the study did not specifically focus on IBS patients nor did it provide any specific findings or references to deviations in IBS patients.\n\nMitra (2013) demonstrated the effectiveness of SOLiD mate-pair sequencing, a type of Shotgun sequencing, in analyzing a complex microbiome. The study was conducted on a human fecal sample, but it did not specifically target IBS patients in India. The sequencing technology used in the study was not explicitly stated.\n\nQiu (2017) explored the diversity of intestinal microbes in IBD (inflammatory bowel disease) patients. However, the study did not specify the sequencing technology used, and it was not specific to an Indian population.\n\nMalinen (2005) analyzed the fecal microbiota of IBS patients and healthy controls using real-time PCR assays, a different method of analysis from Shotgun sequencing. The study did not specify whether it was focused on an Indian population.\n\nAttri (2018) conducted a study on the gut microbial communities in healthy Indian infants. However, the study did not specify whether it targeted IBS patients or used Shotgun sequencing.\n\nSaulnier (2011) associated specific microbiome signatures with pediatric IBS using 16S metagenomics by PhyloChip DNA hybridization and deep 454 pyrosequencing. The study did not specify whether it used Shotgun sequencing or was specific to an Indian population.\n\nZyoud (2021) conducted a bibliometric analysis of the literature pertaining to the microbiome in IBS. However, the study did not provide any information about the specific methods used, such as whether Shotgun sequencing was used or if the analysis was specific to an Indian population.\n\nKrogius-Kurikka (2009) analyzed the microbial community of IBS-D patients using a high-resolution sequencing method, indicating that the study did not use Shotgun sequencing. The study did not specify whether it was specific to an Indian population.\n\nIn conclusion, while several studies have explored the gut microbiome in relation to IBS, none of the papers collected specifically used Shotgun sequencing to study the gut microbiome of IBS patients in India. Further research is needed in this specific area."}, {"query": "how pregnancy reshape the mother brain to cope with sleep deprivation", "paper_list_string": "Pardo 2016:\n\nTitle: Effects of sleep restriction during pregnancy on the mother and fetuses in rats\n\nAbstract: The present study aimed to analyze the effects of sleep restriction (SR) during pregnancy in rats. The following three groups were studied: home cage (HC pregnant females remained in their home cage), Sham (females were placed in tanks similar to the SR group but with sawdust) and SR (females were submitted to the multiple platform method for 20 h per day from gestational days (GD) 14 to 20). Plasma corticosterone after 6 days of SR was not different among the groups. However, the relative adrenal weight was higher in the SR group compared with the HC group, which suggests possible stress impact. SR during pregnancy reduces the body weight of the female but no changes in liver glycogen, cholesterol and triglycerides, and muscle glycogen were detected. On GD 20, the fetuses of the females submitted to SR exhibited increased brain derived neurotrophic factor (BDNF) in the hippocampus, which indicates that sleep restriction of mothers during the final week of gestation may affect neuronal growth factors in a fetal brain structure, in which active neurogenesis occurs during the deprivation period. However, no changes in the total reactive oxygen species (ROS) in the cortex, hippocampus, or cerebellum of the fetuses were detected. SR females showed no major change in the maternal behavior, and the pups' preference for the mother's odor on postpartum day (PPD) 7 was not altered. On GD 20, the SR females exhibited increased plasma prolactin (PRL) and oxytocin (OT) compared with the HC and Sham groups. The negative outcomes of sleep restriction during delivery could be related, in part, to this hormonal imbalance. Sleep restriction during pregnancy induces different changes compared with the changes described in males and affects both the mother and offspring.\n\n==\n\nBrunton 2008:\n\nTitle: The expectant brain: adapting for motherhood\n\nAbstract: A successful pregnancy requires multiple adaptations of the mother's physiology to optimize fetal growth and development, to protect the fetus from adverse programming, to provide impetus for timely parturition and to ensure that adequate maternal care is provided after parturition. Many of these adaptations are organized by the mother's brain, predominantly through changes in neuroendocrine systems, and these changes are primarily driven by the hormones of pregnancy. By contrast, adaptations in the mother's brain during lactation are maintained by external stimuli from the young. The changes in pregnancy are not necessarily innocuous: they may predispose the mother to post-partum mood disorders.\n\n==\n\nBrunton 2015:\n\nTitle: Maternal Brain Adaptations in Pregnancy\n\nAbstract: During pregnancy multiple physiological adaptations take place in the mother to optimize the chances of a successful pregnancy outcome. These adaptations play a critical role in reproductive physiology, serving to (1) increase the supply of oxygen and nutrients to the placenta and maternal organs supporting the pregnancy; (2) optimize fetal growth and development; (3) prepare expanded oxytocin stores for when they are in demand at parturition and for lactation; (4) protect the pregnancy from premature birth; (5) protect the fetus from adverse programming, e.g., by maternal stress; (6) ensure adequate milk production and delivery at lactation; and (7) ensure that the appropriate brain circuits are primed and behaviors altered to deliver sufficient maternal care after parturition. In this chapter we focus on these pregnancy-related adaptations that are organized by the maternal brain, many of which are induced by the increased levels of pregnancy hormones.\n\n==\n\nPires 2020:\n\nTitle: Effects of sleep modulation during pregnancy in the mother and offspring: Evidences from preclinical research\n\nAbstract: Disturbed sleep during gestation may lead to adverse outcomes for both mother and child. Animal research plays an important role in providing insights into this research field by enabling ethical and methodological requirements that are not possible in humans. Here, we present an overview and discuss the main research findings related to the effects of prenatal sleep deprivation in animal models. Using systematic review approaches, we retrieved 42 articles dealing with some type of sleep alteration. The most frequent research topics in this context were maternal sleep deprivation, maternal behaviour, offspring behaviour, development of sleep\u2013wake cycles in the offspring, hippocampal neurodevelopment, pregnancy viability, renal physiology, hypertension and metabolism. This overview indicates that the number of basic studies in this field is growing, and provides biological plausibility to suggest that sleep disturbances might be detrimental to both mother and offspring by promoting increased risk at the behavioural, hormonal, electrophysiological, metabolic and epigenetic levels. More studies on the effects of maternal sleep deprivation are needed, in light of their major translational perspective.\n\n==\n\nChang 2010:\n\nTitle: Sleep deprivation during pregnancy and maternal and fetal outcomes: is there a relationship?\n\nAbstract: Sleep duration in the population has been declining. Women occupy an increasingly prominent place in the work force without reducing most of their responsibilities at home. Consequently, sleep needs are often pushed to the bottom of women's daily priority list. Prior research has indicated that sleep deprivation is associated with higher levels of pro-inflammatory serum cytokines. This is important because higher plasma concentrations of pro-inflammatory serum cytokine levels are associated with postpartum depression and adverse birth outcomes such as preterm delivery. However, little research has directly examined how sleep deprivation may affect maternal and fetal outcomes. This review summarizes the existing data on the effect of sleep deprivation during pregnancy on maternal and fetal outcomes. We review supporting evidence for the hypotheses that sleep deprivation during pregnancy increases the risk of preterm delivery and postpartum depression, and that systemic inflammation is the causal mechanism in the association. Prior research on sleep in pregnancy has been limited by varying data collection methods, subjective self-reported sleep measures, small and non-representative samples, cross-sectional designs; descriptive or non-hypothesis driven studies. Future research with longitudinal study designs is needed to allow examination of the effect of sleep deprivation on adverse maternal and fetal outcomes.\n\n==\n\nSlattery 2008:\n\nTitle: No stress please! Mechanisms of stress hyporesponsiveness of the maternal brain\n\nAbstract: The time around birth is accompanied by behavioural and physiological adaptations of the maternal brain, which ensure reproductive functions, maternal care and the survival of the offspring. In addition, profound neuroendocrine and neurobiological adaptations have been described with respect to behavioural and neuroendocrine stress responsiveness in rodents and human mothers. Thus, the hormonal response of the hypothalamo\u2010pituitary\u2010adrenal (HPA) axis and the response of the sympathetic nervous system to emotional and physical stressors are severely attenuated. Moreover, anxiety\u2010related behaviour and emotional responsiveness to stressful stimuli are reduced with the result of general calmness. These complex adaptations of the maternal brain are likely to be a consequence of an increased activity of brain systems with inhibitory effects on the HPA axis (such as the oxytocin and prolactin systems) and of a reduced activity of excitatory pathways (noradrenaline (norepinephrine), corticotrophin\u2010releasing factor and opioids). Experimental manipulation of these systems using complementary approaches indeed demonstrates their importance in these maternal brain adaptations. Maternal stress adaptations are not only important for the healthy prenatal development of the offspring by preventing excessive glucocorticoid responses and in the promotion of postnatal maternal behaviour, but are also vital for the well\u2010being of the mother and her mental health.\n\n==\n\nHedman 2002:\n\nTitle: Effects of pregnancy on mothers' sleep.\n\nAbstract: OBJECTIVES\nTo survey the effects of pregnancy on mothers' sleep.\n\n\nMETHODS\nMothers were interviewed during and after pregnancy with a series of five questionnaires to assess alterations in their sleep. The first questionnaire covered the 3 months before becoming pregnant, the next three the trimesters of pregnancy and the last the 3 months after delivery. The study was carried out in a central hospital and the maternity care units in the nearby rural community. Altogether, 325 pregnant women completed all five questionnaires.\n\n\nRESULTS\nThe total amounts of reported sleep and of nocturnal sleep increased significantly during the first trimester of pregnancy, began to decrease thereafter and were shortest during the 3 months after pregnancy. During late pregnancy expectant mothers over 30 years of age reported less sleep than those under 30. During the whole pregnancy, but increasingly toward the end of pregnancy, sleep became more restless and fragmentary and its subjective quality worsened, due at least partly to increased restless legs and nightly awakenings increasing with advancing pregnancy.\n\n\nCONCLUSIONS\nThe subjective quality of sleep is disturbed as early as the first trimester of pregnancy, although total sleeping time increases. The amount of reported sleep begins to decrease in the second trimester. The frequency of reported sleep disturbances, such as restless legs syndrome and nocturnal awakenings, is maximum in the third trimester but is about normal within 3 months after delivery.\n\n==\n\nTikotzky 2009:\n\nTitle: Maternal sleep-related cognitions and infant sleep: a longitudinal study from pregnancy through the 1st year.\n\nAbstract: Infant sleep is a major source of concern for many parents. The aims of this longitudinal study were to assess: (a) the development of sleep patterns among infants, (b) the development of maternal cognitions regarding infant sleep, and (c) the relations between these domains during the 1st year of life. Eighty-five mothers were recruited during pregnancy and completed a questionnaire aimed at assessing maternal sleep-related cognitions. After delivery (at 1, 6, and 12 months) sleep was assessed using actigraphy and sleep logs, and maternal cognitions were reassessed. The findings demonstrated significant predictive and concomitant links between maternal cognitions and infant sleep. Maternal soothing behaviors mediated the relations between these domains.", "final_summary": "The intricate relationship between pregnancy and sleep deprivation has been the subject of numerous studies, revealing intriguing insights into how the maternal brain adapts to ensure both the mother's well-being and the optimal development of the fetus.\n\nPardo (2016) found that sleep restriction during pregnancy led to increased levels of brain-derived neurotrophic factor (BDNF) in the fetuses' hippocampus, suggesting that maternal sleep deprivation may affect fetal brain development. Despite this, the study found no significant changes in maternal behavior or the pups' preference for the mother's odor, indicating that the maternal brain may adapt to cope with sleep deprivation.\n\nThis adaptive capacity of the maternal brain is further explored by Brunton (2008, 2015), who posits that the hormonal changes during pregnancy drive multiple physiological adaptations in the mother's brain. These adaptations optimize fetal growth and development, protect the fetus from adverse programming, and ensure adequate maternal care post-parturition. However, Brunton (2008) also warns that these changes may predispose the mother to post-partum mood disorders.\n\nPires (2020) and Chang (2010) both highlight the potential adverse outcomes of sleep deprivation during pregnancy for both the mother and the child, including behavioral, hormonal, electrophysiological, metabolic, and epigenetic risks. However, Chang (2010) also emphasizes the need for more research to directly examine the effect of sleep deprivation on maternal and fetal outcomes.\n\nSlattery (2008) provides a comprehensive overview of the neuroendocrine and neurobiological adaptations in the maternal brain during pregnancy, which result in reduced anxiety-related behavior and emotional responsiveness to stressful stimuli. These adaptations are likely driven by an increased activity of brain systems with inhibitory effects on the hypothalamo\u2010pituitary\u2010adrenal (HPA) axis and a reduced activity of excitatory pathways.\n\nHedman (2002) found that the subjective quality of sleep is disturbed as early as the first trimester of pregnancy, although total sleeping time increases. Tikotzky (2009) found significant predictive and concomitant links between maternal sleep-related cognitions and infant sleep, with maternal soothing behaviors mediating the relations between these domains.\n\nIn conclusion, the maternal brain undergoes significant adaptations during pregnancy to cope with sleep deprivation. These adaptations ensure the optimal development of the fetus and the well-being of the mother, but they may also predispose the mother to post-partum mood disorders. More research is needed to fully understand the complex interplay between pregnancy and sleep deprivation."}, {"query": "cognitive load theory in prekindergarten education", "paper_list_string": "Meissner 2013:\n\nTitle: Towards Cognitive Load Theory as Guideline for Instructional Design in Science Education.\n\nAbstract: We applied cognitive load theory in an heuristic out-of-school science lesson. The lesson comprises experimentsconcerning major attributes of NaCl and was designed for 5th to 8th grade students. Our interest focused on wethercognitive load theory provides sufficient guidelines for instructional design in the field of heuristic science education.We extracted student clusters derived from pre-knowledge and learning success. We characterised students, based oncognitive achievement, mental effort, and instructional efficiency. Cluster analyses revealed three student clusterswith quite satisfying results. Two further clusters showed improvable results, two showed no learning success, whichmay point to difficulties in coping with the learning setting. Motivational characterisation will refine the results, andmay confirm starting points to advance cognitive load theory in heuristic science education.\n\n==\n\nMoos 2014:\n\nTitle: Student teacher challenges: using the cognitive load theory as an explanatory lens\n\nAbstract: Cognitive load theory (CLT) can explain the challenges faced by student teachers. This study, guided by the CLT, included 26 pre-service teachers. Participants completed a cognitive load self-report questionnaire and were interviewed at two points during their student teaching. Results revealed that student teachers decreased mental effort related to monitoring their students\u2019 level of attention, meeting needs of diverse learners, and managing internal and external distractions. Qualitative analysis revealed: (1) student teachers became aware of limited cognitive resources; (2) lesson planning imposes cognitive load during student teaching; and (3) cognitive overload limits the ability to make modifications during teaching.\n\n==\n\nSpringer 2010:\n\nTitle: Cognitive load theory, educational research, and instructional design: some food for thought\n\nAbstract: Cognitive load is a theoretical notion with an increasingly central role in the educational research literature. The basic idea of cognitive load theory is that cognitive capacity in working memory is limited, so that if a learning task requires too much capacity, learning will be hampered. The recommended remedy is to design instructional systems that optimize the use of working memory capacity and avoid cognitive overload. Cognitive load theory has advanced educational research considerably and has been used to explain a large set of experimental findings. This article sets out to explore the open questions and the boundaries of cognitive load theory by identifying a number of prob lematic conceptual, methodological and application-related issues. It concludes by pre senting a research agenda for future studies of cognitive load.\n\n==\n\nJong 2010:\n\nTitle: Cognitive load theory, educational research, and instructional design: some food for thought\n\nAbstract: Cognitive load is a theoretical notion with an increasingly central role in the educational research literature. The basic idea of cognitive load theory is that cognitive capacity in working memory is limited, so that if a learning task requires too much capacity, learning will be hampered. The recommended remedy is to design instructional systems that optimize the use of working memory capacity and avoid cognitive overload. Cognitive load theory has advanced educational research considerably and has been used to explain a large set of experimental findings. This article sets out to explore the open questions and the boundaries of cognitive load theory by identifying a number of problematic conceptual, methodological and application-related issues. It concludes by presenting a research agenda for future studies of cognitive load.\n\n==\n\nKennedy 2021:\n\nTitle: Cognitive Load Theory: An Applied Reintroduction for Special and General Educators\n\nAbstract: There are numerous reasons why students with disabilities struggle in school. A key reason is professionals in the field may not pay enough attention to students\u2019 overwhelmed cognitive capacity. Cognitive load theory explains that all humans have limited capacity at any given time to use their auditory, visual, and tactile inputs (independently or collectively) to acquire new information and store it in long-term memory. When available cognition is overwhelmed \u2013 which can be caused by any number of reasons \u2013 learning cannot occur. In this article, we introduce the key aspects of cognitive load theory and give specific examples of how special educators can use this information to shape their instruction to support students\u2019 unique needs.\n\n==\n\nSweller 2019:\n\nTitle: Cognitive load theory and educational technology\n\nAbstract: Cognitive load theory provides instructional recommendations based on our knowledge of human cognition. Evolutionary psychology is used to assume that knowledge should be divided into biologically primary information that we have specifically evolved to acquire and biologically secondary information that we have not specifically evolved to acquire. Primary knowledge frequently consists of generic-cognitive skills that are important to human survival and cannot be taught because they are acquired unconsciously while secondary knowledge is usually domain-specific in nature and requires explicit instruction in education and training contexts. Secondary knowledge is first processed by a limited capacity, limited duration working memory before being permanently stored in long-term memory from where unlimited amounts of information can be transferred back to working memory to govern action appropriate for the environment. The theory uses this cognitive architecture to design instructional procedures largely relevant to complex information that requires a reduction in working memory load. Many of those instructional procedures can be most readily used with the assistance of educational technology.\n\n==\n\nCooper 1990:\n\nTitle: Cognitive load theory as an aid for instructional design\n\nAbstract: This paper attempts to draw together several recent findings in educational psychology that have led to the development and application of cognitive load theory to the format of instruction (Chandler and Sweller, unpublished manuscript; Sweller, 1988). These findings are directly related to the processes involved in learning, and will ultimately affect the way instructional design is approached. Cognitive load may be viewed as the level of 'mental energy' required to process a given amount of information. As the amount of information to be processed increases, so too does the associated cognitive load. Cognitive load theory suggests that effective instructional material promotes learning by directing cognitive resources towards activities that are relevant to learning rather than to processes that are an adjunct to learning. Cognitive load theory grew out of research investigating the differences between experts and novices. This research will be discussed briefly before returning to the issue of cognitive load theory.\n\n==\n\nMoreno 2010:\n\nTitle: Cognitive Load Theory: Cognitive Load Theory: Historical Development and Relation to Other Theories\n\nAbstract: The goal of this introductory chapter is to provide a historical review of the assumptions underlying Cognitive Load Theory (CLT) and to place the theory into the broader context of the learning sciences. The chapter focuses on the theoretical developments that guided the research on cognitive load and learning for the past twenty years and is organized in the following way. First, we examine the nature of the cognitive load construct and compare it to similar psychological constructs. Second, we present a historical review of the development of CLT's assumptions in the following four stages: (a) extraneous cognitive load in problem solving, (b) intrinsic cognitive load and the first additivity hypothesis, (c) germane cognitive load and the second additivity hypothesis, and (d) the evolutionary interpretation of CLT. Finally, we conclude the chapter by examining the constructs and assumptions of CLT in relation to other theories in psychology and education. THE COGNITIVE LOAD CONSTRUCT CLT is a psychological theory because it attempts to explain psychological or behavioral phenomena resulting from instruction. Psychological theories are concerned with the possible relationships among psychological constructs or between a psychological construct and an observable phenomenon of practical consequence. A psychological construct is an attribute or skill that happens in the human brain. In CLT, the main constructs of interest are cognitive load , hence the name of the theory, and learning . CLT was developed to explain the effects of instructional design on these two constructs.", "final_summary": "Cognitive Load Theory (CLT) is a central concept in educational psychology, focusing on the limitations of working memory and the optimization of learning environments (Jong 2010; Springer 2010). In the context of prekindergarten education, the application of CLT can be both challenging and rewarding.\n\nMeissner (2013) applied CLT in an out-of-school science lesson, demonstrating that student clusters based on pre-knowledge and learning success could be identified. This suggests that CLT can provide guidelines for instructional design, even in heuristic science education. However, the study also highlighted the need for further refinement, particularly in relation to student motivation.\n\nMoos (2014) used CLT to explain the challenges faced by student teachers, revealing that cognitive overload can limit the ability to make modifications during teaching. This study underscores the importance of awareness and management of cognitive load in the teaching process, particularly in meeting the needs of diverse learners.\n\nKennedy (2021) reintroduced CLT in the context of special and general education, emphasizing the need to shape instruction to support students' unique needs. This study suggests that understanding and applying CLT can help address the cognitive challenges faced by students with disabilities.\n\nSweller (2019) discussed the role of educational technology in applying CLT, suggesting that instructional procedures designed to reduce working memory load can be effectively implemented with the aid of technology. This highlights the potential of technology in optimizing the application of CLT in prekindergarten education.\n\nCooper (1990) discussed the development and application of cognitive load theory to instructional design, but there is no specific mention of a historical perspective or the differences between experts and novices. Moreno (2010) provides a historical review of the assumptions underlying cognitive load theory and its relation to other theories in psychology and education, but there is no specific mention of the differences between experts and novices.\n\nIn conclusion, the application of CLT in prekindergarten education is a complex but promising endeavor. It requires a nuanced understanding of cognitive load, careful instructional design, and the effective use of educational technology. Further research is needed to refine the application of CLT in this context, particularly in relation to diverse learners and those with special needs (Meissner 2013; Moos 2014; Kennedy 2021; Sweller 2019; Cooper 1990; Moreno 2010)."}, {"query": "What are the competitive dynamics of a mid sized company in government contracting", "paper_list_string": "Stumpf 2000:\n\nTitle: Competitive pressures on middle\u2010market contractors in the UK\n\nAbstract: Medium\u2010sized regional building contractors in the UK are exhibiting poorer performance in the 1980s and the 1990s and are less likely to survive than their larger or smaller counterparts. The market structure of contracting appears to be changing, putting pressure on these intermediate firms. Evidence drawn from the Department of the Environment (DoE) statistical series shows industry composition is changing, in particular the gradual decline over time of the middle market. An analysis of company accounts for a sample of approximately 200 contractors shows that medium\u2010sized firms are also displaying inferior business ratios. Possible explanations are offered, including barriers to entry, such as capitalization, economies of scale (pecuniary and market), along with changes in construction demand.\n\n==\n\nPayne 2009:\n\nTitle: Competitive Dynamics among Service SMEs\n\nAbstract: In an effort to further our understanding of competitive dynamics, the three constructs of firm specialization, environmental munificence, and rivalry intensity are examined in relation to financial performance in service\u2010intensive Small and Medium\u2010Sized Enterprises (SMEs). Using a sample of physician organizations, direct and interaction relationships are empirically examined using multivariate regression analyses. Findings confirm a three\u2010way interaction that exists among these factors in relationship to overall performance; this supports a more complex, configurations approach to competitive dynamics research, particularly among service\u2010intensive SMEs.\n\n==\n\nHefetz 2004:\n\nTitle: Privatization and Its Reverse: Explaining the Dynamics of the Government Contracting Process\n\nAbstract: Empirical evidence shows local government contracting is a dynamic process that includes movements from public delivery to markets and from market contracts back to in-house delivery.This \u2018\u2018reversecontracting\u2019\u2019reflectsthe complexityofpublicserviceprovisionin aworld where market alternatives are used along with public delivery. We develop a methodology to link responses to national surveys and create a longitudinal data set that captures the dynamics of the contracting process. We present a framework that incorporates principal agent problems, government management, monitoring and citizen concerns, and market structure. Our statistical analysis finds government management, monitoring, and principal agent problems to be most important in explaining both new contracting out and contracting back-in. Professional managers recognize the importance of monitoring and the need for public engagement in the service delivery process. The results support the new public service that argues public managers do more than steer a market process; they balance technical and political concerns to secure public value.\n\n==\n\nFlammer 2017:\n\nTitle: Competing for Government Procurement Contracts: The Role of Corporate Social Responsibility\n\nAbstract: Research Summary: This study examines whether corporate social responsibility (CSR) improves firms\u2019 competitiveness in the market for government procurement contracts. To obtain exogenous variation in firms\u2019 social engagement, I exploit a quasi\u2010natural experiment provided by the enactment of state\u2010level constituency statutes, which allow directors to consider stakeholders\u2019 interests when making business decisions. Using constituency statutes as instrumental variable (IV) for CSR, I find that companies with higher CSR receive more procurement contracts. The effect is stronger for more complex contracts and in the early years of the government\u2010company relationship, suggesting that CSR helps mitigate information asymmetries by signaling trustworthiness. Moreover, the effect is stronger in competitive industries, indicating that CSR can serve as a differentiation strategy to compete against other bidders. Managerial Summary: This study examines how companies can strategically improve their competitiveness in the market for government procurement contracts\u2014a market of economic importance (15\u201320% of GDP). It shows that companies with higher social and environmental performance (CSR) receive more procurement contracts. This effect is stronger for more complex contracts, in the early years of the government\u2013company relationship, and in more competitive industries. These findings indicate that firms\u2019 CSR can serve as a signaling and differentiation strategy that influences the purchasing decision of government agencies. Accordingly, managers operating in the business\u2010to\u2010government (B2G) sector could benefit from integrating social and environmental considerations into their strategic decision making.\n\n==\n\nStr\u00f6mb\u00e4ck 2015:\n\nTitle: Contract Size and Small Firm Competition in Public Procurement\n\nAbstract: The European Commission encourages public authorities to split procurement contracts into multiple contracts in order to increase the competiveness of small and medium sized enterprises (SMEs). In ...\n\n==\n\nJing 2012:\n\nTitle: Is Competitive Contracting Really Competitive? Exploring Government\u2013Nonprofit Collaboration in China\n\nAbstract: ABSTRACT Local governments in China have adopted competitive contracting in social service delivery to restructure burgeoning government\u2013nonprofit collaborations. The stated purpose of using competitive contracting is to forge formal, effective, and accountable collaborative relations. By applying institutional, transaction cost, and resource dependence theories to China's context, we reveal the institutional pressures of adopting a competitive contracting policy and identify the challenges of economizing transaction costs and handling mutual resource dependence in its implementation. We find the prevalence of \u201cdecoupling\u201d to meet these competing pressures at the sacrifice of competition: that is, many contracts only appear competitive. We conducted interviews, participant observations, and a questionnaire survey to study a large-scale, social-service-contracting program in Shanghai. The results provide evidence of weak competition. Local governments in Shanghai were not able to effectively coordinate competitive bidding and thus strategically kept a decentralized implementation and management system based on existing relations.\n\n==\n\nJohnston 2012:\n\nTitle: Government Contracts and \u201cManaging the Market\u201d\n\nAbstract: Theory tells us that competition is the chief driver of improved efficiency and effectiveness in government contracting, yet contract provider markets are often noncompetitive. This study offers a detailed, contextualized examination of public administrative responses to thin contract markets. Following an inductive approach with data from semistructured interviews with contract administrators, the authors offer a preliminary typology of the conditions that give rise to thin markets, and the \u201cmarket management\u201d strategies used to create, enhance, and sustain competition in the markets from where governments purchase goods and services. The authors then review the efficacy and implications of these strategies for public services to citizens.\n\n==\n\nBrunjes 2020:\n\nTitle: Competition and Federal Contractor Performance\n\nAbstract: \n Contracts are increasingly used in government as a way to insert competition into public agencies and policy implementation. Competition among contractors is theorized to drive down costs, encourage innovation, and boost accountability. However, there have been few studies on whether competition actually leads to improved performance among contractors. This analysis examines nearly 25,000 federal contracts to determine how competition is related to performance. Findings indicate that competitively sourced contracts are more likely to suffer from performance problems. However, relationships between contractor and agency correlate with fewer performance problems. This suggests that competition may not be leading to the benefits that many might expect. Instead, relationships and shared experiences, along with other factors, may matter more when seeking to improve contractor performance.", "final_summary": "The competitive dynamics of mid-sized companies in government contracting are multifaceted and influenced by various factors. Stumpf (2000) highlights the challenges faced by medium-sized contractors in the UK, including barriers to entry and changes in construction demand. This suggests that market structure and industry trends significantly impact the competitive dynamics of these firms.\n\nPayne (2009) emphasizes the role of firm specialization, environmental munificence, and rivalry intensity in shaping the competitive dynamics among service-intensive SMEs. This implies that the nature of the industry and the firm's strategic focus can influence its competitive position.\n\nHefetz (2004) underscores the dynamic nature of government contracting, highlighting the importance of government management, monitoring, and principal-agent problems in the contracting process. This suggests that the competitive dynamics of mid-sized companies in government contracting are also shaped by the nature of their relationship with the government.\n\nFlammer (2017) provides evidence that corporate social responsibility (CSR) can enhance a firm's competitiveness in government procurement contracts. This indicates that non-economic factors, such as CSR, can also influence the competitive dynamics of mid-sized companies in government contracting.\n\nStr\u00f6mb\u00e4ck (2015) focuses on the European Commission's encouragement to split procurement contracts to increase competitiveness for small and medium-sized enterprises (SMEs). This suggests that the structure and process of government contracting can significantly impact the competitive position of these firms. Jing (2012), on the other hand, discusses the challenges and weak competition in the implementation of competitive contracting in China's government-nonprofit collaborations, indicating that the competitive dynamics can vary significantly across different contexts.\n\nJohnston (2012) and Brunjes (2020) both emphasize the role of competition and relationships in influencing contractor performance. This implies that the competitive dynamics of mid-sized companies in government contracting are not only shaped by market competition but also by the nature of their relationship with the government.\n\nIn conclusion, the competitive dynamics of mid-sized companies in government contracting are influenced by a range of factors, including market structure, firm strategy, the nature of the contracting process, and the firm's relationship with the government. These findings underscore the complexity of the competitive dynamics in this context and highlight the need for mid-sized companies to adopt a multifaceted strategy to enhance their competitiveness."}, {"query": "how long a patient with generalized anxiety disorder should take medication for", "paper_list_string": "Lam 2006:\n\nTitle: Generalized anxiety disorder: how to treat, and for how long?\n\nAbstract: Generalized anxiety disorder (GAD) is a common, chronic and disabling anxiety disorder with considerable comorbidity with depression as well as with other anxiety disorders. Although tricyclic antidepressants and benzodiazepines have been found to be efficacious in patients with GAD, tolerability problems and other risks limit their use in clinical practice. In placebo-controlled, acute (<8\u2009weeks) trials, several medications, including the selective serotonin reuptake inhibitors ([SSRIs] escitalopram, paroxetine, and sertraline) and others (venlafaxine, buspirone, pregabalin), have demonstrated efficacy in patients with GAD. Indeed, current guidelines for the treatment of GAD recommend SSRIs as first-line pharmacological therapy because of their efficacy and tolerability profiles. Although GAD is a chronic condition that is usually present for years, with symptoms typically fluctuating in intensity over time, there have been few randomized, controlled trials of pharmacotherapy beyond the acute phase of treatment. However, data from recent relapse-prevention studies and longer-term maintenance studies with paroxetine, venlafaxine and escitalopram strongly support the value of continued treatment for at least a further 6 months. This article focuses on pharmacological treatment, and reviews recently available data from acute, long-term and relapse-prevention trials in patients with GAD. In addition, issues relating to the natural course of GAD are highlighted as important considerations to guide selection of pharmacotherapy.\n\n==\n\nRouillon 2004:\n\nTitle: Long term therapy of generalized anxiety disorder\n\nAbstract: Abstract Generalized anxiety disorder (GAD) is a common (lifetime prevalence: 5.1%), recurrent condition, which often heralds other psychiatric disorders, notably depression. As by definition it is a disorder progressing over months, treatment should be designed on a long term basis. And yet, few studies have been conducted beyond the classical 6\u20138 weeks characterizing the acute treatment phase. This is especially true of anxiolytics, but also of antidepressants, with the exception of paroxetine and venlafaxine, which are the only drugs approved in this indication in Western countries. The efficacy of psychotherapy, notably relaxation and cognitive-behavioral therapy, is established in the treatment of GAD, but its preferred indications and possible combination with antidepressants are still to be specified. Long term, not to say very long term studies of GAD, as well as depression, will still be required in the future to improve its management and specify therapeutic modalities (combination treatment, optimal duration, continuous or intermittent therapy, choice of psychotherapeutic techniques or agents, \u2026). Early and adequately prolonged treatment should not only result in more numerous remission periods, but also in decreased frequency of co-morbidities whether depressive, addictive, or of another nature, and should also reduce the social impact of GAD.\n\n==\n\nMah\u00e9 2000:\n\nTitle: Long\u2010term pharmacological treatment of generalized anxiety disorder\n\nAbstract: &NA; Generalized anxiety disorder (GAD) is one of the most common anxiety disorders and has a poor prognosis, although it is often thought to be a minor complaint. This disorder has a chronic course of 5\u201015 years and longer. Long\u2010term treatment with the commonly used benzodiazepines is controversial because of concerns over tolerance and dependence. We performed a thorough search of the literature for clinical trials of a duration of over 2 months conducted in patients with generalized anxiety disorder in order to identify any successful long\u2010term treatment of this disorder. Only eight long\u2010term reports of studies conducted in well\u2010defined homogeneous groups of patients diagnosed with generalized anxiety disorder were found with the methodology of these studies presenting a number of limiting factors. The results are inconclusive and no reference drug could be identified. In addition, an adequate evaluation of the long\u2010term treatment of GAD has not yet been performed.\n\n==\n\nRickels 2010:\n\nTitle: Time to relapse after 6 and 12 months' treatment of generalized anxiety disorder with venlafaxine extended release.\n\nAbstract: CONTEXT\nGeneralized anxiety disorder (GAD) is a chronic disorder in need of reliable data to guide long-term treatment.\n\n\nOBJECTIVES\nTo assess the benefits of 6 and 12 months' treatment of GAD with venlafaxine hydrochloride extended release (XR) in patients who improved after 6 months' open-label venlafaxine XR treatment.\n\n\nDESIGN\nAfter 6 months' open-label venlafaxine XR treatment, improved patients were randomized to venlafaxine XR or placebo for 6 months. All venlafaxine XR patients still in the study at 12 months were randomized to receive venlafaxine XR or placebo, and all placebo patients continued taking placebo for another 6 months.\n\n\nSETTING\nOne urban site (5 locations).\n\n\nPATIENTS\nOf 268 patients with a diagnosis of GAD entering the open-label venlafaxine XR treatment phase, 158 (59.0%) completed 6 months, and 136 (50.7%) entered relapse phase 2 (6-12 months). Fifty-nine (43.4%) of 136 patients entered phase 3 (12-18 months).\n\n\nINTERVENTION\nSix months' open-label treatment with venlafaxine XR, followed by double-blind venlafaxine XR or placebo for 2 relapse phases, each lasting 6 months.\n\n\nMAIN OUTCOME MEASURES\nTime to relapse while receiving venlafaxine XR or placebo after 6 and after 12 months of treatment. Relapse was strictly defined to safeguard against assigning patients with venlafaxine XR discontinuation symptoms or temporary anxiety increase as relapse.\n\n\nRESULTS\nFor objective 1, relapse rates in phase 2 (months 6-12) were 9.8% on venlafaxine XR and 53.7% on placebo (P < .001). For objective 2, relapse rates after 12 months on placebo (32.4%) were lower than after 6 months on venlafaxine XR (53.7%) (P < .03).\n\n\nCONCLUSIONS\nTreatment of GAD with an antidepressant should be continued for at least 12 months. Preliminary data demonstrate that improved patients who relapse while off their antianxiety medication after at least 6 months of treatment will again most likely respond to a second course of treatment with the same medication. Trial Registration clinicaltrials.gov Identifier: NCT00183274.\n\n==\n\nGoodman 2005:\n\nTitle: Treatment of generalized anxiety disorder with escitalopram: pooled results from double-blind, placebo-controlled trials.\n\nAbstract: BACKGROUND\nEscitalopram 10 mg/day is an effective and well-tolerated antidepressant. Three randomized controlled trials recently evaluated the safety and efficacy of escitalopram in the treatment of generalized anxiety disorder (GAD).\n\n\nMETHODS\nThe trial designs were virtually identical, allowing data to be pooled across studies. Male and female outpatients, ages 18-80 years, with DSM-IV-defined GAD were randomized to double-blind treatment with escitalopram or placebo for 8 weeks. Escitalopram dose was fixed at 10 mg/day for the first 4 weeks, after which increases to 20 mg/day were permitted. The primary efficacy variable was the mean change from baseline in total Hamilton Anxiety Scale (HAMA) score.\n\n\nRESULTS\nApproximately 850 patients were randomized to double-blind treatment. In each individual study, escitalopram was significantly superior to placebo (p<0.05) as measured by change from baseline in HAMA score. By-visit analyses of data pooled across studies revealed significantly greater improvement (p<0.05) in the escitalopram group beginning at week 1 or 2 and continuing through week 8 for all primary and secondary efficacy variables. The mean change in HAMA total score from baseline to endpoint also was significantly greater for patients maintained at escitalopram 10 mg/day than for those receiving placebo. Escitalopram was generally well tolerated.\n\n\nLIMITATIONS\nThe studies included in this analysis were of short-term duration and excluded patients with significant medical and psychiatric comorbidities, such as major depressive disorder.\n\n\nCONCLUSION\nResults from the individual trials and the pooled analysis demonstrate that escitalopram is effective and well tolerated for the treatment of GAD.\n\n==\n\nBarlow 1992:\n\nTitle: Behavioral treatment of generalized anxiety disorder\n\nAbstract: Sixty-five carefully diagnosed patients with generalized anxiety disorder were treated with either relaxation, cognitive therapy, or their combination. These three active treatment conditions were compared to a wait-list control group. On several measures, in-cluding measures of worry, treated patients were significantly better than those in the wait-list control group at post treatment. These gains were maintained across the two-year follow-up period. Notably, these therapeutic gains were accompanied by substantial reductions in anxiolytic medication use over the period of follow-up. No differences emerged, however, among treatments at any point of comparison. In addition, drop-out rates among the active treatment groups were high (range = 5% to 38%). Moreover, most patients were left with residual anxiety suggesting the need for the development of more focused and efficient psychological treatments for generalized anxiety disorder.\n\n==\n\nKeller 2002:\n\nTitle: The long-term clinical course of generalized anxiety disorder.\n\nAbstract: Although generalized anxiety disorder (GAD) is a common disorder associated with significant levels of morbidity, little is known of its long-term course and outcomes. During the first 5 years, GAD follows a chronic course with low rates of remission and moderate rates of relapse/recurrence following remission. Retrospective studies suggest that this chronic pattern may last up to 20 years. It is hoped that, as with depression, long-term prospective studies in GAD will provide insight into the course, nature, and outcomes of the disorder over time. The studies will also identify any changes in the duration and severity of episodes of GAD over time, enabling treatments to effectively reflect the course of the disorder. Studies of other anxiety disorders and depression suggest that the course and outcome of the disorder may be influenced by certain factors such as stressful life events, anxiety sensitivity/negative affect, gender, subsyndromal symptoms, and comorbid disorders. Currently, studies are underway to determine the effects of these factors on the risk of relapse/recurrence, maintenance of full symptoms, and development of subsyndromal symptoms in GAD. GAD is currently underrecognized and undertreated, but it is hoped that this will change with the ever-increasing awareness of anxiety disorders. As treatment for GAD becomes more common, future prospective studies will identify the effect of therapy on the course and nature of the disorder, leading to increased understanding of GAD and the development of effective treatment strategies tailored for individual patients.\n\n==\n\nKessler 2005:\n\nTitle: Rethinking the duration requirement for generalized anxiety disorder: evidence from the National Comorbidity Survey Replication\n\nAbstract: Background. The proposed revisions of the ICD and DSM diagnostic systems have led to increased interest in evaluation of diagnostic criteria. This report focuses on the DSM-IV requirement that episodes of generalized anxiety disorder (GAD) must persist for at least 6 months. Community epidemiological data are used to study the implications of changing this requirement in the range 1\u201312 months for estimates of prevalence, onset, course, impairment, co-morbidity, associations with parental GAD, and sociodemographic correlates. Method. Data come from the US National Comorbidity Survey Replication (NCS-R), a US household survey carried out during 2001\u20132003. Version 3.0 of the WHO Composite International Diagnostic Interview (WMH-CIDI) was used to assess DSM-IV anxiety disorders, mood disorders, substance disorders, and impulse-control disorders. Results. Lifetime, 12-month, and 30-day prevalence estimates of DSM-IV GAD changed from 6\u00b71%, 2\u00b79%, and 1\u00b78% to 4\u00b72\u201312\u00b77%, 2\u00b72\u20135\u00b75%, and 1\u00b76\u20132\u00b76% when the duration requirement was changed from 6 months to 1\u201312 months. Cases with episodes of 1\u20135 months did not differ greatly from those with episodes of [ges ]6 months in onset, persistence, impairment, co-morbidity, parental GAD, or sociodemographic correlates. Conclusions. A large number of people suffer from a GAD-like syndrome with episodes of <6 months duration. Little basis for excluding these people from a diagnosis is found in the associations examined here.", "final_summary": "Generalized Anxiety Disorder (GAD) is a chronic condition that necessitates long-term treatment, but the exact duration varies. Lam (2006) underscores the chronic nature of GAD and suggests that treatment should be continued for at least six months, a conclusion supported by data from relapse-prevention studies. However, the need for more research into long-term treatment strategies is evident, as highlighted by Mah\u00e9 (2000) and Barlow (1992). Mah\u00e9 (2000) notes the absence of a reference drug for long-term treatment, while Barlow (1992) advocates for the development of more focused and efficient psychological treatments for GAD.\n\nRickels (2010) provides evidence that treatment with an antidepressant should be continued for at least 12 months. Goodman (2005) found that escitalopram was effective and well-tolerated for the treatment of GAD over an 8-week period, but the study does not explicitly suggest that longer-term treatment could be beneficial.\n\nKeller (2002) discusses the long-term clinical course of GAD, suggesting that GAD follows a chronic course with low rates of remission and moderate rates of relapse/recurrence. Kessler (2005), however, presents a different perspective, suggesting that a large number of people suffer from a GAD-like syndrome with episodes of less than 6 months duration.\n\nIn conclusion, while the exact duration of medication treatment for GAD varies, the consensus from the collected papers suggests that it is a chronic condition that requires long-term treatment. Further research is needed to determine the optimal duration and therapeutic modalities for treatment."}, {"query": "Give me references about gay culture and sexual harassment", "paper_list_string": "Fileborn 2012:\n\nTitle: Sexual violence and gay, lesbian, bisexual, trans, intersex, and queer communities\n\nAbstract: Although the vast majority of literature and research on sexual violence has focused on the experiences of heterosexual women, a burgeoning body of work has highlighted the occurrence of sexual violence within and against gay, lesbian, bisexual, trans, intersex, and queer (GLBTIQ) communities. Research suggests that members of GLBTIQ communities may face significant levels of abuse, harassment and violence (Leonard, Mitchell, Pitts, Patel, & Fox, 2008; NSW Attorney General\u2019s Department [NSW AGD], 2003). Experiencing sexual or physical violence, or other forms of abuse and victimisation, is often associated with a range of negative health and social outcomes\u2014such as post-traumatic stress disorder, depression, anxiety, suicide, and drug and alcohol abuse (Ryan & Rivers, 2003). However, it should also be recognised that members of GLBTIQ communities show great resilience in the face of social exclusion, discrimination and abuse (Scourfield, Roen, & McDermott, 2008).\n\n==\n\nHerek 1993:\n\nTitle: Documenting prejudice against lesbians and gay men on campus: the Yale Sexual Orientation Survey.\n\nAbstract: College and university communities recently have begun to confront the problems of harassment, discrimination, and violence against lesbians, gay men, and bisexual people on campus. A first step in responding to attacks against gay and bisexual people is to document their frequency and the forms that they take. The present article reports the methodology and results of a survey conducted at Yale University in 1986, which subsequently has been replicated on several other campuses. The Yale survey revealed that many lesbians, gay men, and bisexual people on campus lived in a world of secretiveness and fear. Although experiences of physical assault on campus were relatively infrequent, many respondents reported other forms of discrimination and harassment. A majority reported that they feared antigay violence and harassment on campus, and that such fears affected their behavior. Replications on other campuses have yielded similar results. Suggestions are offered for researchers who wish to conduct such a survey on their own campus.\n\n==\n\nHuebner 2004:\n\nTitle: Experiences of harassment, discrimination, and physical violence among young gay and bisexual men.\n\nAbstract: OBJECTIVES\nWe examined the 6-month cumulative incidence of anti-gay harassment, discrimination, and violence among young gay/bisexual men and documented their associations with mental health.\n\n\nMETHODS\nGay/bisexual men from 3 cities in the southwestern United States completed self-administered questionnaires.\n\n\nRESULTS\nThirty-seven percent of men reported experiencing anti-gay verbal harassment in the previous 6 months; 11.2% reported discrimination, and 4.8% reported physical violence. Men were more likely to report these experiences if they were younger, were more open in disclosing their sexual orientation to others, and were HIV positive. Reports of mistreatment were associated with lower self-esteem and increased suicidal ideation.\n\n\nCONCLUSIONS\nAbsent policies preventing anti-gay mistreatment, empowerment and community-building programs are needed for young gay/bisexual men to both create safe social settings and help them cope with the psychological effects of these events.\n\n==\n\nRabelo 2014:\n\nTitle: Two sides of the same coin: gender harassment and heterosexist harassment in LGBQ work lives.\n\nAbstract: This project investigated the incidence, interplay, and impact of gender- and sexuality-based harassment, as experienced by lesbian, gay, bisexual, and queer (LGBQ) employees in higher education. Unlike much queer empirical research, participants in this study were residents of noncoastal regions of the U.S. that are predominantly White, rural, and conservative (i.e., \"red states\"). They completed surveys about their harassment experiences (gender harassment-sexist, gender harassment-policing, and heterosexist harassment), perceived support systems (from supervisors and organizations), and job attitudes (job burnout, job stress, and job satisfaction). Results showed that gender harassment-both sexist and policing subtypes-rarely occurred absent heterosexist harassment, and vice versa. Harassment severity (experiencing moderate to high levels of all three harassment types) was significantly associated with greater levels of job burnout (both disengagement and exhaustion) and job dissatisfaction. Even infrequent experiences of harassment related to large increases in the \"threat\" variety of job stress (i.e., sense of feeling hassled and overwhelmed on the job). Additionally, employees who perceived the lowest organizational support reported the most harassment. We interpret results in light of research on organizational behavior and LGBQ psychology. Moreover, we discuss our findings in the context of Title VII, currently interpreted to protect against harassment based on gender, sex, and sex stereotyping, but not sexual orientation. Our results can inform several possible avenues of expanding gay civil rights in employment: broadening judicial interpretations of Title VII, passing new legislation (e.g., the Employment Non-Discrimination Act, or ENDA), and strengthening organizational supports and policies that protect against sexuality-based abuses.\n\n==\n\nD\u2019augelli 1992:\n\nTitle: Lesbian and Gay Male Undergraduates' Experiences of Harassment and Fear on Campus\n\nAbstract: Harassment and discrimination based on sexual orientation was studied in a sample of 121 undergraduate students between 19 and 22 years of age. Over three fourths of the respondents reported verbal abuse and over one fourth had been threatened with violence. Other students were the most frequent victimizers. Few reported victimization to authorities. Fear for one's personal safety on campus was related to frequency of personal harassment. The implications of harassment and discrimination on the development of young lesbians and gay men are discussed.\n\n==\n\nTaulke\u2010Johnson 2008:\n\nTitle: Moving beyond homophobia, harassment and intolerance: gay male university students\u2019 alternative narratives\n\nAbstract: This paper draws on a small-scale qualitative study of the lived experiences of gay male students in their final year of undergraduate study at a UK university. In contrast to the narratives almost universally reported in academic literature, anti-gay victimisation and harassment were not experienced or framed as dominant discourses in the stories of the six participants. I discuss how despite heterosexuality being the assumed, expected and compulsory discourse at university, the participants made positive sense of their experiences, and how through careful negotiation they were able to address, explore and engage with their (homo)sexual identities and orientation. I challenge the common and (mostly) unquestioned practices of defining gay students solely on the basis of their negative accounts of their experiences, labelling them all as victims, and locating the entire population within a pathologised framework. Instead I advocate a nuanced and balanced perspective which acknowledges the alternate and non-victimised accounts of gay students to provide a more inclusive, comprehensive, fuller and richer understanding of their lived experiences at university.\n\n==\n\nD'Augelli 1989:\n\nTitle: Lesbians' and gay men's experiences of discrimination and harassment in a university community\n\nAbstract: A survey of 125 lesbians and gay men in a university community was conducted to determine the incidence of discrimination, harassment, and violence. Nearly three fourths had experienced verbal abuse; 26% were threatened with violence; and 17% had personal property damaged. Students and roommates were most often those responsible. Most incidents were not reported to authorities, and many made changes in their daily routines to avoid harm. Over half of the sample feared for their personal safety; their fear was related to the amount of harassment and previous property damage. Men were more often victimized than women.\n\n==\n\nBiaggio 1997:\n\nTitle: Sexual Harassment of Lesbians in the Workplace\n\nAbstract: Abstract This article explores the literature addressing sexual harassment of lesbians and discusses the ways in which one form of prejudice, that against lesbians, contributes to this problem. The first section of the article discusses the lesbian experience of violence, harassment, and discrimination. In exploring the above experiences of lesbians it becomes apparent that lesbians are the victims of a specific form of prejudice\u2013homophobia. Homophobia impacts lesbians in two ways: (1) in the form of negative attitudes and denigrating or destructive acts; and (2) by means of actual discrimination, whether overt or subtle. Unlike sexism and racism, homophobia has had little public discussion, and the article concludes with a discussion of the impact of homophobia on lesbians in the workplace.", "final_summary": "The collected papers provide a comprehensive exploration of the intersection between gay culture and experiences of sexual harassment. \n\nFileborn (2012) highlights the significant levels of abuse, harassment, and violence faced by members of the gay, lesbian, bisexual, trans, intersex, and queer (GLBTIQ) communities. Similarly, Herek (1993) documents the prevalence of discrimination and harassment against gay and bisexual individuals on university campuses, with many respondents reporting fear of anti-gay violence. Huebner (2004) further corroborates these findings, reporting that anti-gay harassment and violence are associated with negative mental health outcomes, such as lower self-esteem and increased suicidal ideation.\n\nRabelo (2014) explores the dual nature of harassment experienced by LGBQ employees, encompassing both gender-based and heterosexist harassment. The study emphasizes the need for stronger organizational supports and policies to protect against sexuality-based abuses. D\u2019augelli (1992) and D'Augelli (1989) both focus on the experiences of gay and lesbian undergraduates, revealing high rates of verbal abuse, threats of violence, and fear for personal safety on campus.\n\nHowever, Taulke\u2010Johnson (2008) provides an alternative narrative, suggesting that not all gay students experience university life as a series of victimizations. The study emphasizes the importance of acknowledging the positive experiences and resilience of gay students, challenging the dominant discourse of victimhood.\n\nBiaggio (1997) specifically addresses the sexual harassment of lesbians in the workplace, highlighting the role of homophobia in contributing to this problem. The study underscores the need for greater public discussion and awareness of homophobia and its impacts.\n\nIn conclusion, the body of literature suggests that while harassment and discrimination are unfortunately common experiences within gay culture, it is crucial to also recognize the resilience of these communities and the importance of institutional support in mitigating these issues (Fileborn, 2012; Rabelo, 2014; Taulke\u2010Johnson, 2008)."}, {"query": "Cognitive Load Theory toward visualization", "paper_list_string": "Khalil 2005:\n\nTitle: Design of interactive and dynamic anatomical visualizations: the implication of cognitive load theory.\n\nAbstract: In improving the teaching and learning of anatomical sciences, empirical research is needed to develop a set of guiding principles that facilitate the design and development of effective dynamic visualizations. Based on cognitive load theory (CLT), effective learning from dynamic visualizations requires the alignment of instructional conditions with the cognitive architecture of learners and their levels of expertise. By improving the effectiveness and efficiency of dynamic visualizations, students will be able to be more successful in retaining visual information that mediates their understanding of complex and difficult aspects of anatomy. This theoretical paper presents instructional strategies generated by CLT and provides examples of some instructional implications of CLT on the design of dynamic visualizations for teaching and learning of anatomy.\n\n==\n\nHuang 2009:\n\nTitle: Measuring Effectiveness of Graph Visualizations: A Cognitive Load Perspective\n\nAbstract: Graph visualizations are typically evaluated by comparing their differences in effectiveness, measured by task performance such as response time and accuracy. Such performance-based measures have proved to be useful in their own right. There are some situations, however, where the performance measures alone may not be sensitive enough to detect differences. This limitation can be seen from the fact that the graph viewer may achieve the same level of performance by devoting different amounts of cognitive effort. In addition, it is not often that individual performance measures are consistently in favor of a particular visualization. This makes design and evaluation difficult in choosing one visualization over another. In an attempt to overcome the above-mentioned limitations, we measure the effectiveness of graph visualizations from a cognitive load perspective. Human memory as an information processing system and recent results from cognitive load research are reviewed first. The construct of cognitive load in the context of graph visualization is proposed and discussed. A model of user task performance, mental effort and cognitive load is proposed thereafter to further reveal the interacting relations between these three concepts. A cognitive load measure called mental effort is introduced and this measure is further combined with traditional performance measures into a single multi-dimensional measure called visualization efficiency. The proposed model and measurements are tested in a user study for validity. Implications of the cognitive load considerations in graph visualization are discussed.\n\n==\n\nKhalil 2005:\n\nTitle: Interactive and dynamic visualizations in teaching and learning of anatomy: a cognitive load perspective.\n\nAbstract: With the increasing use of computers in the classroom and the advancement of information technology, a requirement to investigate and evaluate different strategies for the presentation of verbal information in interactive and dynamic visualizations has risen to a high level of importance. There is a need for research efforts that apply cognitive load theory (CLT), cognitive learning strategies, and established principles of multimedia design to conduct empirical research that will add to our knowledge of designing and developing dynamic visualizations for teaching and learning anatomy. The impact of improved teaching and learning of anatomical sciences and the development of a set of guiding principles to facilitate the design and development of effective dynamic visualizations represent a significant achievement for medical education with wide application. This theoretical paper presents the foundations of CLT, cognitive learning strategies, and principles of multimedia design to guide the needed research on dynamic visualizations.\n\n==\n\nCastro-Alonso 2019:\n\nTitle: Instructional Visualizations, Cognitive Load Theory, and Visuospatial Processing\n\nAbstract: There are basically two formats used in instructional visualizations, namely, static pictures and dynamic visualizations (e.g., animations and videos). Both can be engaging and fun for university students in the fields of health and natural sciences. However, engagement by itself is not always conducive to learning. Consequently, teachers, lecturers, and instructional designers need to utilize the cognitive processing advantages of visualizations as well as engagement to achieve full instructional effectiveness. A cognitive processing focus has outlined many ways in which instructional visualization can be optimized. Specifically, cognitive load theory and the cognitive theory of multimedia learning are two research paradigms that provide several methods for directing the design of visualizations by considering how learners process visuospatial information. In this chapter, we describe five methods based on these cognitive theories: (a) the split attention effect and spatial contiguity principle, (b) the modality effect, (c) the redundancy effect and coherence principle, (d) the signaling principle, and (e) the transient information effect. For each of these effects, examples of applications for education in health and natural sciences are provided, where the influence of visuospatial processing is also considered. We end this chapter by discussing instructional implications for science education and providing future directions for research.\n\n==\n\nMurtianto 2022:\n\nTitle: Cognitive Load Theory on Virtual Mathematics Laboratory: Systematic Literature Review\n\nAbstract: The primary goal of cognitive load theory is to improve the learning of complex cognitive tasks by transforming current scientific knowledge on how cognitive structures and processes are organized into guidelines for instructional design. Cognitive load theory assumes that the bottleneck for acquiring new secondary biological knowledge is the limited working memory capacity. In the ideal situation, the working memory resources required for learning do not exceed the available resources. Despite this, in reality, there will often be a high cognitive load, or even \u201doverload,\u201d for two reasons. First, dealing with interactive information elements in complex cognition imposes a high intrinsic working memory load. Second, learners also have to use working memory resources for activities that are extraneous to performing and learning tasks, that is, activities that are not productive for learning. Virtual Laboratory is a form of animation that can visualize abstract phenomena or complex experiments in natural laboratories to increase learning activities and develop problem-solving skills. A virtual math laboratory was created to optimize dual coding memory, namely verbal and audio learning. The investigation tracked the approved reporting Items for Systematics Reviews and Meta-Analysis (PRISMA) guidelines, illustrating the outcomes of the literature searches and articles selection process. It is used to provide that the selection process is replicable and transparent. We accomplished a computerized bibliometric analysis from 2002-2022 for articles retrieved from the SCOPUS database. Data were collected in July 2022. \nKeywords: cognitive load theory, virtual laboratory, mathematics education\n\n==\n\nCook 2006:\n\nTitle: Visual representations in science education: The influence of prior knowledge and cognitive load theory on instructional design principles\n\nAbstract: Visual representations are essential for communicating ideas in the science classroom; however, the design of such representations is not always beneficial for learners. This paper presents instructional design considerations providing empirical evidence and integrating theoretical concepts related to cognitive load. Learners have a limited working memory, and instructional representations should be designed with the goal of reducing unnecessary cognitive load. However, cognitive architecture alone is not the only factor to be considered; individual differences, especially prior knowledge, are critical in determining what impact a visual representation will have on learners' cognitive structures and processes. Prior knowledge can determine the ease with which learners can perceive and interpret visual representations in working memory. Although a long tradition of research has compared experts and novices, more research is necessary to fully explore the expert\u2013novice continuum and maximize the potential of visual representations. \u00a9 2006 Wiley Periodicals, Inc. Sci Ed90:1073\u20131091, 2006\n\n==\n\nZagermann 2016:\n\nTitle: Measuring Cognitive Load using Eye Tracking Technology in Visual Computing\n\nAbstract: In this position paper we encourage the use of eye tracking measurements to investigate users' cognitive load while interacting with a system. We start with an overview of how eye movements can be interpreted to provide insight about cognitive processes and present a descriptive model representing the relations of eye movements and cognitive load. Then, we discuss how specific characteristics of human-computer interaction (HCI) interfere with the model and impede the application of eye tracking data to measure cognitive load in visual computing. As a result, we present a refined model, embedding the characteristics of HCI into the relation of eye tracking data and cognitive load. Based on this, we argue that eye tracking should be considered as a valuable instrument to analyze cognitive processes in visual computing and suggest future research directions to tackle outstanding issues.\n\n==\n\nAnderson 2011:\n\nTitle: A User Study of Visualization Effectiveness Using EEG and Cognitive Load\n\nAbstract: Effectively evaluating visualization techniques is a difficult task often assessed through feedback from user studies and expert evaluations. This work presents an alternative approach to visualization evaluation in which brain activity is passively recorded using electroencephalography (EEG). These measurements are used to compare different visualization techniques in terms of the burden they place on a viewer's cognitive resources. In this paper, EEG signals and response times are recorded while users interpret different representations of data distributions. This information is processed to provide insight into the cognitive load imposed on the viewer. This paper describes the design of the user study performed, the extraction of cognitive load measures from EEG data, and how those measures are used to quantitatively evaluate the effectiveness of visualizations.", "final_summary": "Cognitive Load Theory (CLT) plays a significant role in the design and effectiveness of visualizations in various fields, including anatomy, graph visualizations, and mathematics education. \n\nKhalil (2005) emphasizes the importance of aligning instructional conditions with the cognitive architecture of learners to improve the effectiveness of dynamic visualizations in anatomy. Similarly, another study by Khalil (2005) underscores the need for research that applies CLT and multimedia design principles to develop effective dynamic visualizations for anatomy education.\n\nHuang (2009) proposes measuring the effectiveness of graph visualizations from a cognitive load perspective. The study introduces a cognitive load measure called mental effort, which, when combined with traditional performance measures, forms a multi-dimensional measure called visualization efficiency.\n\nCastro-Alonso (2019) discusses five methods based on cognitive theories for optimizing instructional visualization. These methods consider how learners process visuospatial information and provide instructional implications for science education.\n\nMurtianto (2022) applies CLT to the design of a virtual mathematics laboratory. The study highlights the importance of considering working memory resources and the cognitive load imposed by complex cognition when designing instructional tools.\n\nCook (2006) emphasizes the influence of prior knowledge and cognitive load on the design of visual representations in science education. The study suggests that prior knowledge can determine how easily learners perceive and interpret visual representations.\n\nZagermann (2016) encourages the use of eye tracking measurements to investigate users' cognitive load during human-computer interaction. The study presents a refined model that embeds the characteristics of HCI into the relation of eye tracking data and cognitive load.\n\nAnderson (2011) proposes an alternative approach to visualization evaluation using electroencephalography (EEG) to passively record brain activity and measure the cognitive load imposed on the viewer.\n\nIn conclusion, these studies collectively highlight the importance of considering cognitive load in the design and evaluation of visualizations. They suggest that effective visualizations should align with the cognitive architecture of learners, consider the cognitive load imposed by complex cognition, and take into account individual differences in prior knowledge and cognitive processes (Khalil, 2005; Huang, 2009; Castro-Alonso, 2019; Murtianto, 2022; Cook, 2006; Zagermann, 2016; Anderson, 2011)."}, {"query": "what is design thinking in teachers developing digital educational games?", "paper_list_string": "Jan 2018:\n\nTitle: Understanding Teachers' Design Thinking in Designing Game-Based Activities\n\nAbstract: There is a need to reframe teachers' roles from content area experts to that of learning experience designers because of 21st century teaching challenges. As learning experience designers, teachers help students develop 21st century competencies via guided cognitive and social participation in designed learning activities such as games and gamified activities. In this qualitative case study, we explore teachers' design thinking in designing a lesson plan that involves the use of a card game designed for complex system understanding. Six teachers' thoughts about learning activity design are unpacked via the following activities: playing a card game, crafting a game-based lesson plan, and reporting their design thinking via semi-structural interviews. We discuss similarities in the teachers' views on game-based learning and the structure of their designed lessons, as well as implications of the study. This baseline study helps us map out how teachers think about learning experience design. Such understanding is critical for developing teachers as designers.\n\n==\n\nAnnetta 2019:\n\nTitle: Teaching Technology Design: Practicing Teachers Designing Serious Educational Games\n\nAbstract: This chapter operationally defines the term design thinking and gives the historical and theoretical basis of design thinking. We further review current practices of design thinking in education. In a study of practicing K-12 science and instructional technology teachers designing Serious Educational Games (SEGs) (Annetta, 2008), this chapter illustrates how teaching and learning design changes how teachers think. It is the ultimate goal that this change in teachers\u2019 design thinking will enable teachers to transfer their approach to their students so their students learn to design using technologies beyond just Serious Educational Games.\n\n==\n\nLlorent-Vaquero 2022:\n\nTitle: Digital Creativity through Design Thinking in teacher training\n\nAbstract: The rapid technological and cultural changes in today's society have inevitably affected the field of education. Thus, future teachers must be prepared to face the challenges of this changing society. A powerful tool for adapting to change and resolving conflicts is creativity. This paper shows how Design Thinking can contribute to foster creativity in students of degrees related to the educational world. Specifically, it presents an experience of teaching innovation in which the Design Thinking process was used for the development of digital educational projects in three degrees of education at the University of Ja\u00e9n. In order to explore the effects of the methodology on the students, a quantitative study was carried out to collect the students' perceptions through a questionnaire. A total of 163 students from the education degrees participated in the study: 65 from the Degree in Primary Education, 61 from the Degree in Early Childhood Education and 37 from the Degree in Social Education. The results show an increase in creativity, along with other dimensions analyzed, in the development of creative digital projects after the Design Thinking process. Therefore, it is concluded that it is interesting to introduce this creative process in teacher training as a preparation for the future educational challenges they will have to face.\n\n==\n\nArtym 2016:\n\nTitle: Pre-Service Teachers Designing and Constructing \"Good Digital Games\".\n\nAbstract: There is a growing interest in the application of digital games to enhance learning across many educational levels. This paper investigates pre-service teachers\u2019 ability to operationalize the learning principles that are considered part of a good digital game (Gee, 2007) by designing digital games in Scratch. Forty pre-service teachers, enrolled in an optional educational technology course, designed and constructed their own digital games in an authentic learning context. The course was structured to prepare pre-service teachers to use game design and construction in their future pedagogical practice. These pre-service teachers had various levels of game-playing experience, but little-to-no previous game-design/building experience. To evaluate the digital games, we created the Game Design Assessment Survey, which determined the degree to which a core set of learning principles, identified from the literature, were present in the digital games constructed by the pre-service teachers. Results suggested that pre-service teachers were generally unaware of the learning principles that should be included in the design of a good digital game, but were familiar with quality principles of interface usability. In addition, no relationship was found between the amount of time pre-service teachers played digital games and their ability to design and construct a good game.\n\n==\n\nFrossard 2015:\n\nTitle: Teachers Designing Learning Games\n\nAbstract: Abstract Creativity has become a key educational objective. How can game-based learning enhance creative pedagogies? This chapter proposes an approach in which teachers become game designers. It provides a model which analyzes creativity according to three dimensions: process, product, and teaching. We describe practical experiences in which teachers designed and applied their own learning games. Results highlight that game design promotes teaching practices that foster students\u2019 creativity.\n\n==\n\nAn 2017:\n\nTitle: Examining the Characteristics of Digital Learning Games Designed by In-service Teachers\n\nAbstract: In\ufefforder\ufeffto\ufeffbetter\ufeffunderstand\ufeffteachers\u2019\ufeffperspectives\ufeffon\ufeffthe\ufeffdesign\ufeffand\ufeffdevelopment\ufeffof\ufeffdigital\ufeffgamebased\ufeff learning\ufeff environments,\ufeff this\ufeff study\ufeff examined\ufeff the\ufeff characteristics\ufeff of\ufeff digital\ufeff learning\ufeff games\ufeff designed\ufeffby\ufeffteachers.\ufeffIn\ufeffaddition,\ufeffthis\ufeffstudy\ufeffexplored\ufeffhow\ufeffgame\ufeffdesign\ufeffand\ufeffpeer\ufeffcritique\ufeffactivities\ufeff influenced\ufefftheir\ufeffperceptions\ufeffof\ufeffdigital\ufeffgame-based\ufefflearning\ufeffenvironments\ufeffand\ufefflearning\ufeffthrough\ufeffgame\ufeff design.\ufeffQualitative\ufeffdata\ufeffwere\ufeffcollected\ufefffrom\ufefffifty\ufeffgame\ufeffdesign\ufeffdocuments\ufeffand\ufeffparticipant\ufeffresponses\ufeff to\ufeff reflection\ufeffquestions.\ufeffThe\ufeffanalysis\ufeffof\ufeffgame\ufeffdesign\ufeffdocuments\ufeff showed\ufeff that\ufeff the\ufeffmajority\ufeffof\ufeff the\ufeff participants\ufeffdesigned\ufeffimmersive\ufeffgame-based\ufefflearning\ufeffenvironments\ufeffwhere\ufeffplayers\ufeffare\ufeffrequired\ufeffto\ufeff use\ufeffhigher\ufefforder\ufeffthinking\ufeffand\ufeffreal-world\ufeffskills\ufeffas\ufeffwell\ufeffas\ufeffacademic\ufeffcontent\ufeffto\ufeffcomplete\ufeffmissions\ufeff or\ufeffsolve\ufeffproblems.\ufeffThe\ufeffresults\ufeffof\ufeffthis\ufeffstudy\ufeffprovide\ufeffimportant\ufeffimplications\ufefffor\ufeffteacher\ufeffprofessional\ufeff development\ufeffas\ufeffwell\ufeffas\ufefffor\ufeffeducational\ufeffgame\ufeffdevelopment. KEywoRDS Challenges, Digital Learning Games, Educational Game Design, Engagement, Game Design Strategies, GameBased Learning, Scaffolding, Teacher Perceptions, Teachers as Game Designers\n\n==\n\nSantos 2020:\n\nTitle: Adaptando o Design Thinking para a Defini\u00e7\u00e3o e Desenvolvimento de um Jogo Educacional N\u00e3o Digital no Ensino de Gerenciamento de Riscos\n\nAbstract: An alternative to traditional teaching is the use of educational games that can motivate students. However, there are still difficulties in the development of educational games when selecting their content and when designing their dynamics. This paper presents the experience report of the adaptation of the design thinking methodology in the development process of the non-digital educational game: Risking (Arriscando). Design Thinking combines the focus on the end user with multidisciplinary collaboration and iterative improvement to produce innovative products. A course content analysis was applied to define the subject of the game, as well as brainstorming and validation meetings with the students. Resumo. Uma alternativa para o ensino tradicional \u00e9 o uso de jogos educacionais que podem motivar os alunos. No entanto, ainda h\u00e1 dificuldade no desenvolvimento de jogos educacionais ao selecionar os conte\u00fados presentes e ao projetar a din\u00e2mica do jogo. Este trabalho apresenta o relato de experi\u00eancia da adapta\u00e7\u00e3o da metodologia design thinking no processo de desenvolvimento do jogo educacional n\u00e3o digital: Arriscando. O Design Thinking combina o foco no usu\u00e1rio final com colabora\u00e7\u00e3o multidisciplinar e aprimoramento iterativo para produzir produtos inovadores. Foi aplicada uma an\u00e1lise de conte\u00fados de disciplinas para definir o assunto do jogo, assim como reuni\u00f5es de brainstorming e valida\u00e7\u00e3o com os discentes.\n\n==\n\nBowden 2020:\n\nTitle: Designing and assessing digital games in a classroom: an emerging culture of critique\n\nAbstract: ABSTRACT This study explores situated practices of game design critique in a Swedish 4th grade classroom. The analyses are based on video recordings of peer feedback activities within the context of a project on computational thinking using the software Scratch. Drawing on ethnomethodology and conversation analysis, the interactional and collaborative accomplishment of design critique is examined, focusing on how the participants make relevant norms and values concerning what constitutes a \u2018good\u2019 game. The results of the study show that the children and their teacher orient to different themes that concern aesthetic, functional, and ethical aspects of the games and the design process, at the same time as a moral order in and for the conduct of critique is accomplished in interaction. The study sheds light on the emergence of a local culture of critique as the children learn to formulate and respond to peer feedback, thus negotiating and developing digital literacy.", "final_summary": "Design thinking in teachers' development of digital educational games is a multifaceted process that involves creativity, understanding of learning principles, and iterative improvement. \n\nJan (2018) emphasizes the need for teachers to transition from content experts to learning experience designers, using games as a tool to develop 21st-century competencies in students. Similarly, Annetta (2019) highlights the transformative power of design thinking in changing how teachers approach teaching and learning design, particularly in the context of Serious Educational Games (SEGs).\n\nThe role of design thinking in fostering creativity in teacher training is underscored by Llorent-Vaquero (2022), who found that the use of Design Thinking in the development of digital educational projects increased creativity among education degree students. Frossard (2015) also supports this, suggesting that game design promotes teaching practices that foster students' creativity.\n\nHowever, the ability of teachers to operationalize learning principles in game design is not always straightforward. Artym (2016) found that pre-service teachers were generally unaware of the learning principles that should be included in the design of a good digital game, despite being familiar with principles of interface usability.\n\nSantos (2020) highlights the importance of designing immersive game-based learning environments that require the use of higher-order thinking, real-world skills, and academic content. They also emphasize the importance of iterative improvement and focus on the end user in the game design process.\n\nFinally, Bowden (2020) explores the emergence of a culture of critique in a classroom setting, where students learn to formulate and respond to peer feedback, thus negotiating and developing digital literacy.\n\nIn conclusion, design thinking in teachers' development of digital educational games involves a shift in teaching approach, fostering creativity, understanding and applying learning principles, and creating a culture of critique and iterative improvement (Jan, 2018; Annetta, 2019; Llorent-Vaquero, 2022; Frossard, 2015; Artym, 2016; Santos, 2020; Bowden, 2020)."}, {"query": "The Gross Domestic Product (GDP) in Vietnam was worth 408.80 billion US dollars in 2022, according to official data from the World Bank. The GDP value of Vietnam represents 0.18 percent of the world economy.", "paper_list_string": "Thai 2020:\n\nTitle: Vietnam Economic Issue: Looking from Enterprise Sector, Export and Import Situations\n\nAbstract: At first glance, Vietnam has relatively high growth rate in the region and in the world, the average growth in the period of 2011 - 2018 is about 6.2%. As soon as the Covid 19 pandemic became active, causing most countries to have a negative GDP growth rate, but Vietnam's GDP growth in the first quarter was still 3.82%. \nThis study tried to describe the situation in the context of the current economic situation in Viet Nam through the production results of the enterprise sector, import and export. \nThe study used official data sources from Vietnam General Statistics Office.\n\n==\n\nKalra 2015:\n\nTitle: Vietnam: The Global Economy and Macroeconomic Outlook\n\nAbstract: After almost a decade of high growth, Vietnam\u2019s growth rate fell during 2011\u201313. Since 2001, the country has also experienced two bouts of high inflation, booms and busts in equity and real estate markets, and episodes of large capital inflows and outflows. Against the backdrop of the global economy, this paper provides an account of macroeconomic developments in Vietnam during 2011 to 2013, examines the imbalances that came to a head in 2011, the macroeconomic stabilization achieved during 2012 to 2014, and the outlook and challenges going forward. The paper concludes that successfully designing and implementing a broad set of policies \u2014 staying the course on macroeconomic stabilization, while accelerating the pace of structural reform significantly, and integrating into the global economy \u2014 will allow Vietnam to further advance the remarkable gains that it has already made in poverty alleviation and achieving its Millenium Development Goals.\n\n==\n\nNguyen 2020:\n\nTitle: Vietnam a country in transition: health challenges\n\nAbstract: Vietnam is experiencing a significant change in its economic conditions, such a change has been accompanied by significant changes in the pattern of morbidity and mortality. The country\u2019s Gross Domestic Product (GDP) is increasing at a fast pace: from US$31 176 000 000 in 2000 to US$241 272 000 000 in 2018; and the country has experienced remarkable gains in combatting poverty and hunger. Gross National Income per capita has increased from US$110 in 2000 to US$2400 in 2018.1 At the moment the country is ranked in the low middle income countries by the world population review. Parallel with the economic growth, the country has experienced a fast and wide process of urbanisation. Over the past 20 years, more and more Vietnamese have moved from the rural areas to large urban areas with a projected urbanisation rate of 40% by early 2020. The rapid urbanisation has been accompanied by environment deterioration, increased air pollution, deterioration of lifestyle habits (including physical inactivity and diet changes) and a big strain in healthcare services.2\u20134 This remarkable financial success has been accompanied by a significant reduction in communicable diseases that have been substituted in the ranking of major causes of mortality and morbidity by non-communicable diseases (NCDs), as shown in figure 1. The burden of communicable diseases in Vietnam has significantly diminished from representing 38% and 33% of the morbidity and mortality burden in 1996 to a projected 18% and 6%, respectively, in 2026. The projected mortality burden of 6% for communicable diseases is significantly lower than the projected average burden for other countries in South East Asia (SEA). The successful efforts in combatting communicable diseases has been the main reason for the observed increase in life expectancy. The average life expectancy in Vietnam in 2017 was 79.2 years for women \u2026\n\n==\n\nTarp 2002:\n\nTitle: Trade and Income Growth in Vietnam: Estimates from a New Social Accounting Matrix\n\nAbstract: Economic reforms and greater outward orientation are giving rise to extensive structural change in the Vietnamese economy. Because of the leverage that global markets can exert on an emerging economy, such adjustments will be particularly significant in the composition of domestic supply and demand. As domestic protection levels are reduced and external market access increases, trade growth and shifting trade patterns will have pervasive effects on income distribution in Vietnam. In this paper, we use a newly estimated Vietnam social accounting matrix to elucidate the links between trade and income in the country. With matrix decomposition methods, we show how the Vietnamese economy propagates the direct effects of external demand across the spectrum of domestic activities, factors, and households. This detailed analysis provides a blueprint for policies to improve economic participation of activities and households with relatively weak linkages to the rest of the economy.\n\n==\n\nGlewwe 2004:\n\nTitle: Economic Growth, Poverty, and Household Welfare in Vietnam\n\nAbstract: Viet Nam is an economic success story - it transformed itself from a country in the 1980s as one of the poorest in the world, to a country in the 1990s with one of the world's highest growth rates. With the adoption of a new market-oriented policies, Viet Nam averaged an economic growth rate of 8 percent per year from 1990 to 2000, a growth rate accompanied by a large reduction in poverty, stemming from significant increases in school enrollment, and a rapid decrease in child malnutrition. The book uses an unusually rich set of macroeconomic, and household survey data, to examine several topics: the causes of the economic turnaround, and prospects for future growth; the impact of economic growth on household welfare, as measured by consumption expenditures, health, education, and other socioeconomic indicators; and, the nature of poverty in Viet Nam, and the effectiveness of government policies, intended to reduce same. Although the country's past achievements are impressive, future progress is by no means ensured.\n\n==\n\nBinh 2013:\n\nTitle: APPLYING GRAVITY MODEL TO ANALYZE TRADE ACTIVITIES OF VIETNAM\n\nAbstract: This paper applies gravity model in order to analyze bilateral trade activities between Vietnam and 60 countries from 2000 to 2010. We exploited the panel data on international trade of Vietnam taken from the data banks of International Trade Centre, International \u201eMonetary Fund and World Bank. The estimated results reveal that economic size of Vietnam, economic size and market size of foreign partners, distance and culture have huge effects on bilateral trade flows between Vietnam and these 60 countries. By applying method of speed of convergence, we also find out that Vietnam has trade potential especially with some new markets such as Africa and Western Asia.\n\n==\n\nNghiep 2000:\n\nTitle: Measuring the Impact of Doi Moi on Vietnam\u2019s Gross Domestic Product\n\nAbstract: In 1986 a wide range of policy measures, known as Doi Moi, was introduced to promote Vietnam's transition to a market economy. This paper represents the first attempt to measure the effect of Doi Moi on Vietnam's GDP. In the paper the level of GDP actually reached is compared with the level that would have been reached had the policy not been implemented, i.e. without the improvements in productivity and the increases in investment ratio that can be directly attributed to Doi Moi. Cross-time changes in GDP were depicted by a production function of capital stock, economically active labour force and technical progress. It was found that, after a time lag, Doi Moi appeared to have a significant positive effect on productivity, which by 1998 accounted for a 42% increase in GDP.\n\n==\n\nShultz 1994:\n\nTitle: Vietnam\n\nAbstract: Vietnam is an evolving political economy and marketing system. Since the implementation of Doi Moi, the 1986 policy to invoke a shift from central economic planning to a more market-oriented system, the country has made extraordinary progress on several socioeconomic indicators. Some observers contend Vietnam is a development model; others suggest the country still has numerous challenges to overcome before it can reach its development goals. This article provides an overview of Vietnam\u2019s socioeconomic development; it introduces eight refereed articles and four commentaries that comprise the scholarly contributions to the first special issue of the Journal of Macromarketing to feature research on a single country. Vietnam is that country. Contributors provide detailed research, analysis, and reflection on the interplay of markets, marketing, and society. Topics studied include system complexity and entrepreneurship, retailing evolution, consumption dynamics and societal wellness, family policy and consumption, education and human resource development, living standards and quality of life, ethical/unethical foreign direct investment, ritualistic consumption, and marketing, trade and protectionism, land policy and environmental sustainability, and implications for Vietnam\u2019s economic and geopolitical future.", "final_summary": "The collected papers provide a comprehensive overview of Vietnam's economic growth and its Gross Domestic Product (GDP) value. \n\nVietnam has experienced a significant economic transformation over the past few decades. Thai (2020) highlights that despite the global impact of the Covid-19 pandemic, Vietnam maintained a positive GDP growth rate, demonstrating the resilience of its economy. This growth is further supported by Nguyen (2020), who notes a rapid increase in Vietnam's GDP from 2000 to 2018, paralleling the country's urbanization and economic transition.\n\nThe country's economic success is not without challenges. Kalra (2015) discusses the fluctuations in Vietnam's growth rate and the macroeconomic imbalances that emerged in 2011. Despite these challenges, the country achieved macroeconomic stabilization between 2012 and 2014, suggesting a robust and adaptable economy.\n\nTarp (2002) and Binh (2013) both emphasize the role of trade in Vietnam's economic growth. Tarp (2002) uses a social accounting matrix to demonstrate how external demand influences domestic activities, factors, and households. Binh (2013) applies a gravity model to analyze Vietnam's bilateral trade activities, revealing the significant impact of Vietnam's economic size, market size, and culture on its trade flows.\n\nGlewwe (2004) and Nghiep (2000) both attribute Vietnam's economic success to the implementation of market-oriented policies. Glewwe (2004) notes the significant reduction in poverty and increase in school enrollment that accompanied Vietnam's high growth rates. Nghiep (2000) quantifies the impact of the Doi Moi policy on Vietnam's GDP, finding a 42% increase in GDP by 1998 due to improved productivity and increased investment ratios.\n\nIn conclusion, the papers collectively illustrate the remarkable economic growth of Vietnam, driven by market-oriented policies, trade activities, and resilience in the face of global challenges. Despite fluctuations and imbalances, Vietnam's GDP has consistently grown, contributing to poverty reduction and improved living standards. The country's economic journey offers valuable insights into the interplay of policy, trade, and macroeconomic factors in driving economic growth."}, {"query": "what are Value sensitive design methods?", "paper_list_string": "Friedman 2018:\n\nTitle: A Survey of Value Sensitive Design Methods\n\nAbstract: Value sensitive design is a theoretically grounded approach to the designof technology that accounts for human values in a principled andsystematic manner throughout the design process. In this article weprovide a survey of 14 value sensitive design methods: 1 direct andindirect stakeholder analysis; 2 value source analysis; 3 co-evolutionof technology and social structure; 4 value scenario; 5 value sketch;6 value-oriented semi- structured interview; 7 scalable informationdimensions; 8 value-oriented coding manual; 9 value-oriented mockup,prototype, or field deployment; 10 ethnographically informed inquiryregarding values and technology; 11 model of informed consentonline; 12 value dams and flows; 13 value sensitive action-reflectionmodel; and 14 Envisioning Cards TM. Each of these methods is honedto the investigation of values in technology, serving such purposesas stakeholder identification and legitimation, value representation andelicitation, and values analysis. While presented individually, the methodsare intended to be integrated in a robust value sensitive designprocess. The survey article begins with a brief summary of value sensitivedesign methodology and theoretical constructs. We next providean overview of the 14 methods. Then, we turn to a broader discussion ofvalue sensitive design practice, focussing on some methodological strategiesand heuristics to support skillful value sensitive design practice.Following the broad discussion of practice, we illustrate one method inaction-value scenarios-providing details on its range of purposes andcontexts. We conclude with reflections on core characteristics of valuesensitive design methodology, and heuristics for innovation.\n\n==\n\nFriedman 2002:\n\nTitle: Value Sensitive Design: Theory and Methods\n\nAbstract: Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three case studies. The first study concerns information and control of web browser cookies, implicating the value of informed consent. The second study concerns using high-definition plasma displays in an office environment to provide a virtual window to the outside world, implicating the values of physical and psychological well-being and privacy in public spaces. The third study concerns an integrated land use, transportation, and environmental simulation system to support public deliberation and debate on major land use and transportation decisions, implicating the values of fairness (and specifically freedom from bias), accountability, and support for the democratic process, as well as a highly diverse range of values that might be held by different stakeholders, such as environmental sustainability, opportunities for business expansion, or walkable neighborhoods. We conclude with direct and practical suggestions for how to engage in Value Sensitive Design.\n\n==\n\nFriedmanBatya 2017:\n\nTitle: A Survey of Value Sensitive Design Methods\n\nAbstract: Value sensitive design is a theoretically grounded approach to the designof technology that accounts for human values in a principled andsystematic manner throughout the design process. In this art...\n\n==\n\nDantec 2009:\n\nTitle: Values as lived experience: evolving value sensitive design in support of value discovery\n\nAbstract: The Value Sensitive Design (VSD) methodology provides a comprehensive framework for advancing a value-centered research and design agenda. Although VSD provides helpful ways of thinking about and designing value-centered computational systems, we argue that the specific mechanics of VSD create thorny tensions with respect to value sensitivity. In particular, we examine limitations due to value classifications, inadequate guidance on empirical tools for design, and the ways in which the design process is ordered. In this paper, we propose ways of maturing the VSD methodology to overcome these limitations and present three empirical case studies that illustrate a family of methods to effectively engage local expressions of values. The findings from our case studies provide evidence of how we can mature the VSD methodology to mitigate the pitfalls of classification and engender a commitment to reflect on and respond to local contexts of design.\n\n==\n\nFriedman 2013:\n\nTitle: Value Sensitive Design and Information Systems\n\nAbstract: Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three case studies. The first study concerns information and control of web browser cookies, implicating the value of informed consent. The second study concerns using high-definition plasma displays in an office environment to provide a \u201cwindow\u201d to the outside world, implicating the values of physical and psychological well-being and privacy in public spaces. The third study concerns an integrated land use, transportation, and environmental simulation system to support public deliberation and debate on major land use and transportation decisions, implicating the values of fairness, accountability, and support for the democratic process, as well as a highly diverse range of values that might be held by different stakeholders, such as environmental sustainability, opportunities for business expansion, or walkable neighborhoods. We conclude with direct and practical suggestions for how to engage in Value Sensitive Design.\n\n==\n\nMithun 2018:\n\nTitle: The Realism of Value Sensitive Design on User Interface Development\n\nAbstract: Value Sensitive Design (VSD) is the theoretical approach that applied to technology design, in particular for Human-Computer Interaction (HCI). The User Interface (UI) design is one of the concerns in HCI, being multidisciplinary, it ensures the user's satisfaction and perception of technology which follows during the development process. The VSD method takes human values in a comprehensive manner throughout three investigations called tripartite methodology, and the investigations applied as the techniques to investigate the requirements of the development from various stakeholders values. Since variations of stakeholders, VSD method follows the designers, direct and indirect stakeholders to take out required values into the development. In spite of the widely used VSD method undetermined the values that to be considered on design as designers, direct or indirect stakeholders. The argument considering the values has been researching since the concept applied to HCI design which indicates the method limitation. In this research considered two values, designers and stakeholders and the key concern to observe the effects and realism of the applications of VSD method into the design of user interface also reviewed earlier researches on the application of VSD. The analysis through the development of hospital management system interface to investigate whose values to be considered in the HCI design. Performed user survey based on two criteria that evaluates the variations of stakeholders values which effects on the user interface design that is the contribution of the research, and it enlightens the concept of values of VSD should considered in the future user interface design.\n\n==\n\nYoo 2013:\n\nTitle: A value sensitive action-reflection model: evolving a co-design space with stakeholder and designer prompts\n\nAbstract: We introduce a design method for evolving a co-design space to support stakeholders untrained in design. Specifically, the purpose of the method is to expand and shape a co-design space so that stakeholders, acting as designers, focus not only on the form and function of a tool being envisioned but also on the social context of its use and values that lie with individuals, groups, and societies. The method introduces value sensitive stakeholder prompts and designer prompts into a co-design process, creating a particular kind of reflection-on-action cycle. The prompts provide a means for bringing empirical data on values and theoretical perspective into the co-design process. We present the method in terms of a general model, the Value Sensitive Action-Reflection Model; place the model within discourse on co-design spaces; and illustrate the model with a discussion of its application in a lo-fi prototyping activity around safety for homeless young people. We conclude with reflections on the model and method.\n\n==\n\nManders-Huits 2009:\n\nTitle: The Need for a Value-Sensitive Design of Communication Infrastructures\n\nAbstract: In this chapter, we will discuss Value-Sensitive Design as a general approach to thinking about design in the TUC \u2013 Towards Ultrafast Communication \u2013 project from an ethical perspective. We begin with the historical context by describing developments both in information technology and ethics that preceded Value-Sensitive Design. Both ethics and computer science demonstrate a converging development towards interest in the relation between values and technical design, respectively coined by us as \u201cThe Value Turn\u201d in (information) technology and \u201cThe Design Turn\u201d in ethics. After a sketch of the development and history of Value-Sensitive Design we will discuss the methodology of both Value-Sensitive Design and a related approach called \u201cValues at Play\u201d. Their methodology consists of three modes of inquiry, respectively the conceptual, empirical, and technical mode and the discovery, translation, and verification phase. These three parts of the methodology are used to deal with concerns arising from moral theory with respect to the way technology affects moral practices in direct and indirect ways. Towards the end of the chapter we will discuss in general how Value-Sensitive Design applies to the TUC project.", "final_summary": "Value Sensitive Design (VSD) is a theoretical approach to technology design that systematically and comprehensively incorporates human values throughout the design process (Friedman 2002, Friedman 2013, Friedman 2018). It employs a tripartite methodology, consisting of conceptual, empirical, and technical investigations (Friedman 2002, Friedman 2013, Friedman 2018).\n\nThe VSD methodology involves direct and indirect stakeholder analysis, value source analysis, co-evolution of technology and social structure, value scenario, value sketch, value-oriented semi-structured interview, scalable information dimensions, value-oriented coding manual, value-oriented mockup, prototype, or field deployment, ethnographically informed inquiry regarding values and technology, model of informed consent online, value dams and flows, value sensitive action-reflection model, and Envisioning Cards TM (Friedman 2018).\n\nHowever, the VSD methodology has been critiqued for its limitations due to value classifications, inadequate guidance on empirical tools for design, and the ways in which the design process is ordered (Dantec 2009). To overcome these limitations, it has been suggested that the VSD methodology should mature to effectively engage local expressions of values and engender a commitment to reflect on and respond to local contexts of design (Dantec 2009).\n\nIn the context of User Interface (UI) design, VSD takes into account the values of designers, direct and indirect stakeholders to determine the requirements of the development (Mithun 2018). The VSD method has been applied to the design of a hospital management system interface to investigate whose values should be considered in the HCI design (Mithun 2018).\n\nThe Value Sensitive Action-Reflection Model is a design method for evolving a co-design space to support stakeholders untrained in design (Yoo 2013). It introduces value sensitive stakeholder prompts and designer prompts into a co-design process, creating a particular kind of reflection-on-action cycle (Yoo 2013).\n\nIn conclusion, VSD is a comprehensive and systematic approach to technology design that takes into account human values throughout the design process. Despite its limitations, it provides a framework for a value-centered research and design agenda. The VSD methodology continues to evolve to effectively engage local expressions of values and respond to local contexts of design."}, {"query": "The theory of planned behavior and teacher decisions", "paper_list_string": "Lee 2010:\n\nTitle: Theory of Planned Behavior and Teachers' Decisions Regarding Use of Educational Technology\n\nAbstract: According to Ajzen's Theory of Planned Behavior (TPB), behavioral intention (BI) is predicted by attitude toward the behavior (AB), subjective norm (SN), and perceived behavioral control (PBC). Previous studies using the TPB to explain teachers' intentions to use technology have resulted in inconsistent findings. This inconsistency might be due to overly broad definitions of the target behavior. To investigate this potential weakness, we defined a specific target behavior, using computers only to create and deliver lessons, and then used the TPB to investigate teachers' decisions. An elicitation study was used to identify teachers' salient beliefs and develop a closed-ended questionnaire. Results of the closed-ended questionnaire revealed that AB, SN, and PBC all were significant predictors of teachers' intentions. However, AB had twice the influence of SN and three times that of PBC. This finding suggests that teachers must have positive attitudes about using computers to create and deliver lessons. They are less concerned about what others think of this practice, and far less bothered by internal or external constraints. Results provide specific information that can be used to design effective teacher development programs and remind TPB researchers of the importance of using specific definitions of the target behavior.\n\n==\n\nMartin 2004:\n\nTitle: Self-Efficacy Theory and the Theory of Planned Behavior: Teaching Physically Active Physical Education Classes\n\nAbstract: Abstract The purpose of our investigation was to examine determinants of teachers' intentions to teach physically active physical education classes (i.e., spend at least 50% of class time with the students engaged in moderate to vigorous physical activity). Based on the theory of planned behavior, a model was examined hypothesizing that teachers' intentions were determined by subjective norm, attitude, and perceived behavioral control. Grounded in self-efficacy theory, it was hypothesized that program goal importance and hierarchical and barrier self-efficacy would also predict intention. Using a series of hierarchical regression analyses, the theory of planned behavior was supported by accounting for 59% of the variance in intention due to attitude, perceived behavioral control, and subjective norm. Self-efficacy theory based variables received minimal support.\n\n==\n\nTeo 2016:\n\nTitle: Teachers and technology: development of an extended theory of planned behavior\n\nAbstract: Abstract\nThis study tests the validity of an extended theory of planned behaviour (TPB) to explain teachers\u2019 intention to use technology for teaching and learning. Five hundred and ninety two participants completed a survey questionnaire measuring their responses to eight constructs which form an extended TPB. Using structural equation modelling, the results showed that the constructs in the extended TPB were significant in explaining teachers\u2019 intention to use technology in their work. Among the constructs in the research model, attitude towards computer use had the largest positive influence on technology usage intention, followed by perceived behavioral control. However, subjective norm had a negative impact on intention. The inclusion of the antecedent variables had also strengthened the ability of the extended TPB model to explain intention. This study contributes to the growing discussions in applying psychological theories to explain behavioral intention in educational contexts.\n\n==\n\nDunn 2018:\n\nTitle: Using the Theory of Planned Behavior to explore teachers\u2019 intentions to engage in ongoing teacher professional learning\n\nAbstract: Abstract The purpose of this study was to examine the factors influencing teachers\u2019 intentions to engage in professional learning on the mathematics Common Core State Standards (CCSS). The aim was to identify specific factors teachers encounter that impinge upon or facilitates implementing new instructional practices. Ajzen\u2019s (1991) theory of planned behavior was used to examine the influence of three constructs (attitude toward the behavior, subjective norms, and perceived behavioral control) on teachers\u2019 intentions to implement the instructional expectations associated with the CCSS in mathematics. Data for this quantitative study was collected from 152 teachers in an urban school district in California. The result of the study provides support for the adequacy of the theory of planned behavior for predicting and understanding teachers\u2019 behavioral intention toward professional learning. Intention was predicted significantly by perceived behavioral control, subjective norm and attitude toward the behavior.\n\n==\n\nMacFarlane 2013:\n\nTitle: Teacher attitudes and behavior toward the inclusion of children with social, emotional and behavioral difficulties in mainstream schools: An application of the theory of planned behavior\n\nAbstract: Abstract The Theory of Planned Behavior (TPB) was used to examine relationships between teacher attitudes and behavior toward children with social, emotional and behavioral difficulties (SEBD). One hundred and eleven elementary school teachers completed questionnaires. Teacher perception of their school principals' expectations (subjective norm) predicted teacher behaviors. Teachers who had attended more in-service training (INSET) sessions held more positive feelings, but teachers with more experience were less willing to work with children with SEBD. Findings suggest that school principals have a central role in promoting an inclusive ethos within their schools. INSET could focus more on challenging beliefs.\n\n==\n\nStanec 2009:\n\nTitle: The Theory of Planned Behavior: Predicting Teachers\u2019 Intentions and Behavior during Fitness Testing\n\nAbstract: The twofold purpose of this study was to develop and validate an instrument that assessed teachers\u2019 intentions, attitudes, subjective norm, and perceived behavior control to administer fitness tests effectively, and to determine how well the instrument could predict teachers\u2019 intentions and actual behavior based on Ajzen\u2019s (1985, 1991) theory of planned behavior. In the development phase of the study, 104 physical educators completed the pilot version of the survey to refine the instrument. In the prediction of behavior phase of the study, a convenience sample of 195 physical educators completed (a) the Teachers\u2019 Intentions to Administer Physical Fitness Tests Effectively (TIAPFTE) before fitness testing and (b) a behavior self-report after they administered fitness testing. Standard multiple regression analyses showed perceived behavioral control and attitude significantly predicted intention. Furthermore, results showed that attitude significantly predicted teachers\u2019 behavior directly.\n\n==\n\nCrawley 1990:\n\nTitle: Intentions of Science Teachers To Use Investigative Teaching Methods: A Test of the Theory of Planned Behavior.\n\nAbstract: The purpose of this study was to explore the utility of the theory of planned behavior for predicting the behavioral intentions of teachers enrolled in the Institute in Physical Science, an EESA, Title II program funded by the Texas Higher Education Coordinating Board. In particular, the study investigated three determinants of teachers' behavioral intentions (BI) set forth in the theory of planned behavior, namely, attitude toward the behavior (AB), subjective norm (SN), and perceived behavioral control (PBC). The behavior of interest in this study was the intention of teachers in grades 5/6 or 9/10 who were enrolled in the Institute to use 50% of the activities and investigations completed in the program with students they would teach during the next school year. Data were collected from 50 elementary and secondary teachers. Simple and hierarchical regression analyses were used to determine the relative contributions of attitude, subjective norm, and perceived behavioral control (and their interactions) to the prediction of behavioral intention. Results of this study indicated that attitude, subjective norm, and perceived behavioral control made significant contributions to the prediction of behavioral intention. Examination of the data blocked on external variables revealed attitude toward the behavior to be the single most important predictor of behavioral intention. Intent to perform the behavior appears to be totally under the control of most teachers, with little need for social support and with ample resources and sufficient opportunities available to perform the behavior.\n\n==\n\nKnauder 2019:\n\nTitle: Individualized student support in primary school teaching: A review of influencing factors using the Theory of Planned Behavior (TPB)\n\nAbstract: Abstract According to the Theory of Planned Behavior (TPB; Ajzen, 1991), each behavior results from a behavioral intention, which in turn depends on attitudes, subjective norms and perceived behavioral control. In this study, the TPB was investigated with respect to individualized student support and teachers' intrinsic and extrinsic support intentions involving 488 primary school teachers. Employing individualized support interventions was predicted directly by self-efficacy beliefs and the intrinsic support intention as well as indirectly by teachers\u2019 attitudes. More practically experienced teachers implement support measures more often, while multi-groupcomparisons regarding advanced training showed no differences in the tested model for the TPB.", "final_summary": "The Theory of Planned Behavior (TPB) has been widely used to understand teacher decisions across various educational contexts. The TPB posits that behavioral intention, which is a significant predictor of actual behavior, is influenced by attitudes, subjective norms, and perceived behavioral control (Lee, 2010; Martin, 2004; Teo, 2016; Dunn, 2018; MacFarlane, 2013; Stanec, 2009; Crawley, 1990; Knauder, 2019).\n\nIn the realm of educational technology, Lee (2010) found that teachers' attitudes towards using computers for lesson creation and delivery had the most significant influence on their intentions, overshadowing the impact of subjective norms and perceived behavioral control. Similarly, Teo (2016) found that teachers' attitudes towards computer use had the largest positive influence on their intention to use technology.\n\nWhen it comes to physical education, Martin (2004) found that the TPB could explain 59% of the variance in teachers' intentions to teach physically active classes. However, self-efficacy theory-based variables received minimal support. Stanec (2009) also found that teachers' attitudes and perceived behavioral control significantly predicted their intentions to administer fitness tests effectively.\n\nIn the context of professional learning, Dunn (2018) found that teachers' intentions to engage in professional learning were significantly predicted by perceived behavioral control, subjective norm, and attitude towards the behavior. Similarly, Crawley (1990) found that attitude towards the behavior was the single most important predictor of teachers' intentions to use investigative teaching methods.\n\nIn terms of inclusive education, MacFarlane (2013) found that teachers' perceptions of their school principals' expectations (subjective norm) predicted their behaviors towards children with social, emotional, and behavioral difficulties. Knauder (2019) found that employing individualized student support was directly predicted by self-efficacy beliefs and intrinsic support intention, and indirectly by teachers\u2019 attitudes.\n\nIn conclusion, the TPB provides a useful framework for understanding teacher decisions across various educational contexts. However, the relative importance of attitudes, subjective norms, and perceived behavioral control may vary depending on the specific context and behavior in question."}, {"query": "informal economy and environmental economy", "paper_list_string": "Elgin 2014:\n\nTitle: Pollution and Informal Economy\n\nAbstract: In this study, we investigate the relationship between the size of the informal economy and the level of environmental pollution/energy use. To this end, we first use different indicators of environmental pollution along with a measure of energy use intensity in a panel dataset consisting of 152 countries over the period 1999\u20132009 and empirically examine the relationship between pollution and the shadow economy. The estimation results show that there is an inverse-U relationship between the size of the informal economy and environmental pollution, that is, small and large sizes of the informal economy are associated with lower environmental pollution and medium levels of informality are associated with higher levels of environmental pollution. Next, we build a two sector dynamic general equilibrium model to suggest an economic mechanism for this observation. Our model identifies two channels through which informality might affect environmental pollution: The scale effect, whereby a larger (smaller) informal economy size is associated with a lower (higher) level of environmental pollution, and the deregulation effect, whereby a larger (smaller) informal economy is associated with higher (lower) pollution levels. As these two effects work in opposite directions, the changing relative strength of one with respect to the informal sector size creates the inverted-U relationship between pollution indicators and informality.\n\n==\n\nEren 2022:\n\nTitle: The moderating role of informal economy on financial development induced ekc hypothesis in turkey\n\nAbstract: This study conducts an empirical investigation about the moderating role of the informal economy on Turkey's environmental performance by employing advanced econometric techniques that account numerous structural breaks in series. In this extent, we created three interaction variables by captivating the impact of informal economic activities on CO2 emissions through income, energy use, and financial sector development. Besides, we built a main effect model without the interaction variables to assess the direct effects of our variables on global environmental degradation. The outcomes of the carried analyses produced supporting evidence toward the confirmation of the Environmental Kuznets Curve (EKC) assumption. Obtained findings shown that energy use, financial development and the informal economy in Turkey transmit a deteriorating impact on environmental well-being. Furthermore, the moderating role of the informal economy was found to be statistically significant factor in terms of both economic and environmental efficiency.\n\n==\n\nSmit 2015:\n\nTitle: Towards connecting green economy with informal economy in South Africa : a review and way forward\n\nAbstract: The informal economy is a vibrant and growing phenomenon, offering both opportunities and lessons on resilience and innovation. When considering global social, economic and environmental challenges, resilience and innovation are valuable response strategies. The notion of a green economy has similarly inspired a number of ideological, geopolitical and institutional responses, yet a review of the dominant approach indicates the propensity to undervalue or ignore the informal economy. Within the context of sustainable development and poverty eradication, connecting the informal economy with the green economy is imperative. This paper explores possible connections between the green economy and the informal economy in South Africa and argues that by engaging the informal economy in discussions on the green economy, a more informed policy and planning environment may ensue, resulting in more socially equitable and environmentally sustainable development.\n\n==\n\nMazhar 2013:\n\nTitle: Environmental Regulation, Pollution and the Informal Economy\n\nAbstract: The regulation of environmental pollution is challenging. Particularly, the presence of institutional weaknesses like informal economy may not allow effective regulation. In this context, this paper addresses three related questions: (a) How stringent environmental regulation affects pollution? (b) What is the link between stringency of environmental regulation and the size of the informal economy? (c) How informal economy affects formal sector pollution? We use a data set of more than 100 countries from 2007 to 2010, a multivariate framework that controls for the influence of important factors and an index of perceived stringency of environmental regulation. The main findings of the paper, in line with theoretical reasoning, are that (i) stringent environmental regulation reduces pollution and (ii) stringent environmental regulation increases the size of the informal economy. This evidence suggests that informal economy helps avoid environmental regulation by being outside the regulatory sphere. An additional support to this finding is provided by the robust negative correlation between the size of the informal economy and the formal sector pollution. Our findings are based on interactive and non-linear effects that are tested and verified. In this regard, the paper raises new issues about possible mechanisms to reduce pollution in the presence of the informal economy.\n\n==\n\nPerera 1996:\n\nTitle: Accommodating the Informal Sector: A Strategy for Urban Environmental Management\n\nAbstract: Abstract Livelihoods of the urban poor, particularly the informal economic activities operating on streets and other public places, are usually seen as undesirable for environmental management by urban authorities which are preoccupied with keeping their cities clean. Hence, informal sector activities are often seen as \u00ab\u00abeye-sores\u00bb\u00bb and are evicted from city centers in the name of \u00ab\u00abpublic cleanliness and orderliness\u00bb\u00bb. However, it is seen that environmental problems associated with the informal sector are mostly manifestations of unresponsive physical planning systems rather than attributes inherent to the sector's respective activities. An environmental impact analysis shows that provision of proper business premises to informal enterprises is an effective measure to curb the environmental problems associated with the sector. From this viewpoint, accommodating the informal sector in the urban built-environment is seen as an effective strategy for urban environmental management.\n\n==\n\nGuibrunet 2017:\n\nTitle: The contribution of the informal economy to urban sustainability \u2013 case study of waste management in Tepito, Mexico City\n\nAbstract: This thesis explores the role of the informal economy in urban environmental management. Cities\u2019 relation with the environment is mediated by the urban infrastructure, which provides services such as transport or waste management. Beyond the implementation of plans by local governments, the daily operation of such infrastructure is also the result of informal work. Yet, little is known about the nature and impacts of informal work in urban service provision. This thesis tackles this research gap by documenting the everyday operation of domestic waste collection and management in a neighbourhood of Mexico City. The aim of this research is twofold. Firstly, it aims to critically analyse the concept of \u201cinformality\u201d in the case of urban waste management, and to document how informality operates in that context. Secondly, it assesses the contribution of the informal economy to the waste management system, by contrasting it to the key components of urban sustainability. The research presents primary data collected through qualitative fieldwork. Using an urban metabolism framework, it documents waste flows through the urban infrastructure, identifying the role of formal and informal waste handlers along the way. In parallel, it explores the normative discourses of informality that are mobilised in the production of Mexico City\u2019s urban sustainability policies. The thesis argues that it is necessary to re-consider the role of informal workers in urban sustainability. In Mexico City, informal and formal waste workers\u2019 relationship is symbiotic. Formal waste collection services are sustained by informal work and cash flows. In parallel, informal waste handlers provide the main input (recyclable materials) to the formal recycling industry \u2013 this is achieved through the reliance on local solidarity networks and techniques of experimentation and innovation which are characteristic of the informal economy. The informal economy appears to contribute positively to the environmental and social components of urban sustainability. Yet, informal workers are not recognised as legitimate actors in policy making. Instead, the concept of informality is mobilised by civil servants to exclude informal workers from the policy process. This challenges the potential for inclusive governance, a key component of urban sustainability.\n\n==\n\nRuzek 2014:\n\nTitle: The Informal Economy as a Catalyst for Sustainability\n\nAbstract: Sustainability typically involves the balancing of three major factors: the economy, the environment, and some notion of equity. Though the economy is already a key aspect, the recognition of the informal economy, seems to be absent from the many possible permutations of these three. This paper will explore the various aspects of the informal economy and how it can make a considerable impact on achieving a more sustainable future. Specifically, this paper focuses on how the informal economy can encourage the sustainable use of goods, while offering an alternative to the regulated market economy. By supporting the informal sectors such as farmers markets, street vendors and non-market activities, a shift away from a car-dominated society and singular economic trajectory can begin. The informal sector can provide, social capital, promote local economies, create jobs and provide the need economic shift toward a sustainable future.\n\n==\n\nKahn 2000:\n\nTitle: Informal Economies, Information and the Environment\n\nAbstract: \"If household and firm activities are not observable by government, then they may not be observable by those affected by environmental degradation either.... [W]e have a regulatory challenge: when information is scarce, private bargaining is unlikely to suffice ... and government will lack a basis for regulatory action.\" Many less developed countries (LDCs) contain sizeable shadow economies. For example, informal economic activity constitutes perhaps 70 percent of the GDP of Nigeria and Egypt, and perhaps as much as 30 percent of the GDP of Chile, Costa Rica, Venezuela, Brazil, Paraguay and Colombia.(1) The magnitude of the shadow economy in these and other countries may have serious environmental consequences. Environmental regulators seeking to provide incentives for environmental protection and conservation, for example, face enormous difficulties in monitoring and enforcing laws in the shadow sector. Groups and individuals operating in this sector recognize that they are not likely to be held accountable for actions that degrade environmental quality. The lack of accountability and incentives to comply with strong environmental standards raises the possibility that parties operating in the shadow sector will engage in activities that threaten the quality of the environment. This includes activities that cause an increase in the number of hazardous waste sites, the degradation of local air and water quality, species loss and total greenhouse gas emissions. While informal sectors exist in all economies, the impact of unobserved economic activity on the environment might be more intense in developing countries for at least four reasons: first, as suggested above, the shadow sectors of LDCs are likely to represent a higher proportion of gross GDP than in developed countries; second, LDCs characteristically have more relatively rural and unpopulated areas, in which the inhabitants lack sufficient incentives (economic or otherwise) to motivate well-defined property rights;(2) next, developing countries are poorer, and thus their governments have fewer resources with which to monitor polluters; and finally, if environmental health is understood as a \"luxury good\"(3) then developing countries may lack the motivation to develop institutions that expose shadow sectors to regulation. Given the challenge shadow economies pose to environmental regulators, are there strategies that a well-intentioned government can pursue to mitigate environmental damage? This paper will investigate three major related questions: 1. How does the existence of shadow sectors affect the design of environmental regulation? 2. How might regulators set environmental policies if they had more information concerning economic activity within shadow sectors? 3. Could economic development (including income growth) and increased international trade help \"lift the shadow\" and thus aid environmental regulators in pursuing accountability? In general, environmental economic policies should provide appropriate incentives for firms and households to reduce environmental degradation. Designed appropriately, such government intervention can guide markets toward outcomes that are socially preferable if, as expected, these actors ignore to some extent the environmental consequences of their actions. Appropriate intervention, however, may require expensive studies to gather useful information regarding actors in the shadow economy. Thus, regulators may choose not to gather this information. Consider the underlying policy problems: firms and households make millions of choices every day which go unobserved by regulators and may have an impact on the environment. These are choices such as: the quantity and type of fuel to use; or, whether to dump hazardous wastes or dispose of them properly If a regulator observed each activity and thereby knew how much damage was caused, they could provide appropriate incentives--facing polluters with the full costs of their activities. \u2026", "final_summary": "The relationship between the informal economy and environmental economy is a complex and multifaceted one, as evidenced by the diverse findings in the literature. \n\nElgin (2014) found an inverse-U relationship between the size of the informal economy and environmental pollution, suggesting that both small and large informal economies are associated with lower pollution levels. This is due to two opposing effects: the scale effect, where a larger informal economy is associated with lower pollution, and the deregulation effect, where a larger informal economy is associated with higher pollution levels.\n\nEren (2022) found that the informal economy in Turkey has a significant moderating effect on the country's environmental performance. The study found that energy use, financial development, and the informal economy all have a deteriorating impact on environmental well-being.\n\nSmit (2015) argued that connecting the informal economy with the green economy is crucial for achieving socially equitable and environmentally sustainable development. The study suggested that the informal economy offers valuable lessons on resilience and innovation that can be applied to the green economy.\n\nMazhar (2013) found that stringent environmental regulation reduces pollution but increases the size of the informal economy. This suggests that the informal economy helps avoid environmental regulation by operating outside the regulatory sphere.\n\nPerera (1996) argued that accommodating the informal sector in urban environmental management strategies can effectively curb environmental problems associated with the sector. The study suggested that environmental problems associated with the informal sector are more a result of unresponsive physical planning systems than inherent attributes of the sector.\n\nGuibrunet (2017) found that the informal economy contributes positively to the environmental and social components of urban sustainability. The study suggested that informal workers play a crucial role in urban waste management and should be recognized as legitimate actors in policy making.\n\nRuzek (2014) argued that the informal economy can make a significant impact on achieving a more sustainable future. The study suggested that supporting informal sectors such as farmers markets and street vendors can promote local economies, create jobs, and shift away from a car-dominated society.\n\nKahn (2000) suggested that the size of the shadow economy in many less developed countries may have serious environmental consequences. The study argued that environmental regulators face enormous difficulties in monitoring and enforcing laws in the shadow sector.\n\nIn conclusion, the informal economy plays a significant and complex role in the environmental economy. While it can contribute to environmental degradation, it also offers potential solutions for achieving environmental sustainability. Further research is needed to fully understand this relationship and to develop effective strategies for integrating the informal economy into environmental management and policy making."}, {"query": "digital parenting and artificial intelligence", "paper_list_string": "Croeser 2019:\n\nTitle: Theories of Parenting and Their Application to Artificial Intelligence\n\nAbstract: As machine learning (ML) systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence (AGI) that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.\n\n==\n\nSiibak 2019:\n\nTitle: Digital parenting and the datafied child\n\nAbstract: Many parents of today are feeling increasingly concerned not only for the well-being and safety of their children, but also for their own abilities to take up the role of a \u201cgood\u201d and \u201cresponsible\u201d parent. Empirical research evidence is used in the chapter to illustrate how the data religion cultivated by tech industry, popular press, marketing discourses and general societal expectations of a \u201cresponsible parent\u201d have created a norm for plugged\u2011in parenting resulting in intimate dataveillance of children, both in online and offline contexts. Various digital parenting tools \u2013 from pregnancy apps and baby monitors to parental controls and tracking devices \u2013 and practices\u00a0\u2013 such as sharenting\u00a0\u2013 are used in the chapter to illustrate how the issues related to the digital rights and privacy of the child are almost entirely discarded against the overprotective and technologically moderated parenting stance leading to both commodification as well as datafication of childhood.\n\n==\n\nK 2021:\n\nTitle: Digital Parenting\n\nAbstract: Abstract Parenting helps socialization in children. The inevitable presence of digital media has a major impact on parent\u2013child relationships and parenting skills. The increased use of digital devices, such as smartphones and tablets, by children forces the parents to change their parenting styles. Digital parenting refers to a process adopted by parents to monitor and regulate the activities of their young ones in handling digital devices. The main aim of this style is to protect their children from various online dangers. Parents need to use several approaches for regulating digital media behaviors of children, such as setting limits, practicing constructive discipline, spending time with their kids, and so forth. However, there are challenges associated with digital parenting. Some of the issues include unlimited access to Internet, parents\u2019 job, reverse socialization, and so on. Pediatric nurses can help parents in successfully managing digital parenting styles through education and advocacy as part of health promotion activities of pediatric clients both in hospital and community settings.\n\n==\n\nBenedetto 2020:\n\nTitle: Digital Parenting: Raising and Protecting Children in Media World\n\nAbstract: Digital media have quickly changed ways in which parents and children communicate, enjoy themselves, acquire information, and solve problems daily (both in ordinary and exceptional circumstances such as COVID-19 home confinement). Very young children are regular users of smartphones and tablet, so their early digital engagement poses new challenges to parent-child relationships and parental role. First, the chapter introduces the \u201cdigital parenting\u201d construct, moving through the literature from \u201ctraditional\u201d parenting styles to more recent studies on \u201cparental mediation,\u201d that is, the different behaviors parents adopt to regulate children\u2019s engagement with the Internet and digital media. Second, the chapter reviews empirical researches on different parental mediation practices (active or restrictive behaviors) and how they are adjusted according to the child\u2019s characteristics (age, digital competences, etc.) or parent\u2019s media competence and beliefs. Finally, from a bidirectional perspective of parent-child relationships, the chapter discusses the role of youths\u2019 social involvement, communication, self-disclosure, and digital skills on parent\u2019s beliefs and practices. Implications for parent education and prevention of risks for early and excessive exposure to digital technologies are discussed.\n\n==\n\nAdhe 2020:\n\nTitle: Digital Parenting Services: University Integrated With Society\n\nAbstract: This research aims to make advancement in the area of parenting by building a digital service. This service integrates parenting information in the surrounding society. Services in the form of websites that can be opened by the public and contain information. The method used in this research are research and development with ADDIE type. The samples consist of 50 participants. Trials are conducted with small trials as well as large trials onwards. The results of this study are carried out starting from the assessment of products and materials and then testing the service that was done. The feasibility assessment by material experts obtained a score of 93.18% with both criteria and categories worthy. Eligibility assessment by media experts obtained a score of 93.75% with good criteria and feasible categories. Assessment of the feasibility of small group trials received a total score of 53.61% and large group trials received a total score of 68.56%. The implication of this research shows that digital parenting services help the public understand the parenting that will be applied to children.\n\n==\n\nK\u00fc\u00e7\u00fckoba 2023:\n\nTitle: Digital parenting and digital childhood: Raising gifted children born into the digital age\n\nAbstract: In an age when technology is effective in many areas, it's unreasonable to keep kids away from it. Children born in the age of technology, who shared their photos against their will from birth, do not hesitate to use technological devices and have the curiosity and ability to use multiple digital media at once. They use digital media and devices as creative, productive, and problem-solving tools. They need direction. Parents and teachers need skills and attitudes to help children use technology positively. Digital parenting and media guidance as new parent roles reflect this perspective. The digital parenting perspective says it's not true to keep kids away from media and devices by focusing on the negatives. Instead of focusing on the negative effects of technology, he suggests teaching children how to use digital media and tools to solve daily problems. The digital parenting approach seems more stimulating for children and adults. We need children who understand digital tools better than us in today's world, where problems can't be solved without technology. The research focused on the positive effects of technology on children in terms of \"being the parent of gifted children born in the digital age\" It allows them to use this content rationally to create new content and share it with new social connections. This research may contribute to social sciences by shedding light on digitalization, gifted children, and related applications.\n\n==\n\nLivingstone 2020:\n\nTitle: Parenting for a Digital Future\n\nAbstract: In the decades it takes to bring up a child, parents face challenges that are both helped and hindered by the fact that they are living through a period of unprecedented digital innovation. Drawing on extensive research with parents both rich and poor, parenting toddlers to teenagers, this book reveals how digital technologies give parenting struggles a distinctive character, as parents determine how to forge new territory with little precedent, or support. It argues that, in late modernity, parents are both more burdened with responsibilities and yet increasingly charged with respecting and developing the agency of their child\u2014leaving much to be negotiated. The book charts how parents enact authority and values through digital technologies\u2014as \u201cscreen time,\u201d videogames, and social media become ways of both being together and of setting boundaries, with digital technologies introducing valued opportunities and new sources of risk. To light their way, parents comb through the hazy memories of their own childhoods and look toward hard-to-imagine futures. This results in deeply diverse parenting in the present, as parents move between embracing, resisting, or balancing the role of technology in their own and their children\u2019s lives. This book moves beyond the panicky headlines to offer a deeply researched exploration of what it means to parent in a period of significant social and technological change. Drawing on qualitative and quantitative research in the United Kingdom, the book offers conclusions and insights relevant to parents, policymakers, educators, and researchers everywhere.\n\n==\n\nLupton 2016:\n\nTitle: Parenting and Digital Media: From the Early Web to Contemporary Digital Society\n\nAbstract: Parents have accessed websites, online discussion forums and blogs for advice, information and support since the early days of the World Wide Web (which first became widely available to users in the mid-1990s). In this century, the advent of mobile media such as smartphones and tablet computers and Wi-Fi has allowed parents to access the web from almost any location. They can use social-media platforms and apps (software applications for mobile devices) as part of their parenting practices. These technologies have brought with them opportunities for parents to seek information and support and exchange details of their experiences with each other in a variety of ways. These practices contribute to datafication, that is, rendering details of people\u2019s lives into digital data formats (van Dijck 2014) - not only of parents themselves but also of their children. The possibilities that now exist not only for voluntary sharing of one\u2019s personal data with others, but also for data leakage and commercial exploitation of this information, are key differences between the early digital media that were available to parents and those that they currently use. \n \nIn this article, we review the literature in sociology and related social research addressing the ways in which digital media have been used for parenting-related purposes. We begin with the longer-established media of parenting websites, online discussion forums, blogs, email, mobile phones and message and video services and then move on to the newer technologies of social media and apps. This is followed by a section on data privacy and security issues. The concluding section summarises some major issues arising from the review and points to directions for further research.", "final_summary": "Digital parenting, a term that refers to the process adopted by parents to monitor and regulate their children's use of digital devices, is a topic of increasing interest in the field of artificial intelligence (AI) and parenting research. \n\nCroeser (2019) introduces the concept of applying parenting theories to AI, suggesting that as AI systems become more autonomous, the principles of parenting could be used to guide their development and integration into society. This perspective is echoed by Benedetto (2020), who discusses the evolution of traditional parenting styles to digital parenting, focusing on parental mediation practices to regulate children's engagement with the internet and digital media.\n\nSiibak (2019) discusses the concept of the datafication of childhood, referring to the intimate surveillance of children's data, driven by societal expectations and the tech industry. However, the abstract does not explicitly state that this surveillance is conducted by parents through digital tools. Similarly, Livingstone (2020) discusses how digital technologies have both aided and complicated parenting, with parents having to navigate new territory with little precedent or support.\n\nK\u00fc\u00e7\u00fckoba (2023) discusses the concept of digital parenting, advocating for an approach that teaches children how to use digital media and tools positively. However, the abstract does not mention any specific challenges and opportunities of digital parenting.\n\nAdhe (2020) and Lupton (2016) discuss the role of digital services in parenting. Adhe (2020) presents a research on the development of a digital parenting service that integrates parenting information into society, while Lupton (2016) reviews the literature on how digital media have been used for parenting-related purposes, from websites and online forums to social media and apps.\n\nIn conclusion, the intersection of digital parenting and AI presents both challenges and opportunities. As AI systems become more autonomous, the principles of parenting could guide their development. However, the increasing surveillance of children's data raises concerns about children's digital rights and privacy. Despite these challenges, digital parenting also offers opportunities for parents to adapt their parenting styles and teach children how to use digital media and tools positively. Further research is needed to fully understand the implications of this intersection for both parents and children."}, {"query": "what is the differential diagnosis for traumatic adult new midfoot collapse?", "paper_list_string": "Steiner 2018:\n\nTitle: Combined Subtalar and Naviculocuneiform Fusion for Treating Adult Acquired Flatfoot Deformity With Medial Arch Collapse at the Level of the Naviculocuneiform Joint\n\nAbstract: Background: A challenge in treating acquired flatfoot deformities is the collapse of the medial arch at the level of the naviculocuneiform (NC) joint. Triple fusions, being a treatment option, may lead to problems such as increased foot stiffness. We thus established a method that combines subtalar (ST) fusion with NC fusion while preserving the Chopart joint. We analyzed the radiographic correction, fusion rate, and patient satisfaction with this procedure. Methods: 34 feet in 31 patients (female, 23; male, 8; age 67 [45-81] years) were treated with a ST and NC joint fusion. In 15 cases, a medial sliding-osteotomy was additionally necessary to fully correct hindfoot valgus. The following radiographic parameters were measured on weightbearing radiographs preoperatively and at 2 years: talo\u2013first metatarsal angle, talocalcaneal angle, calcaneal pitch, talonavicular coverage angle and calcaneal offset. Fusion was radiologically confirmed. Results: All parameters, except the calcaneal pitch, showed a significant improvement. Fusion was observed after 1 year in all but 2 cases (94.1%). One nonunion each occurred at the ST and NC joint without needing any subsequent treatment. One patient developed avascular necrosis of the lateral talus with need for total ankle replacement after 1 year. All patients were satisfied with the obtained results. Conclusion: Our data suggest that a combined fusion of the ST and NC joint was effective and safe when treating adult acquired flatfoot with collapse of the medial arch at the level of the NC joint. Although the talonavicular joint was not fused, its subluxation was significantly reduced. Level of Evidence: Level IV, case series.\n\n==\n\nCheng 2012:\n\nTitle: A Rare Midfoot Injury Pattern: Navicular\u2014Cuneiform and Calcaneal\u2014Cuboid Fracture\u2014Dislocation\n\nAbstract: A rare midfoot injury pattern of navicular\u2014cuneiform and calcaneal\u2014cuboid fracture\u2014dislocation is presented with two typical cases and a systematic review of the literature. This injury usually occurs as a result of high-energy crushing trauma and most often causes plantarly directed dislocation of the midfoot. Initial diagnosis includes a thorough physical examination and adequate radiological imaging, comprising anteroposterior, oblique and lateral X-radiography and computed tomography with three-dimensional reconstruction. Care should be taken to determine any injury to adjacent midfoot joints, in particular the Lisfranc joint. Intrasurgical protection of soft tissue is essential. Open reduction using two parallel incisions with minifragment plate fixation is recommended in more comminuted injuries. The navicular\u2014cuneiform and calcaneal\u2014cuboid joints play important roles in maintaining the arch of the foot and in weight-bearing during locomotion. Without proper therapy in the immediate post-traumatic phase, the long-term results are generally unsatisfactory.\n\n==\n\nSammarco 2009:\n\nTitle: Midtarsal arthrodesis in the treatment of Charcot midfoot arthropathy.\n\nAbstract: BACKGROUND\nFracture-dislocation of the midfoot with collapse of the longitudinal arch is common in patients with neuropathic arthropathy of the foot. In this study, we describe a technique of midfoot arthrodesis with use of intramedullary axial screw fixation and review the results and complications following use of this technique.\n\n\nMETHODS\nA retrospective study of twenty-two patients who had undergone surgical reconstruction and arthrodesis to treat Charcot midfoot deformity was performed. Bone resection and/or osteotomy were required to reduce deformity. Axially placed intramedullary screws, inserted either antegrade or retrograde across the arthrodesis sites, were used to restore the longitudinal arch. Radiographic measurements were recorded preoperatively, immediately postoperatively, and at the time of the last follow-up and were analyzed in order to assess the amount and maintenance of correction.\n\n\nRESULTS\nPatients were evaluated clinically and radiographically at an average of fifty-two months. Complete osseous union was achieved in sixteen of the twenty-two patients, at an average of 5.8 months. There were five partial unions in which a single joint did not unite in an otherwise stable foot. There was one nonunion, with recurrence of deformity. All patients returned to an independent functional ambulatory status within 9.5 months. Weight-bearing radiographs showed the talar-first metatarsal angle, the talar declination angle, and the calcaneal-fifth metatarsal angle to have improved significantly and to have been corrected to nearly normal values by the surgery. All measurements remained significantly improved, as compared with the preoperative values, at the time of final follow-up. There were no recurrent dislocations. Three patients had a recurrent plantar ulcer at the metatarsophalangeal joint that required additional surgery. There were eight cases of hardware failure.\n\n\nCONCLUSIONS\nOpen reduction and arthrodesis with use of multiple axially placed intramedullary screws for the surgical correction of neuropathic midfoot collapse provides a reliable stable construct to achieve and maintain correction of the deformity.\n\n==\n\nAkra 2010:\n\nTitle: An unusual etiology for adult-acquired flatfoot.\n\nAbstract: Rupture of the tibialis posterior tendon is widely believed to result from trauma in the presence of preexisting degenerative changes. We report a case of adult-acquired flatfoot in an otherwise healthy 19-year-old man who sustained a fracture of the medial malleolus.\n\n==\n\nFerris 1995:\n\nTitle: Late reconstruction of the midfoot and tarsometatarsal region after trauma.\n\nAbstract: The management of painful arthritis and deformity after trauma to the midfoot starts with careful assessment by physical examination and appropriate investigation to identify the affected joints. Conservative treatment may be very effective and includes the use of NSAIDs, custom insoles with arch support, and a rocker-bottom sole with extended steel shank with or without a SACH heel. If this treatment fails, usually a year after the injury, then arthrodesis of all the symptomatic joints with restoration of the arch and alignment of the weight-bearing surface is the recommended treatment. The long-term results of these fusions may be compromised by the subsequent development of arthritis in adjacent joints.\n\n==\n\nLi 2016:\n\nTitle: Categorization and surgical management of posttraumatic midfoot malunion\n\nAbstract: Objective To assess a classification system for midfoot injury that was based on the characteristics of the foot malunion and to evaluate the suggested treatment strategies. Methods This retrospective review of data from patients with posttraumatic midfoot malunion categorized each foot deformity into one of three types based on the foot arch and then separated these categories into one of three subtypes based on the forefoot deformity. According to the types of malunion, fascio-cutaneous flap, osteotomy, joint arthrodesis or realignment was used to correct the deformity. Patients were assessed before surgery and at 12 and 24 months postoperation. Results Of the 24 patients identified, six had Lisfranc joint injuries, nine had Lisfranc joint complex injuries combined with cuboid compression fractures and nine had Lisfranc joint complex injuries combined with navicular fractures. Overall, eight patients presented with poor soft tissue and massive scar at the dorsal foot. Visual analogue scale and American Orthopaedic Foot and Ankle Society midfoot scores significantly improved over the 24-month study period. At the end of the study, 21 of 24 patients (87.5%) rated their functional outcome as excellent or good. Conclusion The classification of the midfoot malunion evaluated in this study may be helpful in the decision making process for surgical intervention.\n\n==\n\nClaus 2008:\n\nTitle: [Differential diagnosis of acquired clubfoot (author's transl)].\n\nAbstract: The article introduces a patient of 14 years of age in whom clubfeet as well as paraspasticity had developed from his sixth year of life. Instead of the previously suspected heredodegenerative disease, a lipoma was found in the distal spinal canal which had prevented ascension of the spinal cord and had thus produced the described phenomenon.\n\n==\n\nSalvi 2012:\n\nTitle: [An unusual case of post-traumatic adult complex foot deformity].\n\nAbstract: A peculiar clinical presentation of post-traumatic complex left foot fracture deformity is presented in this report as the result of a motorbike accident. Notwithstanding the significant deformity following forefoot fractures, the patient complained only of the recent onset of metatarsalgia. Of particular interest, is that this severe foot injury as well following deformity was overlooked, probably because patient had sustained head injury that was the main problem to treat due to life risk.", "final_summary": "The differential diagnosis for traumatic adult new midfoot collapse is multifaceted, with several potential causes and treatments identified across the literature. \n\nSteiner (2018) discusses acquired flatfoot deformities, specifically those involving collapse of the medial arch at the naviculocuneiform joint. The study suggests that a combined fusion of the subtalar and naviculocuneiform joint can be an effective treatment. Cheng (2012) presents a rare midfoot injury pattern involving navicular-cuneiform and calcaneal-cuboid fracture-dislocation, typically resulting from high-energy crushing trauma. \n\nSammarco (2009) focuses on neuropathic arthropathy of the foot, a common cause of midfoot fracture-dislocation and arch collapse. The study suggests that midfoot arthrodesis using intramedullary axial screw fixation can be an effective treatment. Akra (2010) presents a case of adult-acquired flatfoot following a fracture of the medial malleolus in a young adult, suggesting that such fractures can lead to midfoot collapse. \n\nFerris (1995) discusses the management of painful arthritis and deformity following midfoot trauma, recommending arthrodesis of all symptomatic joints if conservative treatment fails. Li (2016) presents a classification system for posttraumatic midfoot malunion, suggesting that different types of malunion may require different surgical interventions. \n\nClaus (2008) presents an unusual case of acquired clubfoot, highlighting the need for careful differential diagnosis in cases of adult new midfoot collapse. Salvi (2012) presents a case of post-traumatic complex foot deformity, which, while not directly related to acquired clubfoot, underscores the variety of conditions that can result in midfoot collapse.\n\nIn conclusion, the differential diagnosis for traumatic adult new midfoot collapse can include acquired flatfoot deformities, high-energy crushing trauma, neuropathic arthropathy, medial malleolus fractures, posttraumatic arthritis, and unusual cases such as acquired clubfoot. Treatment options can range from conservative management to various surgical interventions, depending on the specific diagnosis and severity of the condition."}, {"query": "The Relationship between Systems Thinking, Self-Leadership, and Clinical Reasoning of the Nursing Students", "paper_list_string": "Joo 2018:\n\nTitle: The Effects of Critical Thinking Disposition, Clinical Performance Ability and Self-Concept of Nursing Profession in Nursing Students on Self Leadership\n\nAbstract: This study is a descriptive research to examine the relationship between critical thinking disposition, clinical performance ability, and self-concept of nursing profession according to degree of self-leadership among nursing college students. The data were collected from 165 nursing students in 3rd & 4th grade using self-report questionnaire and analyzed by t-test, ANOVA, Pearson`s correlation coefficient, and Multiple Regression. Self-leadership according to general characteristics showed significant differences according to grade, academic achievement, motivation, and major satisfaction. Self-leadership was found to have a positive correlation with critical thinking disposition, clinical performance ability, and self-concept of nursing professional. Analysis of the effects of general characteristics affecting sief-leadership, critical thinking disposition, nursing proFessional selF-concept and clinical performance on selF-leadership showed that they were explained by 49.9%, critical thinking disposition was the most important factor. This suggests that nursing students' self-leadership can be improved effectively by promoting critical thinking disposition. Also in reality, there are many difficulties such as the problems in the system of the majors, but it is necessary to operate a continuous education program to strengthen the self-leadership for the nursing college students by seeking various methods.\n\n==\n\nPark 2015:\n\nTitle: Self-leadership, critical thinking disposition, satisfaction of clinical practice and clinical practice competency of nursing students\n\nAbstract: The purpose of this study was to examine the relationship among self-leadership, critical thinking disposition, satisfaction of clinical practice and clinical practice competency of nursing students. Participants were 199 baccalaureate nursing students (3rd and 4th grades) in 2 cities. The data was collected by questionnaires and were analyzed with the SPSS/Win 21.0 program, using descriptive statistics, Pearson`s correlation coefficient and multiple regression. Significant positive correlations were among self-leadership, critical thinking disposition, satisfaction of clinical practice and clinical practice competency. The regression model explained 30.4% of satisfaction of clinical practice. The significant predictors of satisfaction of clinical practice were clinical experience, satisfaction of major, self-leadership and critical thinking disposition. The regression model explained 23.7% of clinical practice competency. Health status, self-leadership and critical thinking disposition were factors influencing clinical practice competency. It should strengthen self-leadership and encourage critical thinking disposition to improve nursing students` satisfaction of clinical practice and clinical practice competency.\n\n==\n\nDong 2016:\n\nTitle: Influence of Professional Self-concept and Self-leadership on Clinical Competence in Nursing Students\n\nAbstract: Purpose:This study was done in order to identify nursing students\u2019 professional self-concept, self-leadership and clinical competence and to analyze the correlation among the variables and the factors influencing clinical competence.Methods: The research participants were 294 senior nursing students in the nursing departments of 3 universities located in Jeollabuk-do. The students had completed 3 semesters of clinical practice. Data were analyzed using descriptive statistics, t-test, ANOVA, Pearson correlation coefficient, and Multiple Regression. Results:Participants' scores for professional self-concept, self-leadership, clinical competence were 2.78\u00b10.36, 3.63\u00b10.47, 3.80\u00b10.40 respectively. Professional self-concept, self-leadership and clinical competence had positive correlations. Factors influencing nursing students\u2019 clinical competence included professional self-concepts in professional practice, self-expectations in self-leadership, constructive thinking, self-compensation and self-criticism in that order, and these variables explained 48% of the variance in clinical competence.Conclusion: Based on these results, it is important to develop and apply educational programs to increase professional self-concept and self-leadership in order to improve nursing students\u2019 clinical competence.\n\n==\n\nShin 2023:\n\nTitle: Influence of Critical Thinking Disposition and Empathy Ability on Self-Leadership of Nursing Students\n\nAbstract: Objectives The purpose of this study is to understand the relationship between nursing students\u2019 critical thinking disposition, empathy ability and self-leadership, and to determine the effect of nursing students' critical thinking disposition and empathy ability to self-leadership. \nMethods The subjects of the study were nursing students enrolled in the 3rd and 4th grades of four universities in C province. Data analysis was performed using descriptive statistics, t-test, ANOVA, Pearson correlation coefficients, and multiple linear regression analysis using the IBM SPSS 20.0 program. \nResults The findings showed differences in the degree of gender, age, clinical experience, religion, motive of entering nursing department, satisfaction of majors, satisfaction of communication skills, satisfaction of personal relationships and hopeful employment fields in the difference between the degree of critical thinking disposition, empathy ability and self-leadership of nursing students according to the general characteristics of the subject, but there was no difference in grade level and club activities. It was found that there was a positive correlation between self-leadership of nursing students, critical thinking disposition and empathy ability. As the result, the most influential factor to self-leadership of nursing student was critical thinking disposition followed by empathy ability, satisfaction of majors. The explanatory power of the model was 48.7%. \nConclusions This study is significant in that it proved the importance of not only critical thinking disposition but also the humanities literacy factor such as empathy ability as an influencing factor of self-leadership. An educational approach that can improve self-leadership combined with creativity is required, and administrative and financial support for the continuous development and cultivation of self-leadership in the clinical field will be required. However, the process of recognizing the need for self-leadership by nursing students should be preceded.\n\n==\n\nLee 2016:\n\nTitle: The Effects of Critical Thinking Disposition and Self-esteem to Self-leadership of Nursing Student\n\nAbstract: Objectives : The purpose of this study was to evaluate the effect of critical thinking disposition and self-esteem to self-leadership in nursing students. Methods : In this study, 273 data was collected from nursing students of K-university in South Korea from Sep. 2015 to Nov. 2015, and the collected data was analyzed using SPSS 22.0 Statistics Program. Results : The results showed that both of critical thinking disposition and self-efficacy according to school year are significant to self-esteem. It was found that critical thinking disposition, self-esteem and self-leadership has a positive correlation. Critical thinking disposition and self-esteem were predictors of self-leadership with 46% of influence. Conclusions : The nursing training program needs to be developed to promote critical thinking and self-esteem in the nursing curriculum, so that the nursing students can demonstrate the ability as a nursing leader in the various fields of clinical practice and health care.\n\n==\n\nCho 2011:\n\nTitle: Self-Leadership and Self-Concept in the Freshmen of Nursing College\n\nAbstract: Purpose: The purpose of this study was to identify the relationship between selfleadership and self-concept in nursing students. Methods: We used a descriptive correlation survey in a convenient sample of 818 subjects recently admitted into college in Seoul and the Kyungki province, Korea. The self-leadership and self-concept levels were measured using the self-leadership and self-concept scales, respectively. The subjects were given a self-report questionnaire. Collected data were analyzed by frequency, percentage, mean, t-test, ANOVA, scheffe-test and pearson correlation, using the SPSS software version 17.0. Result: Positive correlations were identified between the self-leadership and self-concept in the investigated nursing students. Conclusion: The results of this study indicate that it is important to provide opportunities for the nursing students to participate in the programs designed for self-leadership and self-concept development.\n\n==\n\nSeung 2017:\n\nTitle: Influence of Self\u2013Efficacy and Critical Thinking Disposition on Self-Leadership of Nursing Students\n\nAbstract: This study examined the influence of self-efficacy and critical thinking disposition on the self-leadership of nursing students. The research subjects were114 nursing students in D city and K province. The data werecollected from November 9 to November 27, 2015, and analyzed by an independent t-test, one-way ANOVA, Pearson's correlation, and Stepwise multiple regression using the IBM SPSS/Win 20.0 program. The results showed that self-efficacy was 3.73, critical thinking disposition was 3.73, and self-leadership was 3.73. The self-leadership has positive correlations with self-efficacy and critical thinking disposition. Self-efficacy(\u03b2=.277, p<.008), critical thinking disposition(\u03b2=.274, p=.006), grade(\u03b2=.273, p<.001), religion(\u03b2=.163, p=.023),motivation of nursing(\u03b2=-.155, p=.033) explained 45.6% of total variance of self-leadership of nursing students. The most significant predictor of self-leadership were self-efficacy, followed by critical thinking disposition. Therefore, it is necessary to develop aneducation program to improve the self-efficacy and critical thinking disposition of nursing students.\n\n==\n\nKuiper 2009:\n\nTitle: Promoting the Self-Regulation of Clinical Reasoning Skills in Nursing Students\n\nAbstract: Aim: The purpose of this paper is to describe the research surrounding the theories and models the authors united to describe the essential components of clinical reasoning in nursing practice education. The research was conducted with nursing students in health care settings through the application of teaching and learning strategies with the Self-Regulated Learning Model (SRL) and the Outcome-Present-State-Test (OPT) Model of Reflective Clinical Reasoning. Standardized nursing languages provided the content and clinical vocabulary for the clinical reasoning task. Materials and Methods: This descriptive study described the application of the OPT model of clinical reasoning, use of nursing language content, and reflective journals based on the SRL model with 66 undergraduate nursing students over an 8 month period of time. The study tested the idea that self-regulation of clinical reasoning skills can be developed using self-regulation theory and the OPT model. Results: This research supports a framework for effective teaching and learning methods to promote and document learner progress in mastering clinical reasoning skills. Self-regulated Learning strategies coupled with the OPT model suggest benefits of self-observation and self-monitoring during clinical reasoning activities, and pinpoints where guidance is needed for the development of cognitive and metacognitive awareness. Recommendations and Conclusions: Thinking and reasoning about the complexities of patient care needs requires attention to the content, processes and outcomes that make a nursing care difference. These principles and concepts are valuable to clinical decision making for nurses globally as they deal with local, regional, national and international health care issues.", "final_summary": "The relationship between systems thinking, self-leadership, and clinical reasoning in nursing students is a multifaceted and complex topic. The collected papers provide a comprehensive exploration of these interconnections.\n\nJoo (2018) and Park (2015) both found a positive correlation between self-leadership, critical thinking disposition, and clinical performance ability in nursing students. They suggest that enhancing self-leadership and critical thinking can improve clinical practice competency and satisfaction. Dong (2016) further supports this, finding that professional self-concept and self-leadership positively influence clinical competence.\n\nShin (2023) introduces the role of empathy in addition to critical thinking disposition as significant predictors of self-leadership in nursing students. This suggests that fostering these qualities could enhance self-leadership, which in turn could improve clinical reasoning. Lee (2016) also found that critical thinking disposition and self-esteem are predictors of self-leadership in nursing students, suggesting that self-esteem plays a role in self-leadership.\n\nCho (2011) and Seung (2017) also highlight the importance of self-concept and self-efficacy in relation to self-leadership. They found positive correlations between these factors, suggesting that programs designed to develop self-leadership and self-concept or self-efficacy could be beneficial for nursing students.\n\nFinally, Kuiper (2009) provides a practical application of these findings, demonstrating that teaching and learning strategies based on the Self-Regulated Learning Model can promote the self-regulation of clinical reasoning skills in nursing students.\n\nIn conclusion, the papers collectively suggest that self-leadership, influenced by factors such as critical thinking disposition, empathy ability, self-esteem, self-concept, and self-efficacy, plays a crucial role in the clinical reasoning of nursing students. Therefore, educational programs that enhance these qualities could potentially improve the clinical reasoning skills of nursing students."}, {"query": "0.54 g/L urea as nitrogen source for PHA production", "paper_list_string": "Arumugam 2019:\n\nTitle: Low-cost production of PHA using cashew apple (Anacardium occidentale L.) juice as potential substrate: optimization and characterization\n\nAbstract: Polyhydroxyalkanoates are polyesters of R-hydroxyalkonic acids, prominently used as bioplastics on grounds of their complete biodegradable and environment-friendly characteristics. There is an upsurge in need of an alternative low-cost, renewable carbon source for the production of PHA for enhanced economic and to exert a positive impact on the industries. In the present work, cashew apple juice (CAJ) was supplemented as a carbon source for Cupriavidus necator to produce PHA. (NH 4 ) 2 SO 4 , NH 4 Cl, NH 4 NO 3 and CO(NH 2 ) 2 , and NaNO 3 were tested and urea was found to be the best nitrogen source that supports optimal growth of the microorganism. The production process was then optimized using response surface methodology by incorporating the effects of total reducing sugar concentration, urea concentration, and inoculum size. Under optimized condition, the resulting PHA yield was found to be 15.78\u00a0g/L with total reducing sugar concentration of 50\u00a0g/L, inoculum size of 50\u00a0mL/L, and urea concentration of 3\u00a0g/L. FT-IR, NMR, TGA, and DSC analysis revealed the product to be a copolymer of hydroxybutyrate and hydroxyvalerate. Graphical abstract\n\n==\n\nStanley 2017:\n\nTitle: Fed-Batch Strategies for Production of PHA Using a Native Isolate of Halomonas venusta KT832796 Strain\n\nAbstract: In this study, polyhydroxyalkanoates (PHA) accumulation by Halomonas venusta KT832796, a moderate halophilic bacteria isolated from marine source was studied. Both nutritional requirements and process parameters for submerged cultivation of the organism in bioreactor have been standardized. From the shake flask studies, glucose and ammonium citrate as carbon and nitrogen source produced maximum PHA at a ratio 20 with 3.52\u00a0g/L of dry cell weight and 70.56% of PHA content. However, ammonium sulfate as the nitrogen source was found to be more suitable for fed-batch cultivation. Several feeding strategies including pH-based fed-batch and variants of pulse feeding were studied to improve the PHA levels. pH-based feeding, although improved PHA level to 26\u00a0g/L, most of the carbon flux was diverted towards biomass formation; hence, the percent PHA was only 39.15% of the dry cell weight. Maximum PHA of 33.4\u00a0g/L, which corresponded to 88.12% of the dry cell, was obtained from high concentration single pulse method. There was a net 8.65-fold increase in PHA using this feeding strategy when compared to batch studies. According to our knowledge, this is the highest amount of PHA reported for a Halomonas venusta strain.\n\n==\n\nLakshman 2004:\n\nTitle: Simultaneous and comparative assessment of parent and mutant strain of Rhizobium meliloti for nutrient limitation and enhanced polyhydroxyalkanoate (PHA) production using optimization studies\n\nAbstract: Abstract Nutrient limitation conditions, optimization and comparison of polyhydroxyalkanoate (PHA) yields and biomass production by parent and mutant strains of Rhizobium meliloti were investigated. Complex interactions among concentrations of sucrose (5\u201355\u00a0g/l), urea (0.05\u20130.65\u00a0g/l) inoculum (10\u2013250\u00a0ml/l) and K2HPO4 (0.5\u20132\u00a0g/l), were studied using central composite rotatable design (CCRD) experiments. Phosphate-limiting medium (0.33\u00a0g K2HPO4/l) in the presence of excess carbon (sucrose 42.5\u00a0g/l) results in more production of PHA (2.2\u00a0g/l) in the parent strain. In comparison, the mutant strain required moderate levels of sucrose (30\u00a0g/l), along with excess of phosphate (1\u00a0g/l) for high PHA content of cell biomass (80%) and PHA yield (3.3\u00a0g/l). Optimised PHA production (biomass 4.8\u00a0g/l and PHA 3.09\u00a0g/l) by the parent strain occurred at: sucrose 51.58\u00a0g/l, urea 0.65\u00a0g/l, K2HPO4 0.48\u00a0g/l and inoculum 10\u00a0ml/l. In the mutant strain, higher yields of biomass (9.05\u00a0g/l) and PHA (5.66\u00a0g/l) were obtained in Optimised medium containing: sucrose 55\u00a0g/l, urea 0.65\u00a0g/l, K2HPO4 1.0\u00a0g/l and inoculum 150.58\u00a0ml/l.\n\n==\n\nKoller 2005:\n\nTitle: Production of polyhydroxyalkanoates from agricultural waste and surplus materials.\n\nAbstract: To be competitive with common plastics, the production costs of polyhydroxyalkanoates (PHAs) have to be minimized. Biotechnological polymer production occurs in aerobic processes; therefore, only about 50% of the main carbon sources and even a lower percentage of the precursors used for production of co-polyesters end up in the products wanted. A second cost factor in normally phosphate-limited production processes for PHAs is the costs for complex nitrogen sources. Both cheap carbon sources and cheap nitrogen sources are available from agricultural waste and surplus materials and make a substantial contribution for minimizing PHA production costs. In this study, fermentations for PHA production were carried out in laboratory-scale bioreactors on hydrolyzed whey permeate and glycerol liquid phase from the biodiesel production using a highly osmophilic organism. Without any precursor, the organism produced a poly[3(hydroxybutyrate-co-hydroxyvalerate)] copolyester on both carbon sources. During the accumulation phases, a constant 3-hydroxyvalerate content of 8-10% was obtained at a total PHA concentration of 5.5 g/L (on hydrolyzed whey permeate) and 16.2 g/L (glycerol liquid phase). In an additional fermentation, an expensive nitrogen source was substituted by meat and bone meal beside the glycerol liquid phase as a carbon source, resulting in a final PHA concentration of 5.9 g/L.\n\n==\n\nRay 2016:\n\nTitle: Optimization and characterization of PHA from isolate Pannonibacter phragmitetus ERC8 using glycerol waste.\n\nAbstract: Polyhydroxyalkanoates (PHAs) have been considered as a good alternative for petrochemical based polymers due to its biodegradability. However, a high production cost limits their acceptance in industries. In present work, efforts have been made to optimize the production of PHA by Pannonibacter phragmitetus ERC8 using glycerol waste as a sole carbon source, with enhanced polymer production in a cost effective way. To check the possibility of growth and polymer accumulation potential of P. phragmitetus ERC8, various low cost substrates such as food waste, mutton tallow, whey, sugarcane bagasse, corn steep liquor and glycerol waste were used. Optimum concentration of selected factors obtained as response of statistical experimental design were 0.8% (v/v) glycerol waste, 0.26% (w/v) BHM and 1.25%OD as an inoculum for the maximum PHA production. The suggested model was validated and maximum 1.36 g/L of PHA production was obtained after 96 h. PHA production of 1.87 g/L was achieved in 5L (working volume 3 L) lab scale bioreactor with the suggested media components by RSM (Response Surface Methodology). Characterization of the PHA by NMR spectroscopy revealed that the polymer was a hetromonomer of (R)-3-hydroxybutyrate and medium chain length 3HA[(R)-3-hydroxyalkanoate] monomers.\n\n==\n\nWen 2010:\n\nTitle: Effects of phosphorus and nitrogen limitation on PHA production in activated sludge.\n\nAbstract: The effects of phosphorus and nitrogen limitation on polyhydroxyalkanoate (PHA) production and accumulation by activated sludge biomass with acetate as a carbon source were investigated. Pre-selected influent carbon-phosphorus (C:P, W/W) of 100, 160, 250, 500 and 750, and carbon-nitrogen (C:N, W/W) of 20, 60, 100, 125 and 180 were applied in the phosphorus limitation experiments and the nitrogen limitation experiments, respectively. The maximum PHA accumulation up to 59% of the cell dry weight with a PHA productivity of 1.61 mg PHA/mg COD consumed was observed at the C:N 125 in the nitrogen limitation experiment. This value was much higher than that obtained in previous studies with a normal substrate feeding. The study showed that activated sludge biomass would produce more polyhydroxybutyrate than polyhydroxyvalerate under the stress of nutrient limitation, especially under phosphorus limitation conditions. The experimental result also indicated that both phosphorus and nitrogen limitation may cause sludge bulking.\n\n==\n\nGowda 2014:\n\nTitle: Agrowaste-based Polyhydroxyalkanoate (PHA) production using hydrolytic potential of Bacillus thuringiensis IAM 12077\n\nAbstract: The study identified the innate enzymatic potential (amylase) of the PHB producing strain B.thuringiensis IAM 12077 and explored the same for cost-effective production of PHB using agrowastes, eliminating the need for pretreatment (acid hydrolysis and/or commercial enzyme). Comparative polyhydroxyalkanoate (PHA) production by B. thuringiensis IAM 12077 in biphasic growth conditions using glucose and starch showed\u00a0 appreciable levels of growth (5.7 and 6.8 g/L) and PHA production (58.5 and 41.5%) with a PHA yield of 3.3 and 2.8 g/L, respectively. Nitrogen deficiency supported maximum PHA yield (2.46 g/L) and accumulation (53.3%). Maximum growth (3.6 g/L), PHB yield (2.6 g/L) and PHA accumulation (72.8%) was obtained with C:N ratio of 8:1 using starch as the carbon source (10 g/L). Nine substrates (agro and food wastes) viz. rice husk, wheat bran, ragi husk, jowar husk, jackfruit seed powder, mango peel, potato peel, bagasse and straw were subjected to two treatments- acid hydrolysis and hydrolysis by innate enzymes, and\u00a0 the reducing sugars released thereby were utilized for polymer production. All the substrates tested supported comparable PHB production with acid hydrolysis (0.96 g/L-8.03 g/L) and enzyme hydrolysis (0.96 g/L -5.16 g/L). Mango peel yielded the highest PHB (4.03 g/L; 51.3%), followed by\u00a0 jackfruit seed powder (3.93 g/L; 29.32%). Varied levels of amylase activity (0.25U-10U) in all the substrates suggested the enzymatic hydrolysis of\u00a0 agrowastes.\n\n==\n\nZhang 2018:\n\nTitle: Polyhydroxyalkanoates (PHA) production from phenol in an acclimated consortium: Batch study and impacts of operational conditions.\n\nAbstract: Microbial intracellular biopolymer PHA was synthesized from toxic pollutant phenol by an acclimated consortium. Various operational conditions were experimented for their effects on biomass growth and PHA accumulation. Carbon to nitrogen ratios from 5 to 40 (w/w) showed little impact, as did the levels of Fe, Ca and Mg in a short term. Acidic pH inhibited both growth and PHA synthesis, and an optimal dissolved oxygen level of 1-4\u202fmg\u202fL-1 was identified. Low temperature (7\u202f\u00b0C) significantly slowed but did not totally repress microbial activities. A 2% NaCl shock retarded reactions and 4% NaCl caused irreversible damage. Various initial phenol (S0) and biomass concentrations (X0) were combined to study the effect of food to microbe (F/M) ratio. High S0 and F/M exerted toxicity, reducing reaction rates but generating higher ultimate PHA wt% in biomass. Increasing X0 alleviated phenol inhibition and improved productivity and carbon conversion from phenol. A pseudo-optimized F/M ratio of 0.2-0.4 and a maximum PHA% rate of 1.15%\u202fmin-1 were identified under medium S0/high X0. This study is the first to systematically investigate the feasibility of toxic industrial waste as the carbon source for PHA production, and likely the only one indicating potential for scaling-up and industrialization.", "final_summary": "Arumugam (2019) found that urea was the optimal nitrogen source for PHA production in Cupriavidus necator using cashew apple juice as a carbon source. However, the study did not specify the optimal urea concentration or the corresponding PHA yield. \n\nStanley (2017) found that ammonium sulfate was more suitable for fed-batch cultivation for PHA production in Halomonas venusta KT832796. The study did not specify the concentration of ammonium sulfate used.\n\nLakshman (2004) found that a urea concentration of 0.65 g/L was used in the mutant strain of Rhizobium meliloti for high PHA content and yield. However, the study did not explicitly state that this concentration resulted in optimized PHA production.\n\nKoller (2005) used agricultural waste and surplus materials as cheap carbon and nitrogen sources for PHA production. The study did not specify the concentration of nitrogen used.\n\nRay (2016) optimized the production of PHA using glycerol waste as a sole carbon source. The study did not specify the concentration of nitrogen used.\n\nWen (2010) investigated the effects of phosphorus and nitrogen limitation on PHA production. The study found that both phosphorus and nitrogen limitation may cause sludge bulking, but did not specify the concentration of nitrogen used.\n\nGowda (2014) found that nitrogen deficiency supported maximum PHA yield in Bacillus thuringiensis IAM 12077. The study did not specify the concentration of nitrogen used.\n\nZhang (2018) found that increasing the biomass concentration improved productivity and carbon conversion from phenol for PHA production. However, the study did not explicitly mention the effect of high initial phenol concentrations on productivity and carbon conversion.\n\nIn conclusion, while some studies found that urea was used as a nitrogen source for PHA production, others found that other nitrogen sources were more suitable. The concentration of urea varied among the studies that specified it. Further research is needed to determine the optimal concentration of urea for PHA production."}, {"query": "why study purchase intention study", "paper_list_string": "Bebber 2017:\n\nTitle: Antecedents of Purchase Intention in the Online Context\n\nAbstract: ABSTRACT The understanding of the determinant factors of customer purchase intention is necessary, and it is equally important to study the online purchase context, since this context is disseminated among customers. A theoretical model has been elaborated on and tested, considering the constructs of information quality, distrust, and perceived risk as antecedents of purchase intention, and aiming to analyze the relationship among these constructs in the online purchase context. A quantitative research study has been performed by means of the application of a survey. Multivariate statistics techniques have been applied for data analyses, including structural equation modeling. This study contributes to the evolution of the empirically tested concepts by providing a greater individual understanding of each construct presented in the theoretical model, as well as the relationship among them as determinants of purchase intention; the indication is that meaningful relationships were found which may impact greater profitability and, consequently, greater competition for online retailers.\n\n==\n\nToldos-Romero 2015:\n\nTitle: Brand personality and purchase intention\n\nAbstract: Purpose \u2013 The purpose of this paper is to analyze the effects of brand personality dimensions on purchase intention. Furthermore, the brand personality dimensions are compared to study the differences between users and non-users of 12 brands. Design/methodology/approach \u2013 An estimated 400 undergraduate students participated. They were given a questionnaire divided into two sessions (six brands of think products in one session and six brands of feel products in another session). In the end, 313 participants completed the questionnaire on the six brands of think products, and 320 completed the questionnaire on the six brands of feel products. Findings \u2013 Multiple regression analysis revealed that Hipness/Vivacity, Success, Sincerity and Sophistication brand personality dimensions are significant predictors of purchase intention. In addition, Domesticity/Emotionality and Professionalism also explain purchase intention but with a negative weight. The results are also broken down into product categories. Compar...\n\n==\n\nYounus 2015:\n\nTitle: Identifying the Factors Affecting Customer Purchase Intention\n\nAbstract: In the worst competitive market the consumer products manufacturing industries pay attention on customer purchase intention for maintain their repute in market and enhanced their goodwill. Because loyal customer are good source for create revenue. This study learns and contributes the factors that affect customer purchase intention. The purpose of this study is to observe the effect of independent variable (customer knowledge ,purchase intention, celebrity endorsement and perceived value ) on dependent variable (purchase intention).The study describe that the relation between dependent variable have significant relationship with purchase intention. This is quantitative study and sample size of this study is 100. And 100 questionnaires were used for collection of data. The results of this study shows that perceived value, customer knowledge, celebrity endorsement have significant relationship with purchase intention.\n\n==\n\nApdillah 2022:\n\nTitle: WORD OF MOUTH RELATIONSHIP ANALYSIS ON PURCHASE INTENTION AND ITS IMPACT ON PURCHASE DECISIONS\n\nAbstract: This study aims to analyze the relationship of Word of Mouth in Purchase Intention toward Purchase Decision. This study is a quantitative study using SEM PLS. The sample in this study was 100 respondents to consumers of Alfamart Kramat Pulo 2 with a non-probability sampling technique, namely a sampling technique that provides equal opportunities for each element (member) of the population to be selected as a member of the sample. The data the analyzed using validity test and hypothesis test to obtain the result. The finding reveals that by empirical analysis, word of mouth has a statistically significant positive impact on purchasing interest, such that as word of mouth improves, so does purchasing intention. Meanwhile, in empirical evidence, purchasing intention has a positive and significant impact on purchasing decisions, such that if the purchasing interest in a product becomes increasingly attached to the minds of customers, the likelihood of making a purchase increases. Furthermore, according to empirical research, word of mouth has a positive and statistically significant impact on purchasing decisions.\n\n==\n\nWang 2013:\n\nTitle: Understanding the purchase intention towards remanufactured product in closed-loop supply chains: An empirical study in China\n\nAbstract: Purpose \u2013 The paper aims to explore the reasons underlying the key assumption in the closed-loop supply chain (CLSC) literature that consumers' purchase intention is lower for remanufactured products than for new products. It aims to complement the predominantly operation-focused CLSC research by examining consumers' perception of and behavior relating to remanufactured products. Design/methodology/approach \u2013 A theoretical model is developed by integrating the concepts of perceived benefits and product knowledge with the theory of planned behavior and the theory of perceived risk. Then the model is examined through an empirical study in the Chinese automobile spare parts industry involving 288 respondents and using structural equation modeling. Findings \u2013 The results indicate that purchase intention is directly influenced by purchase attitude followed by perceived behavioral control and indirectly influenced by perceived risk, perceived benefit and product knowledge via attitude. Therefore, effective meas...\n\n==\n\nTakaya 2019:\n\nTitle: Antecedents Analysis of Purchase Intention\n\nAbstract: The advancement of technology has resulted in the creation of a new form of shopping transactions. This technology is used by residents to shop online. Thus, customers\u2019 involvements in online purchasing have become an important trend.\u00a0 The objective of this research was to identify the determinants of customer purchases online. This study used a surveymethod using questionnaires and the target is an online customer in Central Jakarta.This research used simple regression to determine the effect of purchace intention to factors that influence it. Data questionnaire distributed directly to the respondents who never buy online shopping.\u00a0 Findings revealed that impulse purchase intention, quality orientation, brand orientation, online trust and prior online purchase experience were positively related to the customer online purchase intention.\n\n==\n\nDelafrooz 2011:\n\nTitle: Understanding consumers internet purchase intention in Malaysia\n\nAbstract: This study aims to explore the antecedents relating to the extent of both the attitude and the purchasing intention of online shopping. It examined the factors influencing consumers\u2019 attitude toward online shopping and shopping intention from the Malaysian perspectives. From an e-commerce perspective, the understanding of the Theory of Reasoned Action (TRA), Theory of Planned Behavior (TPB), and Technology Acceptance Model (TAM) could provide a valid basis in explaining and predicting consumers\u2019 intention towards adopting an online shopping behavior. A purposive sampling method was used and the sample comprised of 370 students studying at a public University in Malaysia. Data were collected via self-administered questionnaire. The data were examined using frequency and path analysis. Result of path analysis showed that trust and attitude had stronger direct effect on online shopping intention, whereas utilitarian orientation, convenience, prices wider selection, and income had stronger indirect effect on online shopping intention through the attitude towards online shopping as mediation. This paper outlined the key online shopping intention and events in Malaysia and pioneered the building of an integrated research framework to understand how consumers form their attitude and make purchase intentions toward online shopping. \n \n \u00a0 \n \n Key words:\u00a0Intention, attitude, online shopping, consumer, Malaysia.\n\n==\n\nMorwitz 2006:\n\nTitle: When Do Purchase Intentions Predict Sales?\n\nAbstract: Marketing managers routinely use purchase intentions to predict sales. The purpose of this paper is to identify factors associated with an increased or decreased correlation between purchase intentions and actual purchasing. In two studies, we examine data collected from a wide range of different settings that reflect the real world diversity in how intentions studies are conducted. The results indicate that intentions are more correlated with purchase: 1) for existing products than for new ones, 2) for durable goods than for non-durable goods, 3) for short than for long time horizons, 4) when respondents are asked to provide intentions to purchase specific brands or models than when they are asked to provide intentions to buy at the product category level, 5) when purchase is measured in terms of trial rates than when it is measured in terms of total market sales, and 6) when purchase intentions are collected in a comparative mode than when they are collected monadically.", "final_summary": "Purchase intention studies are crucial in understanding consumer behavior and driving business strategies. They provide insights into the factors that influence consumers' decisions to buy a product or service (Bebber 2017; Toldos-Romero 2015; Younus 2015; Apdillah 2022; Wang 2013; Takaya 2019; Delafrooz 2011; Morwitz 2006).\n\nBebber (2017) emphasizes the importance of understanding the online purchase context, highlighting the role of information quality, distrust, and perceived risk as antecedents of purchase intention. Takaya (2019) also underscores the significance of the online purchase context, identifying impulse purchase intention, quality orientation, brand orientation, online trust, and prior online purchase experience as determinants of customer online purchase intention.\n\nToldos-Romero (2015) underscores the influence of brand personality dimensions on purchase intention, suggesting that certain traits can significantly predict buying behavior. Younus (2015) focuses on the factors that affect customer purchase intention, such as customer knowledge, celebrity endorsement, and perceived value. Apdillah (2022) specifically analyzes the relationship between word of mouth and purchase intention.\n\nWang (2013) delves into consumers' perception of remanufactured products and their influence on purchase intention. Delafrooz (2011) explores the factors influencing consumers' attitude toward online shopping and shopping intention, including trust, attitude, utilitarian orientation, convenience, prices, wider selection, and income.\n\nLastly, Morwitz (2006) provides a comprehensive analysis of when purchase intentions predict sales, identifying several conditions that increase the correlation between intentions and actual purchasing.\n\nIn conclusion, studying purchase intention is crucial as it helps businesses understand the factors that drive consumers' buying decisions, enabling them to tailor their strategies accordingly. These studies collectively highlight the multifaceted nature of purchase intention, influenced by a myriad of factors ranging from brand personality to perceived risk and online trust (Bebber 2017; Toldos-Romero 2015; Younus 2015; Apdillah 2022; Wang 2013; Takaya 2019; Delafrooz 2011; Morwitz 2006)."}, {"query": "Aim. The objective of our study has been to evaluate the WHO-5 as a new early screening instrument for apathy in a group of elderly persons. Methods. The WHO-5 was compared to the Geriatric Depression Scale (GDS-15). The GDS contains five items measuring well-being and ten items measuring depression. The internal validity of the WHO-5 (total score being a sufficient statistic) was evaluated with both parametric and nonparametric item response theory models. The external validity of the WHO-5 and the GDS was evaluated by ROC using depression as index of validity. Results. The item response theory analyses confirmed that the total score of the WHO-5 is a sufficient statistic. The ROC analysis shows an adequate sensitivity (61%) and specificity (84%). The GDS15 and its two subscales obtained low sensitivity (25\u201342%), but high specificity (90\u201398%). Conclusion. The WHO-5 was found both internally and externally valid when considering decreased positive well-being to be an early indication of apathy reflecting that the wind has begun to be taken out of the \u201cmotivation sail.\u201d", "paper_list_string": "Lucas-Carrasco 2012:\n\nTitle: The Validity of the WHO-5 as an Early Screening for Apathy in an Elderly Population\n\nAbstract: Aim. The objective of our study has been to evaluate the WHO-5 as a new early screening instrument for apathy in a group of elderly persons. Methods. The WHO-5 was compared to the Geriatric Depression Scale (GDS-15). The GDS contains five items measuring well-being and ten items measuring depression. The internal validity of the WHO-5 (total score being a sufficient statistic) was evaluated with both parametric and nonparametric item response theory models. The external validity of the WHO-5 and the GDS was evaluated by ROC using depression as index of validity. Results. The item response theory analyses confirmed that the total score of the WHO-5 is a sufficient statistic. The ROC analysis shows an adequate sensitivity (61%) and specificity (84%). The GDS15 and its two subscales obtained low sensitivity (25\u201342%), but high specificity (90\u201398%). Conclusion. The WHO-5 was found both internally and externally valid when considering decreased positive well-being to be an early indication of apathy reflecting that the wind has begun to be taken out of the \u201cmotivation sail.\u201d\n\n==\n\nHeun 1999:\n\nTitle: Internal and external validity of the WHO Well\u2010Being Scale in the elderly general population\n\nAbstract: The objectives of this study were (i) to evaluate the validity of the WHO Well\u2010Being Scale in elderly subjects and (ii) to assess the influence of demographic variables on subjective quality of life. A sample of 254 elderly subjects completed the 22\u2010item WHO Well\u2010Being Scale. The scale had an adequate internal and external validity. However, the short 10\u2010item and 5\u2010item versions were equally valid. Low scores indicating decreased well\u2010being were related to the presence of a psychiatric disorder or, independently, to poor living conditions. The Well\u2010Being Scale and their short versions would appear to be useful instruments for identifying subjects with reduced subjective quality of life.\n\n==\n\nBertens 2017:\n\nTitle: Validity of the three apathy items of the Geriatric Depression Scale (GDS\u20103A) in measuring apathy in older persons\n\nAbstract: The Geriatric Depression Scale (GDS)\u20103A, a three\u2010item subset of the GDS\u201015, is increasingly used as a measure for apathy in research settings to assess factors associating with this neuropsychiatric syndrome. We aimed to assess how accurately the GDS\u20103A discriminates between presence and absence of apathy in two populations of community\u2010dwelling older persons, using the Apathy Scale as reference standard.\n\n==\n\nAllgaier 2013:\n\nTitle: Beside the Geriatric Depression Scale: the WHO\u2010Five Well\u2010being Index as a valid screening tool for depression in nursing homes\n\nAbstract: The aim of the study was to compare criterion validities of the WHO\u2010Five Well\u2010being Index (WHO\u20105) and the Geriatric Depression Scale 15\u2010item version (GDS\u201015) and 4\u2010item version (GDS\u20104) as screening instruments for depression in nursing home residents.\n\n==\n\nHeun 2009:\n\nTitle: Validity of the five-item WHO Well-Being Index (WHO-5) in an elderly population\n\nAbstract: Background Depression has a high prevalence in the elderly population; however it often remains undetected. The WHO 5-item Well-Being Index (WHO-5) is a short screening instrument for the detection of depression in the general population, which has not yet been evaluated. The goals of the present study were: 1) to assess the internal and external validity of WHO-5 and 2) to compare the two recent versions of WHO-5.Study population and methods 367 subjects above 50 years of age were examined with the WHO-5. ICD-10 diagnoses were made using a structured interview (CIDI). The internal validity of the well-being index was evaluated by calculating Loevinger\u2019s and Mokken\u2019s homogeneity coefficients. External validity for detection of depression was evaluated by ROC analysis.Results The scale was sufficiently homogeneous (Loevinger\u2019s coefficient: version 1=0.38, version 2=0.47; Mokken coefficient \u03c4; 0.3 in nearly all items). ROC analysis showed that both versions adequately detected depression. Version 1 additionally detected anxiety disorders, version 2 being more specific for detection of depression.Conclusion The WHO-5 showed a good internal and external validity. The second version is a stronger scale and was more specific for the detection of depression. The WHO-5 is an useful instrument for identifying elderly subjects with depression.\n\n==\n\nYesavage 1982:\n\nTitle: Development and validation of a geriatric depression screening scale: a preliminary report.\n\nAbstract: A new Geriatric Depression Scale (GDS) designed specifically for rating depression in the elderly was tested for reliability and validity and compared with the Hamilton Rating Scale for Depression (HRS-D) and the Zung Self-Rating Depression Scale (SDS). In constructing the GDS a 100-item questionnaire was administered to normal and severely depressed subjects. The 30 questions most highly correlated with the total scores were then selected and readministered to new groups of elderly subjects. These subjects were classified as normal, mildly depressed or severely depressed on the basis of Research Diagnostic Criteria (RDC) for depression. The GDS, HRS-D and SDS were all found to be internally consistent measures, and each of the scales was correlated with the subject's number of RDC symptoms. However, the GDS and the HRS-D were significantly better correlated with RDC symptoms than was the SDS. The authors suggest that the GDS represents a reliable and valid self-rating depression screening scale for elderly populations.\n\n==\n\nHoyl 2000:\n\nTitle: [Depression in the aged: preliminary evaluation of the effectiveness, as an screening instrument, of the 5-item version of the Geriatric Depression Scale].\n\nAbstract: INTRODUCTION\nThe best approach to improve under-recognition of depression is routine screening, ideally using an instrument that is highly effective and easy to administer.\n\n\nOBJECTIVE\nTo test the effectiveness of the 5-item version of the Geriatric Depression Scale (5-GDS) for depression screening in a community-dwelling Chilean elderly population.\n\n\nPARTICIPANTS AND METHODS\n110 subjects were evaluated at the geriatric outpatient clinic of a university teaching hospital. Patients answered a questionnaire that included the Geriatric Depression Scale (GDS), and demographic information. Using the 15-item GDS score as reference standard to classify subjects as depressed, test characteristics of the 5-GDS were evaluated.\n\n\nRESULTS\nSubjects had a mean 15-item GDS score of 5.4; 47% classified as depressed. Depressed and not depressed subjects were similar with regard to demographics, educational level and comorbid conditions. The mean score was 1.9 for the 5-item GDS. Pearson correlation for 15-item and 5-item GDS scores was 0.92, p < 0.001. Using 15-item GDS score as reference standard, the 5-item GDS had a sensitivity of 0.88, specificity 0.90, positive predictive value 0.88 and negative predictive value 0.90.\n\n\nCONCLUSIONS\nThe 5-item GDS seems to be a promising screening tool for depression. If revalidated against clinical evaluation, it might be the preferred screening tool for depression in the Chilean community-dwelling elderly.\n\n==\n\nHalliday 2017:\n\nTitle: Validation of the WHO-5 as a first-step screening instrument for depression in adults with diabetes: Results from Diabetes MILES - Australia.\n\nAbstract: AIMS\nScreening for depression is recommended internationally. The World Health Organization's 5-item Well-being Index (WHO-5) is used clinically to screen for depression but its empirical suitability for this purpose is not well documented. We investigated the psychometric properties of the WHO-5 and its suitability for identifying likely depression in Australian adults with diabetes.\n\n\nMETHODS\nThe Diabetes MILES - Australia study dataset provided a sample of N=3249 who completed the WHO-5 (positively-worded 5-item measure of emotional well-being) and the PHQ-9 (9-item measure of depressive symptoms). Analyses were conducted for the full sample, and separately by diabetes type and treatment (type 1, non-insulin-treated type 2, and insulin-treated type 2 diabetes). Construct (convergent and factorial) validity and reliability of the WHO-5 were examined. ROC analyses were used to examine the sensitivity and specificity of the WHO-5 as a depression screening instrument, comparing two commonly used WHO-5 cut-off values (\u22647 and <13) with the PHQ-9.\n\n\nRESULTS\nFor the whole sample, the WHO-5 demonstrated satisfactory internal consistency reliability (\u03b1=0.90) and convergent validity with the PHQ-9 (r=-0.73, p<0.001). Confirmatory factor analysis partially supported factorial validity: \u03a72(5)=834.94, p<0.001; RMSEA=0.23, 90% CI 0.21-0.24; CFI=0.98, TLI=0.96; factor loadings=0.78-0.92. The AUC was 0.87 (95% CI: 0.86-0.89, p<0.001). The sensitivity/specificity of the WHO-5 for detecting likely depression was 0.44/0.96 for the \u22647 cut-off, and 0.79/0.79 for the <13 cut-off, with similar findings by diabetes type and treatment.\n\n\nCONCLUSIONS\nThese findings support use of a WHO-5 cut-point of <13 to identify likely depression in Australian adults with diabetes, regardless of type/treatment.", "final_summary": "The collective findings from the research papers suggest that the WHO-5 Well-being Index is a valid and reliable tool for early screening of apathy in the elderly population. Lucas-Carrasco (2012) found that the WHO-5 demonstrated both internal and external validity when used as an early screening instrument for apathy. This was supported by Heun (1999), who also found the WHO-5 to be valid in elderly subjects, with low scores indicating decreased well-being related to the presence of a psychiatric disorder or poor living conditions. \n\nIn comparison, the Geriatric Depression Scale (GDS-15) and its subsets showed varying results. Lucas-Carrasco (2012) found that the GDS-15 and its two subscales obtained low sensitivity but high specificity. \n\nHeun (2009) found that the WHO-5 showed good internal and external validity and was useful for identifying elderly subjects with depression. Yesavage (1982) found the GDS to be a reliable and valid self-rating depression screening scale for elderly populations. Hoyl (2000) found that the 5-item version of the GDS had a high correlation with the 15-item GDS score and had high sensitivity and specificity. \n\nHalliday (2017) validated the WHO-5 as a first-step screening instrument for depression in adults with diabetes and found that a cut-point of <13 on the WHO-5 could identify likely depression. \n\nIn conclusion, the WHO-5 appears to be a valid and reliable tool for early screening of apathy in the elderly population, while the GDS-15 and its subsets show varying results. Further research is needed to confirm these findings and to explore the use of these tools in different populations and settings."}, {"query": "recent experiments using the highly selective 5-HT2A receptor antagonist.", "paper_list_string": "Knight 2004:\n\nTitle: Pharmacological characterisation of the agonist radioligand binding site of 5-HT2A, 5-HT2B and 5-HT2C receptors\n\nAbstract: In the present study we compared the affinity of various drugs for the high affinity \u201cagonist-preferring\u201d binding site of human recombinant 5-HT2A, 5-HT2B and 5-HT2C receptors stably expressed in monoclonal mammalian cell lines. To ensure that the \u201cagonist-preferring\u201d conformation of the receptor was preferentially labelled in competition binding experiments, saturation analysis was conducted using antagonist and agonist radiolabels at each receptor. Antagonist radiolabels ([3H]-ketanserin for 5-HT2A receptor and [3H]-mesulergine for 5-HT2B and 5-HT2C receptor) bound to a larger population of receptors in each preparation than the corresponding agonist radiolabel ([125I]-DOI for 5-HT2A receptor binding and [3H]-5-HT for 5-HT2B and 5-HT2C receptor binding). Competition experiments were subsequently conducted against appropriate concentrations of the agonist radiolabels bound to the \u201cagonist-preferring\u201d subset of receptors in each preparation. These studies confirmed that there are a number of highly selective antagonists available to investigate 5-HT2 receptor subtype function (for example, MDL 100907, RS-127445 and RS-102221 for 5-HT2A, 5-HT2B and 5-HT2C receptors respectively). There remains, however, a lack of highly selective agonists. (\u2212)DOI is potent and moderately selective for 5-HT2A receptors, BW723C86 has poor selectivity for human 5-HT2B receptors, while Org 37684 and VER-3323 display some selectivity for the 5-HT2C receptor. We report for the first time in a single study, the selectivity of numerous serotonergic drugs for 5-HT2 receptors from the same species, in mammalian cell lines and using, exclusively, agonist radiolabels. The results indicate the importance of defining the selectivity of pharmacological tools, which may have been over-estimated in the past, and highlights the need to find more selective agonists to investigate 5-HT2 receptor pharmacology.\n\n==\n\nSoto 2017:\n\nTitle: Novel Bivalent 5-HT2A Receptor Antagonists Exhibit High Affinity and Potency in Vitro and Efficacy in Vivo.\n\nAbstract: The 5-HT2A receptor (5-HT2AR) plays an important role in various neuropsychiatric disorders, including substance use disorder and schizophrenia. Homodimerization of this receptor has been suggested, but tools that allow direct assessment of the relevance of the 5-HT2AR:5-HT2AR homodimer in these disorders are necessary. We chemically modified the selective 5-HT2AR antagonist M100907 to synthesize a series of homobivalent ligands connected by ethylene glycol linkers of varying lengths that may be useful tools for probing 5-HT2AR:5-HT2AR homodimer function. We tested these molecules for 5-HT2AR antagonist activity in a cell line stably expressing the functional 5-HT2AR and quantified a downstream signaling target, activation (phosphorylation) of extracellular regulated kinases 1/2 (ERK1/2), in comparison to in vivo efficacy of altering spontaneous or cocaine-evoked locomotor activity in rats. All of the synthetic compounds inhibited 5-HT-mediated phosphorylation of ERK1/2 in the cellular signaling assay; the potency of the bivalent ligands varied as a function of linker length, with the intermediate linker lengths being the most potent. The Ki values for the binding of bivalent ligands to 5-HT2AR were only slightly lower than the values for the parent (+)-M100907 compound, but significant selectivity for 5-HT2AR over 5-HT2BR or 5-HT2CR binding was retained. In addition, the 11-atom-linked bivalent 5-HT2AR antagonist (2 mg/kg, intraperitoneally) demonstrated efficacy on par with that of (+)-M100907 in inhibiting cocaine-evoked hyperactivity. As we develop further strategies for ligand-evoked receptor assembly and analyses of diverse signaling and functional roles, these novel homobivalent 5-HT2AR antagonist ligands will serve as useful in vitro and in vivo probes of 5-HT2AR structure and function.\n\n==\n\nWillmann 2021:\n\nTitle: Radiosynthesis and Biological Evaluation of [18F]R91150, a Selective 5-HT2A Receptor Antagonist for PET-Imaging.\n\nAbstract: Serotonergic 5-HT2A receptors in cortical and forebrain regions are an important substrate for the neuromodulatory actions of serotonin in the brain. They have been implicated in the etiology of many neuropsychiatric disorders and serve as a target for antipsychotic, antidepressant, and anxiolytic drugs. Positron emission tomography imaging using suitable radioligands can be applied for in vivo quantification of receptor densities and receptor occupancy for therapy evaluation. Recently, the radiosynthesis of the selective 5-HT2AR antagonist [18F]R91150 was reported. However, the six-step radiosynthesis is cumbersome and time-consuming with low radiochemical yields (RCYs) of <5%. In this work, [18F]R91150 was prepared using late-stage Cu-mediated radiofluorination to simplify its synthesis. The detailed protocol enabled us to obtain RCYs of 14 \u00b1 1%, and the total synthesis time was reduced to 60 min. In addition, autoradiographic studies with [18F]R91150 in rat brain slices revealed the typical uptake pattern of 5-HT2A receptor ligands.\n\n==\n\nSchmidt 1992:\n\nTitle: The 5-HT2 receptor antagonist, MDL 28,133A, disrupts the serotonergic-dopaminergic interaction mediating the neurochemical effects of 3,4-methylenedioxymethamphetamine.\n\nAbstract: The selective 5-HT2 receptor antagonist MDL 28,133A dose dependently-blocked the long-term deficits in rat brain 5-HT concentrations produced by the substituted amphetamine analogue 3,4-methylenedioxymethamphetamine (MDMA). This protective effect of MDL 28,133A could be abolished by coadministration of the dopamine precursor, L-dihydroxyphenylalanine (L-DOPA). Electrophysiological experiments demonstrated that the ability of MDL 28,133A to block the MDMA-induced slowing of A9 dopaminergic neurons was also sensitive to L-DOPA administration. Both sets of experiments suggest an interaction of MDL 28,133A at the level of dopamine synthesis. Consistent with this explanation, MDL 28,133A antagonized the MDMA-induced stimulation of dopamine synthesis in vivo. MDMA-induced 5-HT release did not reduce the firing rate of dopaminergic neurons as assessed by dopamine depletion following synthesis inhibition with alpha-methyl-p-tyrosine (alpha-MPT). This indicates that the effect of 5-HT2 receptor antagonists on MDMA-induced dopamine synthesis is not due simply to the removal of an inhibitory serotonergic input followed by an increase in dopamine cell firing and autoreceptor activation. MDL 28,133A was also shown to be without effect on the sensitivity of terminal dopamine autoreceptors. The results are consistent with the hypothesis that 5-HT2 receptors are permissive for the stimulation of dopamine synthesis necessary to support MDMA-induced transmitter efflux.\n\n==\n\nBartoszyk 2003:\n\nTitle: EMD 281014, a new selective serotonin 5-HT2A receptor antagonist.\n\nAbstract: The 5-HT2A receptor ligand 7-[4-[2-(4-fluoro-phenyl)-ethyl]-piperazine-1-carbonyl]-1H-indole-3-carbonitrile HCl (EMD 281014) selectively binds to human (h) and rat 5-HT2A receptors (IC50 values 0.35 and 1 nM, respectively; vs. 1334 nM for h5-HT2C) and inhibited 5-HT-stimulated [35S]guanosine 5'-O-3-thiotriphosphate (GTPgammaS)-accumulation in h5-HT2A transfected Chinese hamster ovary cells (IC50 9.3 nM). EMD 28014 counteracted the N-ethoxycarbonyl-2-ethoxy-1,2-dihydroquinoline (EEDQ)-induced decrease of [3H]ketanserin binding in rat frontal cortex (ID50 0.4 mg/kg p.o.) and R-(-)-1-(2,5-dimethoxy-4-iodophenyl)-aminopropane (DOI)-induced head-twitch behaviour in mice (ID50 0.01 mg/kg s.c., 0.06 mg/kg p.o.), demonstrating unique selectivity and efficacy.\n\n==\n\nBONHAUS 1997:\n\nTitle: RS-102221: A Novel High Affinity and Selective, 5-HT2C Receptor Antagonist\n\nAbstract: The 5-HT2C receptor is one of three closely related receptor subtypes in the 5-HT2 receptor family. 5-HT2A and 5-HT2B selective antagonists have been described. However, no 5-HT2C selective antagonists have yet been disclosed. As part of an effort to further explore the function of 5-HT2C receptors, we have developed a selective 5-HT2C receptor antagonist, RS-102221 (a benzenesulfonamide of 8-[5-(5-amino-2,4-dimethoxyphenyl) 5-oxopentyl]-1,3,8-triazaspiro[4.5]decane-2,4-dione). This compound exhibited nanomolar affinity for human (pKi = 8.4) and rat (pKi = 8.5) 5-HT2C receptors. The compound also demonstrated nearly 100-fold selectivity for the 5-HT2C receptor as compared to the 5-HT2A and 5-HT2B receptors. RS-102221 acted as an antagonist in a cell-based microphysiometry functional assay (pA2 = 8.1) and had no detectable intrinsic efficacy. Consistent with its action as a 5-HT2C receptor antagonist, daily dosing with RS-102221 (2 mg/kg intraperitoneal) increased food-intake and weight-gain in rats. Surprisingly, RS-102221 failed to reverse the hypolocomotion induced by the 5-HT2 receptor agonist 1-(3-chlorophenyl)piperazine (m-CPP). It is concluded that RS-102221 is the first selective, high affinity 5-HT2C receptor antagonist to be described.\n\n==\n\nBryant 1996:\n\nTitle: A novel class of 5-HT2A receptor antagonists: aryl aminoguanidines.\n\nAbstract: Local delivery of serotonin (5-HT) produces a rapid edematous response in soft tissues via increased fluid extravasation which is prevented by 5-HT2 antagonists such as ketanserin or mianserin. Here we report the effects of a new class of aminoguanidine 5-HT2 antagonists, with relative selectivity for 5-HT2A receptors which are potent inhibitors of 5-HT-induced paw edema in the rat. Radioligand binding studies with 125I DOI on human 5-HT2A and 5-HT2C receptors and with 3H-5-HT on human 5-HT2B receptors demonstrated that, LY314228, and LY320954 displayed some selectivity for the 5-HT2A receptor. When compared to binding at other 5-HT2 receptor subtypes, LY314228 had an 18.6-fold greater affinity for the 5-HT2A site over the 5-HT2B site, and 2.6 fold greater at the 5-HT2C site. LY320954 displayed similar preference for 5-HT2A sites. Both compounds also inhibited 5-HT-induced paw swelling in rats, with ED50's of 6.4 and 4.8 mg/kg (for LY314228 and LY320954, respectively). These studies offer evidence for a novel class of pharmacophores for the 5-HT2 receptor family which show greater relative affinities for the 5-HT2A receptor subclass.\n\n==\n\nStevenson 2000:\n\nTitle: 2-Aryl tryptamines: selective high-affinity antagonists for the h5-HT2A receptor.\n\nAbstract: A series of 2-aryl tryptamines have been identified as high-affinity h5-HT2A antagonists. Structure-activity relationship studies have shown that h5-HT2A affinity can be attained via modifications to the tryptamine side chain and that selectivity over h5-HT2C and hD2 receptors can be controlled by suitable C-2 aryl groups.", "final_summary": "The body of research on the use of highly selective 5-HT2A receptor antagonists provides valuable insights into the field of neuropsychiatric disorders. \n\nKnight (2004) underscored the need for more selective agonists to investigate 5-HT2 receptor pharmacology, emphasizing the importance of defining the selectivity of pharmacological tools. Soto (2017) developed a series of homobivalent ligands that showed significant selectivity for 5-HT2AR over 5-HT2BR or 5-HT2CR binding, suggesting potential tools for probing 5-HT2AR:5-HT2AR homodimer function.\n\nWillmann (2021) discussed the use of the selective 5-HT2AR antagonist [18F]R91150 for in vivo quantification of receptor densities and receptor occupancy, indicating its potential for therapy evaluation. Schmidt (1992) explored the interaction of the selective 5-HT2 receptor antagonist MDL 28,133A at the level of dopamine synthesis, suggesting a permissive role for 5-HT2 receptors in the stimulation of dopamine synthesis.\n\nBartoszyk (2003) introduced EMD 281014, a 5-HT2A receptor ligand that demonstrated unique selectivity and efficacy. In a separate study, Bonhaus (1997) presented RS-102221 as the first selective, high affinity 5-HT2C receptor antagonist. Bryant (1996) reported a new class of aminoguanidine 5-HT2 antagonists with relative selectivity for 5-HT2A receptors, and Stevenson (2000) identified a series of 2-aryl tryptamines as high-affinity h5-HT2A antagonists.\n\nIn conclusion, these studies collectively highlight the potential of highly selective 5-HT2A receptor antagonists in the investigation and treatment of neuropsychiatric disorders. They emphasize the importance of selectivity in pharmacological tools and suggest various avenues for further exploration in the quest for more effective therapeutic interventions."}, {"query": "In the context of autonomous vehicles, the importance of the control zone, where vehicles exchange information about the autonomous intersection, and the case of overlaps of different intersections", "paper_list_string": "Li 2020:\n\nTitle: Intersection management for autonomous vehicles with vehicle-to-infrastructure communication\n\nAbstract: This paper proposes an intersection management strategy for autonomous vehicles under the vehicle-to-infrastructure circumstance. All vehicles are supposed to be fully autonomous and can communicate with the intersection management unit to check the traffic situation. Priority of passing the intersection is decided by a static conflict matrix which represents the potential conflict between lanes of different directions and a dynamic information list which could capture the real-time occupation of each lane in the intersection. Compared with the existing approaches in the literature, the intersection management unit in our strategy is more like a database rather than a computational center, and therefore, requires less computational resource and more likely satisfies the real-time requirement in heavy traffic situations. Simulations are conducted using SUMO (Simulation of Urban MObility), in which the proposed strategy is compared with both fixed and adaptive traffic light methods. The results indicate that the proposed strategy could significantly reduce the average time delay caused by the intersection and the corresponding variance, which shows the efficiency and fairness of the proposed strategy in intersection management.\n\n==\n\nWuthishuwong 2015:\n\nTitle: Safe trajectory planning for autonomous intersection management by using vehicle to infrastructure communication\n\nAbstract: The development of autonomous vehicle or self-driving car integrates with the wireless communication technology which would be a forward step for road transportation in the near future. The autonomous crossing of an intersection with an autonomous vehicle will play a crucial role in the future of intelligent transportation system (ITS). The fundamental objectives of this work are to manage autonomous vehicles crossing an intersection with no collisions, maintaining that a vehicle drives continuously, and to decrease the waiting time at an intersection. In this paper, a discrete model of the one-way single intersection is designed. The vehicle-to-infrastructure (V2I) communication is implemented to exchange information between a vehicle and an intersection manager which is the roadside infrastructure. The safe trajectory of autonomous vehicles for the autonomous intersection management is determined and presented by using discrete mathematics.\n\n==\n\nHausknecht 2011:\n\nTitle: Autonomous Intersection Management: Multi-intersection optimization\n\nAbstract: Advances in autonomous vehicles and intelligent transportation systems indicate a rapidly approaching future in which intelligent vehicles will automatically handle the process of driving. However, increasing the efficiency of today's transportation infrastructure will require intelligent traffic control mechanisms that work hand in hand with intelligent vehicles. To this end, Dresner and Stone proposed a new intersection control mechanism called Autonomous Intersection Management (AIM) and showed in simulation that by studying the problem from a multiagent perspective, intersection control can be made more efficient than existing control mechanisms such as traffic signals and stop signs. We extend their study beyond the case of an individual intersection and examine the unique implications and abilities afforded by using AIM-based agents to control a network of interconnected intersections. We examine different navigation policies by which autonomous vehicles can dynamically alter their planned paths, observe an instance of Braess' paradox, and explore the new possibility of dynamically reversing the flow of traffic along lanes in response to minute-by-minute traffic conditions. Studying this multiagent system in simulation, we quantify the substantial improvements in efficiency imparted by these agent-based traffic control methods.\n\n==\n\nWuthishuwong 2017:\n\nTitle: Consensus-based local information coordination for the networked control of the autonomous intersection management\n\nAbstract: Autonomous intersection management (AIM) will be a future method for improving traffic efficiency in the urban area. Instead of using the traffic signal control like nowadays, it uses wireless communication with autonomous vehicles to support the management of road traffic more safely and efficiently. A single AIM shows an exceptional performance in managing traffics at an intersection. However, it could not be represented a traffic in the real world, which is composed of multiple intersections. We show that coordination of traffic information among vehicles and infrastructures is an essential part of macroscopic traffic management. Coordination of traffic information among the network of AIMs is the key to improve the overall traffic flow throughout the network not only has an optimal flow in some intersections and very heavy traffic in others. In this paper, we introduce the distributed control to a graph-based intersection network to control traffic in a macroscopic level. Vehicle to infrastructure and infrastructure to infrastructure communication are used to exchange the traffic information between a single autonomous vehicle to the network of autonomous intersections. We implement a discrete time consensus algorithm to coordinate the traffic density of an intersection with its neighborhoods and determine the control policy to maximize a traffic throughput of each intersection as well as stabilizing the overall traffic in the network. We use the Greenshields traffic model to define the boundary condition of various traffic flows to the corresponded traffic density and velocity. Our proposed method represents the ability to maintain traffic flow rate of each intersection without having a back up traffic. As well, every intersection operates under the uncongested flow condition. The simulation results of the graph-based networked control of a multiple autonomous intersection showed that the overall traffic flow in the network achieves up to $$20\\%$$20% higher than using traffic signal system.\n\n==\n\nAzimi 2014:\n\nTitle: STIP: Spatio-temporal intersection protocols for autonomous vehicles\n\nAbstract: Autonomous driving is likely to be the heart of urban transportation in the future. Autonomous vehicles have the potential to increase the safety of passengers and also to make road trips shorter and more enjoyable. As the first steps toward these goals, many car manufacturers are investing in designing and equipping their vehicles with advanced driver-assist systems. Road intersections are considered to be serious bottlenecks of urban transportation, as more than 44% of all reported crashes in U.S. occur within intersection areas which in turn lead to 8,500 fatalities and approximately 1 million injuries every year. Furthermore, the impact of road intersections on traffic delays leads to enormous waste of human and natural resources. In this paper, we therefore focus on intersection management in Intelligent Transportation Systems (ITS) research. In the future, when dealing with autonomous vehicles, it is critical to address safety and throughput concerns that arise from autonomous driving through intersections and roundabouts. Our goal is to provide vehicles with a safe and efficient passage method through intersections and roundabouts. We have been investigating vehicle-to-vehicle (V2V) communications as a part of co-operative driving in the context of autonomous driving. We have designed and developed efficient and reliable intersection protocols to avoid vehicle collisions at intersections and increase traffic throughput. In this paper, we introduce new V2V intersection protocols to achieve the above goals. We show that, in addition to intersections, these protocols are also applicable to vehicle crossings at roundabouts. Additionally, we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols and suggest required modifications to guarantee their safety and efficiency despite these impairments. Our simulation results show that we are able to avoid collisions and also increase the throughput of the intersections up to 87.82% compared to common traffic-light signalized intersections.\n\n==\n\nDai 2016:\n\nTitle: Quality-of-Experience-Oriented Autonomous Intersection Control in Vehicular Networks\n\nAbstract: Recent advances in autonomous vehicles and vehicular communications are envisioned to enable novel approaches to managing and controlling traffic intersections. In particular, with intersection controller units (ICUs), passing vehicles can be instructed to cross the intersection safely without traffic signals. Previous efforts on autonomous intersection control mainly focused on guaranteeing the safe passage of vehicles and improving intersection throughput, without considering the quality of the travel experience from the passengers' perspective. In this paper, we aim to design an enhanced autonomous intersection control mechanism, which not only ensures vehicle safety and enhances traffic efficiency but also cares about the travel experience of passengers. In particular, we design the metric of smoothness to quantitatively capture the quality of experience. In addition, we consider the travel time of individual vehicles when passing the intersection in scheduling to avoid a long delay of some vehicles, which not only helps with improving intersection throughput but also enhances the system's fairness. With the above considerations, we formulate the intersection control model and transform it into a convex optimization problem. On this basis, we propose a new algorithm to achieve an optimal solution with low overhead. Finally, we build the simulation model and implement the algorithm for performance evaluation. Comprehensive simulation results demonstrate the superiority of the proposed algorithm.\n\n==\n\nWuthishuwong 2013:\n\nTitle: Coordination of multiple autonomous intersections by using local neighborhood information\n\nAbstract: Traffic congestion is considered one of the most important problems of road transportation. The concept of Intelligent Transportation System (ITS) has been introduced to improve the efficiency of the traffic flow. The autonomous vehicle technology is slowly becoming a reality, alongside wireless communication technology, which is becoming a major role in vehicle communication. Wireless communication devices will be generally used in vehicles for the future road transportation. The purpose of this research work is to use the local traffic information to reduce delay time of a vehicle at an intersection. In this paper, the fully autonomous intersection system is represented as a single intersection model. In addition, the multiple intersections are created based on the connection of a single autonomous intersection agent. The communication topology of the intersection network is modeled by using the undirected graph. The traffic information in the network is able to be exchanged by Infrastructure-to-Infrastructure (I2I) communication. This discrete consensus algorithm is implemented for coordinating the local traffic information and the neighborhood intersection. The proposed method works, based on the concept Green Wave, which maximizes the throughput, as well as minimizing the delay time. It maintains the traffic density of every intersection in the network to manage every vehicle for a continuous drive with a minimal delay time. The simulation of variable traffic flow rate of multiple autonomous intersections is implemented based on the proposed method.\n\n==\n\nGregoire 2016:\n\nTitle: Hybrid centralized/distributed autonomous intersection control: Using a job scheduler as a planner and inheriting its efficiency guarantees\n\nAbstract: We propose a hybrid centralized/distributed architecture to coordinate autonomous vehicles at intersections. The centralized job scheduler assigns and updates a service time for every vehicle - the time at which the vehicle should go through the intersection at maximum speed. Then, a distributed controller ensures service time is achieved while avoiding collisions. Efficiency bounds for the coordination system are inherited from efficiency guarantees of the job scheduling policy. It is remarkable that provably safe and efficient coordination can be performed without explicitly planning precise trajectories.", "final_summary": "In the realm of autonomous vehicles, the control zone, where vehicles exchange information about the autonomous intersection, plays a pivotal role in ensuring efficient and safe traffic management. This is particularly crucial in scenarios where different intersections overlap.\n\nLi (2020) proposes an intersection management strategy where the intersection management unit acts as a database, reducing the need for computational resources and enhancing real-time responses in heavy traffic situations. This strategy significantly reduces average time delay at intersections, demonstrating its efficiency and fairness.\n\nWuthishuwong (2015) emphasizes the importance of vehicle-to-infrastructure (V2I) communication in managing autonomous vehicles at intersections. The study presents a safe trajectory planning method for autonomous intersection management using discrete mathematics, ensuring no collisions and reduced waiting time at intersections.\n\nHausknecht (2011) extends the study of intersection management to a network of interconnected intersections. The study explores the potential of Autonomous Intersection Management (AIM) in controlling a network of intersections, highlighting the possibility of dynamically reversing traffic flow based on real-time traffic conditions.\n\nWuthishuwong (2017) introduces a distributed control to a graph-based intersection network to control traffic at a macroscopic level. The study emphasizes the importance of coordinating traffic information among the network of AIMs to improve overall traffic flow.\n\nAzimi (2014) introduces new V2V intersection protocols to increase the safety and efficiency of autonomous vehicles at intersections and roundabouts. The study also explores the impact of position inaccuracy of GPS devices on these protocols.\n\nDai (2016) proposes an enhanced autonomous intersection control mechanism that not only ensures vehicle safety and enhances traffic efficiency but also improves the travel experience of passengers. The study introduces the metric of smoothness to quantitatively capture the quality of experience.\n\nWuthishuwong (2013) uses local traffic information to reduce vehicle delay time at an intersection. The study introduces a discrete consensus algorithm for coordinating local traffic information and the neighborhood intersection.\n\nGregoire (2016) proposes a hybrid centralized/distributed architecture to coordinate autonomous vehicles at intersections. The study highlights the efficiency of the coordination system, which can be guaranteed by the efficiency of the job scheduling policy.\n\nIn conclusion, these studies collectively highlight the importance of the control zone in managing autonomous vehicles at intersections, particularly in cases of intersection overlaps. They emphasize the need for efficient communication and coordination strategies, such as V2I and V2V communications, to ensure safe and efficient traffic management. Future research should continue to explore innovative strategies for managing autonomous intersections, particularly in complex traffic scenarios involving multiple overlapping intersections."}, {"query": "Recurrence of gonadotroph pituitary adenomas after primary surgery", "paper_list_string": "Dubois 2007:\n\nTitle: Relevance of Ki-67 and prognostic factors for recurrence/progression of gonadotropic adenomas after first surgery.\n\nAbstract: OBJECTIVE\nGonadotropin-secreting pituitary adenomas carry a high risk of local recurrence or progression (R/P) of remnant tumor after first surgery. The clinical characteristics and the long-term outcome of these silent adenomas, which show no signs of endocrine hyperfunction, differ from those of other types of pituitary adenomas. However, to date, no study has focused specifically on gonadotropic adenomas.\n\n\nMATERIALS AND METHODS\nTo identify prognostic factors of R/P of remnants, we studied the postoperative outcome of 32 gonadotropic pituitary adenomas, defined on immunohistochemical staining, according to their clinical and radiological characteristics as well as the Ki-67 labeling index (LI).\n\n\nRESULTS\nThe Ki-67 LI failed to provide independent information for the identification of patients at risk of progression of remnants or recurrence. Multivariate survival analysis (Cox regression) showed that neither invasiveness nor remnant tumors nor hyposomatotropism influenced tumor recurrence. The strongest predicting factors of R/P were the antero-posterior (AP) diameter in the sagittal plane (P = 0.014), and the age of the patient at surgery (P = 0.047), with younger patients being at greater risk. Hazard ratios were 2.11 for each 5 mm increase in AP diameter and 0.57 for every 10 years of age.\n\n\nCONCLUSION\nThe two simple clinical criteria revealed by our study, the AP diameter of the tumor and the age of the patient, should be helpful in planning clinical management and radiological monitoring after first surgery of gonadotropic adenomas, while awaiting the identification of other pathological parameters.\n\n==\n\nManieri 2000:\n\nTitle: Gonadotroph cell pituitary adenomas in males.\n\nAbstract: BACKGROUND\nConsidered exceptional in the past, gonadotroph cell pituitary adenomas account for 3.5-6.4% of total surgically excised pituitary adenomas when examined with immunospecific staining. The aim of this study was to describe the clinical, hormonal, radiological and immunohistochemical features, the management and the follow-up of our patients with gonadotroph adenoma.\n\n\nMETHODS\nIn this retrospective study we describe 14 male subjects aged 19-70 yrs affected by gonadotroph cell pituitary adenomas; the patients were studied by hormonal, radiological and immunohistochemical investigations and followed up for 3-13 yrs by ambulatory and/or hospitalized care.\n\n\nRESULTS\nVisual impairment and/or decreased libido and erectile dysfunction were the symptoms at presentation. Increased serum gonadotropin concentrations were shown in 3 patients. Reduced levels of testosterone were present in 9 patients, and normal in the remainder. At diagnosis all patients had pituitary macroadenomas, with wide extrasellar extension in 12. All patients underwent trans-sphenoidal surgery and immunohistochemical staining of surgically excised specimens showed the presence of gonadotroph and alpha-subunit cells in all pituitary adenomas. After surgery 3 patients had clear radiological evidence of normal pituitary; in the others a doubtful MRI picture or a residual adenomatous tissue were present. In the patients who did not undergo radiotherapy immediately after surgery, a regrowth of tumoral tissue was shown in 1-10 yrs.\n\n\nCONCLUSIONS\nWe stress the importance of a close follow-up of patients with gonadotroph adenomas after surgery, and we raise the question of whether radiotherapy may be useful for avoiding any further adenomatous regrowth.\n\n==\n\nSalmi 1982:\n\nTitle: Recurrence of chromophobe pituitary adenomas after operation and postoperative radiotherapy\n\nAbstract: The rate of recurrence is reported in a prospective study of 56 patients (28 men, 28 women) with large chromophobe pituitary adenoma (with or without hyperprolactinemia). The surgical approach was transfrontal in 44 and transseptosphenoidal in 12 patients. Cryoapplication was combined with the transsphenoidal operation. All but one patient received post\u2010operative pituitary irradiation. Altogether, 11 (20 %) clinical relapses (10 men) occurred between 0.5 and 6 years after the transfrontal operation. Patients that relapsed had had larger tumors than those remaining in remission. Occurrence of the tumors appeared with a deterioration of the visual field defect in 9 patients. There were no differences in the degrees of hypopituitarism in patients who relapsed as compared to patients remaining in remission.\n\n==\n\nLosa 2004:\n\nTitle: Endocrine inactive and gonadotroph adenomas: diagnosis and management\n\nAbstract: Endocrine inactive pituitary adenomas represent about one quarter of all pituitary tumors. By immunocytochemistry, most of these tumors are positive for intact gonadotropins and/or their subunits. Clinical presentation is usually secondary to mass effect symptoms, such as visual disturbances, headache, and hypopituitarism. Differential diagnosis is usually accomplished by neuroradiologic studies, even though in selected cases positron emission tomography and/or single photon emission tomography may aid to distinguish pituitary adenomas from other endocrine inactive lesions, such as meningiomas and craniopharyngiomas. Surgical management is usually considered the first choice treatment for patients with endocrine inactive pituitary adenomas because it is very effective in ameliorating symptoms of chiasmal compression and headache. Radical removal of the tumor, however, is difficult to obtain because of the frequent invasiveness into the cavernous sinus. Radiation therapy diminishes the likelihood of tumor recurrence, especially in patients with demonstrable tumor remnants after surgery. Medical therapy with dopaminergic drugs, somatostatin analogs, or gonadotropin-releasing hormone agonists or antagonists causes mild reduction of tumor size in few patients and, therefore, seems to be of limited value in the therapeutic management of patients with endocrine inactive pituitary adenomas.\n\n==\n\nSnyder 1987:\n\nTitle: Gonadotroph cell pituitary adenomas.\n\nAbstract: The frequency of gonadotroph cell adenomas among all unselected pituitary adenomas is likely much higher than previously suspected. The prevalence in one series of 139 men with pituitary macroadenomas was 17 per cent (24 per cent if adenomas secreting only alpha subunit are included). The clinical characteristics of patients with gonadotroph cell adenomas are similar. Most are middle-aged men who have a history of normal pubertal development and a normal fertility history and by examination are normally virilized and have testes of normal size. They are brought to medical attention because of visual impairment, which is the result of the enormous size of the adenoma. The most common hormonal characteristics of gonadotroph cell adenomas in vivo is hypersecretion of FSH, which is often accompanied by hypersecretion of FSH-beta and alpha subunits and less often by hypersecretion of LH-beta or intact LH. Another common characteristic is secretion of FSH and/or LH-beta in response to TRH. A few patients with gonadotroph cell adenomas hypersecrete intact LH and therefore have supranormal serum testosterone concentrations. A larger number have secondary hypogonadism, because the adenomas are not secreting intact LH but are compressing the normal gonadotroph cells and impairing LH secretion. These patients have concentrations of intact LH that are not elevated, despite subnormal testosterone concentrations. The testosterone increases markedly in response to human chorionic gonadotropin. Both the clinical and hormonal characteristics of gonadotroph cell adenomas usually make them readily distinguishable from pituitary enlargement due to long-standing primary hypogonadism. Most gonadotroph cell adenomas are now managed first by transsphenoidal surgery to attempt to restore vision as quickly as possible, and then by supervoltage radiation to prevent regrowth of the remaining adenomatous tissue. Surgery usually does improve vision, as well as the pretreatment hormonal abnormalities, and radiation reduces FSH hypersecretion further. Dopamine agonist therapy is experimental but warrants further trial. The hormonal abnormalities detected prior to treatment, such as supranormal basal concentrations of FSH, alpha, and FSH-beta and the FSH and LH-beta responses to TRH, can be used to monitor the response to therapy.\n\n==\n\nMor 2005:\n\nTitle: Diagnosis of pituitary gonadotroph adenomas in reproductive-aged women.\n\nAbstract: OBJECTIVE\nTo describe the clinical symptoms associated with the diagnosis of pituitary gonadotroph adenoma in premenopausal women.\n\n\nDESIGN\nReport of three separate cases.\n\n\nSETTING\nUniversity medical center.\n\n\nPATIENT(S)\nThree patients: a 31-year-old woman with primary infertility, recurrent adnexal masses, and highly elevated estradiol level; a 30-year-old woman with recurrent multicystic ovaries following multiple cystectomies and transvaginal cyst aspirations, and elevated estradiol level; a 43-year-old woman with bilateral complex cystic adnexal masses and an elevated estradiol level, who underwent a total abdominal hysterectomy and bilateral salpingo-oophorectomy for a suspected granulosa cell tumor.\n\n\nINTERVENTION(S)\nTranssphenoidal resection of a pituitary mass.\n\n\nMAIN OUTCOME MEASURE(S)\nSerum estradiol, FSH, and LH levels; transvaginal ultrasonography of the ovaries; histologic examination of pituitary tumors.\n\n\nRESULT(S)\nTranssphenoidal resection of pituitary adenomas resulted in normalization of serum estradiol and FSH levels and resolution of adnexal masses in two of the women.\n\n\nCONCLUSION(S)\nPituitary gonadotroph adenoma must be considered in the differential diagnosis in reproductive-aged women presenting with the clinical symptom triad of new onset oligomenorrhea, bilateral cystic adnexal masses, and elevated estradiol and FSH levels with suppressed levels of LH; timely diagnosis may prevent unnecessary and potentially damaging surgical procedures.\n\n==\n\nLanglois 2017:\n\nTitle: Clinical profile of silent growth hormone pituitary adenomas; higher recurrence rate compared to silent gonadotroph pituitary tumors, a large single center experience\n\nAbstract: PurposeStudy and comparison of characteristics of silent growth hormone adenomas (SGHA), silent corticotroph adenomas (SCA), and silent gonadotroph adenomas (SGA) in a single institution cohort of surgically treated pituitary adenomas.MethodsRetrospective analysis of SGHA surgically resected over 10 years: SGHA was defined as no clinical or biochemical evidence of acromegaly and positive GH immunostaining.ResultsOf 814 pituitary surgeries; 2.1% (n\u2009=\u200917) were SGHA, 4.5% (n\u2009=\u200937) SCA, and 18.9% (n\u2009=\u200970/371; 2011\u20132016) SGA. Mean age at SGHA diagnosis was 43 years, with a large female predominance (82%). Mean tumor size and cavernous/sphenoid sinus invasiveness for SGHA, SCA, and SGA were 1.5\u2009\u00b1\u20091.0\u2009cm and 25%, 2.5\u2009\u00b1\u20091.2\u2009cm and 43%, 2.9\u2009\u00b1\u20092.0\u2009cm and 41%, respectively (tumor size p\u2009=\u20090.009, SGHA vs. SGA, and invasion p; not-significant). During mean follow-up of 3.9 years, two patients (11%) developed elevated insulin-like growth factor-1 and five patients (29%) required a second surgery for tumor recurrence. Rate of surgical reintervention was similar to SCA (31%), but higher than SGA (10%) (p\u2009=\u20090.035, SGHA vs. SGA), and 18% underwent radiation therapy, similar to SCA (19%, p; not-significant) but higher than SGA (2.9%, p\u2009=\u20090.018).ConclusionThis is the largest single center study characterizing SGHA behavior with SGA and SCA control groups in a cohort of surgically resected pituitary adenomas. SGHA present mostly in young females, and should be closely followed due to their higher likelihood of recurrence and potential of progression to clinical acromegaly. We propose that a complete hormonal staining panel be routinely performed for all pituitary adenomas.\n\n==\n\nYoung 1996:\n\nTitle: Gonadotroph adenoma of the pituitary gland: a clinicopathologic analysis of 100 cases.\n\nAbstract: OBJECTIVE\nTo determine the clinical and pathologic features in a large cohort of randomly selected patients with gonadotroph pituitary adenomas.\n\n\nDESIGN\nWe retrospectively reviewed clinical, surgical, and pathologic findings in 100 patients (79 men and 21 women, 30 to 82 years old) with this tumor.\n\n\nRESULTS\nDiagnosis of a pituitary tumor was prompted by visual loss (43%), symptoms of hypopituitarism (22%), headache (8%), or a combination of these findings (10%); 17% of the patients were asymptomatic. Visual field defects were present in 68% of the study group, and complete or partial anterior pituitary failure was present in 77%. Serum prolactin concentrations were increased (maximum, 110 ng/mL) in 33% of patients. Hypersecretion of follicle-stimulating hormone (FSH) and luteinizing hormone (LH) was documented in 11 and 5 patients, respectively. The serum level of alpha-subunit was increased in 1 of 29 patients in whom it was measured. All patients had pituitary macroadenomas, only 21% of which were grossly invasive. The surgical approach was transsphenoidal in all but two patients, who underwent transfrontal craniotomy; gross total tumor resection was achieved in 90%. By definition, all tumors had at least 10% immunoreactivity for LH, FSH, or both. All tumors were chromophobic. Ultrastructurally, the tumors were characterized as gonadotroph adenomas of \"male\" (45%) or \"female\" (9%) type as well as null-cell adenomas of the oncocytic (35%) or nononcocytic (11%) type. After a median follow-up of 4.3 years, 69% of the patients who had had visual field defects noted normalization or improvement. Persistent or clinically recurrent pituitary tumor tissue was present in 42%. A second pituitary surgical procedure was required in eight patients.\n\n\nCONCLUSION\nMost patients with clinically evident gonadotroph pituitary tumors have loss of vision and hypopituitarism. Hypersecretion of FSH or LH is unusual, and no distinct hormone-dependent clinical phenotype is present. Transsphenoidal surgical treatment generally yields normalization or improvement of visual field defects.", "final_summary": "Gonadotroph pituitary adenomas, a type of pituitary tumor, have been the subject of numerous studies due to their high recurrence rate after primary surgery. \n\nDubois (2007) found that the age of the patient at surgery and the antero-posterior diameter of the tumor were the strongest predictors of recurrence or progression. Younger patients and those with larger tumors were at a higher risk. Manieri (2000) emphasized the importance of close follow-up after surgery, suggesting that radiotherapy might be beneficial in preventing further adenomatous regrowth. \n\nSalmi (1982) reported a 20% clinical relapse rate between 0.5 and 6 years after surgery, with larger tumors being more likely to recur. Losa (2004) noted that radiation therapy reduces the likelihood of tumor recurrence, especially in patients with demonstrable tumor remnants after surgery. \n\nSnyder (1987) suggested that supervoltage radiation after surgery can prevent regrowth of the remaining adenomatous tissue. Young (1996) reported that persistent or clinically recurrent pituitary tumor tissue was present in 42% of patients, with a second pituitary surgical procedure required in eight patients. \n\nIn conclusion, the recurrence of gonadotroph pituitary adenomas after primary surgery is a significant concern. Factors such as the patient's age at surgery, tumor size, and the presence of tumor remnants after surgery can influence the likelihood of recurrence. Close follow-up and consideration of additional treatments such as radiotherapy may be beneficial in managing this condition. The findings from each paper contribute to our understanding of the recurrence of gonadotroph pituitary adenomas after primary surgery (Dubois, 2007; Manieri, 2000; Salmi, 1982; Losa, 2004; Snyder, 1987; Young, 1996)."}, {"query": "What do scenarios mean for the future of geopolitics? How are scenarios being used in the Arctic?", "paper_list_string": "Nilsson 2019:\n\nTitle: Towards improved participatory scenario methodologies in the Arctic\n\nAbstract: ABSTRACT Participatory scenario methodologies are increasingly used for studying possible future developments in the Arctic. They have the potential to contribute to several high-priority tasks for Arctic research, such as integration of indigenous and local knowledge in futures studies, providing a platform for activating Arctic youth in shaping their futures, identifying Arctic-relevant indicators for sustainable development, and supporting decision-making towards sustainable futures. Yet, to achieve this potential, several methodological challenges need to be addressed. These include attention to whose voices are amplified or silenced in participatory research practices, with special attention to diversification and the engagement of youth. Given the historic and potential future role of disruptive events for Arctic development trajectories, methods are needed in participatory scenario exercises to include attention to the dynamics and consequences of such events and regime shifts. Participatory scenarios can also be further improved through approaches that effectively combine qualitative and quantitative information. Finally, there is a need for systematic studies of how the results of scenario exercises influence decision-making processes. This article elaborates on ways in which attention to these aspects can help make scenarios more robust for assessing a diversity of potential Arctic futures in times of rapid environmental and social change.\n\n==\n\nErokhin 2020:\n\nTitle: Regional scenarios of the Arctic futures: A review\n\nAbstract: The future of the Arctic region is a subject of heated debates in both scientific and policy circles. The region has an enormous economic potential as a storehouse of mineral resources and as a provider of shorter and more cost-effective transportation between Europe and Asia. The Arctic is therefore an essential strategic element of the domestic and foreign policies of all Arctic states. In addition, there is an increasing economic interest in the region on the part of non-Arctic states. However, at present, the future of the Arctic region development remains highly uncertain. Scenario building is a suitable methodology to imagine alternative plausible futures of such a complex and multi-dimensional process and to elaborate successful and robust development strategies. This paper provides an overview of the scenario frameworks of Arctic futures presented in the literature and analyses key factors that determine these scenarios. Overall, we find a growing interest of the international foresight research community in the Arctic region that is evident from a number of thorough scenario-building exercises published recently. At the same time, we observe two drawbacks. First, the existing studies lack a numerical element, that is, the overwhelming majority of the scenario frameworks that can be found in the literature are fully qualitative. Quantitative estimates would strengthen the scenario narratives and enrich communication, which make them a useful addition to support a qualitative scenario framework. Second, the existing studies use a mixture of both internal and external factors to describe the underlying uncertainty. This limits the number of factors that can be taken into consideration and may be confusing for a potential user of these scenario frameworks due to the lack of a systemic view. Such a confusion can happen, for example, if some of the external factors underpinning a particular scenario suddenly develop in a direction that was not anticipated within the scenario framework. The effect of such a change on the set of scenarios and the validity of the scenarios despite this change will be of interest to the user, and a clear systems perspective would be conducive to address these questions. Separating internal and external factors in a scenario building exercise is particularly useful given that the volatility of the global geopolitical, geoeconomic and environmental dynamics is only increasing. It is our intention to address these two drawbacks in a scenario building exercise within the \u201cEmerging trade routes between Europe and Asia\u201d scenario-building project led by IIASA within the Northern Dimension Institute (NDI) Think Tank Action co-funded by the European Union and coordinated by Aalto University, Finland.\n\n==\n\nHaavisto 2016:\n\nTitle: Socio-economic scenarios for the Eurasian arctic by 2040\n\nAbstract: Improved weather and marine services (WMS) can have a role to play in the safe and secure development of the Arctic region through either a demand-pull (enhanced by growth in activity) or a supply-push (enhances growth in activity) process. To analyse the nature of the process and the future use and benefits of WMS, a better understanding of possible future developments in the Eurasian Arctic is needed. This report presents six socio-economic scenarios for the Eurasian Arctic by 2040, and a brief synopsis of the implications of each scenario for WMS. The scenarios focus on the development of shipping, resource extraction and tourism industries. The scenario futures, called Wild West, Silicon Valley, Exploited Colony, Shangri La, Conflict Zone and Antarctic, describe the scale and scope of activities in the Eurasian Arctic by 2040. The scenarios have three dimensions: open \u2013 closed, public private and dirty \u2013 clean, which describe the political, economic, social, technological and environmental aspects of different futures. The scenarios are based on a literature review, pre-survey, expert workshop and restructuring and analysis of this material. The methodology used for scenario construction is described in detail and may be used widely by other scenario developers. Our analysis shows that plenty of potential pressures for major changes in the Eurasian Arctic exist. Environmental changes, political shifts and technological development can all push forward drastic new developments in the region. Then again, it is possible that despite all the hype and interest, the Eurasian Arctic remains backwater areas in the global economy. This emphasizes the need for any decision-maker to be able to respond to very different futures. Therefore, robust decision making, a good eye for weak signals and tipping points, and the ability to prepare for risks and seize opportunities as they emerge is required in the Eurasian Arctic. The development of WMS is important in ensuring the safe and secure development of the Eurasian Arctic, unless the development follows the path of \u201cAntarctica\u201d with tourism and research as main activities in the marine regions.\n\n==\n\nKeys 2021:\n\nTitle: Visions of the Arctic Future: Blending Computational Text Analysis and Structured Futuring to Create Story\u2010Based Scenarios\n\nAbstract: The future of Arctic social systems and natural environments is highly uncertain. Climate change will lead to unprecedented phenomena in the pan\u2010Arctic region, such as regular shipping traffic through the Arctic Ocean, urban growth, military activity, expanding agricultural frontiers, and transformed Indigenous societies. While intergovernmental to local organizations have produced numerous synthesis\u2010based visions of the future, a challenge in any scenario exercise is capturing the \u201cpossibility\u201d space of change. In this work, we employ a computational text analysis to generate unique thematic input for novel, story\u2010based visions of the Arctic. Specifically, we develop a corpus of more than 2,000 articles in publicly accessible, English\u2010language Arctic newspapers that discuss the future in the Arctic. We then perform a latent Dirichlet allocation, resulting in 10 distinct topics and sets of associated keywords. From these topics and keywords, we design ten story\u2010based scenarios employing the M\u0101noa mashup, science fiction prototyping, and other methods. Our results demonstrate that computational text analysis can feed directly into a creative futuring process, whereby the output stories can be traced clearly back to the original topics and keywords. We discuss our findings in the context of the broader field of Arctic scenarios and show that the results of this computational text analysis produce complementary stories to the existing scenario literature. We conclude that story\u2010based scenarios can provide vital texture toward understanding the myriad possible Arctic futures.\n\n==\n\nZaikov 2019:\n\nTitle: Scenarios for the development of the Arctic region (2020\u20132035)\n\nAbstract: . The importance of selecting the development of the Arctic seems to be relevant since rapid and irreversible changes are taking place there. Climate change and globalization are their prominent examples. A complex of factors has both positive and negative impacts on the use of natural resources and the positioning of states located not only within the Arctic but also outside it. The questions arise: what is the significance of these changes for geography, politics, and the management system? How should the compre-hension of these processes be built? The relevance of the topic is enhanced by the fact that Russia has the most significant Arctic sector among the states with access to the Arctic Ocean. Therefore, our country has a leading role in working out strategies for the development of the Arctic. The comprehensive approach (considering the economic and political-geographical positions) is central in the article to analyze the directions of development of the Arctic territories. The method reveals the possibilities of sustainable development, which will provide Russia with strategic benefits within the Arctic and globally. The article discusses scenarios for the development of the Arctic, including the Arctic zone of the Russian Federation, in the long-term perspective (until 2035). Substantiation of the long-term prospects for the development of the Arctic, despite Russian and foreign research, seems to be unrealistic due to lack of knowledge about the nature and consequences of climatic changes currently observed in this region and affecting global environmental management. The authors concluded that the priority directions of the Arctic development should be the ones based on positive and innovative trends.\n\n==\n\nBrigham 2008:\n\nTitle: The Future of Arctic Marine Navigation in Mid-Century - Scenario Narratives\n\nAbstract: This document serves as the final Scenario Narratives Report for the Future of Arctic Marine Navigation in Mid-Century, a project of the Arctic Council\u2019s Protection of the Arctic Marine Environment (PAME) working group and Global Business Network (GBN), a member of the Monitor Group. The purpose of this project, and these scenarios, is to systematically consider the long-term social, technological, economic, environmental, and political impacts on Arctic Marine Navigation of Key Finding #6 of the Arctic Climate Impact Assessment (ACIA) published by the Arctic Council and the International Arctic Science Committee in November 2004. These scenarios are meant to summarize and communicate a set of plausible and different stories of the future in which critical uncertainties play out in ways that challenge planning decisions being made in the present. For this scenarios project on the Future of Arctic Marine Navigation, we convened two workshops to gather the perspectives and ideas of a highly diverse group of stakeholders. The first workshop was held at the GBN office in San Francisco in April 2007, and the second at the offices of Aker Arctic Technology in Helsinki in July 2007. Because this project rests on Key Finding #6, all of the scenarios assume continued global climate change that results in significantly less Arctic ice cover, at least in the summer, throughout the 2030s and 2040s. It is our intention that these scenarios will provide material for deeper discussions about the future and earlier decisions by the countries, peoples, and industries active in the Arctic region.\n\n==\n\nWormbs 2017:\n\nTitle: Arctic Futures: Agency and Assessing Assessments\n\nAbstract: The future is a common theme in discussions of the Arctic, whether in media, policy, or scientific communications. The future is not a given, and there are several possible futures that different actors strive to enable at any given time. At present considerable attention is given to monolithic \u201cdrivers\u201d of change in this region, including melting sea ice, technological development, and global resource geopolitics; and although this discourse is far from new (Doel et al. 2014b), the end of the Cold War and the amplification of climate and global change have reframed the discussion (Christensen et al. 2013). The media has played a large role in propagating the \u201cdrivers\u201d discourse, usually understating the role of human agency. While recognizing that these \u201cdriving\u201d factors are important, this chapter will analyze some of this \u201cfuture-talk\u201d, in relation to the future of the Arctic. We would argue that there is considerable discursive power (Foucault 2002) in these images of the future, which explains why they are so visible and articulated with such fervor. It is not our intention to suggest that this talking about the future in and of itself constructs the future that actually unfolds. Rather, we hold that the genres of future-talk are closely connected to real interests connected to particular versions of Arctic futures.\n\n==\n\nKauppila 2022:\n\nTitle: China\u2019s rise and the Arctic region up to 2049 \u2013 three scenarios for regional futures in an era of climate change and power transition\n\nAbstract: ABSTRACT Although China has emerged as an increasingly influential global actor over recent decades, it is unclear whether a more fundamental transformation is shaking processes of regionalisation in this context. Our scenario-based case study considers the spatial reconfiguration of the rapidly globalising Arctic with varying degrees of Chinese engagement. More specifically, we examine alternative and transformational configurations of the Arctic in 2049, and ponder upon the plausibility of the hypothesised changes in light of three schools of thought on International Relations \u2013 realism, liberal institutionalism and relationalism. Hence, we explore how the rise of China could potentially alter the regional dynamics and whether, consequently, regions should be rethought both empirically and theoretically. We conclude that pluralistic discussion on the multiple regional outcomes is a necessary precondition for achieving a balanced and democratic future in the Arctic and beyond.", "final_summary": "The future of geopolitics, particularly in the Arctic region, is a complex and multifaceted issue. Scenario-building methodologies are increasingly being used to explore potential future developments in this rapidly changing region (Nilsson 2019; Erokhin 2020; Haavisto 2016; Keys 2021; Zaikov 2019; Brigham 2008; Wormbs 2017; Kauppila 2022).\n\nThe Arctic's future is influenced by a myriad of factors, including climate change, globalization, political shifts, and technological advancements (Zaikov 2019; Erokhin 2020; Haavisto 2016). These factors can lead to a variety of scenarios, from the Arctic becoming a hub of economic activity due to its mineral resources and strategic location (Erokhin 2020), to it remaining a relatively undeveloped region (Haavisto 2016).\n\nScenario-building exercises have been used to explore these potential futures. For instance, Nilsson (2019) emphasizes the need for participatory scenario methodologies that include diverse voices, particularly those of indigenous communities and youth. Similarly, Keys (2021) uses computational text analysis to generate unique thematic input for novel, story-based visions of the Arctic.\n\nHowever, these scenarios are not without their challenges. Erokhin (2020) notes the need for a systemic view in scenario-building exercises, particularly given the volatility of global geopolitical, geoeconomic, and environmental dynamics. Brigham (2008) also highlights the importance of considering the long-term impacts of key findings on Arctic Marine Navigation.\n\nIn conclusion, scenario-building methodologies provide a valuable tool for exploring the potential future of geopolitics in the Arctic. However, these exercises must be inclusive, systemic, and adaptable to the rapidly changing dynamics of the region (Nilsson 2019; Erokhin 2020; Keys 2021). As the Arctic continues to undergo significant changes, these scenarios will play a crucial role in informing decision-making processes and strategies for sustainable development."}, {"query": "what is the relationship between conflict communication styles and marital satisfaction? give 20 studies summary", "paper_list_string": "Kurdek 1995:\n\nTitle: Predicting change in marital satisfaction from husbands' and wives' conflict resolution styles.\n\nAbstract: Identifying what specific conflict resolution styles are linked to change in marital satisfaction is important because managing conflict is one of the central tasks of maintaining a marriage (Gottman, 1994) and because declines in marital satisfaction herald a series of processes indicative of a deteriorating marriage (Gottman & Levenson, 1992). Findings regarding the concurrent link between conflict resolution styles and marital satisfaction have been consistent in indicating that each spouse's marital satisfaction is positively related to the frequency with which each spouse uses constructive strategies to resolve conflict (such as agreement, compromise, and humor) and negatively related both to the frequency with which each spouse uses destructive strategies to resolve conflict (such as conflict engagement, withdrawal, and defensiveness) and to the joint frequency with which the wife uses conflict engagement and the husband uses withdrawal (the \"demand-withdraw\" pattern). This pattern of findings has been obtained regardless of whether conflict resolution styles were assessed by means of brief behavioral observations (Gottman & Krokoff, 1989; Heavey, Layne, & Christensen, 1993; Noller, Feeney, Bonnell, & Callan, 1994) or by means of self-report and partner-report questionnaire data (Christensen, 1988; Heavey et al., 1993; Huston & Vangelisti, 1991; Noller et al., 1994). Given that the concurrent link between conflict resolution styles and marital satisfaction is well-established, researchers have used longitudinal data to assess the plausibility of two causal relations involving conflict resolution styles and marital satisfaction. The first causal relation--the assumption that the use of certain conflict resolution styles causes marital satisfaction--is based on interdependence theory (Rusbult, 1983) which posits that perceived rewards to a relationship (such as the frequent use of constructive conflict resolution strategies) and perceived costs to the relationship (such as the frequent experience of negative conflict resolution styles) determine satisfaction with the relationship. The longitudinal data consistent with this causal relation would indicate that the frequency with which certain conflict resolution styles are used at Time 1 predicts change in marital satisfaction. The second causal relation--the assumption that the level of marital satisfaction is causally related to the frequency with which certain conflict resolution styles are used--is based on self-fulfilling prophecy theory (e.g., Snyder, Tanke, & Berscheid, 1977) which posits that one's attitude (e.g., level of satisfaction with the marriage) provides a psychological environment that elicits behavior (e.g., conflict resolution styles) that reinforces and is consistent with the initial attitude. The longitudinal data consistent with this causal relation would indicate that the level of marital satisfaction at Time 1 predicts the degree of change in the frequency with which certain conflict resolution styles are used. Unfortunately, longitudinal findings relevant to the plausibility of either causal relation have been inconsistent (Gottman & Krokoff, 1989; Heavey et al., 1993; Huston & Vangelisti, 1991; Noller et al., 1994). In part, this could be due to four methodological and two conceptual limitations of these studies. Regarding the methodological limitations, first, because Gottman and Krokoff (1989) and Heavey et al. (1993) used measures of marital satisfaction that also tapped frequency of conflict and disagreement, the correlations from these measures may have been inflated (see review by Fincham & Bradbury, 1987). Second, only Huston and Vangelisti (1991) reported that marital satisfaction scores changed appreciably over the time interval studied, raising the possibility that, in the other studies, a restriction of range accounted for nonsignificant findings. Third, because the first assessments made by Huston and Vangelisti (1991) and Noller et al. \u2026\n\n==\n\nGreeff 2000:\n\nTitle: Conflict management style and marital satisfaction.\n\nAbstract: The aim of this study was to investigate whether there is one conflict management style that correlated more significantly with marital satisfaction than any other. In addition, spousal satisfaction with how marital conflict is managed was also examined, as were gender differences. Fifty-seven couples who had been married for at least 10 years took part in the study. Results showed that the collaborative conflict management style has the highest correlation with both marital satisfaction and spousal satisfaction with conflict management in the marriage. In contrast, where one or both of the spouses used the competitive conflict management style, the lowest marital satisfaction was reported. The results were also interpreted in terms of cultural and gender differences.\n\n==\n\nBruyne 2000:\n\nTitle: Conflict Management Style and Marital Satisfaction\n\nAbstract: The aim of this study was to investigate whether there is one conflict management style that correlated more significantly with marital satisfaction than any other. In addition, spousal satisfaction with how marital conflict is managed was also examined, as were gender differences. Fifty-seven couples who had been married for at least 10 years took part in the study. Results showed that the collaborative conflict management style has the highest correlation with both marital satisfaction and spousal satisfaction with conflict management in the marriage. In contrast, where one or both of the spouses used the competitive conflict management style, the lowest marital satisfaction was reported. The results were also interpreted in terms of cultural and gender differences.\n\n==\n\nRussell-Chapin 2001:\n\nTitle: The Relationship of Conflict Resolution Styles and Certain Marital Satisfaction Factors to Marital Distress\n\nAbstract: The data presented in this article report significant predictors and relationships among high levels of marital distress, lack of couple time together, and problems associated with lack of communication. The results are explored in terms of counseling implications.\n\n==\n\nUnal 2020:\n\nTitle: Conflict resolution styles as predictors of marital adjustment and marital satisfaction: an actor\u2013partner interdependence model\n\nAbstract: The aim of the study was to examine the mediating role of marital adjustment on the relationship between conflict resolution styles and marital satisfaction in terms of actor and partner effects. I...\n\n==\n\nRehman 2011:\n\nTitle: Marital Satisfaction and Communication Behaviors During Sexual and Nonsexual Conflict Discussions in Newlywed Couples: A Pilot Study\n\nAbstract: The way couples communicate during conflict discussions has been found to be a reliable predictor of marital satisfaction. However, in previous research, there has been little experimental control over the selection of topics. The present study examined, in a sample of 15 newlywed couples, whether affective displays during the discussion of a sexual and a nonsexual conflict topic differentially predict current marital satisfaction. Communication behaviors were coded using an adaptation of the Specific Affect Coding System, resulting in composite \u201cnegative behavior\u201d and \u201cpositive behavior\u201d categories. Data were analyzed using multilevel modeling. Negative behaviors displayed during the nonsexual conflict discussions were not significantly related to concurrent self-reported relationship satisfaction. In contrast, for wives, negative behaviors displayed during the discussion of a sexual problem were significantly related to lower levels of relationship satisfaction. For the sexual and nonsexual conflict discussions, positive behaviors were positively associated with relationship satisfaction, although this effect did not reach statistical significance. Overall, the authors\u2019 findings emphasize the importance of incorporating sexual variables in the study of marriage. Furthermore, their study represents an important step in recognizing that marital research benefits from an examination of specific topics of conflict as a factor to consider in studies of marital functioning.\n\n==\n\nSchudlich 2011:\n\nTitle: Relations between spouses' depressive symptoms and marital conflict: a longitudinal investigation of the role of conflict resolution styles.\n\nAbstract: This study investigated longitudinal relations between spouses' depressive symptoms and styles of conflict resolution displayed by husbands and wives in marital conflict, including angry, depressive, and constructive patterns of expression. Behavioral observations were made from a community sample of 276 couples during marital conflict resolution tasks once a year for 3 years. Couples were observed engaging in a major and minor conflict resolution task. Constructive, angry, and depressive conflict resolution styles were derived from the behavioral observation coding. Couples self-reported on depressive symptoms and marital dissatisfaction. Path analyses provided support for an extension of the marital discord model of depression (Beach, Sandeen, & O'Leary, 1990). Specifically, angry, depressive, and constructive styles of conflict each mediated the link between marital dissatisfaction and depressive symptoms. Significant cross-spouse effects were found. Implications for the treatment of depressed and/or relationally discordant couples are discussed.\n\n==\n\nRands 1981:\n\nTitle: Patterns of Conflict Resolution and Marital Satisfaction*\n\nAbstract: an intensive survey of fertility decision-making, both Spouses in 244Northern California marriages responded to questions a bout the kinds of they encounter, their style of conflict resolution, the expected oftheir conflict, and their marital satisfaction. Four main \"tYPes\"0f resolution were found based on the 488 individual partnen' reports: (1) a nonintimaie-aggressivc pattern, (11) a nonintimatc-nonaggressivc one, ('11) an intimate-aggressive pattern, and (1V) an intimate-nonaggressive One. who perceived their marriage in Type 1 were generally least aatisficd. tlnd those in Type I V most satisfied, with Types I1 and 111 intermediate. The meaning of this typology and its correlates, as well as its wider implications. are discussed.", "final_summary": "The relationship between conflict communication styles and marital satisfaction has been extensively studied, revealing a complex interplay between these two variables. \n\nKurdek (1995) found that constructive conflict resolution strategies, such as agreement, compromise, and humor, were positively related to marital satisfaction. Conversely, destructive strategies, such as conflict engagement, withdrawal, and defensiveness, were negatively related to marital satisfaction. \n\nGreeff (2000) and Bruyne (2000) both found that the collaborative conflict management style was most strongly correlated with marital satisfaction. In contrast, the competitive conflict management style was associated with the lowest marital satisfaction. \n\nRussell-Chapin (2001) reported that high levels of marital distress were linked to lack of couple time together and problems associated with lack of communication. \n\nUnal (2020) examined the mediating role of marital adjustment on the relationship between conflict resolution styles and marital satisfaction, suggesting a complex interplay between these variables. \n\nRehman (2011) found that negative behaviors displayed during conflict discussions, particularly those related to sexual issues, were significantly related to lower levels of relationship satisfaction for wives. \n\nSchudlich (2011) found that angry, depressive, and constructive styles of conflict each mediated the link between marital dissatisfaction and depressive symptoms. \n\nFinally, Rands (1981) identified four main types of conflict resolution, with those who perceived their marriage as nonintimate-aggressive being generally least satisfied, and those in intimate-nonaggressive most satisfied.\n\nIn conclusion, the studies collectively suggest that constructive and collaborative conflict resolution styles are positively associated with marital satisfaction, while destructive and competitive styles are negatively associated. Furthermore, the way conflict is managed in specific areas, such as sexual issues, can have a significant impact on marital satisfaction. These findings underscore the importance of effective conflict communication in maintaining marital satisfaction."}, {"query": "flexible markov switching", "paper_list_string": "Deschamps 2006:\n\nTitle: A Flexible Prior Distribution for Markov Switching Autoregressions With Student-T Errors\n\nAbstract: This paper proposes an empirical Bayes approach for Markov switching autoregressions that can constrain some of the state-dependent parameters (regression coefficients and error variances) to be approximately equal across regimes. By flexibly reducing the dimension of the parameter space, this can help to ensure regime separation and to detect the Markov switching nature of the data. The permutation sampler with a hierarchical prior is used for choosing the prior moments, the identification constraint, and the parameters governing prior state dependence. The empirical relevance of the methodology is illustrated with an application to quarterly and monthly real interest rate data.\n\n==\n\nDewachter 2001:\n\nTitle: Can Markov switching models replicate chartist profits in the foreign exchange market\n\nAbstract: Abstract In this paper we show that the Markov switching model is a relevant statistical alternative to the classical martingale model for exchange rates. By extending the standard Markov switching model we decisively reject the martingale model. Moreover, the model generates autocorrelations and linear structures in line with what is observed in reality. Subsequently, we test whether this model can explain chartist profits. We find that the extended Markov switching model is able to explain the profitability of a simple MA-30 rule. Finally, we decompose the profitability of the MA-30 rule into a linear and nonlinear part. We find that, although the implied linear structure of the Markov model explains a substantial part of the profitability, part of the profits of the MA-30 rule can be attributed to the specific nonlinearities implicit in the Markov model.\n\n==\n\nGuidolin 2011:\n\nTitle: Markov Switching Models in Empirical Finance\n\nAbstract: I review the burgeoning literature on applications of Markov regime switching models in empirical finance. In particular, distinct attention is devoted to the ability of Markov Switching models to fit the data, filter unknown regimes and states on the basis of the data, to allow a powerful tool to test hypotheses formulated in light of financial theories, and to their forecasting performance with reference to both point and density predictions. The review covers papers concerning a multiplicity of sub-fields in financial economics, ranging from empirical analyses of stock returns, the term structure of default-free interest rates, the dynamics of exchange rates, as well as the joint process of stock and bond returns.\n\n==\n\nDark 2015:\n\nTitle: Futures hedging with Markov switching vector error correction FIEGARCH and FIAPARCH\n\nAbstract: Markov switching vector error correction asymmetric long memory volatility models with fat tailed innovations are proposed. Bivariate two state versions of the models are applied to a futures hedge of the S&P500. Regime switches occur between high and low cost of carry states via changes in the error correction term or basis. Regime identification is therefore dominated by switches in the mean, not volatility. Relative to a number of alternatives, the proposed models provide superior out of sample forecasts of the covariance matrix particularly for horizons greater than 10days ahead. When hedging, Markov switching with long memory improves the tail risk of hedged returns beyond 10day horizons, however there is mixed support for models with volatility asymmetries. These findings have important implications for the development of multivariate models and other applications including portfolio management, spread option pricing and arbitrage.\n\n==\n\nBauwens 2007:\n\nTitle: Theory and Inference for a Markov Switching GARCH Model\n\nAbstract: We develop a Markov-switching GARCH model (MS-GARCH) wherein the conditional mean and variance switch in time from one GARCH process to another. The switching is governed by a hidden Markov chain. We provide sufficient conditions for geometric ergodicity and existence of moments of the process. Because of path dependence, maximum likelihood estimation is not feasible. By enlarging the parameter space to include the state variables, Bayesian estimation using a Gibbs sampling algorithm is feasible. We illustrate the model on SP500 daily returns.\n\n==\n\nSheu 2014:\n\nTitle: Optimal Futures Hedging Under Multichain Markov Regime Switching\n\nAbstract: Most of the existing Markov regime switching GARCH-hedging models assume a common switching dynamic for spot and futures returns. In this study, we release this assumption and suggest a multichain Markov regime switching GARCH (MCSG) model for estimating state-dependent time-varying minimum variance hedge ratios. Empirical results from commodity futures hedging show that MCSG creates hedging gains, compared with single-state-variable regime-switching GARCH models. Moreover, we find an average of 24% cross-regime probability, indicating the importance of modeling cross-regime dynamic in developing optimal futures hedging strategies. \u00a9 2012 Wiley Periodicals, Inc. Jrl Fut Mark 34:173\u2013202, 2014\n\n==\n\nEngel 1992:\n\nTitle: Can the Markov Switching Model Forecast Exchange Rates?\n\nAbstract: A Markov-switching model is fit for eighteen exchange rates at quarterly and monthly frequencies. This model fits well in-sample at the quarterly frequency for many exchange rates. By the mean-squared-error or mean-absolute-error criterion. the Markov model does not generate superior forecasts at a random walk or at the forward rate. There appears to be some evidence that the forecast of the Markov model are superior at predicting the direction of change of the exchange rate.\n\n==\n\nDueker 1997:\n\nTitle: Markov Switching in GARCH Processes and Mean-Reverting Stock-Market Volatility\n\nAbstract: This article introduces four models of conditional heteroscedasticity that contain Markov-switching parameters to examine their multiperiod stock-market volatility forecasts as predictions of options-implied volatilities. The volatility model that best predicts the behavior of the options-implied volatilities allows the Student-t degrees-of-freedom parameter to switch such that the conditional variance and kurtosis are subject to discrete shifts. The half-life of the most leptokurtic state is estimated to be a week, so expected market volatility reverts to near-normal levels fairly quickly following a spike.", "final_summary": "Markov Switching models, characterized by their ability to transition between different states or regimes, have been extensively applied in various financial contexts. \n\nDeschamps (2006) proposed an empirical Bayes approach for Markov switching autoregressions, which can constrain some state-dependent parameters to be approximately equal across regimes. This approach aids in detecting the Markov switching nature of the data and ensuring regime separation. \n\nDewachter (2001) extended the standard Markov switching model to reject the martingale model for exchange rates, demonstrating the model's ability to generate autocorrelations and linear structures observed in reality. The study also found that the extended Markov switching model could explain the profitability of a simple MA-30 rule in the foreign exchange market.\n\nGuidolin (2011) reviewed the application of Markov Switching models in empirical finance, highlighting their ability to fit data, filter unknown regimes, test hypotheses, and forecast both point and density predictions. \n\nDark (2015) proposed Markov switching vector error correction asymmetric long memory volatility models, which were applied to a futures hedge of the S&P500. The models provided superior out-of-sample forecasts of the covariance matrix, particularly for horizons greater than 10 days ahead.\n\nBauwens (2007) developed a Markov-switching GARCH model where the conditional mean and variance switch from one GARCH process to another, governed by a hidden Markov chain. The model was illustrated on SP500 daily returns.\n\nSheu (2014) suggested a multichain Markov regime switching GARCH model for estimating state-dependent time-varying minimum variance hedge ratios. The model was found to create hedging gains compared to single-state-variable regime-switching GARCH models.\n\nEngel (1992) fitted a Markov-switching model for eighteen exchange rates, finding that the model fits well in-sample at the quarterly frequency for many exchange rates. However, the model did not generate superior forecasts compared to a random walk or the forward rate.\n\nDueker (1997) introduced four models of conditional heteroscedasticity with Markov-switching parameters to examine their multiperiod stock-market volatility forecasts. The model that best predicted the behavior of the options-implied volatilities allowed the Student-t degrees-of-freedom parameter to switch, leading to discrete shifts in the conditional variance and kurtosis.\n\nIn conclusion, Markov Switching models have demonstrated their versatility and efficacy in various financial contexts, from exchange rates to stock-market volatility. They offer a flexible and robust approach to modeling financial data, capturing the dynamic nature of financial markets."}, {"query": "The increased importance of cassava in agricultural and economic development as well as in food security particularly in Nigeria should give its processing and waste handling more attention. Presently, the methods involved in cassava processing are referred to as crude and unstandardized, lacking scientific principles and waiting for possible upgrade (Okunade and Adekalu, 2013). ", "paper_list_string": "Kolawole 2010:\n\nTitle: Sustaining world food security with improved cassava processing technology: the Nigeria experience.\n\nAbstract: Cassava is a very important food crop that is capable of providing food security. However, a lot of problems prevent the development and use of modern equipment for its production. Most of the cassava produced still comes from peasant farmers who depend on manual tools for their field operations and these farmers have made Nigeria the world\u2019s largest producer of the crop. An increase in production of cassava to sustain the world food security needs improved machinery to allow its continuous cultivation and processing. Reasons for the low success recorded in the mechanization of cassava harvesting and processing were traced, and the attempts that have been made in the recent past by various engineers in Nigeria researching towards achieving mechanized harvesting and processing of cassava are well explained. The machinery required for cassava production in Africa, the development of new machines, and the need for more research and development in harvesting and processing machineries, which can reduce poverty worldwide and make food available and accessible for all, are also discussed. Research efforts made and the challenges facing the engineers, farmers, scientists and food processors towards achieving mechanical harvesting and processing of cassava are presented. Breeding a cassava variety with a regular shape for easy mechanization is one solution that could help the engineers worldwide.\n\n==\n\nAchi 2018:\n\nTitle: Cassava Processing Wastes: Options and Potentials for Resource Recovery in Nigeria\n\nAbstract: Agro-food processing industries are major contributors of wastes in most developing countries. With Nigeria leading in cassava food production, little attention has been paid to provide a sustainable and profit-oriented solution to the problem of solid waste resulting from cassava processing industries.\n\n==\n\nAgbaeze 2020:\n\nTitle: Management of Food Crop for National Development: Problems and Challenges of Cassava Processing in Nigeria\n\nAbstract: This article examined the problems and challenges of managing cassava processing in Nigeria. Based on the findings of our study, agricultural policy framework, level of mechanization, infrastructure, agricultural technical experts, and access to capital are the major factors that affect cassava processing in Nigeria, while the challenges are policy inconsistency, high cost of mechanization and inadequate capital investment in infrastructure, inadequate extension agents, as well as stringent credit facility conditions. This study, therefore, concludes that for cassava to play a major role in Nigeria\u2019s national development government must ensure that there is a full exploration of all opportunities that have the potential to yield strong advantage. This could be achieved through a holistic assessment of the cassava process that is geared toward improving the quality of life and well-being of citizens based on Nigeria\u2019s comparative advantage in cassava production and processing.\n\n==\n\nOnyenwoke 2014:\n\nTitle: Cassava post-harvest processing and storage in Nigeria: A review\n\nAbstract: Cassava is an important root crop consumed as a staple food, boiled, baked or often fermented into other foods and beverages all over the world. It is a very good vehicle for addressing some health related problems and also serve as security food. Cassava undergoes postharvest physiological deterioration (PPD) once the tubers are separated from the main plant. PPD is one of the main obstacles currently preventing farmers from exporting fresh cassava abroad thereby generating income from foreign exchange. Cassava can be preserved in various ways such as coating with wax and freezing. Recent development in plant breeding has resulted in cassava that is tolerant to PPD. Genetic manipulation was considered most appropriate to solving the PPD challenge by adding new traits to elite genotypes without altering other desired characteristics. Processing cassava affects the nutritional value of cassava roots through modification and losses in nutrients of high value. The processing methods include peeling, boiling, steaming, slicing, grating, soaking or seeping, fermenting, pounding, roasting, pressing, drying, and milling. The products from cassava are: High Quality Cassava Flour (HQCF), cassava chips, garri, starch, ethanol etc.\n\n==\n\nJimoh 2012:\n\nTitle: An Automated Cassava Peeling System for the Enhancement of Food Security in Nigeria\n\nAbstract: ABSTRACT Cassava is second only to sweet potato as the most important starchy root crop of the tropical world. In most parts of the tropics, cassava is grown on small plots. However, in some countries, e.g. Mexico, Brazil and Nigeria, large plantations have been started and interest in mechanization is growing. The degree of mechanization depends on the size of the land and availability of machines for each unit operation involved in cassava processing. However, of all the unit operations involved in cassava processing, cassava peeling remains a serious global challenge to process engineers. An automated cassava peeling machine was thus developed in the Department of Agricultural Engineering, Federal University of Technology, Akure. The design of the machine was based on the development and modification of the peeling tool of previous cassava peeling machines and the peeling principle is by impact. This machine was evaluated using newly harvested improved variety of cassava tuber (TMS 30572) grouped into different sizes: 100\u00a0\u2264\u00a0L\n\n==\n\nOdebode 2008:\n\nTitle: Appropriate Technology for Cassava Processing in Nigeria: User\u2019s Point of View\n\nAbstract: This study examined appropriate agricultural extension technological needs of users in cassava processing activities in Nigeria. Purposive sampling technique was used in selecting 160 participating and non-participating users making a total of 320 users in Oyo state. Data was collected with Interview Schedule and analysed using chi-square and t-test. (58%) of women (users) use traditional processing equipments in cassava processing. Improved processing technologies used include vibrating sieve, abrasive peeler, motorised grater, drum drier, and screw \u2013jack . Processed cassava products include \u201cgari\u201d, \u201clafun\u201d, starch, and \u201cfufu\u201d. Significant relationships exist between the use of improved technologies for processing and age (X 2 = 6.15, p= 0.05), educational Status (X 2 = 5.80, p= 0.05), religion (X 2 = 12.20, p= 0.05) and type of technology utilized. Significant difference exists between mean adoption scores of participating and nonparticipating users (t= 6.53, p= 0.05) . Problems encountered by the users include high cost of processing equipment, transportation difficulties, poor infrastructural facilities, shortage of labour, poor access to market, lack of fund and poor shortage facilities. Time-saving and simpler prototype processing equipment should be introduced to the users of cassava processing technologies during extension training.\n\n==\n\nEwebiyi 2021:\n\nTitle: Constraints to utilization of improved processing technologies among cassava processors in Oyo State, Nigeria\n\nAbstract: This study investigated the constraints associated with utilization of improved processing technologies among cassava processors in Oyo State, Nigeria. A structured questionnaire was used to collect data from a random sample of 176 respondents. Data was analyzed using descriptive statistics and Pearson Product Moment Correlation. The findings were that most (56.8%) of the respondents had a positive attitude towards utilization of mechanical grater, screw press hydraulic and Hammer mill technologies. However, inadequate funds, lack of technical know-how, lack of information, high cost of purchase and inadequate engineers for the improved processing technologies were identified as severe constraints militating against utilization of improved processing technologies. Significant relationship was established between these constraints and utilization of the improved processing technologies. It is recommended that government 1) builds factories that will lower the cost of the technologies; and 2) ensures that research institutes, departments of Agricultural engineering in universities and polytechnics and adult education programs in the community are adequately funded to promote knowledge of the technologies.\n\n==\n\nEhinmowoO 2014:\n\nTitle: Analysis of Technical Efficiency of Cassava Processing Methods among Small Scale Processors in South - West, Nigeria\n\nAbstract: This study employed a stochastic frontier model to analyze the productivity and technical efficiency (TE) of cassava processing methods among small scale processors in South \u2013 West, Nigeria. The study was carried out in three states of Ogun, Oyo and Ondo, Nigeria. Data were obtained from primary sources using a set of structured questionnaire assisted with interview schedule. The multistage sampling technique was used. Data were analyzed using: descriptive statistics and the stochastic frontier production function using a farm level survey data collected from 373 small scale cassava processors. Results showed that cassava processing under local and modern methods was in stage one (stage of inefficiency) of the production region and that processors using the local method of processing cassava were more technically efficient than those using the modern method.", "final_summary": "Cassava, a crucial food crop, is a cornerstone of Nigeria's agricultural sector, contributing significantly to the country's economy as Nigeria is the world's largest producer of the crop (Kolawole, 2010). However, the current methods of cassava processing are considered crude and unstandardized, necessitating a focus on improving these methods (Okunade and Adekalu, 2013).\n\nThe importance of mechanization in cassava processing is emphasized by Kolawole (2010), who suggests that improved machinery could enhance cassava production and contribute to global food security. Similarly, Jimoh (2012) discusses the development of an automated cassava peeling machine, indicating progress in mechanization. However, Onyenwoke (2014) and Odebode (2008) highlight that traditional processing methods are still prevalent, with the latter noting that 58% of women use traditional equipment in cassava processing.\n\nThe challenges facing cassava processing in Nigeria are multifaceted. Agbaeze (2020) identifies policy inconsistency, high cost of mechanization, inadequate infrastructure, and stringent credit facility conditions as key obstacles. Similarly, Odebode (2008) and Ewebiyi (2021) point out issues such as high equipment costs, transportation difficulties, and lack of technical know-how. EhinmowoO (2014) adds that processors using traditional methods are more technically efficient than those using modern methods, suggesting a need for improved training and support for modern processing techniques.\n\nWaste management from cassava processing is another area requiring attention. Achi (2018) notes that despite Nigeria's leading position in cassava production, little effort has been made to address the solid waste generated by cassava processing industries.\n\nIn conclusion, while cassava holds significant potential for contributing to Nigeria's agricultural and economic development, there are substantial challenges to overcome. These include the need for improved and standardized processing methods, mechanization, waste management, and addressing the various socio-economic and infrastructural challenges faced by cassava processors (Agbaeze, 2020; Achi, 2018; Kolawole, 2010)."}, {"query": "Knee cartilage repair", "paper_list_string": "Richter 2015:\n\nTitle: Knee Articular Cartilage Repair and Restoration Techniques\n\nAbstract: Context: Isolated chondral and osteochondral defects of the knee are a difficult clinical challenge, particularly in younger patients for whom alternatives such as partial or total knee arthroplasty are rarely advised. Numerous surgical techniques have been developed to address focal cartilage defects. Cartilage treatment strategies are characterized as palliation (eg, chondroplasty and debridement), repair (eg, drilling and microfracture [MF]), or restoration (eg, autologous chondrocyte implantation [ACI], osteochondral autograft [OAT], and osteochondral allograft [OCA]). Evidence Acquisition: PubMed was searched for treatment articles using the keywords knee, articular cartilage, and osteochondral defect, with a focus on articles published in the past 5 years. Study Design: Clinical review. Level of Evidence: Level 4. Results: In general, smaller lesions (<2 cm2) are best treated with MF or OAT. Furthermore, OAT shows trends toward greater longevity and durability as well as improved outcomes in high-demand patients. Intermediate-size lesions (2-4 cm2) have shown fairly equivalent treatment results using either OAT or ACI options. For larger lesions (>4 cm2), ACI or OCA have shown the best results, with OCA being an option for large osteochondritis dissecans lesions and posttraumatic defects. Conclusion: These techniques may improve patient outcomes, though no single technique can reproduce normal hyaline cartilage.\n\n==\n\nWelton 2018:\n\nTitle: Knee Cartilage Repair and Restoration: Common Problems and Solutions.\n\nAbstract: Focal cartilage defects in the knee are commonly found on MRI and arthroscopically. When these lesions are symptomatic and fail nonoperative management several surgical strategies are available. Common surgical techniques include reparative (ie, microfracture) and restorative procedures (ie, autologous chondrocyte implantation, particulated juvenile allograft cartilage, osteochondral autograft transfer, and osteochondral allograft). Each of these surgical procedures have shared and novel complications associated with their use. This article provides a detailed, case-based discussion of common complications encountered in surgical procedures for focal cartilage defects of the knee, highlighting causes, clinical recognition, and how to address and avoid these complications.\n\n==\n\nKalson 2010:\n\nTitle: Current strategies for knee cartilage repair\n\nAbstract: Defects in knee articular cartilage (AC) can cause pain and disability and present the clinician with an extremely challenging clinical situation. This article describes the most up\u2010to\u2010date surgical techniques that aim to repair and/or regenerate symptomatic focal defects in AC, which include arthroscopic debridement, microfracture bone marrow stimulation and autologous osteochondral allografting, with an emphasis on autologous chondrocyte implantation. In the future, refinement of tissue\u2010engineering approaches promises to further improve outcome for these patients.\n\n==\n\nBrittberg 2016:\n\nTitle: Cartilage repair in the degenerative ageing knee\n\nAbstract: Background and purpose \u2014 Cartilage damage can develop due to trauma, resulting in focal chondral or osteochondral defects, or as more diffuse loss of cartilage in a generalized organ disease such as osteoarthritis. A loss of cartilage function and quality is also seen with increasing age. There is a spectrum of diseases ranging from focal cartilage defects with healthy surrounding cartilage to focal lesions in degenerative cartilage, to multiple and diffuse lesions in osteoarthritic cartilage. At the recent Aarhus Regenerative Orthopaedics Symposium (AROS) 2015, regenerative challenges in an ageing population were discussed by clinicians and basic scientists. A group of clinicians was given the task of discussing the role of tissue engineering in the treatment of degenerative cartilage lesions in ageing patients. We present the outcomes of our discussions on current treatment options for such lesions, with particular emphasis on different biological repair techniques and their supporting level of evidence. Results and interpretation \u2014 Based on the studies on treatment of degenerative lesions and early OA, there is low-level evidence to suggest that cartilage repair is a possible treatment for such lesions, but there are conflicting results regarding the effect of advanced age on the outcome. We concluded that further improvements are needed for direct repair of focal, purely traumatic defects before we can routinely use such repair techniques for the more challenging degenerative lesions. Furthermore, we need to identify trigger mechanisms that start generalized loss of cartilage matrix, and induce subchondral bone changes and concomitant synovial pathology, to maximize our treatment methods for biological repair in degenerative ageing joints.\n\n==\n\nSalzmann 2018:\n\nTitle: Articular Cartilage Repair of the Knee in Children and Adolescents\n\nAbstract: Articular cartilage predominantly serves a biomechanical function, which begins in utero and further develops during growth and locomotion. With regard to its 2-tissue structure (chondrocytes and matrix), the regenerative potential of hyaline cartilage defects is limited. Children and adolescents are increasingly suffering from articular cartilage and osteochondral deficiencies. Traumatic incidents often result in damage to the joint surfaces, while repetitive microtrauma may cause osteochondritis dissecans. When compared with their adult counterparts, children and adolescents have a greater capacity to regenerate articular cartilage defects. Even so, articular cartilage injuries in this age group may predispose them to premature osteoarthritis. Consequently, surgery is indicated in young patients when conservative measures fail. The operative techniques for articular cartilage injuries traditionally performed in adults may be performed in children, although an individualized approach must be tailored according to patient and defect characteristics. Clear guidelines for defect dimension\u2013associated techniques have not been reported. Knee joint dimensions must be considered and correlated with respect to the cartilage defect size. Particular attention must be given to the subchondral bone, which is frequently affected in children and adolescents. Articular cartilage repair techniques appear to be safe in this cohort of patients, and no differences in complication rates have been reported when compared with adult patients. Particularly, autologous chondrocyte implantation has good biological potential, especially for large-diameter joint surface defects.\n\n==\n\nGobbi 2011:\n\nTitle: One-Step Cartilage Repair with Bone Marrow Aspirate Concentrated Cells and Collagen Matrix in Full-Thickness Knee Cartilage Lesions\n\nAbstract: Objective: The purpose of our study was to determine the effectiveness of cartilage repair utilizing 1-step surgery with bone marrow aspirate concentrate (BMAC) and a collagen I/III matrix (Chondro-Gide, Geistlich, Wolhusen, Switzerland). Materials and Methods: We prospectively followed up for 2 years 15 patients (mean age, 48 years) who were operated for grade IV cartilage lesions of the knee. Six of the patients had multiple chondral lesions; the average size of the lesions was 9.2 cm2. All patients underwent a mini-arthrotomy and concomitant transplantation with BMAC covered with the collagen matrix. Coexisting pathologies were treated before or during the same surgery. X-rays and MRI were collected preoperatively and at 1 and 2 years\u2019 follow-up. Visual analog scale (VAS), International Knee Documentation Committee (IKDC), Knee injury and Osteoarthritis Outcome Score (KOOS), Lysholm, Marx, SF-36 (physical/mental), and Tegner scores were collected preoperatively and at 6, 12, and 24 months\u2019 follow-up. Four patients gave their consent for second-look arthroscopy and 3 of them for a concomitant biopsy. Results: Patients showed significant improvement in all scores at final follow-up (P < 0.005). Patients presenting single lesions and patients with small lesions showed higher improvement. MRI showed coverage of the lesion with hyaline-like tissue in all patients in accordance with clinical results. Hyaline-like histological findings were also reported for all the specimens analyzed. No adverse reactions or postoperative complications were noted. Conclusion: This study showed that 1-step surgery with BMAC and collagen I/III matrix could be a viable technique in the treatment of grade IV knee chondral lesions.\n\n==\n\nSiclari 2013:\n\nTitle: Cartilage repair in the knee with subchondral drilling augmented with a platelet-rich plasma-immersed polymer-based implant\n\nAbstract: PurposeThe aim of our study was to analyse the clinical and histological outcome after the treatment of focal cartilage defects in non-degenerative and degenerative knees with bone marrow stimulation and subsequent covering with a cell-free resorbable polyglycolic acid\u2013hyaluronan (PGA-HA) implant immersed with autologous platelet-rich plasma (PRP).MethodsFifty-two patients (mean age 44\u00a0years) with focal chondral defects in radiologically confirmed non-degenerative or degenerative knees were subjected to subchondral drilling arthroscopically. Subsequently, defects were covered with the PGA-HA implant immersed with autologous PRP. At 2-year follow-up, the patients\u2019 situation was assessed using the Knee Injury and Osteoarthritis Outcome Score (KOOS) and compared to the pre-operative situation and 3\u201312-month follow-up. Biopsies (n\u00a0=\u00a04) were harvested at 18\u201324\u00a0months after implantation and were analysed by histology and collagen type II immune staining.ResultsAt 1- and 2-year follow-up, the KOOS showed clinically meaningful and significant (p\u00a0<\u00a00.05) improvement in all subcategories compared to baseline and to 3-month follow-up. There were no differences in KOOS data obtained after 2\u00a0years compared to 1\u00a0year after the treatment. Histological analysis of the biopsy tissue showed hyaline-like to hyaline cartilage repair tissue that was rich in cells with a chondrocyte morphology, proteoglycans and type II collagen.ConclusionsCovering of focal cartilage defects with the PGA-HA implant and PRP after bone marrow stimulation improves the patients\u2019 situation and has the potential to regenerate hyaline-like cartilage.Level of evidenceCase series, Level IV.\n\n==\n\nNiemeyer 2016:\n\nTitle: Cartilage repair surgery for full-thickness defects of the knee in Germany: indications and epidemiological data from the German Cartilage Registry (KnorpelRegister DGOU)\n\nAbstract: PurposeTreatment of cartilage defects of the knee remains an important issue with high relevance. In October 2013 the German Cartilage Registry (KnorpelRegister DGOU) was initiated in order to study indications, epidemiology and (clinical) outcome of different cartilage repair techniques. The present evaluation of the registry baseline data was initiated to report common practices of cartilage repair surgery in Germany.Materials and methods1065 consecutive patients who underwent surgical cartilage treatment of the knee have been included (complete data sets available in 1027 cases; FU rate 96.4\u00a0%) between October 1, 2013 and June 30, 2015. Data collection was performed using a web-based RDE System. All data were provided by the attending physician at the time of arthroscopic or open surgery of the affected knee.ResultsIn 1027 cartilage repair procedures, single defects were treated in 80\u00a0% of the cases with the majority of the defects located on the medial femoral condyle, followed by the patella. Degenerative defects grade III or IV according to ICRS were treated in 60\u00a0% of the cases and therefore were found more frequently compared to traumatic or post-traumatic lesions. Autologous chondrocyte implantation (ACI) was the most common technique followed by bone marrow stimulation (BMS) and osteochondral transplantation (OCT). While ACI was performed in defects with a mean size of 4.11\u00a0cm2 SD SD 2.16), BMS and OCT (1.51\u00a0cm2, SD 1.19; p\u00a0<\u00a00.01) were applied in significantly smaller defects (both p\u00a0<\u00a00.01). Independent of defect size, the ratio of ACI versus BMS applications differed between different defect locations. ACI was used preferably in defects located on the patella.ConclusionThe present analysis of data from the German Cartilage Registry shows that the vast majority of cartilage repair procedures were applied in degenerative, non-traumatic cartilage defects. Experts in Germany seem to follow the national and international guidelines in terms that bone marrow stimulation is applied in smaller cartilage defects while cell-based therapies are used for the treatment of larger cartilage defects. In patellar cartilage defects a trend towards the use of cell-based therapies has been observed.", "final_summary": "Knee cartilage repair is a complex and evolving field, with a variety of techniques being employed to address the issue. The papers collectively suggest that the choice of technique often depends on the size and location of the cartilage defect.\n\nRichter (2015) and Welton (2018) both discuss a range of techniques, including microfracture, autologous chondrocyte implantation (ACI), and osteochondral autograft (OAT). Richter (2015) suggests that smaller lesions are best treated with microfracture or OAT, while larger lesions may benefit from ACI or osteochondral allograft (OCA). Welton (2018) also discusses the potential complications associated with these techniques.\n\nKalson (2010) and Brittberg (2016) both highlight the challenges of treating cartilage defects, particularly in degenerative knees. Kalson (2010) emphasizes the potential of autologous chondrocyte implantation, while Brittberg (2016) suggests that more research is needed to improve repair techniques for degenerative lesions.\n\nSalzmann (2018) focuses on cartilage repair in children and adolescents, suggesting that while these patients have a greater capacity to regenerate cartilage, surgical intervention may still be necessary. The paper suggests that techniques used in adults can also be used in this younger population, but an individualized approach is necessary.\n\nGobbi (2011) and Siclari (2013) both discuss innovative one-step procedures. Gobbi (2011) reports positive results from a technique using bone marrow aspirate concentrate (BMAC) and a collagen matrix, while Siclari (2013) discusses the use of a platelet-rich plasma-immersed polymer-based implant following subchondral drilling.\n\nFinally, Niemeyer (2016) provides an overview of cartilage repair surgery in Germany, noting that the majority of procedures are applied to degenerative, non-traumatic cartilage defects. The paper suggests that bone marrow stimulation is typically used for smaller defects, while cell-based therapies are used for larger defects.\n\nIn conclusion, the papers collectively suggest that a range of techniques are available for knee cartilage repair, with the choice of technique often depending on the size and location of the defect. However, more research is needed to improve these techniques and to develop new approaches, particularly for degenerative lesions."}, {"query": "attentional engagement", "paper_list_string": "Zivony 2018:\n\nTitle: Contingent Attentional Engagement: Stimulus- and Goal-Driven Capture Have Qualitatively Different Consequences\n\nAbstract: We examined whether shifting attention to a location necessarily entails extracting the features at that location, a process referred to as attentional engagement. In three spatial-cuing experiments (N = 60), we found that an onset cue captured attention both when it shared the target\u2019s color and when it did not. Yet the effects of the match between the response associated with the cued object\u2019s identity and the response associated with the target (compatibility effects), which are diagnostic of attentional engagement, were observed only with relevant-color onset cues. These findings demonstrate that stimulus- and goal-driven capture have qualitatively different consequences: Before attention is reoriented to the target, it is engaged to the location of the critical distractor following goal-driven capture but not stimulus-driven capture. The reported dissociation between attentional shifts and attentional engagement suggests that attention is best described as a camera: One can align its zoom lens without pressing the shutter button.\n\n==\n\nBecker 2020:\n\nTitle: The attentional blink: A relational accountof attentional engagement\n\nAbstract: Visual attention allows selecting relevant information from cluttered visual scenes and is largely determined by our ability to tune or bias visual attention to goal-relevant objects. Originally, it was believed that this top-down bias operates on the specific feature values of objects (e.g., tuning attention to orange). However, subsequent studies showed that attention is tuned to in a context-dependent manner to the relative feature of a sought-after object (e.g., the reddest or yellowest item), which drives covert attention and eye movements in visual search. However, the evidence for the corresponding relational account is still limited to the orienting of spatial attention. The present study tested whether the relational account can be extended to explain attentional engagement and specifically, the attentional blink (AB) in a rapid serial visual presentation (RSVP) task. In two blocked conditions, observers had to identify an orange target letter that could be either redder or yellower than the other letters in the stream. In line with previous work, a target-matching (orange) distractor presented prior to the target produced a robust AB. Extending on prior work, we found an equally large AB in response to relatively matching distractors that matched only the relative color of the target (i.e., red or yellow; depending on whether the target was redder or yellower). Unrelated distractors mostly failed to produce a significant AB. These results closely match previous findings assessing spatial attention and show that the relational account can be extended to attentional engagement and selection of continuously attended objects in time.\n\n==\n\nOakes 2004:\n\nTitle: Attentional Engagement in Infancy: The Interactive Influence of Attentional Inertia and Attentional State.\n\nAbstract: We evaluated the interactive influences of attentional state and attentional inertia on infants' level of attentional engagement. We assessed infants' distraction latencies longitudinally at 6.5 and 9 months as they explored toys, and we coded both their attentional state (focused vs. casual) and how long they had been looking at the toy at each distractor onset. Consistent with previous results, both attentional state and attentional inertia contributed to differences in distraction latency. Importantly, the level of attentional engagement was interactively determined by attentional state and attentional inertia. Infants were most resistant to distraction when they were judged to be in a state of focused attention following relatively long looks to the toy, and they were equivalently less resistant to distraction under all other conditions. These results are consistent with a general conceptualization of attentional engagement resulting from the interaction of multiple processes.\n\n==\n\nAbuhamdeh 2012:\n\nTitle: Attentional involvement and intrinsic motivation\n\nAbstract: The current study used the Experience Sampling Method to examine attentional involvement\u2014the degree to which one\u2019s attention is devoted to moment-to-moment activity\u2014as a potential mediator of two previously identified relationships within the field of intrinsic motivation: (1) the positive relationship between a balance of challenges and skills and enjoyment, and (2) the positive relationship between competence valuation and enjoyment. Multilevel, within-person analyses indicated attentional involvement fully mediated both relationships. Implications of the findings for intrinsic motivation processes are discussed.\n\n==\n\nZivony 2020:\n\nTitle: Distractor intrusions are the result of delayed attentional engagement: A new temporal variability account of attentional selectivity in dynamic visual tasks.\n\nAbstract: When observers must identify targets among distractors in a rapid serial visual presentation (RSVP) stream, distractor intrusion errors are frequent, demonstrating the difficulty of allocating attention to the right object at the right moment in time. However, the mechanisms responsible for such intrusion errors remain disputed. We propose a new attentional engagement account of selective visual processing in RSVP tasks. Engagement is triggered by the preattentive detection of target-defining features. Critically, the success versus failure of target identification is determined by the speed of such engagement processes on individual trials. To test this account, we measured electrophysiological markers of attentional engagement (N2pc components) in 3 experiments where observers had to report the identity of a target digit in 1 of 2 lateral RSVP streams. On most trials, the target was immediately followed by a digit distractor, resulting in many posttarget distractor intrusions. Critically, N2pcs components measured on distractor intrusion trials were significantly delayed relative to trials with correct target reports. This was the case regardless of whether the target was defined by a shape cue or by its color and even when the location of shape-defined targets was known in advance. These findings show that distractor intrusions are the result of delayed attentional engagement. They demonstrate that temporal variability in attentional selectivity across trials can strongly affect visual awareness and perceptual reports. Our temporal variability account of attentional engagement offers a new framework for assessing the temporal dynamics of attention in visual object recognition. (PsycInfo Database Record (c) 2020 APA, all rights reserved).\n\n==\n\nLi 2020:\n\nTitle: Closed-Loop Attention Restoration Theory for Virtual Reality-Based Attentional Engagement Enhancement\n\nAbstract: Today, as media and technology multitasking becomes pervasive, the majority of young people face a challenge regarding their attentional engagement (that is, how well their attention can be maintained). While various approaches to improve attentional engagement exist, it is difficult to produce an effect in younger people, due to the inadequate attraction of these approaches themselves. Here, we show that a single 30-min engagement with an attention restoration theory (ART)-inspired closed-loop software program (Virtual ART) delivered on a consumer-friendly virtual reality head-mounted display (VR-HMD) could lead to improvements in both general attention level and the depth of engagement in young university students. These improvements were associated with positive changes in both behavioral (response time and response time variability) and key electroencephalography (EEG)-based neural metrics (frontal midline theta inter-trial coherence and parietal event-related potential P3b). All the results were based on the comparison of the standard Virtual ART tasks (control group, n = 15) and closed-loop Virtual ART tasks (treatment group, n = 15). This study provides the first case of EEG evidence of a VR-HMD-based closed-loop ART intervention generating enhanced attentional engagement.\n\n==\n\nPerra 2012:\n\nTitle: Attention engagement in early infancy.\n\nAbstract: We report a longitudinal study investigating developmental changes in the structure of attention engagement during early infancy. Forty-three infants were observed monthly from 2 to 4 months. Attention engagement was assessed from play interactions with parents, using a coding system developed by Bakeman and Adamson (1984). The results indicated a developmental transition in attention engagement at 3 months: after this age infants engaged for longer periods and in a wider variety of states. Most infants displayed person engagement at 2 months, passive joint engagement at 3 months, and object engagement at 4 months. To address whether emerging abilities of attention engagement allow infants to follow the attention of social partners, we compared attention engagement to performance on an experimental measure of attention control (reported by Perra & Gattis, 2010). Analyses revealed a positive relation between passive joint engagement and checking back, suggesting that changes in passive joint engagement reflect the development in attention control.\n\n==\n\nBrockmole 2009:\n\nTitle: Should I stay or should I go? Attentional disengagement from visually unique and unexpected items at fixation.\n\nAbstract: Distinctive aspects of a scene can capture attention even when they are irrelevant to one's goals. The authors address whether visually unique, unexpected, but task-irrelevant features also tend to hold attention. Observers searched through displays in which the color of each item was irrelevant. At the start of search, all objects changed color. Critically, the foveated item changed to an unexpected color (it was novel), became a color singleton (it was unique), or both. Saccade latency revealed the time required to disengage overt attention from this object. Singletons resulted in longer latencies, but only if they were unexpected. Conversely, unexpected items only delayed disengagement if they were singletons. Thus, the time spent overtly attending to an object is determined, at least in part, by task-irrelevant stimulus properties, but this depends on the confluence of expectation and visual salience.", "final_summary": "Attentional engagement, a captivating area of study, refers to the degree to which one's attention is devoted to moment-to-moment activity. The papers collectively provide a comprehensive understanding of this concept.\n\nZivony (2018) suggests that attentional engagement is not necessarily a result of shifting attention to a location. Instead, it is contingent on the relevance of the stimulus, indicating that goal-driven and stimulus-driven captures have different consequences. This idea is further supported by Becker (2020), who proposes a relational account of attentional engagement, suggesting that attention is tuned in a context-dependent manner to the relative feature of a sought-after object.\n\nThe influence of attentional state and attentional inertia on attentional engagement is explored by Oakes (2004). The study found that infants were most resistant to distraction when they were in a state of focused attention following relatively long looks at the toy. This suggests that attentional engagement is determined by the interaction of multiple processes.\n\nAbuhamdeh (2012) links attentional engagement to intrinsic motivation, suggesting that attentional involvement mediates the positive relationship between a balance of challenges and skills and enjoyment, as well as the positive relationship between competence valuation and enjoyment.\n\nZivony (2020) proposes a new attentional engagement account of selective visual processing in RSVP tasks. The study suggests that the success versus failure of target identification is determined by the speed of engagement processes on individual trials.\n\nLi (2020) introduces a novel approach to enhancing attentional engagement using a Virtual ART-inspired closed-loop software program delivered on a VR-HMD. The study found improvements in both general attention level and the depth of engagement in young university students.\n\nPerra (2012) provides a longitudinal study investigating developmental changes in the structure of attention engagement during early infancy. The study found a developmental transition in attention engagement at 3 months, with infants engaging for longer periods and in a wider variety of states.\n\nFinally, Brockmole (2009) addresses whether visually unique, unexpected, but task-irrelevant features also tend to hold attention. The study found that the time spent overtly attending to an object is determined, at least in part, by task-irrelevant stimulus properties.\n\nIn conclusion, these papers collectively suggest that attentional engagement is a complex process influenced by various factors such as the relevance of the stimulus, attentional state and inertia, intrinsic motivation, speed of engagement processes, and task-irrelevant stimulus properties. Further research is needed to fully understand the mechanisms underlying attentional engagement."}, {"query": "Psychology and novel character development", "paper_list_string": "Retnomurti 2022:\n\nTitle: Psychological Approach in Personality Development of Theodore Decker\u2019s Character in \u201cThe Goldfinch\u201d\n\nAbstract: The research aims to describe the personality and the personality development of Theodore Decker through psychosocial development perspectives in the main character of The Goldfinch novel. His personality becomes an example of how family and social environment contribute to the development of one\u2019s personality. The research method used in understanding and in analysing the novel was descriptive qualitative. The research results are: According to Erik Erikson\u2019s eight stages in psychosocial development theory, Theo has passed four major stages they are the stages of initiative vs guilt (14%), industry vs inferiority (14%), identity vs role confusion (46%), and intimacy vs isolation (29%). Theo struggles to pass the stage of identity vs role confusion. It has led him to the situation where he faces difficulties on understanding his identity and role within society. Theo is grown up as a person with less empathy for others, struggles with his drug addiction, and has trouble making commitments; Erik Erikson\u2019s psychosocial development of the personality emphasizes the social influences in the development of the ego. Erikson\u2019s theory of psychosocial development has eight distinct stages. Completing each step results in a healthy personality and the acquisition of the ego to resolve subsequent crises. Conversely, failure to complete a set will lead to ability reduction in completing further stages and an unhealthier personality and sense of self. Instead of focusing on sexual development, this theory is interesting in how children socialize and how social environment affects their senses.\n\n==\n\nWibowo 2022:\n\nTitle: Defining the Study of Humanistic Personality Psychology of the Main Character of the Novel Si Anak Pelangi by Tere Liye: The Value of Character Education and Its Relevance to Learning\n\nAbstract: The purpose of this study is to examine the psychological aspects of the main character in the novel \u201cSi Anak Pelangi\u201d by Tere Liye. Moreover, this study also investigates the value of character education and its relevance to learning Indonesian in elementary school. The research method uses a descriptive qualitative approach. The data was the document of the novel which was analyzed using content analysis method. The results of this study are related to the Haberman's humanistic psychological theory, namely (1) technical learning, (2) practical learning, and (3) emancipatory learning. In addition, there are several values of character education such as (1) hard work in the form of doing activities diligently and accompanied by sincere intentions, (2) politeness in speech and behavior, (3) giving awards by sincerely accepting someone's gift, (4 ) positive, in acting both in sports and interacting with others, and (5) being patient in dealing with insults. This study concludes that this novel has suitable content for elementary school students since it contains positive values and educating characters.\n\n==\n\nYuan 2022:\n\nTitle: Research on the psychology of fictional characters based on artificial intelligence\u2014an example study on The Family\n\n\nAbstract: \n Using ecological recognition based on artificial intelligence technology and Chinese psychology analysis system, by comparing the characters\u2019 dialogs in The Family, we got the main characters\u2019 big-five personality scores. In order to confirm the validity of this method, we compared the predicted scores with documents related to characters\u2019 psychological analysis and novel descriptions. The prediction results are supported by the literature and plot. This indicates that the analysis of characters\u2019 personalities by literary intelligence is effective. This research method provides data support for literary critics and quantitatively reflects the personality differences of characters in novels.\n\n==\n\nAlmond 1989:\n\nTitle: Psychological change in Jane Austen's Pride and Prejudice.\n\nAbstract: In the spirit of Jerome Frank's (1973) pioneering studies of universal characteristics in psychotherapeutic change, I have tried to demonstrate that character development in Pride and Prejudice can be understood in terms of psychoanalytic process. In the course of this thesis certain observations have emerged. Change occurs in the context of a relationship intense enough to disturb the tendency of personality to homeostasis (engagement). The change-inducing relationship is composed of a sequence of effects and countereffects (mutual influence). For these influences to be salutory (therapeutic) there must be a directional pull provided by the attitude of the primary agent of change, a pull that resonates with important motivations of the object of change (directionality). In Pride and Prejudice we recurrently found self-concept, and particularly the question of worth, to be an important interface of these phenomena. As therapists we are familiar with this as a clinical issue. Study of the novel suggests that self-esteem may play a central role in motivating therapeutic change. This observation raises intriguing unanswered questions regarding the conceptualization of self-esteem in psychoanalytic thinking.\n\n==\n\nHalperin 1996:\n\nTitle: Writing Great Characters : The Psychology of Character Development in Screenplays\n\nAbstract: This valuable book identifies and solves a major problem for writers - creating characters who are so real they literally jump of the page. Halperin has developed an easy-to-understand, logical system, which gives all screenwriters a foolproof and fail-proof method of developing great characters. \"Writing Great Characters\" is a book for all writers, from the expert who is looking to polish his techniques to the novice who wants to learn the craft from an expert.\n\n==\n\nLapsley 2006:\n\nTitle: Character Psychology and Character Education\n\nAbstract: This collection of essays provides new perspectives on the nature of character and moral education by utilizing insights from the disciplines of moral psychology, moral philosophy, and education. The volume draws from personality and developmental research as well as educational and ethical theory. Character Psychology and Character Education distinguishes itself by bringing moral philosophers, who believe that ethical reflection about virtue and character must be tied to defensible notions of personality and selfhood, into dialogue with academic psychologists, who believe that the developmental study of the moral self requires adequate grounding in various psychological literatures. The first group embraces a \"naturalized\" ethics, while the second group favors a \"psychologized\" morality. Among the topics explored in this volume are the constructs of moral selfhood, personality, and identity, as well as defensible models of character education. One of the primary arguments of the volume is that problems of character education cannot be addressed until an adequate model of character psychology is developed. In addition to the excellent theoretical essays, this collection includes applied chapters that consider the challenge of character education in the context of schools, families, and organized sports. This book will be an invaluable resource both for scholars and practitioners in the fields of psychology and education.\n\n==\n\nSamuels 2005:\n\nTitle: A social psychological view of morality: why knowledge of situational influences on behaviour can improve character development practices\n\nAbstract: Results from research in social psychology, such as findings about the fundamental attribution error and other situational influences on behaviour, are often used to justify attacking the existence of character traits. From this perspective, character development is an illusion, an impossibility, or both. We offer a different interpretation of how these issues interact with character development concerns. Rather than undermining the very idea of character traits, social psychology actually sheds light on the manner in which character development can occur. It reaffirms Spinozistic and Aristotelian points about character, namely that: (1) knowledge of the fundamental attribution error can help us minimize the influence environment and situation have on our behaviour, and (2) proper habituation only takes place in appropriately structured environments. Acknowledging these important results from social psychology helps us improve some of our character education practices rather than destroying their basis.\n\n==\n\nUtami 2013:\n\nTitle: The Psychological Changes of a Main Character in a Novel A Walk To Remember by Nicholas Sparks\n\nAbstract: Literature is a field in which most of the human\u2019s life is included on it. It is able to symbolize the presence of any literature related to human beings; behavior, thoughts, attitude, and others. Part of the literature which is generally cites many human live is a novel. In analyzing characters\u2019 characteristics in a novel, the writer examines the psychological aspects associated with the literary novel in titled A Walk to Remember by Nicholas Sparks. The objectives of the study were: to find out Landon Carter\u2019s characteristics before and after he fell in love with Jamie Sullivan and to find out the cause that makes his characteristics changed. This research uses qualitative method, which describes the data. It helps the readers to understand that love influences someone\u2019s characteristics and it also can make someone\u2019s characteristics changed. After analyzing the data, the writer found that Landon\u2019s characteristics before fall in love with Jamie are impolite, naughty, heartless, helpful, and annoying. His characteristics after fall in love with Jamie are polite, helpful, responsible, and religious. In addition, all those changes happened because of love, \nwhich is the main factor that makes Landon\u2019s characteristics changed.", "final_summary": "The exploration of the intersection between psychology and novel character development reveals a rich tapestry of insights and theories. The papers collectively suggest that psychological theories and principles play a crucial role in shaping and understanding character development in novels.\n\nRetnomurti (2022) applies Erik Erikson\u2019s psychosocial development theory to analyze the personality development of Theodore Decker in \"The Goldfinch\". The study highlights the influence of family and social environment on character development, emphasizing the struggles and stages the character undergoes in his journey of self-discovery.\n\nSimilarly, Wibowo (2022) uses Haberman's humanistic psychological theory to examine the main character in the novel \"Si Anak Pelangi\". The study identifies values of character education such as hard work, politeness, and patience, suggesting that these values can be integrated into learning processes.\n\nIn a more technologically advanced approach, Yuan (2022) employs artificial intelligence to analyze the psychology of fictional characters in \"The Family\". The study validates the effectiveness of using AI in literary analysis, providing quantitative data on character personalities.\n\nAlmond (1989) uses a psychoanalytic lens to explore character development in \"Pride and Prejudice\". The study suggests that self-esteem plays a central role in motivating therapeutic change, thus influencing character development.\n\nHalperin (1996) provides a practical guide for writers, offering a system for developing realistic and engaging characters. The book emphasizes the importance of understanding character psychology in creating compelling narratives.\n\nLapsley (2006) and Samuels (2005) both delve into the realm of character education, arguing that an understanding of character psychology is crucial for effective character education. They suggest that insights from moral psychology and social psychology can improve character development practices.\n\nLastly, Utami (2013) explores the psychological changes of the main character in \"A Walk To Remember\". The study demonstrates how love can significantly influence a character's traits and behavior.\n\nIn conclusion, these papers collectively underscore the importance of psychological theories and principles in understanding and shaping character development in novels. They highlight the multifaceted nature of character psychology, suggesting that it is influenced by a range of factors including social environment, personal experiences, and inherent personality traits (Retnomurti, 2022; Wibowo, 2022; Almond, 1989; Utami, 2013). Furthermore, they emphasize the potential of integrating technology and psychological insights in literary analysis (Yuan, 2022), and the practical implications of character psychology in education and writing (Halperin, 1996; Lapsley, 2006; Samuels, 2005)."}, {"query": "false step kinematics", "paper_list_string": "Sato 1997:\n\nTitle: Kinematical Bound States of Steps Caused by Asymmetry in Step Kinetics.\n\nAbstract: We study time evolution of parallel straight steps with repulsive interaction between steps. If step kinetics is asymmetric in the upper and the lower terraces (Schwoebel effect), a vicinal face becomes unstable when undersaturation exceeds a critical value, and an array of large bunches described by the Benney equation appears. In the one-sided model (the extreme limit of the asymmetry) a pairing instability occurs. In this case the instability always ends up with formation of step pairs, and with large undersaturation hierarchical bound states of step pairs are formed. On the contrary many-body bound states appear in the general asymmetric model.\n\n==\n\nLi 2011:\n\nTitle: Implementation of kinematic mechanism data exchangebased on step\n\nAbstract: In this paper, the first known valid implementation of kinematic mechanism based on STEP(ISO 10303, STandard for the Exchange of Product data) is presented. The result includes a generalconceptual ...\n\n==\n\nZhang 1992:\n\nTitle: Forward kinematics of a class of parallel (Stewart) platforms with closed-form solutions\n\nAbstract: This article studies the geometrical condition for closed-form solutions of forward kinematics of parallel platforms. It is shown that closed-form solutions are available if 1 rotational degree of freedom (dof) of the moving platform is decoupled from the other 5 dof. Geometrically, this condition is satisfied when five end-points at the moving platform (or at the base) are colinear. A general case that these five points do not coincide with each other is studied first and is shown to have 16 possible closed-form solutions. The variations of parallel platforms that satisfy the above-mentioned geometrical condition are then discussed. Some of them have the additional feature that the three rotational dof are fully decoupled from the 3 translational dof and their closed-form solutions are further simplified. One particular case has extremely simple forward kinematics and could be used as an alternative to the Stewart platform.\n\n==\n\nZhang 1991:\n\nTitle: Forward kinematics of a class of parallel (Stewart) platforms with closed-form solutions\n\nAbstract: The condition under which closed-form solutions of forward kinematics of parallel platforms are obtainable is explored. It is found that forward position analysis has closed-form solutions if one rotational degree of freedom (DOF) of a parallel platform is decoupled from the other five DOFs. Geometrically, this condition is satisfied when five end points at the platform or at the base are on the same line. A general case in which these five points do not coincide with each other is studied first and is shown to have 16 possible closed-form solutions. The computation of these solutions is very efficient compared to that of Stewart platforms. The different geometries of parallel platforms with closed-form solutions are discussed.<<ETX>>\n\n==\n\nZarrugh 1979:\n\nTitle: Computer generation of human gait kinematics.\n\nAbstract: Abstract The paper describes a computer program that generates absolute motion variables of human gait from predetermined relative motions. Relative displacements are measured over a range of step rates during both free (self-determined step rate at different speeds) and forced (forced step rate at a constant speed) walking, converted into harmonic coefficients and stored in an array as a function of step rate. Only six variable identifiers need to be specified to compute any absolute variable or its derivatives at any desirable step rate. The paper displays some examples of measured relative motions and reconstituted absolute variables.\n\n==\n\nWu 2010:\n\nTitle: Goal-directed stepping with momentum control\n\nAbstract: This paper proposes a technique for animating simulated characters to perform controlled steps. The desired step is controlled by high-level goals, namely step position and step duration. These stepping goals guide the desired time-varying values for the center of mass and the stepping foot which in turn lead to objectives dictating the desired changes in momentum and joint angles over the duration of the step. Our approach employs a multiobjective optimization to solve for joint accelerations from the objectives and uses inverse dynamics to compute joint torques. Our approach can guide a character with purposeful, directable steps for controlling careful navigation of the character's position and orientation. In addition, the same system can be used to create protective steps to prevent falling as a reaction to a disturbance. A novel supervisory routine automatically chooses when and where to step based on an analysis of the momentum conditions for the character. We contrast this approach to previous methods for step recovery using the inverted pendulum.\n\n==\n\nShoham 1985:\n\nTitle: Naive Kinematics: One Aspect of Shape\n\nAbstract: Ways in which physical objects interact are explored, and in particular the concept of freedom is analysed. Intuitively, the fit between two shapes in a given spatial configuration is a statement about how much one shape needs to mutilated in order to be made identical to the other. The freedom of one object with respect to another specifies what motions the First object can go through without the second one moving. The formulations, termed naive kinematics, are compared to work that was done in the kinematics of machinery in the 10th century and that has since been somewhat neglected.\n\n==\n\nSol\u00e0 2015:\n\nTitle: Quaternion kinematics for the error-state Kalman filter\n\nAbstract: A tutorial of quaternion algebra, especially suited for the error-state Kalman filter, with the aim of building Visual-Inertial SLAM and odometry systems.", "final_summary": "The research question \"false step kinematics\" is not directly addressed in the provided papers. The papers discuss various aspects of kinematics, but none specifically focus on \"false step kinematics\". Therefore, a summary cannot be provided based on the given information."}, {"query": "epidermal growth factor receptors overexpression mesothelium", "paper_list_string": "Rena 2011:\n\nTitle: Epidermal growth factor receptor overexpression in malignant pleural mesothelioma: Prognostic correlations\n\nAbstract: To evaluate epidermal growth factor receptor (EGFR) phenotypic expression and related gene status in malignant pleural mesothelioma (MPM) and to correlate the results with patients' prognosis.\n\n==\n\nVelcheti 2009:\n\nTitle: Absence of mutations in the epidermal growth factor receptor (EGFR) kinase domain in patients with mesothelioma.\n\nAbstract: To the Editor: Malignant mesothelioma, a debilitating and often fatal malignancy occurs most commonly in patients with a history of exposure to asbestos. Majority of patients with pleural malignant mesothelioma present with unresectable disease. Epidermal Growth Factor Receptor (EGFR) is one of erbB family of receptor tyrosine kinases (TK) known to play a critical role in the cell proliferation and survival in malignant neoplasms. Several studies have reported an increased expression of EGFR in mesothelioma. EGFR-expressing mesothelioma cell lines seemed to be sensitive to treatment with EGFR tyrosine kinase inhibitors.1 After these observations, the Cancer and Leukemia Study Group B conducted a phase II trial of gefitinib, an EGFR TK inhibitor in 43 previously untreated patients with mesothelioma.2 Of the 28 samples tested for EGFR expression, 27 (97%) had an overexpression of EGFR. Despite EGFR overexpression, gefitinib was not active in malignant mesothelioma. EGFR expression is not an independent prognostic factor in patients with mesothelioma.2,3 There is a striking discordance between EGFR expression and lack of response to EGFR TK inhibitors in malignant mesothelioma. Mutations in exons 18\u201321 of the EGFR are associated with response to EGFR TK inhibitors in non-small cell lung cancer. There are only limited data on the prevalence of EGFR TK mutations in mesothelioma. We sought to investigate the presence of any EGFR mutations in our patients with mesothelioma. A total of 32 patients with a histologically confirmed diagnosis of mesothelioma seen at the Washington University School of Medicine were included in this study. DNA samples were obtained from paraffin-embedded tissue blocks containing representative malignant cells. High-throughput (96well plate) bidirectional dideoxynucleotide sequencing of polymerase chain reactionamplified gene products was performed at the Genome Sequencing Center (Washington University in St. Louis) as per standard protocol http://genome.wustl. edu/activity/med_seq/protocols.cgi. Exonic regions 18 \u201321, including the exon/intron boundaries of EGFR (Entrez GeneID 2065) were amplified by polymerase chain reaction. The sequence traces were assembled and scanned for variations from the reference sequence through the use of PolyPhred and PolyScan. All detailed protocols and primer sequences are available through the Washington University School of Medicine GSC website (http://genome.wustl. edu/platforms.cgi?id 7). After sequencing the DNA from the 32 mesothelioma samples, we found no evidence of mutations in our set of patients with mesothelioma. Our findings are consistent with the three earlier published studies.3\u20135 We further confirm the findings that although increased expression of EGFR is common in patients with mesothelioma, mutations of the EGFR kinase region commonly occurring in non-small cell lung cancer seem to be uncommon in mesothelioma. Lack of activating mutations in the EGFR TK domain may explain the inactivity of EGFR tyrosine kinase inhibitors in mesothelioma.\n\n==\n\nItakura 1994:\n\nTitle: Epidermal growth factor receptor overexpression in esophageal carcinoma. An immunohistochemical study correlated with clinicopathologic findings and DNA amplification\n\nAbstract: Background. Many studies have reported the increased expression of epidermal growth factor receptor (EGFR) in various human malignancies and its association with the biologic behavior of the tumors.\n\n==\n\nRamael 2005:\n\nTitle: Immunohistochemical distribution patterns of epidermal growth factor receptor in malignant mesothelioma and non-neoplastic mesothelium\n\nAbstract: An immunohistochemical study of the epidermal growth factor (EGF) receptor in non-neoplastic pleural mesothelium (35 cases) and in human malignant mesothelioma (36 cases) was made, using a murine monoclonal antibody OM-11-951. All malignant mesotheliomas and non-neoplastic pleural biopsies exhibited a strong cytoplasmic immunoreactivity in mesothelial cells. Nuclear immunoreactivity was detected in mesothelial cells of all specimens of both malignant and non-neoplastic pleura. No statistically significant differences were found between malignant mesothelioma and non-neoplastic pleural mesothelium. There were differences, between the three subtypes of mesothelioma, in the number of cells that exhibited nuclear staining. Statistically significant differences were noted between the epithelial subtype and the mesenchymal subtype (P< 0.005), epithelial subtype versus the mixed cell type (P< 0.005) and between the mesenchymal component of the mixed cell type and the mesenchymal type (P<0.0005). We conclude that there is strong expression of EGF receptor in both malignant mesothelioma and in non-neoplastic pleural mesothelium. Different staining patterns are seen when comparing the different subtypes of mesotheliomas with each other. EGF receptor expression cannot be used to distinguish between malignant and benign mesothelium.\n\n==\n\nPalm\u00e9r 1999:\n\nTitle: Epidermal growth factor receptor ligands are chemoattractants for normal human mesothelial cells.\n\nAbstract: Signalling through epidermal growth factor (EGF) receptor leads to several cellular responses including cell division and cell migration. Since EGF receptors are expressed on normal mesothelial cells, this study investigated whether EGF receptor ligands act as chemoattractants on these cells. The study used Boyden chambers fitted with filters coated with the adhesive matrix proteins fibronectin, laminin, collagen type IV and the nonmatrix adhesive molecule poly-L-lysine, for the migration studies. Normal mesothelial cells migrated to EGF receptor ligands such as EGF, transforming growth factor (TGF)-alpha and heparin-binding epidermal growth factor (HB-EGF) at concentrations ranging 0.024-100 ng x mL(-1) (with a peak stimulation at 6.25 ng x mL(-1)), if matrix proteins were present as adhesive substrates. This migration was integrin-dependent, since the same cells failed to migrate in the absence of extracellular matrix molecules or when the Boyden chamber assay was performed in the presence of anti-beta1 integrin monoclonal antibodies. These findings describe for the first time epidermal growth factor receptor ligands acting as chemoattractants on normal mesothelial cells, and that signalling through epidermal growth factor receptors leading to mesothelial cell migration also requires the activation of integrins.\n\n==\n\nGill 1987:\n\nTitle: Epidermal growth factor and its receptor\n\nAbstract: Epidermal growth factor (EGF) binds with high affinity and specificity to a single site on the external domain of its transmembrane receptor to activate the tyrosine protein kinase activity of its cytoplasmic portion. The EGF receptor gene is amplified and over-expressed in several human tumors, suggesting that increased concentrations of the proto-oncogene leads to constitutive activity similar to that seen with oncogene erb B. Synthesis and degradation of the EGF receptor are regulated, in addition, covalent modification by phosphorylation regulates activity of the receptor protein. Intramolecular self-phosphorylation of Tyr1173 removes a competitive inhibitory constraint to enhance phosphorylation of substrates. Phosphorylation of Thr654 by protein kinase C decreases high affinity EGF binding and EGF-stimulated tyrosine protein kinase activity, providing a mechanism for heterologous regulation of the EGF receptor by tumor promoters and other ligand X receptor complexes. Extensive regulation contributes to normal growth control, abrogation of regulatory controls contributes to uncontrolled growth as seen with erb B transformation and EGF receptor gene amplification in human tumors.\n\n==\n\nClark 1985:\n\nTitle: Epidermal growth factor regulates the expression of its own receptor.\n\nAbstract: The epidermal growth factor (EGF) receptor gene is the cellular homolog of the avian erythroblastosis virus erbB oncogene. Control of EGF receptor expression determines cellular responsiveness to EGF and might play an important role in neoplastic development. Using RNA blot hybridization, we have found that exposure of human KB carcinoma cells to EGF results in elevated levels of EGF receptor mRNA. The phorbol ester 4 beta-phorbol 12-myristate 13-acetate also stimulates EGF receptor RNA accumulation. Immunoprecipitation of metabolically labeled (30 min) EGF receptor protein revealed that synthesis of new EGF receptor follows the increase in receptor RNA. Addition of cycloheximide together with EGF further enhances EGF receptor RNA accumulation. Results of nuclear runoff-transcription experiments suggest that the stimulatory effects of EGF and cycloheximide are most likely due to a posttranscriptional control mechanism.\n\n==\n\nGreen 1983:\n\nTitle: Distribution and number of epidermal growth factor receptors in skin is related to epithelial cell growth.\n\nAbstract: Epidermal growth factor (EGF), a low-molecular-weight polypeptide (G. Carpenter and S. Cohen, 1979, Annu. Rev. Biochem. 48, 193-216), stimulates the proliferation and keratinisation of cultured embryonic epidermis (S. Cohen, 1965, Dev. Biol. 12, 394-407) and promotes epidermal growth, thickening, and keratinisation when injected into neonatal mice (S. Cohen and G.A. Elliott, 1963, J. Invest. Dermatol, 40, 1-5). We have determined the distribution of the available receptors for epidermal growth factor in rat skin using autoradiography following incubation of explants with 125I-labelled mouse EGF. EGF receptors are detected on the epithelial cells overlying the basement membranes of the epidermis, sebaceous gland, and regions of the hair follicle all of which have proliferative capacity. In marked contrast, tissues which have started to differentiate and lost their growth potential, carry either an undetectable or sharply reduced number of EGF receptors. The EGF receptor number and receptor affinity of epidermal basal cells freshly isolated from rats of increasing age has also been determined. We find that receptor affinity remains unchanged (3.3 nM) but that basal cell surface receptor number decreases markedly with age. This decrease in receptor number is similar in trend to the known drop in basal cell [3H]thymidine labelling index which occurs over the same time period. The data suggest that the distribution of EGF receptors and EGF cell surface receptor number in skin are important in the spatial and temporal control of epithelial proliferation.", "final_summary": "The research question \"epidermal growth factor receptors overexpression mesothelium\" has been addressed in several studies. \n\nRena (2011) evaluated the phenotypic expression of epidermal growth factor receptor (EGFR) in malignant pleural mesothelioma (MPM) and its correlation with patients' prognosis. Velcheti (2009) found that while EGFR expression is common in mesothelioma, mutations in the EGFR kinase region, which are common in non-small cell lung cancer, are uncommon in mesothelioma. \n\nItakura (1994) reported increased EGFR expression in various human malignancies and its association with tumor behavior. Ramael (2005) found strong EGFR expression in both malignant mesothelioma and non-neoplastic pleural mesothelium, with different staining patterns seen in different subtypes of mesotheliomas. \n\nPalm\u00e9r (1999) found that EGFR ligands act as chemoattractants on normal mesothelial cells, indicating that EGFR signaling leads to mesothelial cell migration. Gill (1987) discussed how EGFR binds with high affinity and specificity to activate the tyrosine protein kinase activity of its cytoplasmic portion, and how the EGF receptor gene is amplified and overexpressed in several human tumors. \n\nClark (1985) found that exposure of human KB carcinoma cells to EGF results in elevated levels of EGF receptor mRNA, suggesting that control of EGF receptor expression determines cellular responsiveness to EGF. Green (1983) found that the distribution of EGF receptors and EGF cell surface receptor number in skin are important in the spatial and temporal control of epithelial proliferation.\n\nIn conclusion, these studies collectively suggest that EGFR is overexpressed in both malignant and non-neoplastic mesothelium, and this overexpression may play a role in cell proliferation and migration. However, mutations in the EGFR kinase region, which are common in other cancers, are uncommon in mesothelioma."}, {"query": "information summarization large language models", "paper_list_string": "Verma 2023:\n\nTitle: Large Scale Multi-Lingual Multi-Modal Summarization Dataset\n\nAbstract: Significant developments in techniques such as encoder-decoder models have enabled us to represent information comprising multiple modalities. This information can further enhance many downstream tasks in the field of information retrieval and natural language processing; however, improvements in multi-modal techniques and their performance evaluation require large-scale multi-modal data which offers sufficient diversity. Multi-lingual modeling for a variety of tasks like multi-modal summarization, text generation, and translation leverages information derived from high-quality multi-lingual annotated data. In this work, we present the current largest multi-lingual multi-modal summarization dataset (M3LS), and it consists of over a million instances of document-image pairs along with a professionally annotated multi-modal summary for each pair. It is derived from news articles published by British Broadcasting Corporation(BBC) over a decade and spans 20 languages, targeting diversity across five language roots, it is also the largest summarization dataset for 13 languages and consists of cross-lingual summarization data for 2 languages. We formally define the multi-lingual multi-modal summarization task utilizing our dataset and report baseline scores from various state-of-the-art summarization techniques in a multi-lingual setting. We also compare it with many similar datasets to analyze the uniqueness and difficulty of M3LS. The dataset and code used in this work are made available at \u201chttps://github.com/anubhav-jangra/M3LS\u201d.\n\n==\n\nChouikhi 2022:\n\nTitle: Deep Transformer Language Models for Arabic Text Summarization: A Comparison Study\n\nAbstract: Large text documents are sometimes challenging to understand and time-consuming to extract vital information from. These issues are addressed by automatic text summarizing techniques, which condense lengthy texts while preserving their key information. Thus, the development of automatic summarization systems capable of fulfilling the ever-increasing demands of textual data becomes of utmost importance. It is even more vital with complex natural languages. This study explores five State-Of-The-Art (SOTA) Arabic deep Transformer-based Language Models (TLMs) in the task of text summarization by adapting various text summarization datasets dedicated to Arabic. A comparison against deep learning and machine learning-based baseline models has also been conducted. Experimental results reveal the superiority of TLMs, specifically the PEAGASUS family, against the baseline approaches, with an average F1-score of 90% on several benchmark datasets.\n\n==\n\nVarab 2021:\n\nTitle: MassiveSumm: a very large-scale, very multilingual, news summarisation dataset\n\nAbstract: Current research in automatic summarisation is unapologetically anglo-centered\u2013a persistent state-of-affairs, which also predates neural net approaches. High-quality automatic summarisation datasets are notoriously expensive to create, posing a challenge for any language. However, with digitalisation, archiving, and social media advertising of newswire articles, recent work has shown how, with careful methodology application, large-scale datasets can now be simply gathered instead of written. In this paper, we present a large-scale multilingual summarisation dataset containing articles in 92 languages, spread across 28.8 million articles, in more than 35 writing scripts. This is both the largest, most inclusive, existing automatic summarisation dataset, as well as one of the largest, most inclusive, ever published datasets for any NLP task. We present the first investigation on the efficacy of resource building from news platforms in the low-resource language setting. Finally, we provide some first insight on how low-resource language settings impact state-of-the-art automatic summarisation system performance.\n\n==\n\nLiu 2014:\n\nTitle: Enhanced language modeling for extractive speech summarization with sentence relatedness information\n\nAbstract: Extractive summarization is intended to automatically select a set of representative sentences from a text or spoken document that can concisely express the most important topics of the document. Language modeling (LM) has been proven to be a promising framework for performing extractive summarization in an unsupervised manner. However, there remain two fundamental challenges facing existing LM-based methods. One is how to construct sentence models involved in the LM framework more accurately without resorting to external information sources. The other is how to additionally take into account the sentence-level structural relationships embedded in a document for important sentence selection. To address these two challenges, in this paper we explore a novel approach that generates overlapped clusters to extract sentence relatedness information from the document to be summarized, which can be used not only to enhance the estimation of various sentence models but also to allow for the sentencelevel structural relationships for better summarization performance. Further, the utilities of our proposed methods and several state-of-the-art unsupervised methods are analyzed and compared extensively. A series of experiments conducted on a Mandarin broadcast news summarization task demonstrate the effectiveness and viability of our method.\n\n==\n\nPerez-Beltrachini 2022:\n\nTitle: Models and Datasets for Cross-Lingual Summarisation\n\nAbstract: We present a cross-lingual summarisation corpus with long documents in a source language associated with multi-sentence summaries in a target language. The corpus covers twelve language pairs and directions for four European languages, namely Czech, English, French and German, and the methodology for its creation can be applied to several other languages. We derive cross-lingual document-summary instances from Wikipedia by combining lead paragraphs and articles\u2019 bodies from language aligned Wikipedia titles. We analyse the proposed cross-lingual summarisation task with automatic metrics and validate it with a human study. To illustrate the utility of our dataset we report experiments with multi-lingual pre-trained models in supervised, zero- and few-shot, and out-of-domain scenarios.\n\n==\n\nLi 2016:\n\nTitle: GuideRank: A Guided Ranking Graph Model for Multilingual Multi-document Summarization\n\nAbstract: Multilingual multi-document summarization is a task to generate the summary in target language from a collection of documents in multiple source languages. A straightforward approach to this task is automatically translating the non-target language documents into target language and then applying monolingual summarization methods, but the summaries generated by this method is often poorly readable due to the low quality of machine translation. To solve this problem, we propose a novel graph model based on guided edge weighting method in which both informativeness and readability of summaries are taken into consideration fully. In methodology, our model attempts to choose from the target language documents the sentences which contain important shared information across languages, and also retains the salient sentences which cannot be covered by documents in other language. The experimental results on our manually labeled dataset (It will be released to the public.) show that our method significantly outperforms other baseline methods.\n\n==\n\nLawrie 2003:\n\nTitle: Language models for hierarchical summarization\n\nAbstract: Hierarchies have long been used for organization, summarization, and access to information. In this dissertation we define summarization in terms of a probabilistic language model and use this definition to explore a new technique for automatically generating topic hierarchies. We use the language model to characterize the documents that will be summarized and then apply a graph-theoretic algorithm to determine the best topic words for the hierarchical summary. This work is very different from previous attempts to generate topic hierarchies because it relies on statistical analysis and language modeling to identify descriptive words for a document and organize the words in a hierarchical structure. \nWe compare our new technique to previous methods proposed for constructing topic hierarchies, including subsumption and lexical hierarchies. We also compare the words chosen to be part of the hierarchy to the top ranked words using TF.IDF in terms of how well each summarizes the document set. Our results show that the language modeling approach performs as well as or better than these other techniques in non user-based evaluations. We also show that the hierarchies provide better access to the documents described in the summary than does a ranked list using one of the non-user based evaluations we have developed. In a user study that compares the ability of users to find relevant instances using both the hierarchy and a ranked list to using the ranked list alone, we find that users like the information provided by the hierarchy and after some practice can use it as effectively as they can a ranked list.\n\n==\n\nSingh 2018:\n\nTitle: Long-Span Language Models for Query-Focused Unsupervised Extractive Text Summarization\n\nAbstract: Effective unsupervised query-focused extractive summarization systems use query-specific features along with short-range language models (LMs) in sentence ranking and selection summarization subtasks. We hypothesize that applying long-span n-gram-based and neural LMs that better capture larger context can help improve these subtasks. Hence, we outline the first attempt to apply long-span models to a query-focused summarization task in an unsupervised setting. We also propose the A cross S entence B oundary LSTM-based LMs, ASB LSTM and bi ASB LSTM, that is geared towards the query-focused summarization subtasks. Intrinsic and extrinsic experiments on a real word corpus with 100 Wikipedia event descriptions as queries show that using the long-span models applied in an integer linear programming (ILP) formulation of MMR criterion are the most effective against several state-of-the-art baseline methods from the literature.", "final_summary": "The research question \"information summarization large language models\" has been addressed in various ways across the selected papers. \n\nVerma (2023) introduced a large-scale multi-lingual multi-modal summarization dataset, M3LS, which is derived from news articles published by the British Broadcasting Corporation (BBC) over a decade and spans 20 languages. This dataset could potentially enhance the capabilities of large language models in summarizing information from diverse sources and languages.\n\nChouikhi (2022) explored the effectiveness of deep Transformer-based Language Models (TLMs) in Arabic text summarization. The study found that TLMs, particularly the PEAGASUS family, outperformed baseline models, indicating the potential of large language models in summarizing complex natural languages.\n\nVarab (2021) presented a large-scale multilingual summarization dataset containing articles in 92 languages. This dataset could be instrumental in improving the performance of large language models in summarizing information from low-resource languages.\n\nLiu (2014) proposed a novel approach to enhance the estimation of sentence models and to account for sentence-level structural relationships for better summarization performance. This approach could potentially improve the capabilities of large language models in extractive speech summarization.\n\nPerez-Beltrachini (2022) introduced a cross-lingual summarization corpus with long documents in a source language associated with multi-sentence summaries in a target language. This corpus could be beneficial for large language models in cross-lingual summarization tasks.\n\nLi (2016) proposed a graph model for multilingual multi-document summarization, which takes into account both the informativeness and readability of summaries. This model could potentially enhance the performance of large language models in summarizing information from multiple source languages.\n\nLawrie (2003) explored a new technique for automatically generating topic hierarchies using a probabilistic language model. This technique could potentially improve the capabilities of large language models in hierarchical summarization.\n\nSingh (2018) proposed the use of long-span n-gram-based and neural LMs for query-focused unsupervised extractive text summarization. This approach could potentially enhance the performance of large language models in summarizing information based on specific queries.\n\nIn conclusion, the selected papers collectively suggest that large language models can be effectively used for information summarization across diverse languages and modalities. The introduction of large-scale multi-lingual datasets, novel summarization approaches, and advanced language models could potentially enhance the capabilities of large language models in summarizing complex and diverse information."}, {"query": "why is forecasting weekly oil prices useful? who would benefit from that? how are weekly oil prices more useful than monthly oil prices?", "paper_list_string": "Degiannakis 2018:\n\nTitle: Forecasting oil prices: High-frequency financial data are indeed useful\n\nAbstract: The paper examines the importance of combining high frequency financial information, along with the oil market fundamentals, in order to gain incremental forecasting accuracy for oil prices. Inspired by French et al. (1987) and Bollerslev et al. (1988), who maintain that future asset returns are also influenced by past volatility, we use daily volatilities and returns from financial and commodity markets to generate real out-of-sample forecasts for the monthly oil futures prices. Our results convincingly show that although the oil market fundamentals are useful for long-run forecasting horizons, the combination of the latter with high-frequency financial data significantly improve oil price forecasts, by reducing the RMSE of the no-change forecast by approximately 68%. Results are even more impressive during the oil price collapse period of 2014\u201315. These findings suggest that we cannot ignore the information extracted from the financial markets when forecasting oil prices. Our results are both statistically and economically significant, as suggested by several robustness tests.\n\n==\n\nYanagisawa 2009:\n\nTitle: Usefulness of the Forward Curve in Forecasting Oil Prices\n\nAbstract: When people analyse oil prices, the forward curve is often referred to as it reflects the average view among market participants. In this paper, to what extent the forward curve provides useful information in forecasting oil prices was analysed quantitatively. Although the usefulness of the forward curve is confirmed in forecasting oil prices, the effect in reducing forecast error is small. Additionally, the forward curve is actually useful for one week ahead and for one month ahead in daily and weekly forecasts, respectively. However, the forward curve is scarcely useful in long-term forecast.\n\n==\n\nBaumeister 2012:\n\nTitle: What Central Bankers Need to Know About Forecasting Oil Prices\n\nAbstract: Recent research has shown that recursive real-time VAR forecasts of the real price of oil tend to be more accurate than forecasts based on oil futures prices of the type commonly employed by central banks worldwide. Such monthly forecasts, however, differ in several important dimensions from the forecasts central banks require when making policy decisions. First, central banks are interested in forecasts of the quarterly real price of oil rather than forecasts of the monthly real price of oil. Second, many central banks are interested in forecasting the real price of Brent crude oil rather than any of the U.S. benchmarks. Third, central banks outside the United States are interested in forecasting the real price of oil measured in domestic consumption units rather than U.S. consumption units. Addressing each of these three concerns involves modeling choices that affect the relative accuracy of alternative forecasting methods. In addition, we investigate the costs and benefits of allowing for time variation in VAR model parameters and of constructing forecast combinations. We conclude that quarterly forecasts of the real price of oil from suitably designed VAR models estimated on monthly data generate the most accurate forecasts among a wide range of methods including forecasts based on oil futures prices, nochange forecasts and forecasts based on models estimated on quarterly data.\n\n==\n\nAmin-Naseri 2007:\n\nTitle: A Hybrid Artificial Intelligence Approach to Monthly Forecasting of Crude Oil Price Time Series\n\nAbstract: Due to the important role of crude oil in the global economy, oil price is a key factor affecting economic plans and decisions of governments and commercial firms. Therefore, proactive knowledge of its future movements can lead to better decisions in various governmental and managerial levels. However, oil price forecasting with a satisfying accuracy has proved to be a difficult task because of the complex underlying mechanism governing oil price evolution. This paper proposes a hybrid artificial intelligence model for monthly crude oil price forecasting by means of feed-forward neural networks, genetic algorithm and k-means clustering. In order to evaluate the performance of the model, its forecasts are compared with those of the econometric model of Energy Information Administration as the best representative of econometric models, as well as three artificial intelligence models from the literature. Results show that our proposed model outperforms the above mentioned models.\n\n==\n\nZhang 2019:\n\nTitle: Do high-frequency stock market data help forecast crude oil prices? Evidence from the MIDAS models\n\nAbstract: Extensive studies have used stock market information to forecast crude oil prices, and stock market can more easily derive high-frequency data than crude oil market due to no revisions, which raises a question that whether high-frequency stock market data can improve the forecast performance of crude oil prices. Therefore, this paper employs the MIDAS model and the high-frequency data of four stock market indices to forecast WTI and Brent crude oil prices at lower frequency. The results indicate that the high-frequency stock market indices have certain advantage over the lower-frequency data in forecasting monthly crude oil prices, and the MIDAS model using high-frequency data proves superior to the ordinary model.\n\n==\n\nBaumeister 2013:\n\nTitle: Forecasting the Real Price of Oil in a Changing World: A Forecast Combination Approach\n\nAbstract: The U.S. Energy Information Administration (EIA) regularly publishes monthly and quarterly forecasts of the price of crude oil for horizons up to 2 years, which are widely used by practitioners. Traditionally, such out-of-sample forecasts have been largely judgmental, making them difficult to replicate and justify. An alternative is the use of real-time econometric oil price forecasting models. We investigate the merits of constructing combinations of six such models. Forecast combinations have received little attention in the oil price forecasting literature to date. We demonstrate that over the last 20 years suitably constructed real-time forecast combinations would have been systematically more accurate than the no-change forecast at horizons up to 6 quarters or 18 months. The MSPE reductions may be as high as 12% and directional accuracy as high as 72%. The gains in accuracy are robust over time. In contrast, the EIA oil price forecasts not only tend to be less accurate than no-change forecasts, but are much less accurate than our preferred forecast combination. Moreover, including EIA forecasts in the forecast combination systematically lowers the accuracy of the combination forecast. We conclude that suitably constructed forecast combinations should replace traditional judgmental forecasts of the price of oil.\n\n==\n\nChatziantoniou 2019:\n\nTitle: Futures-based forecasts: How useful are they for oil price volatility forecasting?\n\nAbstract: Oil price volatility forecasts have recently attracted the attention of many studies in the energy finance field. The literature mainly concentrates its attention on the use of daily data, using GARCH-type models. It is only recently that efforts to use more informative intraday data to forecast oil price realized volatility have been made. Despite all these previous efforts, no study has examined the usefulness of futures-based models for oil price realized volatility forecasting, although the use of such models is extensive for oil price predictions. This study fills this void and shows that futures-based forecasts based on intra-day data provide informative forecasts for horizons that span between 1-day and 66-days ahead. More importantly, these results hold true even during turbulent times for the oil market, such as the Global Financial Crisis of 2007-09 and the oil collapse period of 2014-15.\n\n==\n\nDelis 2021:\n\nTitle: What matters when developing oil price volatility forecasting frameworks\n\nAbstract: Forecasting oil price volatility is considered of major importance for numerous stakeholders, including, policy makers, industries and investors. This paper examines and evaluates the main factors that oil price volatility forecasters should take before constructing their forecasting models. Such factors are related to: i) direct vs iterated forecasts, ii) the incorporation of continuous and jump components, iii) the importance of semi variance volatility measures, and iv) OLS vs time-varying parameter (TVP) estimation procedures. Even more, we evaluate the performance of these factors for both realized and implied volatility measures, based on statistical loss functions, as well \nas, their economic use. The results show that depending on whether end-users are interested in forecasting the realized or the implied volatility, the factors influencing the accuracy of forecasts are different. In particular, for the realized volatility, direct forecasting based on TVP estimation procedure, as well as, using the information obtained in the semi variance measures are capable of producing significantly superior forecasts. By contrast, separating the continuous and the jump components of the realized volatility does not provide any added value to these forecasts. Turning to the OVX, based on the economic evaluation of our forecasts, the TVP estimation procedure \nseems to performbetter. In addition, we find evidence that the continuous component and the semi variance measures of the realized volatility also yield better OVX forecasts in the longer run horizons.", "final_summary": "Forecasting oil prices is a critical endeavor with far-reaching implications for various stakeholders. The ability to predict oil prices with accuracy can significantly influence economic plans and decisions at both governmental and managerial levels (Amin-Naseri, 2007).\n\nHigh-frequency financial data, when combined with oil market fundamentals, significantly improves the accuracy of oil price forecasts (Degiannakis, 2018). This combination can reduce the root mean square error of the no-change forecast by approximately 68%, making it a powerful tool for forecasting.\n\nThe forward curve, which reflects the average view among market participants, is useful in forecasting oil prices, particularly for short-term forecasts (Yanagisawa, 2009). However, the utility of the forward curve diminishes for long-term forecasts, indicating that different forecasting tools may be needed for different time horizons.\n\nHigh-frequency stock market data can improve the forecast performance of crude oil prices (Zhang, 2019). However, the abstract does not provide specific information on the comparison between weekly and monthly forecasts.\n\nForecast combinations, which have received little attention in the oil price forecasting literature, can be more accurate than traditional judgmental forecasts (Baumeister, 2013). These combinations can reduce the mean squared prediction error by as much as 12% and increase directional accuracy to as high as 72%.\n\nIn conclusion, forecasting oil prices is useful for a variety of stakeholders. The use of high-frequency financial data, forward curves, and forecast combinations can significantly improve the accuracy of these forecasts. However, the choice of forecasting tool may depend on the specific needs and time horizons of the user."}, {"query": "topical benefifical", "paper_list_string": "He 2016:\n\nTitle: Extracting Topical Phrases from Clinical Documents\n\nAbstract: \n \n In clinical documents, medical terms are often expressed in multi-word phrases. Traditional topic modelling approaches relying on the \"bag-of-words\" assumption are not effective in extracting topic themes from clinical documents. This paper proposes to first extract medical phrases using an off-the-shelf tool for medical concept mention extraction, and then train a topic model which takes a hierarchy of Pitman-Yor processes as prior for modelling the generation of phrases of arbitrary length. Experimental results on patients' discharge summaries show that the proposed approach outperforms the state-of-the-art topical phrase extraction model on both perplexity and topic coherence measure and finds more interpretable topics.\n \n\n\n==\n\nPorras-Luque 2007:\n\nTitle: [Topical antimicrobial agents in dermatology].\n\nAbstract: Topical antimicrobial agents are chemical substances that, directly applied to the skin, inhibit the growth or destroy any microorganism, either fungi, viruses or bacteria. Within this term, we generally refer to those that are active against the latter. They are divided into antiseptics and antibiotics. Antiseptics are directly applied to a living organism to eradicate the existing microorganism on the mucocutaneous surfaces, preventing their proliferation. Topical antibiotics are either produced by living organisms or manufactured through synthesis and are mainly used to fight infections. Topical antimicrobials represent an important option in the prophylaxis and treatment of primary and secondary superficial bacterial infections. Antibiotics for topical use have a lower incidence of systemic toxicity, secondary effects and development of resistance than parenteral antibiotics. The ideal topical antimicrobial must have a broad spectrum of activity, fast and prolonged antibacterial effect, bactericidal activity, easy diffusion through tissues and detritus, with minimal toxicity, very low or null incidence of irritation or allergy and sparse activity against normal skin flora. The approach of this article is the description of the classical and new antimicrobials that are, more important due to their wide use and proven efficacy. We review their pharmacological characteristics, spectrum of activity, possible secondary effects and interactions, as well as the combinations that improve their antimicrobial activity, main indications and possible development of resistance, all this properly documented.\n\n==\n\n\u30d1\u30f3\u30c9\u30e4\u3001\u30a2\u30ca\u30f3\u30c8\u30fb\u30b1\u30fc 2006:\n\nTitle: Topical compositions and the use thereof\n\nAbstract: Suitable for topical application to the skin, in a composition comprising a fugitive solvent base comprising at least one alcohol, to provide an emollient component. One of the advantages of the present invention is to reduce the irritancy potential of the alcoholic fugitive solvent base with the composition.\n\n==\n\n\u30c0\u30b9\u30b0\u30d7\u30bf\uff0c\u30d3\u30d0\u30c4\u30b7\u30e6\u30fb\u30e9\u30f3\u30b8\u30e4\u30f3 2007:\n\nTitle: Topical compositions and the use thereof\n\nAbstract: Topical compositions and describes a method for improving skin characteristics. Topical composition comprising a mixture of physical scatterers having a fully equipped particle size of up to 2 microns beyond the 100nm below and 300nm 5. The topical compositions of the present invention, when applied, brings the brightness of the desired skin, no redness and blemishes are often characterized, red intensity of less than -1 75 micron thickness of the film of the topical composition exhibit (a).\n\n==\n\nAhmed 2016:\n\nTitle: Topical Gel Formulation : A review\n\nAbstract: The motivation behind composing this survey on pharmaceutical gel was to aggregate the late writing with unique spotlight on discerning way to deal with topical detailing and fundamental parts of topical medication conveyance frameworks. Topical use of medications offers potential favorable circumstances of conveying the medication specifically to the site of activity and acting for an augmented timeframe. Skin is a standout amongst the most broad and promptly open organs on human body for topical organization and is primary course of topical medication conveyance framework. Gels have better potential as a vehicle to controlled medication topically in contrast with balm, since they are non-sticky requires low vitality amid the plan. Topical gels are proposed for skin application on the other hand to certain mucosal surfaces for neighborhood activity or percutaneous infiltration of medicament or for their emollient or defensive activity. Gels are assessed by taking after parameters, for example, pH, drug content, thickness (Brookfield viscometer), spreadability, and extrudability, skin disturbance on templates, in-vitro discharge, in steadiness. By and large, the clinical confirmation shows that topical gel is a sheltered and powerful treatment choice for use in the administration of skin related illnesses.\n\n==\n\nHuang 2000:\n\nTitle: Topical anesthetics in dermatology.\n\nAbstract: Topical anesthetics are valuable tools for the dermatologists. The purpose of this study is to educate dermatologists about the pharmacology and clinical applications of topical anesthetics. A review of the current topical anesthetics was conducted. Several topical anesthetics are available to provide safe and effective cutaneous analgesia. Patient care can be improved by reducing the discomfort of local procedures and by minimizing the side effects of the anesthetics.\n\n==\n\nBoyce 1982:\n\nTitle: Beyond topicality : A two stage view of relevance and the retrieval process\n\nAbstract: Abstract Topicality is an operationally necessary but insufficient condition for requestor judged relevance. Documents are independent of one another as to any judgement of their topicality but not independent as to any judgement of their relevance which is a function of their informativeness to a requestor. Recall depends solely upon topicality but precision depends upon informativeness as well. A retrieval system which aspires to the retrieval of relevant documents should have a second stage which will order the topical set in a manner so as to provide maximum informativeness to the requestor. Should a system be concerned only with topicality then a two stage system which generates a high recall set and discards imprecise documents by measuring their distance from a seed document can be iterated to provide topicality feedback without user input.\n\n==\n\nHadgraft 2016:\n\nTitle: Advanced topical formulations (ATF).\n\nAbstract: Topical formulations aim to target the skin for a variety of cosmetic, protective or therapeutic needs. Despite the use of creams and ointments over the millennia, the bioavailability of actives from topical preparations remains quite low, often not exceeding 1-2% of the applied dose. In this review we examine the reasons underlying the poor performance of topical preparations. We also outline a rational approach, based on Fick's laws of diffusion, to develop advanced topical formulations. Methodologies which are currently used in research and development are critically examined and the importance of understanding the fate of the vehicle as well as the active is emphasised. Advanced topical formulation development will also be facilitated by emerging and sophisticated analytical techniques that are able to probe real time delivery of actives to the skin. A good understanding of the underlying physical chemistry of both the formulation and the skin is crucial in the development of optimised topical products.", "final_summary": "The research question \"topical beneficial\" is broad, but the collected papers provide insights into various aspects of topical applications, from medical to computational fields.\n\nIn the medical field, topical applications have been found to be beneficial in several ways. Porras-Luque (2007) discusses the efficacy of topical antimicrobial agents in dermatology, highlighting their importance in preventing and treating superficial bacterial infections. Huang (2000) emphasizes the value of topical anesthetics in dermatology, providing safe and effective cutaneous analgesia. Ahmed (2016) reviews the advantages of topical gel formulations, which can deliver drugs directly to the site of action for an extended period. \n\nIn the realm of skincare, \u30d1\u30f3\u30c9\u30e4\u3001\u30a2\u30ca\u30f3\u30c8\u30fb\u30b1\u30fc (2006) discusses a topical composition that reduces the irritancy potential of an alcoholic solvent base, contributing to improved skin characteristics. Meanwhile, \u30c0\u30b9\u30b0\u30d7\u30bf\uff0c\u30d3\u30d0\u30c4\u30b7\u30e6\u30fb\u30e9\u30f3\u30b8\u30e4\u30f3 (2007) describes a topical composition that enhances skin brightness and reduces redness and blemishes. Hadgraft (2016) also emphasizes the need for advanced topical formulations to increase the bioavailability of actives from topical preparations.\n\nIn the computational field, He (2016) discusses the extraction of topical phrases from clinical documents, which can improve the interpretation of patient data. Boyce (1982) proposes a two-stage view of relevance in information retrieval, where topicality is a necessary but insufficient condition for requestor-judged relevance.\n\nIn conclusion, the papers collectively suggest that topical applications, whether in the form of medical treatments, skincare products, or computational methods, offer significant benefits. However, further advancements are needed to maximize their potential, particularly in the areas of drug delivery and information retrieval."}, {"query": "job performance definition", "paper_list_string": "Ramawickrama 2017:\n\nTitle: A Synthesis towards the Construct of Job Performance\n\nAbstract: Job performance is highly relevant for organizations and individuals alike. Individual Job performance is the behavioural outcome of an employee which points out that the employee is showing positive attitudes towards his or her organization. Job performance is differently defined and measured in different disciplines in different ways. The main purpose of this paper is to define and to review theoretically and empirically the concept of job performance, measurement dimensions of job performance and empirical findings for measurement dimensions of job performance with reference to the various professions in service oriented organizations. As a desk research, this study reviewed literature regarding job performance and its dynamic nature, compared and analyzed dimensions (taxonomies) related to job performance, created a new definition and explained the importance of job performance adding novelty to the existing literature and provided suggestions for further studies.\n\n==\n\nMing-sum 1998:\n\nTitle: A Job Performance Model for Professional Social Workers\n\nAbstract: This paper defines job performance as a social construct from different perspectives, such as standards, behaviour, and process. A comprehensive model was constructed to explain the multiple realities of the job performance of professional social workers. The different perspectives of job performance: organizational performance, staff performance and quality of service were discussed. The dynamic relationships between various factors behind these components were also identified. Supervision and professional development for social workers, goals setting and strategy formulation for human service organizations, and programme evaluation for service delivery were proposed as guidelines for improving job performance of professional social workers.\n\n==\n\nSonnentag 2010:\n\nTitle: Job Performance\n\nAbstract: Individual performance is of high relevance for organizations and individuals alike. Showing high performance when accomplishing tasks results in satisfaction, feelings of selfefficacy and mastery (Bandura, 1997; Kanfer et aL, 2005). Moreover, high performing individuals get promoted, awarded and honored. Career opportunities for individuals who perform well are much better than those of moderate or low performing individuals (Van Scotter et aI., 2000). This chapter summarizes research on individual performance and addresses performance as a multi-dimensional and dynamic concept. First, we define the concept of performance, next we discuss antecedents of between-individual variation of performance, and describe intraindividual change and variability in performance, and finally, we present a research agenda for future research.\n\n==\n\nViswesvaran 2008:\n\nTitle: Job Performance: Assessment Issues in Personnel Selection\n\nAbstract: An important construct in Industrial, Work and Organizational (TWO) psychology, organizational behavior, and human resources management (personnel selection, training, and performance evaluation) in general, and personnel selection in particular, is the construct of job performance. Job performance is the most important dependent variable in TWO psychology (Schmidt & Hunter, 1992). A general definition of the construct of job performance reflects behaviors (both visually observable and non-observable) that can be evaluated (Viswesvaran, Ones, & Schmidt, 1996). In other words, job performance refers to scalable actions, behaviors, and outcomes that employees engage in or bring about that are linked with and contribute to organizational goals (Viswesvaran & Ones, 2000). To date, most researchers focusing on the construct of job performance have confined themselves to particular situations and settings with no attempt to generalize their findings. Also, there has been an emphasis on prediction and practical application rather than explanation and theory building. The consequence of these two trends has been a proliferation of the various measures of job performance in the extant literature. Virtually every measurable individual differences dimension thought to be relevant to the productivity, efficiency, or profitability of the unit or organization has been used as a measure of job performance. Absenteeism, productivity ratings, violence on the job, and teamwork ratings are some examples of the variety of measures used to measure job performance. There are multiple uses for job performance data. In selection contexts, measures of job performance are used to validate predictors. \u2018Thus, the choice of the job performance measure has important substantive implications for our practice and science of personnel selection. Measures of individual job performance play a central role at each step of the personnel selection function. Consider the first step in selection: recruitment of qualified applicants. One question in recruitment is whether the different sources of recruitment result in attraction of individuals who differ in job performance levels (Barber, 1998). Following successful recruitment efforts, attempts are made to identify individual differences variables that are related to individual differences in job performance, and select individuals based on those characteristics (Guion, 1998). Individual differences in job performance are assessed and those assessments are used in placement and promotion decisions.\n\n==\n\nJohnson 2009:\n\nTitle: A multi-level investigation of overall job performance ratings\n\nAbstract: Multi-level modeling was used to understand how supervisor assign overall job performance ratings. Results indicated a uniform relationship between task and overall performance ratings across supervisors but significant variability in the relationship between contextual and overall performance ratings. Employee and supervisor attributes were examined to explain this variability. Job performance is typically conceptualized as \" actions and behaviors that are under the control of the individual that contribute to the goals of the organization, \" (Rotundo & Sackett, 2002, p. 66). Campbell (1990) asserts that job performance is inherently multidimensional, a view that is shared by the vast majority of performance researchers (e. However, it is also generally accepted that, while multidimensional conceptions of performance are appropriate when conducting research, for decision making in organizations, a unidimensional, or composite criteria is preferred (e.g., Schmidt & Kaplan, 1971). And while the notion that raters will differ in the way they combine information to arrive at an overall rating is far from new (e.g., Naylor & Wherry, 1965), in the job performance domain, research dedicated to explaining this variability is limited. Performance Task performance refers to behaviors and activities that support the organization's technical core, which can involve the execution of technical processes (transforming raw materials into the goods or services provided by the organization) or the maintenance of those processes, for instance by providing raw materials, distributing products, or through planning and coordination functions (Borman & Motowidlo, 1993; Motowidlo, et al., 1997). The term contextual performance was coined by Borman and Motowidlo (1993) who argued that performance measures used in selection research and practice ignored activities such as persisting, helping, and endorsing organizational objectives. They argue that the criterion domain consists of task performance as well as contextual performance, or behaviors that support the broader psychological and social environment in which that technical core must function. An individual's overall performance rating can be thought of as a measure of his or her organizational worth (Motowidlo & Van Scotter, 1994). Implicit in this definition is that job performance must include only those behaviors that contribute to the organization's goals (Campbell, 1990). Accordingly, the rating assigned to an incumbent's overall job performance will depend not only on the level at which he or she performs certain behaviors, but also on the rater's beliefs about the goals of the organization and his or her mental model relating job behaviors to those goals. According to \u2026\n\n==\n\nMotowidlo 2012:\n\nTitle: 5 Job Performance\n\nAbstract: This chapter presents an overview of job performance as it is conceptualized in the industrial\u2013organizational psychology literature. It includes a definition of job performance that emphasizes the behavioral, episodic, and aggregate nature of the construct. The chapter reviews and discusses several well-established multidimensional models of job performance, along with emerging theories of performance such as adaptive performance and trait activation. Causal antecedents of job performance are discussed, including personality traits, cognitive ability, motivation, knowledge, and skill, along with their interrelations. Various dilemmas in theories of performance are explored, including whether organizational citizenship behavior is likely a latent or aggregate construct, whether counterproductive work behavior and organizational citizenship should be considered opposite ends of a single continuum or separate constructs, and how counterproductive work behavior could be a product of knowledge and skill. \n \n \nKeywords: \n \njob performance; \ncontextual performance; \norganizational citizenship behavior; \ncounterproductive work behavior; \nimplicit trait policies\n\n==\n\nCarlos 2016:\n\nTitle: Development and Validation of a Self-Reported Measure of Job Performance\n\nAbstract: The current turbulent context in which we live in requires, more and more, that organisations focus on improving the workers\u2019 performance. Individual performance is more than just the execution of specific tasks and it involves an ample variety of organisational activities that have important implications for the understanding and measurement of job performance. Considering the fact that most individual performance measures are developed in order to be applied in particular job-related contexts or cultures, our goal is to develop a job performance measure that might be applicable across jobs and cultures. After an extensive literature review, and based on studies that were developed in different cultural and job-related contexts, two dimensions\u2014task and contextual\u2014and eight sub dimensions of job performance were found: job knowledge, organisational skills, efficiency, persistent effort, cooperation, organisational consciousness, personal characteristics and interpersonal and relational skills. Confirmatory factorial analysis was used in order to test their relevance. The dimensions \u2018personal characteristics\u2019 and \u2018persistent effort\u2019 were merged. The resulting 29 item scale presents appropriate psychometric properties.\n\n==\n\nAketch 2012:\n\nTitle: Effects of quality of work life on job performance: theoretical perspectives and literature review\n\nAbstract: The concept of quality of work life exhibits positive emotional reactions and attitudes an individual has towards their job. It has been conceptualized as a general attitude toward the job. Job performance is often viewed as the degree to which employees execute their job tasks, responsibilities and assignments adequately. Motivation is the willingness to work at a certain level of effort and drives employees to action. Motivation emerges out of needs, values, goals, intentions and expectations. The study utilizes desk research to review literature on quality of work life that informs organization performance and motivation. The importance of considering these factors is demonstrated in the strong relationship between employee\u2019s well-being at work and performance of such organizations.", "final_summary": "Job performance is a multifaceted construct that is defined and measured in various ways across different disciplines. Ramawickrama (2017) defines it as the behavioral outcome of an employee that reflects positive attitudes towards their organization. Ming-sum (1998) takes a broader perspective, defining job performance as a social construct that encompasses standards, behavior, and process. This definition is echoed by Viswesvaran (2008), who describes job performance as scalable actions, behaviors, and outcomes that employees engage in or bring about that are linked with and contribute to organizational goals.\n\nThe dimensions of job performance are also diverse. Sonnentag (2010) and Johnson (2009) both emphasize the multidimensional nature of job performance, with Johnson (2009) suggesting that it includes both task and contextual performance. Carlos (2016) expands on this by identifying eight sub-dimensions of job performance, including job knowledge, organizational skills, efficiency, persistent effort, cooperation, organizational consciousness, personal characteristics, and interpersonal and relational skills.\n\nMotowidlo (2012) introduces the concept of adaptive performance and trait activation as emerging theories in job performance. He also discusses the causal antecedents of job performance, including personality traits, cognitive ability, motivation, knowledge, and skill. Aketch (2012) adds to this by suggesting that the quality of work life, which exhibits positive emotional reactions and attitudes an individual has towards their job, can also affect job performance.\n\nIn conclusion, job performance is a complex construct that encompasses a wide range of behaviors, outcomes, and dimensions. It is influenced by various factors, including individual traits, skills, motivation, and the quality of work life. Further research is needed to fully understand and measure this construct across different jobs and cultures."}, {"query": "Main findings and conflicts about the urbanization of agricultural land in Chile", "paper_list_string": "Madaleno 2004:\n\nTitle: \u201cUrban versus rural\u201d no longer matches reality: an early public agro-residential development in periurban Santiago, Chile\n\nAbstract: Abstract The paper discusses the validity of the \u201curban versus rural\u201d dichotomy in scientific literature, statistics and urban planning practice, especially in empirical studies developed around the issue of vegetable and animal farming inside and around cities. Santiago\u2019s rural\u2013urban interface is presented as a case study. Sample results extracted in early 2003 have shown that farming mixed with other land uses is even now a reality in Southern Santiago, supported in solidarity alliances and a characteristic life philosophy, giving researchers the possibility of exploring a living ecocity laboratory.\n\n==\n\nBarrado 2020:\n\nTitle: Promoted Urbanization of the Countryside: The Case of Santiago\u2019s Periphery, Chile (1980\u20132017)\n\nAbstract: Urbanization of the countryside affects rural areas, especially in the immediate surroundings of large cities. Normally, this occurs as an unpromoted process, but in Chile, it is driven by the legal framework. This research focuses on rural residential plots (RRPs) around the capital city, Santiago. The analysis seeks to understand the significance and consequences of RRPs during the last four decades and the role of a favorable legal framework in affecting their development. By examining data and official cartography on rural residential plots, the analysis shows a large phenomenon of rapid RRP development in the Metropolitan Region of Santiago de Chile (MR). The study confirms the existence of an ongoing process that is still partially latent and potentially both uncontrolled and evolving. This work demonstrates the negative effect that land liberalization policies can have by promoting territorial transformations that policymakers cannot subsequently control. The conclusions provide a critical perspective on the counter-urbanization process in the context of fragility and scarce resources.\n\n==\n\nContesse 2018:\n\nTitle: Is urban agriculture urban green space? A comparison of policy arrangements for urban green space and urban agriculture in Santiago de Chile\n\nAbstract: Urban green spaces are crucial for citizens\u2019 wellbeing. Nonetheless, many Latin American cities struggle to provide sufficient and equitable green space distribution for their citizens. By looking at the Chilean capital Santiago as an example, this paper examines whether the growing urban agriculture movement provides a feasible opportunity to increase public urban green space access. It does so by using the policy arrangement approach to analyse change and stability in two policy domains: urban green space planning and urban agriculture. The paper investigates urban green spaces and urban agriculture and the role of practitioners, urban planners and policymakers. The analysis found opportunities for urban agriculture to facilitate the expansion of urban green spaces in Santiago if policy mechanisms enable private or public spaces to be maintained by citizen organizations. Such mechanisms may, however, encounter resistance from public agencies, as it is unresolved who is involved and who benefits from urban agriculture. The paper concludes that urban agriculture is an opportunity for urban greening in Santiago, although changes are needed in how green areas are planned and conceived. Additionally, urban agriculture should not be understood as a substitute for parks but as a complementary form of green space provision with a distinctive value.\n\n==\n\nPauchard 2006:\n\nTitle: Multiple effects of urbanization on the biodiversity of developing countries: the case of a fast-growing metropolitan area (Concepci\u00f3n, Chile).\n\nAbstract: Urbanization is increasingly homogenizing the biota of less developed countries. Even though urban sprawl is a worldwide problem, most studies on the effects of urbanization, and the conceptual models have focused on developed countries. South America has not escaped urbanization, and here we discuss the potential impacts of urban sprawl with respect to three ecosystems in the metropolitan area of Concepcion, Chile. We consider this area a good model and fairly representative of other cities in developing countries which are also experiencing rapid and uncontrolled growth. We found that the impacts of urban sprawl on biodiversity in the metropolitan area of Concepcion differ little from cities in other parts of the world: native ecosystems are replaced by pavements and buildings and what is left of the natural soil is covered with green areas dominated by non-native ornamental species. Wetlands and other peri-urban ecosystems are rapidly being destroyed, fragmented or invaded by non-native species. We found that from a study area of 32,000 ha, there was a net loss to urbanization of 1734 ha of wetlands (23% of the original) and 1417 ha (9%) of agricultural, forest and shrub land cover types between 1975 and 2000. From the total area urbanized (3151 ha), 55% corresponded to wetlands and 45% to agricultural, forest and shrub lands cover types. We see the lack of environmental awareness as a major cause of the increasing deterioration of biodiversity in urban areas of developing countries. More research is needed to fully understand the effects of urban sprawl on the biodiversity of developing countries to include these ecosystems in global conservation strategies.\n\n==\n\nGeisse 1978:\n\nTitle: Urbanizaci\u00f3n e industrializaci\u00f3n en Chile.\n\nAbstract: INTRODUCCION En este trabajo se sintetizaran los principales aspectos desarrollados con mayor detalle en un proyecto de investigacion mas extenso sobre las interrelaciones entre desarrollo economico y la urbanizacion en el caso chileno durante el presente siglo hasta fines de la decada de los anos sesenta . El campo de referencia del estudio es el sistema urbano nacional, entendido como el componente principal de la integracion territorial de las diferentes actividades economicas y de las estructuras de clases y de poder. El supuesto basico, entonces, ha sido que los sistemas urbanos no son sino manifestaciones espaciales de este conjunto de estructuras y relaciones sociales. Por ello, desde un punto de vista metodologico, el estudio de su funcionamiento y transformaciones solo puede ser abordado a la luz de las leyes que rigen el\n\n==\n\nFoster 2016:\n\nTitle: Geographic disparities in rural land appreciation in a transforming economy: Chile, 1980 to 2007\n\nAbstract: This paper reports on a research effort to gather and analyze rural land value data during a period of unprecedented growth in Chilean agriculture. This information is important to understand the geographical distribution of gains associated with the transformation of the rural sector during a period of rapid development, trade liberalization and transition toward a predominant emphasis on export earnings in agriculture. A large set of data of rural land transactions for 1980, 1990, 1997 and 2007 were collected from a sample of land registry offices. Results show notable declines in the physical size of transactions, significant average annual rates of increase in real per-hectare values, and a small-parcel premium for rural land associated with non-farm land use. Overall real land values have increased faster than the average annual growth rates in the agricultural sector\u2019s value added, suggesting that land owners have gained proportionately more than other claimants to sectoral income. Tests show significant geographic disparities in annual rates of land appreciation across regions and municipalities. Consistent with differential net gains due to integration into world markets and the geographic heterogeneity of suitability for different land uses, northern areas, with greater emphasis on export-oriented crops, have experienced the highest average rates of annual real per-hectare value growth, in the order of 7 percent, while southern areas, emphasizing traditional crops and pastures/livestock, have experienced growth rates of half that. Geographic disparities are also explained by proximity to urban population and income centers.\n\n==\n\nMarques 2012:\n\nTitle: Urban Agriculture in the Metropolitan Area of Santiago de Chile An Environmental Instrument to Create a Sustainable Urban Model\n\nAbstract: Currently, the city of Santiago de Chile represents an example of the rapid and expansive development experienced by capitals of developing countries. If Santiagos urban model should be classified, the balance tips towards the dispersed city model, characterized by consuming large amounts of resources faster than it can produce them. This paper explores the possible implementation strategies of urban agriculture in Santiago de Chile, related to urban policies and regulations. Urban agriculture is part of the toolkit that enable sustainable social and\n\n==\n\nAz\u00f3car 2007:\n\nTitle: Urbanization patterns and their impacts on social restructuring of urban space in Chilean mid-cities: The case of Los Angeles, Central Chile\n\nAbstract: Abstract Urban growth patterns of Chilean mid-cities are explored, analyzing land use/land cover changes between 1955\u20131978 and 1978\u20131998 in Los Angeles, a mid-city located in central Chile. Urban sprawl is analyzed with respect to population growth and possible effects on the city's socio-spatial structure. Markov's matrix was used to analyze sequences of land use/land cover changes, whereas regression analysis was used to study the relationship between city and population growth. The study is based on remotely sensed data and processed in a GIS environment using Arcview 3.2, IDRISI, and Arc-Info N.T.", "final_summary": "The urbanization of agricultural land in Chile is a complex and multifaceted issue, with various studies highlighting different aspects and impacts of this process.\n\nMadaleno (2004) challenges the traditional \"urban versus rural\" dichotomy, suggesting that farming and other land uses coexist in periurban areas of Santiago. This suggests a blending of urban and rural characteristics in these areas, rather than a straightforward replacement of agricultural land with urban development.\n\nBarrado (2020) highlights the role of legal frameworks in promoting the urbanization of rural areas around Santiago. The study suggests that this process is not only uncontrolled but also potentially harmful, as it leads to territorial transformations that policymakers struggle to manage.\n\nContesse (2018) explores the potential of urban agriculture as a means to increase public access to urban green spaces in Santiago. The study suggests that urban agriculture could complement traditional green spaces, such as parks, but notes that this would require changes in planning and policy.\n\nPauchard (2006) discusses the negative impacts of urban sprawl on biodiversity in the metropolitan area of Concepcion, Chile. The study highlights the loss of wetlands and other peri-urban ecosystems due to urbanization.\n\nGeisse (1978) discusses the interrelations between economic development and urbanization in Chile, but does not specifically focus on the urbanization of agricultural land. Foster (2016) examines the geographical disparities in rural land appreciation in Chile during a period of agricultural transformation, but does not explicitly link this to urbanization.\n\nMarques (2012) suggests that urban agriculture could be a tool for creating a more sustainable urban model in Santiago, but does not specifically discuss urbanization patterns. Az\u00f3car (2007) explores the impacts of urban growth patterns on the socio-spatial structure of Los Angeles, but does not discuss urbanization patterns in Santiago.\n\nIn conclusion, the urbanization of agricultural land in Chile is a complex process influenced by various factors, including legal frameworks, economic development, and urban planning practices. The impacts of this process are diverse, affecting biodiversity, land values, and the socio-spatial structure of cities. Further research is needed to fully understand these impacts and to develop strategies for managing urbanization in a way that balances economic development with environmental sustainability and social equity."}, {"query": "What is the definition of primary data?", "paper_list_string": "Khuc 2021:\n\nTitle: Primary data survey: a step-by-step procedure for researchers in social sciences and humanities\n\nAbstract: Primary data, pilot survey, final survey\n\n==\n\nKhuc 2021:\n\nTitle: Primary data\n\nAbstract: The primary investigation, primary data, questionnaire method, model,\n\n==\n\nKhuc 2020:\n\nTitle: Primary data\n\nAbstract: The primary investigation, primary data, questionnaire method, model,\n\n==\n\nBoslaugh 2007:\n\nTitle: Secondary Data Sources for Public Health: An Introduction to Secondary Data Analysis\n\nAbstract: What Are Secondary Data? In the fields of epidemiology and public health, the distinction between primary and secondary data depends on the relationship between the person or research team who collected a data set and the person who is analyzing it. This is an important concept because the same data set could be primary data in one analysis and secondary data in another. If the data set in question was collected by the researcher (or a team of which the researcher is a part) for the specific purpose or analysis under consideration, it is primary data . If it was collected by someone else for some other purpose, it is secondary data . Of course, there will always be cases in which this distinction is less clear, but it may be useful to conceptualize primary and secondary data by considering two extreme cases. In the first, which is an example of primary data , a research team conceives of and develops a research project, collects data designed to address specific questions posed by the project, and performs and publishes their own analyses of the data they have collected. In this case, the people involved in analyzing the data have some involvement in, or at least familiarity with, the research design and data collection process, and the data were collected to answer the questions examined in the analysis.\n\n==\n\nSilman 2018:\n\nTitle: Use of secondary data\n\nAbstract: Primary data collection is challenging and with increasing electronic data capture in routine healthcare and other aspects of life, it is possible to address several epidemiological questions by robust analysis of such \u2018secondary data\u2019. There are considerable advantages in terms of scope, size, and speed of study to be balanced against the quality and depth of using primary data. Even when such direct contact is not required, there is often the need to extract necessary information from individual subject records such as medical files. There is often no alternative source of information, although the greater digitization of information is changing that scenario with the potential that the availability of such information might preclude the need for primary data.\n\n==\n\nGlass 1976:\n\nTitle: Primary, Secondary, and Meta-Analysis of Research1\n\nAbstract: M y subject is data analysis at three levels. Primary analysis is the original analysis of data in a research study. It is what one typically imagines as the application of statistical methods. Secondary analysis is the re-analysis of data for the purpose of answering the original research question with better statistical techniques, or answering new questions with old data. Secondary analysis is an important feature of the research and evaluation enterprise. Tom Cook (1974) at Northwestern University has written about its purposes and methods. Some of our best methodologists have pursued secondary analyses in such grand style that its importance has eclipsed that of the primary analysis. We can cite with pride some state of the art documents: the MostellerMoynihan secondary analysis of the Coleman study; the Campbell-Erlebacher analysis of the Ohio-Westinghouse Headstart evaluation; and the Elashoff-Snow secondary analysis of Pygmalion in the Classroom, to name three. About all that can effectively be done to insure that secondary analyses of important studies are carried out is to see that the data from the original studies are preserved and that secondary analyses are funded. The preservation of original data could improve. Last month, one of our graduate students, Karl White, spent 15 hours and made 30 phone calls attempting to obtain from the government a copy of the data tapes for the Coleman study only to learn in the end that they had been irretrievably filed in unmarked tape cannisters with some 2,000 other unmarked data tapes. Tom Cook remarked in an Annual Meeting symposium on secondary analysis that you can get the data if you have chutzpah or if you're socio metrically well-connected. The whole business is too important to be treated so casually. On the other extreme, one can point with satisfaction to the ready availability to any researcher of the data tapes from Project TALENT or the National Assessment of Educational Progress. Others are advancing the practice of secondary analysis. My major interest currently is in what we have come to call\u2014not for want of a less pretentious name\u2014the meta-analysis of research. The term is a bit grand, but it is precise, and apt, and in the spirit of \"metamathematics,\" \"meta-psychology,\" and \"meta-evaluation.\" Meta-analysis refers to the analysis of analyses. I use it to refer to the statistical analysis of a large collection of analysis results from individual studies for the purpose of integrating the findings. It connotes a rigorous alternative to the casual, narrative discussions of research studies which typify our attempts to make sense of the rapidly expanding research literature. The need for the meta-analysis of research is clear. The literature on dozens of topics in education is growing at an astounding rate. In five years time, researchers can produce literally hundreds of studies on IQ and creativity, or impulsive vs. reflective cognitive styles, or any other topic.\n\n==\n\nSchindler 2005:\n\nTitle: Webservices Infrastructure for the Registration of Scientific Primary Data\n\nAbstract: Registration of scientific primary data, to make these data citable as a unique piece of work and not only a part of a publication, has always been an important issue. In the context of the project \u201dPublication and Citation of Scientific Primary Data\u201d funded by the German Research Foundation (DFG) the German National Library of Science and Technology (TIB) has become the first registration agency worldwide for scientific primary data. Registration has started for the field of earth science, but will be widened for other subjects in the future. This paper shall give an overview about the technical realization of this important usage field for a digital library.\n\n==\n\nQuinn 1997:\n\nTitle: Primary methods of measurement and primary standards\n\nAbstract: The definition and concept of primary method are explained, taking examples from the fields of thermometry and measurements of amount of substance. It is shown that although few methods can be considered strictly primary, small departures from the strict definition can be quantified and useful practical primary methods are thus available. The links between primary methods of measurement, primary standards of measurement and primary reference materials are discussed.", "final_summary": "Primary data is defined as data collected by a researcher or a team for a specific purpose or analysis under consideration (Boslaugh, 2007). This data is collected through methods that are designed to directly address the research question at hand, such as through a primary investigation or questionnaire method (Khuc, 2021). It is the original analysis of data in a research study (Glass, 1976). Primary data is unique and specific to the research question it was collected to answer, making it a crucial component in various fields of study.\n\nIn conclusion, primary data is the original data collected by a researcher or team for a specific research purpose. It is the first-hand evidence that is gathered through methods designed to directly address the research question, such as primary investigations or questionnaires. This type of data is fundamental to research as it provides unique insights specific to the research question at hand."}, {"query": "adrenal and renal pecoma", "paper_list_string": "Lau 2012:\n\nTitle: Malignant PEComa of the adrenal gland.\n\nAbstract: Perivascular epithelioid cell neoplasms, also known as PEComas, are unique mesenchymal tumors exhibiting perivascular epithelioid cell differentiation, characterized by a mixed myogenic and melanocytic phenotype. PEComas arising in visceral organs outside of the kidney, liver, and lung are rare, and often pose problems in diagnosis. Examples of this neoplasm originating in the adrenal gland are limited. The present report details the clinical and pathologic features of an unusual case of a pure epithelioid PEComa (epithelioid angiomyolipoma) of the adrenal gland exhibiting clinically malignant behavior in the form of pulmonary metastases, a feature not previously described in tumors of this site. The diagnosis was supported by immunohistochemical studies demonstrating expression of myoid and melanocytic antigens. The present case serves to emphasize the potential of PEComa for clinically aggressive behavior and the importance of distinguishing this tumor from other epithelioid neoplasms that are more commonly encountered in the adrenal gland.\n\n==\n\nBattistella 2023:\n\nTitle: Metastatic Adrenal PEComa: Case Report and Short Review of the Literature\n\nAbstract: PEComa has become a widely accepted entity, and increased recognition has led to descriptions of this tumor in a wide variety of anatomic sites, including the adrenal gland. PEComa (perivascular epithelioid cell tumor) is a mesenchymal tumor composed of perivascular cells, and the most frequent sites of PEComas are the uterus and retroperitoneum. The incidence is <1 per 1,000,000 people. We report a case of adrenal metastatic PEComa in a 63-year-old man discovered by a spontaneous hematoma of the rectus abdominis. In our case, PEComa of the adrenal gland was a significant diagnostic dilemma as the morphologic and immunophenotypic features of this neoplasm may easily be confused with those of other more commonly encountered lesions.\n\n==\n\nPant 2015:\n\nTitle: Malignant Perivascular Epithelioid Cell Tumor (PEComa) of the Adrenal Gland: Report of a Rare Case Posing Diagnostic Challenge with the Role of Immunohistochemistry in the Diagnosis\n\nAbstract: Histological diagnosis of adrenal tumors is often challenging as diverse groups of tumors, both primaries and metastatic, may be seen in the adrenal gland with overlapping morphological features. Immunohistochemistry (IHC) plays the most important role in their diagnosis. Perivascular epithelioid cell tumor (PEComa), a rarely reported tumor in the adrenal gland, shares many features with another rare tumor sarcomatoid adrenocortical carcinoma (ACC). Extensive immunohistochemical study is required to distinguish this tumor from adrenocortical carcinoma and from other morphologically similar tumors. The unique combination of immunoreactivity for melanocytic markers, such as HMB-45 and Melan A, and myogenic markers, such as smooth muscle actin, is the hallmark of PEComas biological behavior, and prognosis of malignant PEComas is yet to be fully understood. Few cases of malignant PEComa have been reported in the adrenal gland. We report a case of malignant PEComa of the adrenal gland posing diagnostic challenge and compare its morphological and immunohistochemical features with those of sarcomatoid ACC.\n\n==\n\nIi 2016:\n\nTitle: Angiomyolipoma and Malignant PEComa: Discussion of Two Rare Adrenal Tumors\n\nAbstract: Angiomyolipoma and PEComa are rare tumors descending from perivascular epithelial cells (PECs), with distinctive IHC, morphological, and ultrastructural features. The kidney is the most frequent site of origin, but not the only one; however, adrenal gland angiomyolipomas are extremely rare. We describe two cases being found in the adrenal glands. Given the paucity of literature on the subject, more information on this disease is necessary for diagnosis and treatment. Here, we describe two complete case reports, from presentation to treatment and follow-up, along with imaging and microscopic pathology samples, and provide a comprehensive review as to the history and current literature available regarding these extremely rare tumors.\n\n==\n\nKumar 2010:\n\nTitle: Perivascular epithelioid cell tumour (PEComa) of the inferior vena cava presenting as an adrenal mass\n\nAbstract: Abstract A 54-year-old woman had a mass located in the right suprarenal area. On imaging, this mass appeared to be infiltrating the inferior vena cava (IVC). Exploratory laparotomy was undertaken and excision of the tumour was done with the sleeve of the involved IVC. The mass turned out to be a perivascular epithelioid cell tumour (PEComa) on histopathological examination. This report describes previously reported cases of PEComa in brief and highlights the problems associated with the management of this tumour.\n\n==\n\nMartignoni 2015:\n\nTitle: PEComas of the kidney and of the genitourinary tract.\n\nAbstract: PEComas are mesenchymal tumors composed of histologically and immunohistochemically distinctive perivascular epithelioid cells that are characterized by the coexpression of muscle and melanogenetic markers. This group of lesions includes angiomyolipoma, clear cell \"sugar\" tumor of the lung and extrapulmonary sites, lymphangioleiomyomatosis, clear cell myomelanocytic tumor of the falciform ligament/ligamentum teres, and rare clear cell tumors of other anatomical sites. In the genitourinary tract, PEComas have been described in the kidney, bladder, prostate, testis, and urethra. Although most PEComas behave as benign tumors, some are potentially malignant, and criteria for malignancy have been suggested for both and renal and extrarenal lesions. Recently, the expression of cathepsin K has been demonstrated in a large number of PEComas and has been proposed as a relatively specific marker to distinguish these proliferations from the majority of human cancers. In addition, a distinctive subset of PEComas harboring TFE3 gene fusions has been reported, giving rise to a possible relationship between them and MiTF/TFE family translocation renal cell carcinomas. The genetic alterations of tuberous sclerosis complex that promote activation of the mTOR pathway have been identified in PEComas. Therapy with mTORC1 inhibitors has been shown to be effective in some cases.\n\n==\n\nMcligeyo 1996:\n\nTitle: Renal vein and intracaval invasion by an adrenal phaeochromocytoma with extension Into the right atrium: a case study.\n\nAbstract: A 30 year old female with an unexpected right adrenal phaechromacytoma invading the renal vein, the inferior vena cava and extending into the right atrium is presented. She also had BuddChiari syndrome due to invasion of the hepatic veins by the tumour. Additionally, the tumour had metastasised to the liver and the lungs. Despite elevated 24 hour urinary vanillylmandelic acid (VMA) the patient was normotensive pre-operatively. The patient underwent right adrenalectomy and extended nephrectomy with milking of the tumaur from the inferior vena cava. Unfortunately, the patient developed multiple hypotensive episodes and adult respiratory distress syndrome post-operatively and died three weeks after surgery.\n\n==\n\nZhao 2014:\n\nTitle: Sclerosing PEComa of the kidney: clinicopathologic analysis of 2 cases and review of the literature.\n\nAbstract: Introduction Perivascular epithelioid cell tumors (PEComas) are a group of uncommon neoplasms that include angiomyolipoma, clear cell \u201csugar\u201d tumor of the lung, lymphangiomyomatosis, and a variety of unusual visceral, intra-abdominal, and soft tissue/bone tumors (such as clear cell myomelanocytic tumor of the falciform ligament/ ligamentum teres, abdominopelvic sarcoma of perivascular epithelioid cells, and primary extrapulmonary sugar tumor). The prototypic cell called perivascular epithelioid cell (PEC) has unique morphologic, immunohistochemical, ultrastructural, and genetic features that are characterized by perivascular location, often with a radial arrangement of cells around the vascular lumen. PECs typically show immunoreactivity for both melanocytic markers (such as HMB-45 [human melanoma black], melan-A [MLANA], and microphthalmia transcription factor [MiTF]) and muscle markers (such as smooth muscle actin [SMA] and calponin). Hornick and Fletcher have described a distinctive variant of PEComa having a hyalinized stroma and a striking predilection for the retroperitoneum of middle-aged women, for which they proposed the term sclerosing PEComa. Subsequently, there have been several case reports that are summarized in Table 1.", "final_summary": "Adrenal and renal PEComas, or Perivascular Epithelioid Cell tumors, are rare mesenchymal tumors that exhibit unique characteristics and pose significant diagnostic challenges (Lau, 2012; Battistella, 2023; Pant, 2015; Ii, 2016; Kumar, 2010; Martignoni, 2015; Zhao, 2014). These tumors are characterized by their perivascular location and the coexpression of muscle and melanogenetic markers (Martignoni, 2015; Zhao, 2014).\n\nPEComas have been found in various organs, including the adrenal gland (Lau, 2012; Battistella, 2023; Pant, 2015; Ii, 2016; Kumar, 2010). Lau (2012) reported a case of a pure epithelioid PEComa of the adrenal gland exhibiting clinically malignant behavior in the form of pulmonary metastases. Battistella (2023) also reported a case of adrenal PEComa, but did not specifically mention pulmonary metastases. Pant (2015) reported a case of malignant adrenal PEComa, emphasizing the role of immunohistochemistry in distinguishing this tumor from other similar ones. Ii (2016) discussed two cases of rare adrenal tumors, angiomyolipoma and PEComa, highlighting the need for more information for diagnosis and treatment. Kumar (2010) reported a case of PEComa in the inferior vena cava presenting as an adrenal mass, underscoring the challenges associated with managing this tumor.\n\nIn the renal context, PEComas have been described in the kidney and other parts of the genitourinary tract (Martignoni, 2015; Zhao, 2014). Martignoni (2015) noted that most PEComas behave as benign tumors, but some are potentially malignant. Zhao (2014) discussed a distinctive variant of PEComa, sclerosing PEComa, which has a hyalinized stroma and a predilection for the retroperitoneum of middle-aged women.\n\nIn conclusion, adrenal and renal PEComas are rare, complex tumors that require careful diagnosis and management. While they often behave as benign tumors, some exhibit malignant behavior, necessitating further research to fully understand their biological behavior and prognosis (Lau, 2012; Battistella, 2023; Pant, 2015; Ii, 2016; Kumar, 2010; Martignoni, 2015; Zhao, 2014)."}, {"query": "Critical evaluation of governance structure of Unilever PLC UK. Based the discussion on the UK Corporate Governance Code (April 2016 and July 2018), Financial Reporting Council (FRC). The discussion should focus on board leadership, board committees, risk management strategy, internal control mechanisms and key performance indicators (financial and financial indicators)", "paper_list_string": "Vafeas 1998:\n\nTitle: THE RELATIONSHIP BETWEEN BOARD STRUCTURE AND FIRM PERFORMANCE IN THE UK\n\nAbstract: Abstract This study examines the relationship between board structure with firm performance in the UK by employing data from 250 publicly traded firms. Consistent with general findings from the USA, the tests as a whole do not discern a significant link between board structure (director affiliation and ownership, chairman affiliation, and committee composition) with firm performance. These results are robust to alternative measures of performance, outlier definitions, various model specifications, and statistical estimation techniques. The most significant determinants of corporate performance are the level of R&D spending and current operating performance. These results are consistent with governance needs varying across firms, and contrast the notion that uniform board structures should be mandated.\n\n==\n\nPesqueux 2004:\n\nTitle: A critical view on corporate governance and its performance measurement and evaluation systems\n\nAbstract: Performance measurement and evaluation systems cannot be taken as \u201cpure\u201d tools because they are the product of a given society. That is why they take their meanings and their sense from this society. They are particularly linked with corporate governance which is also linked with the contemporary developments of capitalism seen as a political order. That is why performance measurement and evaluations sytems in today\u2019s companies are the production of a social game which has to be understood. Key indicators like shareholder\u2019s value and actors like auditing firms play a specific role which has to be evaluated. This paper will introduce the discussion about these systems (and moreover management tools) as the production of their society from a posture taken from Political Philosophy. They will be evaluated in relation which what is capitalism today (which has to be understood in relation with what it was yesterday). That is why notions like shareholder\u2019s value will be linked with the increasing weight of investments funds and with the systematic search for financial surplus. It will be induced that all these \u201cconcrete\u201d notions build a system and are reciprocally re\u2010enforced. A special mention will be made to auditing firms as a cartel. Corporate governance will, as well as auditing firms, be presented as the concretisation of a new social game.\n\n==\n\nKyere 2020:\n\nTitle: Corporate governance and firms financial performance in the United Kingdom\n\nAbstract: The objective of this study is to examine empirically the impact of good corporate governance on financial performance of United Kingdom non-financial listed firms. Agency theory and stewardship theory serve as the bases of a conceptual model. Five corporate governance mechanisms are examined on two financial performance indicators, return on assets (ROA) and Tobin's Q, employing cross-sectional regression methodology. The conclusion drawn from empirical test so performed on 252 firms listed on London Stock Exchange for the year 2014 indicates a positive or a negative relationship, but also sometimes no effect, of corporate governance mechanisms impact on financial performance. The implications are discussed. Thereby, so distinguishing effects due to causes, we present a proof that, when the right corporate governance mechanisms are chosen, the finances of a firm can be improved. The results of this research should have some implication on academia and policy makers thoughts.\n\n==\n\nWeir 2001:\n\nTitle: Governance structures, director independence and corporate performance in the UK\n\nAbstract: A number of Committees have been set up in recent years to investigate the governance of UK quoted companies. The key one was the Cadbury Committee, which recommended a number of governance structures as examples of best practice. These included the separation of the posts of CEO and chairman, a significant representation of non\u2010executive directors, the importance of non\u2010executive director independence and the setting up of board subcommittees. This study finds that there has been widespread adoption of the recommended governance structures. However, there is no clear relationship between governance structures and corporate performance. This raises questions about the most effective type of governance mechanism and whether or not the prescriptive recommendations of Cadbury should be replaced with a more flexible approach.\n\n==\n\nPage 2009:\n\nTitle: Corporate governance and corporate performance: UK FTSE 350 companies\n\nAbstract: In the wake of the recent financial crisis, attention has once again turned to corporate governance, with policy reviews of UK corporate governance being undertaken by the FRC and the Walker Review. One key question may relate to the purpose of corporate governance \u2013 is it about the control of risks, the improvement of performance, or both? If this could be clarified, criteria could be developed to measure the success of corporate governance procedures or codes. This research investigates whether companies with particular corporate governance characteristics outperform other companies and have lower levels of risk. The governance characteristics investigated in the report are: board independence; board size; directors\u2019 ownership of equity; and extent of ownership by large block holders. The effects of these characteristics were measured over two three year periods between 1999 and 2004. Three measures of performance were used: one stock market measure (market to book ratio); and two accounting based measures (return on assets and ratio of sales to total assets). Risk was measured in three ways: total risk; systematic risk; and a measure of sudden share price falls. The findings reveal no clear systematic relationship between governance factors and improved performance, and no strong evidence that governance reduces either total or systematic risk. This project was funded by the Scottish Accountancy Trust for Education and Research (SATER). The Research Committee of The Institute of Chartered Accountants of Scotland (ICAS) as also been happy to support this project. The Committee recognises that the views expressed do not necessarily represent those of ICAS itself, but hopes that the project will add to the knowledge about the interaction between corporate governance factors, company performance and risk.\n\n==\n\nShaukat 2017:\n\nTitle: Board Governance and Corporate Performance\n\nAbstract: We examine the link between the monitoring capacity of the board and corporate performance of UK listed firms. We also investigate how firms use the flexibility offered by the voluntary governance regime to make governance choices. We find a strong positive association between the board governance index we construct and firm operating performance. Our results imply that adherence to the board-related recommendations of the UK Corporate Governance Code strengthens the board's monitoring capacity, potentially helping mitigate agency problems, but that investors do not value it correspondingly. Moreover, in contrast to prior UK findings suggesting efficient adoption of Code recommendations, we find that firms at times use the Code flexibility opportunistically, aiming to decrease the monitoring capacity of the board, which is followed by subsequent underperformance. This finding questions the effectiveness of the voluntary approach to governance regulation followed in the UK as in many countries around the world. \n \nThis article is protected by copyright. All rights reserved\n\n==\n\nPaniagua 2018:\n\nTitle: Corporate governance and financial performance: The role of ownership and board structure\n\nAbstract: This study examines how corporate governance and ownership structure relate to the financial performance of firms. We estimated this relationship using fsQCA. We enhanced our analysis using complementary linear and non-linear multiple regression analysis. The panel data used in this study covered 1207 companies from 59 countries across 19 sectors for the period 2013 to 2015. The study makes two main contributions. First, the multiple empirical techniques employed in this study offer a broader approach to the empirical analysis of financial performance. Second, the study aids our understanding of the role of corporate governance and ownership in the financial performance of firms.\n\n==\n\nDalwai 2015:\n\nTitle: A critical review of relationship between corporate governance and firm performance: GCC banking sector perspective\n\nAbstract: Purpose \u2013 The purpose of this paper is to evaluate existing studies on the relationship of corporate governance with firm performance in different regions and address the need for similar analysis for the Gulf Coperation Council (GCC) sector. The banking sector comprises the conventional and Islamic banks in the GCC sector and is important due to their ability to bring stability to this region. Existing studies that measure the relationship of GCC sector conventional banks and firm performance are limited. This study proposes a need for future research on corporate governance in the GCC region. Design/methodology/approach \u2013 This paper will review and analyze the different empirical and theoretical contributions in establishing the relationship between corporate governance and firm performance. Findings \u2013 This paper will create a focus for future research of measuring the impact of corporate governance mechanism on firm performance. The regulators will be encouraged to focus on more research studies for th...", "final_summary": "The governance structure of Unilever PLC UK can be critically evaluated through the lens of various studies that have explored the relationship between corporate governance and firm performance. \n\nVafeas (1998) found no significant link between board structure and firm performance in the UK, suggesting that governance needs may vary across firms. This implies that Unilever's unique governance structure may be tailored to its specific needs. Similarly, Weir (2001) found that while there has been widespread adoption of recommended governance structures in the UK, there is no clear relationship between these structures and corporate performance. This suggests that Unilever's adherence to the UK Corporate Governance Code may not necessarily translate into improved performance.\n\nHowever, Shaukat (2017) found a strong positive association between the board governance index and firm operating performance in UK listed firms. This suggests that Unilever's adherence to the board-related recommendations of the UK Corporate Governance Code could potentially strengthen the board's monitoring capacity and mitigate agency problems. \n\nKyere (2020) found that the right corporate governance mechanisms can improve a firm's financial performance. This suggests that Unilever's financial performance may be influenced by its choice of corporate governance mechanisms. \n\nPage (2009) and Paniagua (2018) found no clear systematic relationship between governance factors and improved performance, and no strong evidence that governance reduces risk. This suggests that Unilever's governance structure may not necessarily lead to improved performance or reduced risk.\n\nIn conclusion, while Unilever's adherence to the UK Corporate Governance Code and its choice of governance mechanisms may potentially improve its board's monitoring capacity and financial performance, the relationship between governance structure and firm performance is not clear-cut. Further research is needed to fully understand the impact of Unilever's governance structure on its performance."}, {"query": "fiscal policy, bank lending ", "paper_list_string": "Melina 2012:\n\nTitle: Fiscal Policy and Lending Relationships\n\nAbstract: This paper studies how fiscal policy affects loan market conditions. First, it conducts a Structural Vector-Autoregression analysis showing that the bank spread responds negatively to an expansionary government spending shock, while lending increases. Second, it illustrates that these results are mimicked by a Real Business Cycle model where the bank spread is endogenized via the inclusion of a banking sector exploiting lending relationships. Third, it shows that lending relationships represent a friction that generates a financial accelerator effect in the transmission of the fiscal shock.\n\n==\n\nMelina 2014:\n\nTitle: Fiscal Policy and Lending Relationships\n\nAbstract: type=\"main\" xml:lang=\"en\"> This paper studies how fiscal policy affects loan market conditions in the United States. First, it conducts a structural vector-autoregression analysis showing that the bank spread responds negatively to an expansionary government spending shock, while lending increases. Second, it illustrates that these results are mimicked by a dynamic stochastic general equilibrium model where the bank spread is endogenized via the inclusion of a banking sector exploiting lending relationships. Third, it shows that lending relationships represent a friction that generates a financial accelerator effect in the transmission of the fiscal shock. (JEL E44, E62)\n\n==\n\nAydin 2011:\n\nTitle: Bank Lending in Turkey: Effects of Monetary and Fiscal Policies\n\nAbstract: We study the impact of monetary and fiscal policies on credit growth in Turkey using bank-level data from the last quarter of 2002 to the first quarter of 2008. We find evidence that the liquidity-constrained banks have a sharper decline in lending during contractionary monetary policies and that the crowding-out effect diminishes more for banks with a retail banking focus when the government adopts fiscal discipline. However, the results are statistically weak. Hence, the evidence is not strong enough to irrefutably document the bank lending channel and the impact of government finances on loan supply in Turkey even though these effects may be operational.\n\n==\n\nHauner 2006:\n\nTitle: Fiscal Policy and Financial Development\n\nAbstract: We examine the effects of public sector borrowing from the domestic banking system on financial development in middle-income countries. While these countries' external debt has been falling, the share of bank credit absorbed by the public sector has been rising rapidly. We argue that this runs the risk of slowing financial development by affecting structural characteristics of the banking systems. We find empirical evidence that too much public sector borrowing harms financial deepening, and that banks mainly lending to the public sector tend to be more profitable but less efficient. We note that these effects add to the costs of fiscal prolificacy.\n\n==\n\nHauner 2006:\n\nTitle: Fiscal Policy and Financial Development\n\nAbstract: We examine the effects of public sector borrowing from the domestic banking system on financial development in middle-income countries. While these countries' external debt has been falling, the share of bank credit absorbed by the public sector has been rising rapidly. We argue that this runs the risk of slowing financial development by affecting structural characteristics of the banking systems. We find empirical evidence that too much public sector borrowing harms financial deepening, and that banks mainly lending to the public sector tend to be more profitable but less efficient. We note that these effects add to the costs of fiscal prolificacy.\n\n==\n\nYan 2021:\n\nTitle: How Does Fiscal Policy Affect Bank Credit? Evidence from China\n\nAbstract: Using a set of Chinese economic data and a structural vector autoregression (SVAR) model, this paper investigates the transmission channels of fiscal policy to bank credit in China. We find that increases in tax revenue can increase bank credit through external financing premium channel, collateral channel, and bank liquidity channel. We also find that increases in government spending can reduce bank credit through bank liquidity channel and increase bank credit through external financing premium channel and collateral channel.\n\n==\n\nKollmann 2012:\n\nTitle: Fiscal Policy, Banks and the Financial Crisis\n\nAbstract: This paper studies the effectiveness of Euro Area (EA) fiscal policy, during the recent financial crisis, using an estimated New Keynesian model with a bank. A key dimension of policy in the crisis was massive government support for banks \u2014 that dimension has so far received little attention in the macro literature. We use the estimated model to analyze the effects of bank asset losses, of government support for banks, and other fiscal stimulus measures, in the EA. Our results suggest that support for banks had a stabilizing effect on EA output, consumption and investment. Increased government purchases helped to stabilize output, but crowded out consumption. Higher transfers to households had a positive impact on private consumption, but a negligible effect on output and investment. Banking shocks and increased government spending explain half of the rise in the public debt/GDP ratio since the onset of the crisis.\n\n==\n\nKollmann 2012:\n\nTitle: Fiscal Policy, Banks and the Financial Crisis\n\nAbstract: This paper studies the effectiveness of Euro Area (EA) fiscal policy, during the recent financial crisis, using an estimated New Keynesian model with a bank. A key dimension of policy in the crisis was massive government support for banks\u2014that dimension has so far received little attention in the macro literature. We use the estimated model to analyze the effects of bank asset losses, of government support for banks, and other fiscal stimulus measures, in the EA. Our results suggest that support for banks had a stabilizing effect on EA output, consumption and investment. Increased government purchases helped to stabilize output, but crowded out consumption. Higher transfers to households had a positive impact on private consumption, but a negligible effect on output and investment. Banking shocks and increased government spending explain half of the rise in the public debt/GDP ratio since the onset of the crisis.", "final_summary": "Fiscal policy, a government's strategy in managing its revenue and expenditure, has a significant impact on bank lending, as evidenced by several studies. Melina (2012, 2014) found that expansionary government spending reduces the bank spread, leading to an increase in lending. This effect is amplified by lending relationships, which act as a friction, creating a financial accelerator effect in the transmission of the fiscal shock.\n\nHowever, the relationship between fiscal policy and bank lending is not always positive. Aydin (2011) found that contractionary monetary policies lead to a sharper decline in lending, especially for liquidity-constrained banks. Furthermore, Hauner (2006) found that excessive public sector borrowing can harm financial deepening, making banks that primarily lend to the public sector more profitable but less efficient.\n\nIn the context of China, Yan (2021) found that increases in tax revenue can increase bank credit through various channels, including the external financing premium channel, collateral channel, and bank liquidity channel. However, increases in government spending can both reduce and increase bank credit through different channels.\n\nDuring the financial crisis, fiscal policy played a crucial role in stabilizing the banking sector. Kollmann (2012) found that government support for banks had a stabilizing effect on output, consumption, and investment. However, increased government spending crowded out consumption, while higher transfers to households had a negligible effect on output and investment.\n\nIn conclusion, fiscal policy has a complex and multifaceted impact on bank lending. While expansionary fiscal policy can stimulate lending, excessive public sector borrowing can harm financial development. Furthermore, the impact of fiscal policy on bank lending can vary depending on the specific economic context and the channels through which fiscal policy affects bank credit."}, {"query": "What are the effects of estrogen on spatial memory?", "paper_list_string": "Luine 1998:\n\nTitle: Estradiol Enhances Learning and Memory in a Spatial Memory Task and Effects Levels of Monoaminergic Neurotransmitters\n\nAbstract: The effects of chronic estrogen treatment on radial arm maze performance and on levels of central monoaminergic and amino acid neurotransmitters were examined in ovariectomized (Ovx) rats. In an eight arms baited paradigm, choice accuracy was enhanced following 12 days but not 3 days of treatment. In addition, performance during acquisition of the eight arms baited maze task was better in estrogen-treated Ovx rats than in Ovx rats. Performance of treated rats was also enhanced in win-shift trials conducted 12 days postestrogen treatment. Working, reference, and working-reference memory was examined when four of the eight arms were baited, and only working memory was improved by estrogen and only after long-term treatment. Activity of Ovx rats on an open field, crossings and rearings, was increased at 5 but not at 35 days following estrogen treatment. In medial prefrontal cortex, levels of NE, DA, and 5-HT were decreased but glutamate and GABA levels were not affected following chronic estrogen treatment. Basal forebrain nuclei also showed changes in monoamines following estrogen. Hippocampal subfields showed no effects of estrogen treatment on monoaminergic or amino acid transmitters. Levels of GABA were increased in the vertical diagonal bands following chronic estrogen. Results show that estrogen enhances learning/memory on a task utilizing spatial memory. Effects in Ovx rats appear to require the chronic (several days) presence of estrogen. Changes in activity of both monoaminergic and amino acid transmitters in the frontal cortex and basal forebrain may contribute to enhancing effects of estrogen on learning/memory.\n\n==\n\nHarburger 2007:\n\nTitle: Effects of estrogen and progesterone on spatial memory consolidation in aged females\n\nAbstract: Interpretation of data illustrating that estrogen, with or without progestin, is detrimental to memory in post-menopausal women is complicated by the fact that little is known about the effects of progestins on memory. The present study examined if estrogen, alone or with progesterone, affects spatial memory consolidation in ovariectomized aged female mice. Mice received eight training trials in a spatial Morris water maze followed immediately by injection of water-soluble 17beta-estradiol (E(2); 0.2 mg/kg) or vehicle. Mice were re-tested 24 h later. All mice learned to find the platform on Day 1. On Day 2, the performance of control, but not E(2) mice, deteriorated, suggesting that E(2) enhanced memory for the platform location. In a second experiment, mice were injected with E(2) and 10 or 20 mg/kg water-soluble progesterone. The 10 mg/kg dose of progesterone did not affect estrogen's ability to enhance spatial memory consolidation, but 20 mg/kg blocked this effect. These data indicate that estrogen can improve spatial memory consolidation in aged females and that this effect can be attenuated by progesterone.\n\n==\n\nRissman 2002:\n\nTitle: Disruption of estrogen receptor \u03b2 gene impairs spatial learning in female mice\n\nAbstract: Here we provide the first evidence, to our knowledge, that estradiol (E2) affects learning and memory via the newly discovered estrogen receptor \u03b2 (ER\u03b2). In this study, ER\u03b2 knockout (ER\u03b2KO) and wild-type littermates were tested for spatial learning in the Morris water maze after ovariectomy, appropriate control treatment, or one of two physiological doses of E2. Regardless of treatment, all wild-type females displayed significant learning. However, ER\u03b2KOs given the low dose of E2 were delayed in learning acquisition, and ER\u03b2KOs administered the higher dose of E2 failed to learn the task. These data show that ER\u03b2 is required for optimal spatial learning and may have implications for hormone replacement therapy in women.\n\n==\n\nVarga 2002:\n\nTitle: Weak if any effect of estrogen on spatial memory in rats\n\nAbstract: In a number of species, males appear to have spatial abilities that are superior to those of females. The favored explanation for this cognitive difference is hormonal: higher testosterone levels in males than in females. An alternative explanation focuses on the role of varying levels of estrogens in females during the estrus cycle; females perform as well as males on days of low estrogen, but more poorly on days of high estrogen. Other investigators have reported that estrogens improve both types of memory processes, which depend on the striatal (nonspatial navigation) and hippocampal (spatial) memory systems. Additionally, estrogens have been found to protect the working memory. These contradictory results initiated the present study, in which ovariectomized female rats were trained to escape in a Morris water maze. The daily trials were preceded by estradiol application in low doses (Experiment I) or in higher doses (Experiment II). In Experiment I, no differences at all were found between the latencies of the treated and control groups to reach a submerged platform in a Morris water maze. In Experiment II, however, the animals treated with the higher dose of estradiol showed a small deficit in the acquisition of the Morris water maze task. This study indicates that estradiol at around the physiological level has no effect on spatial learning and memory functions. + Dedicated to Professor Otto Feher on the occasion of his 75 th birthday.\n\n==\n\nSilverman 1993:\n\nTitle: Effects of estrogen changes during the menstrual cycle on spatial performance\n\nAbstract: Abstract Four sequential, interrelated studies of the relationship of menstrual cycle phase to three-dimensional mental rotations performance were conducted, using both between-and within-subjects designs. All studies showed significant increases in mean mental rotations scores during the menstrual period phase, when estrogen levels were at their lowest. Effects occurred only for mental rotations; relationships with hormonal status did not occur for control tests, which were not of a spatial nature, and a different spatial test (Space Relations). Findings are discussed as they relate to ontogenetic development and evolutionary origins of sex-specific differences in spatial behaviors.\n\n==\n\nGibbs 1999:\n\nTitle: Estrogen Replacement Enhances Acquisition of a Spatial Memory Task and Reduces Deficits Associated with Hippocampal Muscarinic Receptor Inhibition\n\nAbstract: A delayed matching-to-position (DMP) T-maze task was used to examine the effects of estrogen replacement on spatial learning and memory, as well as the ability of estrogen replacement to reduce performance deficits produced by acute systemic and intrahippocampal muscarinic cholinergic inhibition. Two experiments were performed. In Experiment 1, ovariectomized animals were trained to criterion on the DMP task and then tested with increased intertrial delays and following systemic scopolamine administration. The animals then received either continuous estrogen replacement or sham surgery and were retested beginning 10 days later. In Experiment 2, ovariectomized animals received guide cannulae implanted bilaterally into the hippocampus. Half of these animals also began receiving continuous estrogen replacement. Two months later, the animals were trained on the DMP task and then tested with increased intertrial delays and following systemic as well as intrahippocampal scopolamine administration. Animals received the same test battery 8 months later and were then immediately trained on a reversal task. The results indicate that estrogen-treated animals acquired the DMP task at a significantly faster rate than the ovariectomized, non-estrogen-treated controls. In addition, estrogen replacement significantly reduced deficits in DMP performance produced by intrahippocampal, but not systemic, scopolamine administration. This occurred when animals were tested after 3.5 months, as well as after 12 months, of continuous estrogen replacement. No evidence for an effect of estrogen replacement on spatial working memory or reversal learning was detected. These findings demonstrate that estrogen replacement can enhance acquisition of a spatial memory task and reduce performance deficits associated with hippocampal cholinergic impairment.\n\n==\n\nFrick 2002:\n\nTitle: Estrogen replacement improves spatial reference memory and increases hippocampal synaptophysin in aged female mice\n\nAbstract: Estrogen deficiency during menopause is often associated with memory dysfunction. However, inconsistencies regarding the ability of estrogen to improve memory in menopausal women highlight the need to evaluate, in a controlled animal model, the potential for estrogen to alleviate age-related mnemonic decline. The current study tested whether estrogen could ameliorate spatial reference memory decline in aged female mice. At the conclusion of testing, levels of the presynaptic protein synaptophysin, and activities of the synthetic enzymes for acetylcholine and GABA, were measured in the hippocampus and neocortex. Aged (27-28-month-old) female C57BL/6 mice were given daily subcutaneous injections of 1 microg or 5 microg of beta-estradiol-3-benzoate dissolved in sesame oil. Control mice received daily injections of sesame oil or no injections. Estradiol treatment began 5 days prior to behavioral testing and continued throughout testing. Spatial and non-spatial memory were assessed in the Morris water maze. The 5 microg dose of estradiol significantly improved spatial learning and memory in aged females. The performance of 5 microg females improved significantly more rapidly than that of control females; estradiol-treated females performed at asymptotic levels by session 2. Furthermore, 5 microg females exhibited a more robust spatial bias than controls during probe trials. In contrast, 1 microg of estradiol did not improve spatial task performance. Neither dose affected performance of the non-spatial task. In the hippocampus, synaptophysin was increased in 5 microg females relative to controls. Estrogen did not affect enzyme activities in either brain region. This study is the first to examine the effects of estrogen replacement on spatial reference memory and synaptophysin expression in aged post-estropausal female rodents. The results suggest that: (1) estrogen can profoundly improve spatial reference memory in aged females, and (2) this improvement may be related to increased hippocampal synaptic plasticity, but not modulation of the synthetic enzymes for acetylcholine and GABA.\n\n==\n\nSandstrom 2004:\n\nTitle: Spatial memory retention is enhanced by acute and continuous estradiol replacement\n\nAbstract: Estradiol replacement to ovariectomized female rats causes dramatic changes in hippocampal structure and function as well as in performance on hippocampally dependent tasks. Using a delayed matching-to-place version of the water maze, the present study examines the time course of estradiol-induced enhancements in memory retention as well as the effectiveness of acute and continuous patterns of replacement. One 10-microg injection of estradiol administered on each of two successive days resulted in significant improvements in memory retention that persisted for approximately 4 days following the second injection. When estradiol administration continued for 10 consecutive days, these improvements in memory retention persisted. These findings indicate that estradiol replacement can improve memory retention and that these improvements can be maintained by continuous replacement for at least 10 days.", "final_summary": "The effects of estrogen on spatial memory have been extensively studied, with a variety of findings reported. \n\nLuine (1998) found that chronic estrogen treatment enhanced spatial memory in ovariectomized rats, suggesting a positive effect of estrogen on spatial memory. Similarly, Harburger (2007) reported that estrogen improved spatial memory consolidation in aged female mice, and this effect could be attenuated by progesterone. Gibbs (1999) also found that estrogen replacement enhanced the acquisition of a spatial memory task and reduced performance deficits associated with hippocampal cholinergic impairment. \n\nHowever, not all studies found a positive effect of estrogen on spatial memory. Varga (2002) reported that estradiol at around the physiological level had no effect on spatial learning and memory functions. \n\nSome studies found that the effect of estrogen on spatial memory might depend on other factors. Rissman (2002) found that estrogen receptor \u03b2 is required for optimal spatial learning, suggesting that the effect of estrogen on spatial memory might be mediated by its receptors. Silverman (1993) reported that menstrual cycle phase affected spatial performance, with significant increases in mean mental rotations scores during the menstrual period phase, when estrogen levels were at their lowest. \n\nFrick (2002) found that estrogen replacement improved spatial reference memory and increased hippocampal synaptophysin in aged female mice, suggesting that estrogen might improve spatial memory by increasing hippocampal synaptic plasticity. Sandstrom (2004) reported that both acute and continuous estradiol replacement enhanced spatial memory retention.\n\nIn conclusion, while many studies suggest that estrogen has a positive effect on spatial memory, the effect might depend on other factors such as the presence of estrogen receptors, the phase of the menstrual cycle, and the pattern of estrogen replacement. Further research is needed to fully understand the complex relationship between estrogen and spatial memory."}, {"query": "The rise of AI-based decision-making tools in the criminal justice system: implications for judicial integrity", "paper_list_string": "Barabas 2019:\n\nTitle: Beyond Bias: Re-Imagining the Terms of \u2018Ethical AI\u2019 in Criminal Law\n\nAbstract: Data-driven decision-making regimes, often branded as \u201cartificial intelligence,\u201d are rapidly proliferating across the US criminal justice system as a means of predicting and managing the risk of crime and addressing accusations of discriminatory practices. These data regimes have come under increased scrutiny, as critics point out the myriad ways that they can reproduce or even amplify pre-existing biases in the criminal justice system. This essay examines contemporary debates regarding the use of \u201cartificial intelligence\u201d as a vehicle for criminal justice reform, by closely examining two general approaches to, what has been widely branded as, \u201calgorithmic fairness\u201d in criminal law: 1) the development of formal fairness criteria and accuracy measures that illustrate the trade-offs of different algorithmic interventions and 2) the development of \u201cbest practices\u201d and managerialist standards for maintaining a baseline of accuracy, transparency and validity in these systems. The essay argues that attempts to render AI-branded tools more accurate by addressing narrow notions of \u201cbias,\u201d miss the deeper methodological and epistemological issues regarding the fairness of these tools. The key question is whether predictive tools reflect and reinforce punitive practices that drive disparate outcomes, and how data regimes interact with the penal ideology to naturalize these practices. The article concludes by calling for an abolitionist understanding of the role and function of the carceral state, in order to fundamentally reformulate the questions we ask, the way we characterize existing data, and how we identify and fill gaps in existing data regimes of the carceral state.\n\n==\n\nHartmann 2021:\n\nTitle: Uncertainty, risk and the use of algorithms in policy decisions: a case study on criminal justice in the USA\n\nAbstract: Algorithms are increasingly used in different domains of public policy. They help humans to profile unemployed, support administrations to detect tax fraud and give recidivism risk scores that judges or criminal justice managers take into account when they make bail decisions. In recent years, critics have increasingly pointed to ethical challenges of these tools and emphasized problems of discrimination, opaqueness or accountability, and computer scientists have proposed technical solutions to these issues. In contrast to these important debates, the literature on how these tools are implemented in the actual everyday decision-making process has remained cursory. This is problematic because the consequences of ADM systems are at least as dependent on the implementation in an actual decision-making context as on their technical features. In this study, we show how the introduction of risk assessment tools in the criminal justice sector on the local level in the USA has deeply transformed the decision-making process. We argue that this is mainly due to the fact that the evidence generated by the algorithm introduces a notion of statistical prediction to a situation which was dominated by fundamental uncertainty about the outcome before. While this expectation is supported by the case study evidence, the possibility to shift blame to the algorithm does seem much less important to the criminal justice actors.\n\n==\n\nPutera 2022:\n\nTitle: Artificial Intelligence-Powered Criminal Sentencing in Malaysia: A conflict with the rule of law\n\nAbstract: Artificial Intelligence (AI) promises to heighten human decision-making, including in court. AI sentencing would be better at detecting, organizing, and calibrating all of the variables correlated to sentencing, such as prior criminal records, educational background, substance abuse history, and employment history, resulting in consistencies that traditional sentencing may not be able to provide. AI pervades the Malaysian judiciary system when AI criminal sentencing was launched for the first time to augment the process of meting out sentences in Sabah courts. Despite its promising benefits, AI sentencing may infringe the fundamental principle of due process, presents unacceptable risks of error and implicit bias, and reliance on AI to predict recidivism which forms significant components of the rule of law. The rule of law guarantees that all entities are subject to and accountable to a clear and known law. It enables the judicial branch of the government to be independent and to resolve dispute in a fair manner while upholding the presumption of innocence and preventing the exercise of arbitrary powers. The present research, therefore, examines the use of AI in supporting court processes and human judges, discovering its technical characteristics, practical constraints, and legal theoretical consequences for decision-making processes. Employing jurisprudential analysis as the method of research, this research explores an adjudicatory paradigm that prefers standardisation over discretion, leading in the waning of the notion of rule of law pertinent to the justice system. The metamorphosis to AI adjudication will undoubtedly promote the growth of digitalized dispute resolution by providing efficiency and at least a semblance of impartiality, but it is also poised to birth concerns by making the legal system data-driven, alienating, and disillusioning.\nKeywords: Artificial Intelligence, Artificial Intelligence in Courts, Artificial Intelligence in Criminal Justice\u00a0\neISSN: 2398-4287\u00a9 2022. The Authors. Published for AMER ABRA cE-Bs by e-International Publishing House, Ltd., UK. This is an open access article under the CC BYNC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Peer\u2013review under responsibility of AMER (Association of Malaysian Environment-Behaviour Researchers), ABRA (Association of Behavioural Researchers on Asians/Africans/Arabians) and cE-Bs (Centre for Environment-Behaviour Studies), Faculty of Architecture, Planning & Surveying, Universiti Teknologi MARA, Malaysia.\nDOI: https://doi.org/10.21834/ebpj.v7iSI7%20(Special%20Issue).3813\n\n==\n\nYeung 2023:\n\nTitle: How do \"technical\" design-choices made when building algorithmic decision-making tools for criminal justice authorities create constitutional dangers?\n\nAbstract: Automated, internet-enabled digital decision tools, particularly those that utilise some form of machine learning (\u2018ML\u2019), are widely touted as offering transformative \u2018solutions in government\u2019.1 Also called \u2018narrow\u2019 or \u2018taskspecific\u2019 artificial intelligence (\u2018AI\u2019), ML is a computational technique that uses algorithms to find patterns and correlations in large datasets, \u2018learning\u2019 from past experience to create a mathematical model that can generate predictions when applied to unseen data. These models may, in turn, be embedded into a digital tool to inform, or even to automate, organisational decisions. The variety of ML-based algorithmic systems now being rapidly taken-up the public sector within EU Member States is showcased in the \u2018AI Watch\u2019 EU Joint Research Centre\u2019s 2020 Report,2 with public sector adoption continuing apace.3 Examples range from the use of automated image recognition technologies to detect whether agricultural grasslands have been mowed in Estonia, to the automation of welfare benefit decisions in Sweden, and the delivery of personalised social services to the unemployed in Poland.4 British criminal justice authorities are also rapidly embracing these tools, incentivised by the government\u2019s Police Transformation Fund (and its predecessor) from which \u00a3720 million was allocated to projects aimed at transforming how police use technology between 2015-2020 alone.5 The Home Office claims these technologies will \u2018deliver cash savings, as well as improving efficiency by, for example, freeing\n\n==\n\nMcKay 2019:\n\nTitle: Predicting risk in criminal procedure: actuarial tools, algorithms, AI and judicial decision-making\n\nAbstract: ABSTRACT Risk assessments are conducted at a number of decision points in criminal procedure including in bail, sentencing and parole as well as in determining extended supervision and continuing detention orders of high-risk offenders. Such risk assessments have traditionally been the function of the human discretion and intuition of judicial officers, based on clinical assessments, framed by legislation and common-law principles, and encapsulating the concept of individualised justice. Yet, the progressive technologisation of criminal procedure is witnessing the incursion of statistical, data-driven evaluations of risk. Human judicial evaluative functions are increasingly complemented by a range of actuarial, algorithmic, machine learning and Artificial Intelligence (AI) tools that purport to provide accurate predictive capabilities and objective, consistent risk assessments. But ethical concerns have been raised globally regarding algorithms as proprietary products with in-built statistical bias as well as the diminution of judicial human evaluation in favour of the machine. This article focuses on risk assessment and what happens when decision-making is delegated to a predictive tool. Specifically, this article scrutinises the inscrutable proprietary nature of such risk tools and how that may render the calculation of the risk score opaque and unknowable to both the offender and the court.\n\n==\n\nChiao 2019:\n\nTitle: Fairness, accountability and transparency: notes on algorithmic decision-making in criminal justice\n\nAbstract: Abstract Over the last few years, legal scholars, policy-makers, activists and others have generated a vast and rapidly expanding literature concerning the ethical ramifications of using artificial intelligence, machine learning, big data and predictive software in criminal justice contexts. These concerns can be clustered under the headings of fairness, accountability and transparency. First, can we trust technology to be fair, especially given that the data on which the technology is based are biased in various ways? Second, whom can we blame if the technology goes wrong, as it inevitably will on occasion? Finally, does it matter if we do not know how an algorithm works or, relatedly, cannot understand how it reached its decision? I argue that, while these are serious concerns, they are not irresolvable. More importantly, the very same concerns of fairness, accountability and transparency apply, with even greater urgency, to existing modes of decision-making in criminal justice. The question, hence, is comparative: can algorithmic modes of decision-making improve upon the status quo in criminal justice? There is unlikely to be a categorical answer to this question, although there are some reasons for cautious optimism.\n\n==\n\nElyounes 2020:\n\nTitle: Bail or Jail? Judicial Versus Algorithmic Decision-Making in the Pretrial System\n\nAbstract: To date, there are approximately sixty risk assessment tools deployed in the criminal justice system. These tools aim to differentiate between low-, medium-, and high-risk defendants and to increase the likelihood that only those who pose a risk to public safety or who are likely to flee are detained. Proponents of actuarial tools claim that these tools are meant to eliminate human biases and to rationalize the decision-making process by summarizing all relevant information in a more efficient way than can the human brain. Opponents of such tools fear that in the name of science, actuarial tools reinforce human biases, harm defendants\u2019 rights, and increase racial disparities in the system. The gap between the two camps has widened in the last few years. Policymakers are torn between the promise of technology to contribute to a more just system and a growing movement that calls for the abolishment of the use of actuarial risk assessment tools in general and the use of machine learning-based tools in particular. \nThis paper examines the role that technology plays in this debate and examines whether deploying artificial intelligence (\u201cAI\u201d) in existing risk assessment tools realizes the fears emphasized by opponents of automation or improves our criminal justice system. It focuses on the pretrial stage and examines in depth the seven most commonly used tools. Five of these tools are based on traditional regression analysis, and two have a machine-learning component. This paper concludes that classifying pretrial risk assessment tools as AI-based tools creates the impression that sophisticated robots are taking over the courts and pushing judges from their jobs, but that impression is far from reality. Despite the hype, there are more similarities than differences between tools based on traditional regression analysis and tools based on machine learning. Robots have a long way to go before they can replace judges, and this paper does not argue for replacement. The long list of policy recommendations discussed in the last chapter highlights the extensive work that needs to be done to ensure that risk assessment tools are both accurate and fair toward all members of society. These recommendations apply regardless of whether machine learning or regression analysis is used. Special attention is paid to assessing how machine learning would impact those recommendations. For example, this paper argues that carefully detailing each of the factors used in the tools and including multiple options to choose from (i.e., not just binary \u201cyes-or-no\u201d questions) will be useful for both regression analysis and machine learning. However, machine learning would likely lead to more personalized and meaningful scoring of criminal defendants because of the ability of machine learning techniques to \u201czoom in\u201d on the unique details of each individual case.\n\n==\n\nSimmons 2018:\n\nTitle: Big Data, Machine Judges, and the Legitimacy of the Criminal Justice System\n\nAbstract: Predictive algorithms are rapidly spreading throughout the criminal justice system. They are used to more efficiently allocate police resources, identify potentially dangerous individuals, and advise judges at bail hearings and sentencing determinations. These algorithms have the potential to increase the accuracy, efficiency, and fairness of the criminal justice system, and they have been criticized on the grounds that they may reinforce pre-existing biases against minorities. But one aspect of these tools that has not yet been discussed in the literature is whether they will be accepted as legitimate. For centuries, these critical decisions that affect people\u2019s safety and liberty have been made by human beings; now, for the first time in human history, we are delegating large aspects of these decisions to machines. This article addresses whether people will be willing to accept this change, and if not, how we can adapt the algorithms in order to make them more acceptable. \nIn determining whether predictive algorithms are likely to be accepted by criminal defendants, the article draws on the field of procedural justice, which sets out numerous factors that determine whether a participant in a judicial proceeding believes that the process is fair. The article finds that predictive algorithms do not fare particularly well on these factors: they may not be seen as trustworthy or neutral, and they do not give defendants a significant opportunity to participate in the process. The article suggests that criminal defendants would be more likely to view predictive algorithms as legitimate if the algorithms were made more transparent, and if they were designed to ensure that they did not use data that was tainted by past discriminatory practices. \nThe article then examines whether the general population is likely to perceive predictive algorithms as legitimate. It examines various psychological barriers that people have with regard to accepting predictive algorithms. The article presents an original empirical study of six hundred individuals who were presented with a hypothetical case in which a judge uses a predictive algorithm to assist in a bail hearing. The study indicates that individuals are likely to accept predictive algorithms, as long as certain criteria are met.", "final_summary": "The advent of AI-based decision-making tools in the criminal justice system has ignited a complex debate about their implications for judicial integrity. The papers collectively provide a multifaceted perspective on this issue.\n\nBarabas (2019) contends that while AI tools are designed to mitigate bias and enhance accuracy, they may unintentionally perpetuate punitive practices and disparate outcomes. This viewpoint is mirrored by Putera (2022), who posits that AI sentencing, despite its potential advantages, may violate the principle of due process and the rule of law.\n\nHartmann (2021) delves into the transformation of decision-making processes in the criminal justice sector in the USA due to the introduction of risk assessment tools. Hartmann (2021) suggests that the use of AI tools has introduced statistical prediction into a previously uncertain situation, fundamentally altering decision-making. Yeung (2023), on the other hand, discusses the constitutional dangers created by technical design choices when building ML-based algorithmic systems, with a focus on their adoption by criminal justice authorities in the UK.\n\nMcKay (2019) and Chiao (2019) raise ethical concerns about the use of AI tools. McKay (2019) highlights the proprietary nature of these tools, which may render their risk calculations opaque. Chiao (2019) underscores the need for fairness, accountability, and transparency in algorithmic decision-making.\n\nElyounes (2020) and Simmons (2018) explore the potential acceptance of AI tools in the criminal justice system. Elyounes (2020) suggests that while robots cannot replace judges, AI tools can lead to more personalized and meaningful scoring of criminal defendants. Simmons (2018) presents an empirical study indicating that individuals are likely to accept predictive algorithms, provided certain criteria are met.\n\nIn conclusion, the rise of AI-based decision-making tools in the criminal justice system presents both opportunities and challenges. While these tools have the potential to improve efficiency and accuracy, concerns about fairness, transparency, and the reinforcement of existing biases persist. Further research and careful implementation are required to ensure these tools enhance, rather than compromise, judicial integrity."}, {"query": "AI as a teacher assistant", "paper_list_string": "Boulay 2016:\n\nTitle: Artificial Intelligence as an Effective Classroom Assistant\n\nAbstract: The field of artificial intelligence in education (AIED) uses techniques from AI and cognitive science to better understand the nature of learning and teaching and to build systems to help learners gain new skills or understand new concepts. This article studies metareviews and meta-analyses to make the case for blended learning, wherein the teacher can offload some work to AIED systems.\n\n==\n\nBoulay 2016:\n\nTitle: Artificial Intelligence as an Effective Classroom Assistant\n\nAbstract: The field of artificial intelligence in education (AIED) uses techniques from AI and cognitive science to better understand the nature of learning and teaching and to build systems to help learners gain new skills or understand new concepts. This article studies metareviews and meta-analyses to make the case for blended learning, wherein the teacher can offload some work to AIED systems.\n\n==\n\nKim 2020:\n\nTitle: My Teacher Is a Machine: Understanding Students\u2019 Perceptions of AI Teaching Assistants in Online Education\n\nAbstract: ABSTRACT An increase in demand for online education has led to the creation of a new technology, machine teachers, or artificial intelligence (AI) teaching assistants. In fact, AI teaching assistants have already been implemented in a small number of courses in the United States. However, little is known about how students will perceive AI teaching assistants. Thus, the present study investigated students\u2019 perceptions about AI teaching assistants in higher education by use of an online survey. Primary findings indicate that perceived usefulness of an AI teaching assistant and perceived ease of communication with an AI teaching assistant are key to understanding an eventual adoption of AI teaching assistant-based education. These findings provide support for AI teaching assistant adoption. Based on the present study\u2019s findings, more research is needed to better understand the nuances associated with the learning experience one may have from an AI teaching assistant.\n\n==\n\nLiu 2022:\n\nTitle: The application of artificial intelligence assistant to deep learning in teachers' teaching and students' learning processes\n\nAbstract: With the emergence of big data, cloud computing, and other technologies, artificial intelligence (AI) technology has set off a new wave in the field of education. The application of AI technology to deep learning in university teachers' teaching and students' learning processes is an innovative way to promote the quality of teaching and learning. This study proposed the deep learning-based assessment to measure whether students experienced an improvement in terms of their mastery of knowledge, development of abilities, and emotional experiences. It also used comparative analysis of pre-tests and post-tests through online questionnaires to test the results. The impact of technology on teachers' teaching and students' learning processes, identified the problems in the teaching and learning processes in the context of the application of AI technology, and proposed strategies for reforming and optimizing teaching and learning. It recommends the application of software and platforms, such as Waston and Knewton, under the orientation of AI technology to improve efficiency in teaching and learning, optimize course design, and engage students in deep learning. The contribution of this research is that the teaching and learning processes will be enhanced by the use of intelligent and efficient teaching models on the teachers' side and personalized and in-depth learning on the students' side. On the one hand, the findings are helpful for teachers to better grasp the actual conditions of in-class teaching in real time, carry out intelligent lesson preparations, enrich teaching methods, improve teaching efficiency, and achieve personalized and precision teaching. On the other hand, it also provides a space of intelligent support for students with different traits in terms of learning and effectively improves students' innovation ability, ultimately achieving the purpose of \u201cartificial intelligence + education.\u201d\n\n==\n\nZhang 2019:\n\nTitle: Development of an AI based teaching assisting system\n\nAbstract: Nowadays, people are facing the issues of declining birthrates and an aging society. Teaching resources are far from enough and the teachers usually have great amount of works, especially in countryside or small towns. Aim to reduce the work of teachers, we developed a teaching assisting system to automatically track and recognize the motions and behaviors of students based on artificial intelligence (AI) technology. The humans are detected accurately by combing the detection results of OpenPose and human area projection method. Each person is identified by fusing his/her personal information, including the features of color, face and moving history, and tracked by using an extended particle filter method based on Markov Chain Monte Carlo (MCMC). By observing the students' behaviors without getting tired with the sensor system instead of the teachers, the performances and growing processes of all the students can be analyzed, referring the excellent knowledge and experiences of professional teachers. This information can be provided to the teacher which can help the teachers to adjust guidance to the students. Moreover, the information can also be provided to education robot so that the robot can also held a class or have interactions with the students.\n\n==\n\nKim 2019:\n\nTitle: Assisting Teachers with Artificial Intelligence: Investigating the Role of Teachers Using a Randomized Field Experiment\n\nAbstract: This study investigates whether artificial intelligence (AI) can transform the teacher\u2019s role by delivering personalized learning to each individual student. Conducting a randomized controlled trial in collaboration with an education company, we evaluate how providing teachers with AI assistance impacts students\u2019 academic outcomes. We find that providing AI-generated reports to teachers significantly improves students\u2019 study effort and performance, although those effects vary by teacher and class characteristics. Consistent with the organizational literature on technology and worker productivity, our findings indicate that technology overload could undermine teachers\u2019 effective use of AI coaching despite its highly accurate diagnostic ability and ready availability. Although some teachers did not utilize the AI coaching program, we found positive effects among the students of both compliers and non-compliers. We, therefore, decompose our results to identify direct and spillover effects on students\u2019 academic performance and show that failing to account for spillover effects across teachers (externality among peers) and within teachers (learning-by-doing across students) may understate the effects of AI coaching. Finally, we provide practical guidelines for implementing technology in educational settings.\n\n==\n\nEdwards 2018:\n\nTitle: I, teacher: using artificial intelligence (AI) and social robots in communication and instruction*\n\nAbstract: ABSTRACT Human\u2013machine communication has emerged as a new relational context of education and should become a priority for instructional scholarship in the coming years. With artificial intelligence and robots offering personalized instruction, teachers\u2019 roles may shift toward overseers who design and select machine-led instruction, monitor student progress, and provide support. In this essay, we argue that bringing the sensibilities of instructional researchers to bear on these issues involving machine agents, within and outside the traditional classroom walls, is vitally important.\n\n==\n\nal-Zyoud 2020:\n\nTitle: The Role of Artificial Intelligence in Teacher Professional Development\n\nAbstract: This paper aims to present the idea, development, and field of artificial intelligence (AI), considering its impact on teachers' professional development. In addition to the technical applications that support AI in the educational sector, and the problems facing its uses, the importance of artificial intelligence in the education field has been addressed and previous study cases that competed AI with commentary have been identified. This study uses an analytical approach based on a theoretical survey of previous literature to answer the study questions. The results refer to a set of intelligent applications as well as areas of artificial intelligence that can serve the professional development of teachers. Furthermore, we recommend to: Design educational software based on artificial intelligence to raise the qualifications of teachers, Build training pathways based on artificial intelligence for all those working in the field of education, Provide accurate databases in all areas' education including human resources, Provide educational training environments that add to improving the part of artificial intelligence applications in the professional development of teachers, and Link databases within a global network that helps challenge teachers' career paths.", "final_summary": "Artificial Intelligence (AI) is making significant strides in the educational sector, particularly as a teaching assistant. The papers collectively suggest that AI can be an effective tool in the classroom, aiding teachers and enhancing the learning experience for students.\n\nBoulay (2016) advocates for a blended learning approach, where teachers can delegate some tasks to AI systems. This approach allows for a more efficient use of resources and can potentially enhance the learning process. Similarly, Liu (2022) suggests that AI can improve teaching efficiency and promote personalized and in-depth learning. The use of AI technology in deep learning can help teachers better understand their students' learning processes and adapt their teaching methods accordingly.\n\nKim (2020) delves into students' perceptions of AI teaching assistants, finding that students' perceived usefulness and ease of communication with an AI teaching assistant are key factors in the adoption of AI-based education. On the other hand, Kim (2019) investigates the impact of providing teachers with AI assistance on students' academic outcomes, finding that AI-generated reports to teachers significantly improve students' study effort and performance. However, the study also warns of technology overload, suggesting that the effective use of AI in teaching requires careful implementation.\n\nZhang (2019) and Edwards (2018) both discuss the potential of AI in tracking and analyzing student behavior. Zhang (2019) developed a system that can automatically track and recognize students' motions and behaviors, providing valuable insights for teachers. Edwards (2018) suggests that AI and social robots can offer personalized instruction, shifting teachers' roles towards overseers who design and select machine-led instruction.\n\nLastly, al-Zyoud (2020) emphasizes the role of AI in teacher professional development. The paper suggests that AI can help improve teachers' qualifications and provide training pathways for those working in the field of education.\n\nIn conclusion, AI has the potential to revolutionize the role of a teacher's assistant, offering personalized learning experiences, improving teaching efficiency, and aiding in teacher professional development. However, careful implementation is necessary to avoid technology overload and ensure the effective use of AI in the classroom."}, {"query": "write an essay on how general knowledge and awareness correlate with perceived behavioural control in relation to consuming food products especially unfamiliar low trophic level aquaculture products.", "paper_list_string": "Pieniak 2013:\n\nTitle: Consumer knowledge and use of information about fish and aquaculture\n\nAbstract: This paper explores consumers\u2019 knowledge about fish and aquaculture and assesses the use and importance of different information cues about fish. Cross-sectional data were collected in 2008 through a consumer survey (n=3213) in the Czech Republic, Germany, Greece, Italy, Portugal, Romania, Sweden and the UK. Consumers\u2019 knowledge about fish generally, and about aquaculture in particular, was relatively low and differed significantly between countries. Consumers from all countries reported an indication of quality and/or food safety as an information cue when buying fish. The information sources most frequently used by Europeans were labelling and sellers in retail or supermarkets. The Internet was identified by consumers in all of the countries as one of the most important sources of information about sea and freshwater fish products. Policy makers and food marketers are encouraged to develop a simple and easily recognisable mark (relating to quality, food safety and nutrition) to assist consumer decision-making. Information campaigns focusing on issues such as the nutritional benefits of eating fish are also recommended.\n\n==\n\nBanovi\u0107 2019:\n\nTitle: \u201cOne Fish, Two Fish, Red Fish, Blue Fish\u201d: How ethical beliefs influence consumer perceptions of \u201cblue\u201d aquaculture products?\n\nAbstract: Respecting ethical beliefs of consumers is an important precondition for food manufacturers in their attempt to improve their positioning in the European food market. Based on a cross-cultural survey of 2511 European participants, this research demonstrates how ethical beliefs affect consumer perceptions of \u201cblue\u201d (i.e. environmentally friendly) aquaculture products. The study further emphasises that the positive effect of ethical beliefs on purchase intention operates via an indirect route mediated by consumers\u2019 trust in a product category. Consumer involvement has limited moderation effect on the above relationships. To expand its \u201cblue\u201d business, a key policy recommendation to aquaculture product manufacturers and policy makers is to urge stable and reliable standards of control in environmentally responsible aquaculture production so that consumers can rely on the information source and increase their trust in aquaculture products.\n\n==\n\nVerbeke 2005:\n\nTitle: Individual determinants of fish consumption: application of the theory of planned behaviour\n\nAbstract: This study investigates individual determinants of fish consumption behaviour based on cross-sectional data collected in Belgium. Analyses show that determinants as hypothesised by the theory of planned behaviour (TPB) and personal characteristics influence fish consumption intention and frequency. Favourable attitude, high subjective norm and high perceived behavioural control have a positive impact on fish consumption decisions. Significant habit effects are detected when including habit as a separate regressor of behavioural intention and behaviour. Appreciation of the attribute taste emerges as the most important driver for eating fish, followed closely by health. Bones and price constitute the negative attitude factor, which, however, does not directly reduce behavioural intention. Individual determinants pertain to gender, age, children, income, education level and region. Fish consumption frequency in compliance with health recommendations is higher among women and increases with increasing age, while the presence of children in the household leads to lower fish consumption. The lowest income class has the lowest fish consumption frequency. Higher education results in a higher intention to eat fish but has no effect on the consumption frequency itself. The coastal region of West Flanders is the region with the highest consumption. Food involvement correlates positively with fish consumption intention and frequency, whereas no significant impact of food-health awareness is found.\n\n==\n\nAitken 2020:\n\nTitle: The positive role of labelling on consumers\u2019 perceived behavioural control and intention to purchase organic food\n\nAbstract: Abstract The consumption of organic food has increased dramatically in recent years in response to consumers\u2019 concerns with issues related to health, well-being and the environment. However, further increases are restricted by a number of barriers, one of the most important of which is information. Using the Reasoned Action Approach (RAA), this study is the first to examine the role of product specific information (labelling) to understand the gap between consumer attitude and behavioural intention to purchase organic food products. Based on responses from 1,052 New Zealand consumers, analysis using structural equation modelling demonstrates that labelling plays an important role within the perceived behavioural control dimension of the RAA, as well as directly on attitudes, in influencing behavioural intention. The more that respondents agree that labelling is actionable, the more positive their attitude and sense of control, and hence intention and (self-reported) behaviour. These findings suggest that by improving labelling systems to include more actionable information, such as the health, environmental and societal benefits of products, consumers perceived behavioural control can be increased to strengthen intentions to purchase organic products.\n\n==\n\nGempesaw 1995:\n\nTitle: Consumer Perceptions of Aquaculture Products\n\nAbstract: The consumption of seafood products, including aquacultural products, significantly increased during the 1970s and 1980s. However, during the late 1980s and early 1990s, per capita seafood consumption declined. The U.S. Department of Agriculture reports that current per capita consumption of seafood products is estimated at around 14.9 pounds, down from a peak of 16.1 pounds in 1987 (USDA). Uncertainty in the seafood supply has contributed to retail seafood prices rising faster than prices for other meat products (Harvey). To reduce the fluctuation in retail seafood prices, there is a need to stabilize production and offer consistency in supplies. However, consistency in the seafood supply can be expected to come primarily from aquaculture because of the problems of overutilization of natural or wild stocks (United Nations). Proponents of aquaculture are advocating its commercial expansion. However, several problems must be resolved before aquaculture's potential can be achieved. The first problem lies in the biological potential of a species to survive in an aquaculture environment. The second factor deals with the consumer's willingness to buy aquaculture products. The stiff competition offered by the beef, pork, poultry, and wild fisheries industries requires the aquaculture industry to use effective marketing strategies. This study focuses on the second issue raised regar ing the willingness of consumers to purchase products obtained from either the wild fishery or aquaculture. In particular, it deals with consumer preferences for fresh finfish and shellfish products purchased for home preparation and consumption in northeastern and MidAtlantic households.\n\n==\n\nSparks 1992:\n\nTitle: An investigation into the relationship between perceived control, attitude variability and the consumption of two common foods\n\nAbstract: The study reported here takes its lead from the literatures which emphasize the importance of attitude variability and the role of perceived control over action. Within-person variability and perceptions of control are investigated in the context of people's attitudes towards the consumption of two common foods. The role of attitude ambivalence is also examined. The findings indicate that higher attitude variability is associated with weaker relationships between the components of the theory of reasoned action and that attitude variability is negatively related to perceived control. Moreover, perceived control is shown to be related to different sorts of control problem for different behaviours. It is advocated that a more in-depth assessment of attitude variability and the perceived control construct is merited and that recent calls for more serious examination of attitude ambivalence are well-founded.\n\n==\n\nHasan 2020:\n\nTitle: The Influence of Attitude, Subjective Norm and Perceived Behavioral Control towards Organic Food Purchase Intention\n\nAbstract: This study aims to determine the determinants of intention in buying organic food. This thesis focuses on the consumer of organic food products in Indonesia who use online media in Instagram to make their organic food purchases with Theory of Planned Behavior. The data collection in this study was conducted in two stages, namely pre-survey to analyze opinion leaders and types of organic foods that will be used as a limitation of the study and the second stage is the research questionnaire. The analysis technique used is qualitative data analysis. Based on the results of the study conclusions can be drawn as follows:\u00a0 adherence to organic food and perceived behavioral control significantly influence buying intention in organic food, while subjective norms are found to have no significant effect on buying intention in organic food. Attitudes toward organic food have the strongest influence among the other two variables. Subjective norms have the lowest influence. Subjective norms or pressure from other people to consume organic food apparently do not have a significant effect because it does not originate from oneself (external) or is not their own volition, in contrast to the attitude and perceived behavior control variables that originate from oneself.\n\n==\n\nAltintzoglou 2016:\n\nTitle: Fish Quality and Consumers: How Do Consumers\u2019 Knowledge About and Involvement in Fish Quality Define Factors That Influence Fish Buying Behavior?\n\nAbstract: ABSTRACT Despite recommendations, fish consumption is low in several countries. One of the many relevant factors that influence fish consumption is the quality of fish products. However, consumers differ in involvement in and knowledge about fish quality. The aim of this study was to reveal differences in involvement and knowledge about fish quality among consumers and measure how these lead to variations in the importance of factors that influence consumers when buying fresh fish fillets. We aimed to explain these results based on subjective and objective knowledge, fish consumption, and social and demographic characteristics. This study was based on data collected from 738 Norwegian consumers who responded to a questionnaire. The survey included items regarding knowledge and involvement in fish quality, objective and subjective knowledge measures, factors important when buying fish, and finally sociodemographic characteristics. This article suggests that perceived quality is one of the most important factors that influence buying behavior when it comes to fish fillet products in Norway. Consumers can have a high or low involvement with and knowledge about fish quality. However, when confronted with the decision of what fish product they may buy, most consumers reported using fish quality as a main driver.", "final_summary": "The relationship between general knowledge, awareness, and perceived behavioural control in relation to consuming food products, particularly unfamiliar low trophic level aquaculture products, is a complex interplay. \n\nPieniak (2013) found that consumers' knowledge about fish and aquaculture was relatively low, with quality and food safety being the primary concerns. Similarly, Gempesaw (1995) highlighted the importance of consumers' willingness to purchase aquaculture products, suggesting that effective marketing strategies are crucial. \n\nBanovi\u0107 (2019) emphasized the role of ethical beliefs in shaping consumer perceptions of environmentally friendly aquaculture products. The study suggested that trust in a product category mediates the positive effect of ethical beliefs on purchase intention. \n\nVerbeke (2005) applied the theory of planned behaviour to fish consumption, finding that favourable attitudes, high subjective norms, and high perceived behavioural control positively impact fish consumption decisions. \n\nAitken (2020) demonstrated that labelling plays a significant role in influencing consumers' perceived behavioural control and intention to purchase organic food. The study suggested that improved labelling systems could increase consumers' perceived behavioural control and strengthen their intentions to purchase organic products. \n\nSparks (1992) found that higher attitude variability is associated with weaker relationships between the components of the theory of reasoned action and that attitude variability is negatively related to perceived control. \n\nHasan (2020) found that adherence to organic food and perceived behavioural control significantly influence buying intention in organic food, while subjective norms have no significant effect. \n\nAltintzoglou (2016) suggested that perceived quality is one of the most important factors influencing buying behaviour when it comes to fish fillet products in Norway. \n\nIn conclusion, general knowledge and awareness significantly influence perceived behavioural control and intention to consume unfamiliar low trophic level aquaculture products. Factors such as ethical beliefs, labelling, and perceived quality also play crucial roles. However, there is a need for more effective marketing strategies and improved labelling systems to increase consumers' knowledge and awareness, thereby enhancing their perceived behavioural control and intention to consume these products."}, {"query": "are questionaire the common method in social science?", "paper_list_string": "Yufang 1999:\n\nTitle: Some Considerations of Questionaire Method in Education from Angle of Measurement\n\nAbstract: Educational Questionaire Method is a method in common use.In the process of use,however,there are some problems existing in the method,which affect the scientific nature of it.This essay maintains that it will help to improve the scientific nature of questionaire method if we pay attention to the theoretical conception in its designing,make the process of quantification tally with measuring principles,and make textual research to the reliability and validity of the questionaires.\n\n==\n\nTan\u010di\u0107 2019:\n\nTitle: The application of survey questionnaire in the methodology of social sciences in the example of Kosovo and Metohija\n\nAbstract: In the methodology of social and political sciences it is generally known that without survey researches and survey questionnaires it is possible neither to conduct a lot of researches nor to reach new scientific knowledge. That was determined from one hand, by the scientific field, and from another one, by the theme, project task, and conceptual sketch, draft of the scientific idea and relation of conceptualisation toward the pre-research and toward the interpretation and practicing of the research results. The survey is the technique of the inspection method, and survey questionnaire an instrument so that their application requests a lot of scientific and professional activities starting from the planning of survey inspections until the making of survey questionnaire, formulation and forms of questions etc. In scientific fund there are \"two general approaches of the comprehension of survey. From one hand, there are comprehensions which consider the survey as a scientific method... and from another one, there are comprehensions which consider survey as only one of techniques of scientific method of inspection\" (Tancic, Tancic 2019: 185).\n\n==\n\nManabe 2012:\n\nTitle: Social Research and Sociological Theory : Toward an Innovation of Social Analysis through the Questionnaire Method (\u9ad9\u5742\u5065\u6b21\u6559\u6388\u9000\u8077\u8a18\u5ff5\u53f7)\n\nAbstract: With social research positioned as a technique for building sociological theories, the crux of the problem being discussed here becomes clear. (1) It is the problem of an imbalance between public opinion polls and survey research. On one hand, this is due to the prevalence of public opinion polls, and on the other, the stagnation of survey re- search. This is not a healthy situation for building sociological theories. Here, I distin- guish the former as a technique that was developed based on social needs, focuses on social issues, and is oriented toward the description of people\u2019s subjective conscious- ness, as opposed to the latter, which is a technique that was developed based on aca- demic needs, to address a variety of aspects of social living, and is oriented toward the analysis of people\u2019s subjective consciousness. This paper looks at the commonalities between the two, referring to them both as the \u201cquestionnaire method.\u201d (2) There is a problem involving the heavy use, perhaps overuse, of public opinion polls and survey research, namely, the questionnaire method. Among the social phenomena that exist, there are some that can be approached using the questionnaire method and some that cannot. The current problem is that the questionnaire method is being used with too lit- tle regard to whether it is appropriate for a specific research question. (3) It is almost as if the researcher starts with the method, and then uses that method regardless of the nature of the research. Originally, a study began with observations of a subject; meth- ods for approaching that subject were then developed based on those observations. However, in the case of the questionnaire method, this American-made technique was already available, where the research needed to be conducted before observations of the various aspects of people\u2019s subjective consciousness in various societies were made. In Germany, the German expression \u201cLehne ein bisschen ab\u201d was artificially created as a survey term meant to correspond to the English phrase \u201cDisagree a little.\u201d This is a typical example of this problem. Looking at these problems, one might begin to think that the questionnaire method should be methodologically rejected, but that is not at all the case. As is the case with many kinds of tools, no method is perfect or entirely without fault. Tools must be used appropriately for their particular purposes. This is where the exploration of the poten- tial uses of the questionnaire method begins. This is an effort to \u201cconfirm the dimen- sions\u201d for the description, classification and measurement of a specific social phenome- non, namely subjective reality (people\u2019s subjective consciousness).\n\n==\n\nRobinson 2002:\n\nTitle: Ethics of questionnaires \u2014 again\n\nAbstract: In the 6 years I have been writing this column, one piece stands out as having had the greatest response. It whizzed round university campuses like wildfire. It described simple and obvious concerns about ethics in social science research (Robinson, 1996). Apparently, it came as news to many that asking questions was not necessarily harmless. Everyone, it seemed, was concocting their own little questionnaire, and because they weren\u2019t taking blood or biopsies, assumed there could be no ethical problems. Those who applied for approval to any of the three research committees I sat on soon learned otherwise\u2026 I was simply insisting on the same standards of quality and ethics for social science as for clinical research.\n\n==\n\nSimion 2012:\n\nTitle: THEORETICAL AND APPLIED ASPECTS OF EVALUATION THROUGH STATISTICAL QUESTIONAIRE\n\nAbstract: This paper contains a review of the practice and theory of statistical questionnaires. It points out the increasing distance between the theory of selective research and the practice of using surveys. This distance is being amplified nowadays by the economical and social changes in general and those of the academic environment in particular. In addition, the paper points out the danger of diminishing the scientific reputation of these statistical methods and tools through the excessive contact of individuals with different types of questionnaires suggests that the usage of these tools should be adapted correctly by considering the peculiarities of all the field.\n\n==\n\nSmall 1907:\n\nTitle: Are the Social Sciences Answerable to Common Principles of Method? II\n\nAbstract: Having replied in the first part of this paper' to certain specific criticisms in Dr. Hoxie's \"Rejoinder,\" 2 I shall now attempt to express more positively two or three rather elementary methodological principles. I am not yet sure whether there is a real difference of judgment about them between Dr. Hoxie and myself, or whether the argument amounts only to an incident in the race-hatreds between vocabularies. In either case, no better way of approaching an understanding is in sight than the frankest possible use of the words at command on both sides. In reviewing the scope of this discussion I am impressed with the necessity of appeal to primary principles which the social sciences can hardly repudiate. As I see the situation, the occasion for such a discussion as this arises from the degree of inattention to formulation of these principles which has become habitual. It is hard to believe that responsible scholars would deliberately deny their substantial validity. I have the best of reasons for believing that Dr. Hoxie's own practice, for example, is a salutary object-lesson in consistent respect for the very principles about which we find ourselves disagreeing in the abstract. As I said in the first part of this paper, we seem to be unable to use words in a way that makes us sure of each other's ideas. The removal of the misunderstanding, I repeat, is probably not feasible through a process of logical proof. It must be chiefly through increased attention to conditions or elements of the scientific process which are out of sight when we\n\n==\n\nCoccia 2018:\n\nTitle: An Introduction to the Methods of Inquiry in Social Sciences\n\nAbstract: Abstract. The purpose of this paper is an introduction to methods for conducting inquiries in social sciences. A method of inquiry is an organized and systematic scientific approaches used by scholars for controlled investigations and experiments to efficiently solve theoretical and practical problems, generating discoveries and/or science advances. The paper here presents, briefly, the development of models of inquiry in the philosophy of science. After that, it introduces general methods of inquiry (deduction, induction, abduction and hypothetical-deductive approaches) and specific models of scientific inquiry in social sciences, such as multiple working hypotheses. In general, modern scientific research is multifaceted and requires different approaches for generating new concepts, new hypothesis and theories. Different methods of inquiry directed to solve problems in science can be complementary approaches that foster knowledge creation within and between research fields. Keywords. Methods of inquiry, Philosophy of science, Models of inquiry, Scientific research, Conduct of inquiry. JEL. A20, A29, B40, B49.\n\n==\n\nGorard 2003:\n\nTitle: Quantitative methods in social science\n\nAbstract: Introduction - the role of numbers in research finding secondary data - the \"idle\" researcher simple analysis - the index wars and other battles sampling: the basis of all research surveying the field - questionnaire design simple non-parametric statistics - minding your table manners research claims - modelling the social world experimental approaches - a return to the gold standard? elementary parametric statistics - what do they signify? progress via regression - introducing correlations combining approaches - a \"compleat\" researcher.", "final_summary": "Questionnaires are a widely recognized tool in social science research. Yufang (1999) acknowledges their common use in education, but also highlights the need for careful design and validation to ensure their scientific validity. This sentiment is echoed by Tan\u010di\u0107 (2019), who asserts that survey questionnaires are indispensable in social and political sciences, enabling the acquisition of new scientific knowledge. \n\nManabe (2012) further emphasizes the importance of questionnaires in social research, distinguishing it as a technique developed based on academic needs and oriented towards the analysis of people's subjective consciousness. However, Robinson (2002) cautions about the ethical considerations in using questionnaires, arguing that their use is not necessarily harmless and should adhere to the same standards of quality and ethics as clinical research.\n\nSimion (2012) provides a critical perspective, pointing out the increasing gap between the theory of selective research and the practice of using surveys. He warns of the potential danger of diminishing the scientific reputation of these statistical methods due to their excessive use. \n\nGorard (2003) discusses quantitative methods in social science, including questionnaire design, as part of a comprehensive approach to research. \n\nIn conclusion, while questionnaires are widely recognized as a common and valuable tool in social science research, these studies collectively highlight the need for careful design, ethical considerations, and methodological rigor in their use (Yufang, 1999; Tan\u010di\u0107, 2019; Manabe, 2012; Robinson, 2002; Simion, 2012; Gorard, 2003)."}, {"query": "what is the relation between cervical disc pathology and cervical posture?", "paper_list_string": "C\u00e2mara-Souza 2018:\n\nTitle: Cervical posture analysis in dental students and its correlation with temporomandibular disorder\n\nAbstract: Abstract Objective: To evaluate the relationship between temporomandibular disorders (TMD) and craniocervical posture in the sagittal plane measured from lateral radiographs of the head. Methods: The sample was comprised of 80 randomly selected students of dentistry at the Federal University of Rio Grande do Norte. Research Diagnostic Criteria for TMD (RDC/TMD) was used to evaluate the signs and symptoms of TMD. Lateral radiographs of each individual were used to measure the position of the hyoid bone, the craniocervical angle, and the occiput\u2013atlas distance. A chi-square test was used to evaluate the relationships between craniocervical posture measures and TMD. Results: No relationship was found between TMD and the craniocervical posture measured by the positioning of the hyoid bone, head rotation, and the extension/flexion of the head (p > 0.05). Conclusion: It can be concluded, therefore, that no relationship exists between cervical posture in the sagittal plane and TMD.\n\n==\n\nLaat 1998:\n\nTitle: Correlation between cervical spine and temporomandibular disorders\n\nAbstract: Abstract Neuroanatomical interconnections and neurophysiological relationships between the orofacial area and the cervical spine have been documented earlier. The present single-blind study was aimed at screening possible correlations between clinical signs of temporomandibular disorders (TMD) and cervical spine disorders. Thirty-one consecutive patients with symptoms of TMD and 30 controls underwent a standardised clinical examination of the masticatory system, evaluating range of motion of the mandible, temporomandibular joint (TMJ) function and pain of the TMJ and masticatory muscles. Afterwards subjects were referred for clinical examination of the cervical spine, evaluating segmental limitations, tender points upon palpation of the muscles, hyperalgesia and hypermobility. The results indicated that segmental limitations (especially at the C0\u2013C3 levels) and tender points (especially in the m. sternocleidomastoideus and m. trapezius) are significantly more present in patients than in controls. Hyperalgesia was present only in the patient group (12\u201316%).\n\n==\n\nYoo 2009:\n\nTitle: The relationship between the active cervical range of motion and changes in head and neck posture after continuous VDT work.\n\nAbstract: This study investigated the relationship between the active cervical range of motion (ROM) and changes in the head and neck posture after continuous visual display terminal (VDT) work. Twenty VDT workers were recruited from laboratories. The active cervical ROM of the participants was measured and videotaped to capture the craniocervical and cervicothoracic angles using a single video camera before and after VDT work. Pearson correlation coefficients were used to quantify the linear relationship between active cervical ROM measurements and the changes in the craniocervical and cervicothoracic angles after continuous VDT work. Active neck extension (r=-0.84, p<0.01) was negatively correlated with the mean craniocervical angle, and active neck flexion (r=-0.82, p<0.01) and left lateral flexion (r=-0.67, p<0.01) were negatively correlated with the mean cervicothoracic angle.\n\n==\n\nSonnesen 2007:\n\nTitle: Cervical column morphology related to head posture, cranial base angle, and condylar malformation.\n\nAbstract: The present study describes the cervical column as related to head posture, cranial base, and mandibular condylar hypoplasia. Two groups were included in the study. The 'normal' sample comprised 21 subjects, 15 females aged 23-40 years (mean 29.2 years), and six males aged 25-44 years (mean 32.8 years) with neutral occlusion and normal craniofacial morphology. The condylar hypoplasia group comprised the lateral profile radiographs of 11 patients, eight females, and three males, aged 12-38 years (mean 21.6 years). For each individual, a profile radiograph was taken to perform a visual assessment of the morphology of the cervical column. For the normal group only, the profile radiographs were taken in the standardized head posture to measure the head posture and the cranial base angle. Cervical column: Morphological deviations of the cervical column occurred significantly more often in the subjects with condylar hypoplasia compared with the normal group (P < 0.05 and P < 0.01, respectively). The pattern of morphological deviations was significantly more severe in the subjects with condylar hypoplasia compared with the normal group (P < 0.01). Cervical column related to head posture and cranial base: The cervicohorizontal and cranial base angles were statistically larger in females than in males (P < 0.05 and P < 0.01, respectively). No statistically significant age differences were found. Only in females was the cervical lordosis angle (OPT/CVT, P < 0.01), the inclination of the upper cervical spine (OPT/HOR, P < 0.05), and the cranial base angle (n-s-ba, P < 0.05) significantly positively correlated with fusion of the cervical column. These associations were not due to the effect of age.\n\n==\n\nTonetti 2004:\n\nTitle: Morphological cervical disc analysis applied to traumatic and degenerative lesions\n\nAbstract: Trauma and degenerative pathologies at the lower cervical spine are different from lumbar spine pathologies. However, the description of cervical discs is classically taught similarly to that of the lumbar discs. Recent studies have raised this issue, and in 1999, Mercer and Bogduk described ventral annulus fibrosus as a crescent-shaped interosseous ligament. We propose a metric analysis of the different components of the cervical disc to examine this description. We analyzed 140 sagittal and coronal transections of 35 discs. These discs were taken from seven cervical spines at the five lower levels, C2-C3, C3-C4, C4-C5, C5-C6 and C6-C7. We measured quantitative parameters on sagittal, para-sagittal, ventral coronal and dorsal coronal colored transections: disc length (L), ventral annulus thickness (VAF), lateral annulus thickness (LAF), dorsal annulus thickness (DAF), length of the fibrocartilaginous tissue (FC), sagittal and coronal fibrocartilaginous core ratio (% Core) and intra-disc cleft length (Cleft). We also measured two qualitative parameters: degenerative disease of cartilaginous end plates and total intra-disc cleft. Finally, we examined 114 transections, and 18.5% were ruled out. The results showed thick ventral annulus fibrosus, thin lateral annulus and a very thin dorsal annulus. Fibrocartilaginous tissue filled the dorsal sagittal half of the disc. Intra-disc cleft split the fibrocartilaginous tissue and spread through the ventral annulus only six times. The shape of the ventral annulus at the lower cervical spine is compared to a pivot-hinge device. The aspect is functionally discussed in regard to teardrop fractures, unilateral locked facet syndrome and degenerative changes in the unco-vertebral area.\n\n==\n\nRao 2002:\n\nTitle: Neck pain, cervical radiculopathy, and cervical myelopathy: pathophysiology, natural history, and clinical evaluation.\n\nAbstract: Degenerative cervical disk disease is a ubiquitous condition that is, for the most part, asymptomatic. When symptoms do arise as a result of these degenerative changes, they can be easily grouped into axial pain, radiculopathy and myelopathy. While the pathophysiology of radiculopathy and myelopathy is better understood, the source of neck pain remains somewhat controversial. A discussion of the mechanisms of neck and suboccipital pain, and the chemical and mechanical factors responsible for neurologic symptoms is warranted. Examination of the patient with these symptoms will reveal variations in the clinical presentation. A thorough understanding of the natural history of these conditions will allow appropriate treatment to be carried out. The natural history of these conditions suggests that for the most part patients with axial symptoms are best treated without surgery, while some patients with radiculopathy will continue to be disabled by their pain, and may be candidates for surgery. Myelopathic patients are unlikely to show significant improvement, and in most cases will show stepwise deterioration. Surgical decompression and stabilization should be considered in these patients.\n\n==\n\nNakashima 2015:\n\nTitle: Cervical Disc Protrusion Correlates With the Severity of Cervical Disc Degeneration: A Cross-Sectional Study of 1211 Relatively Healthy Volunteers\n\nAbstract: Study Design. Cross-sectional study. Objective. The purposes of this study were (1) to investigate the frequency and degree of cervical disc degeneration and protrusion on cervical spine magnetic resonance (MR) images and (2) to analyze the correlation between the severity of disc degeneration and disc protrusion. Summary of Background Data. Cervical disc degenerative changes or protrusion is commonly observed on MR images in healthy subjects. However, there are few large-scale studies, and the frequency and range of these findings in healthy subjects have not been clarified. Moreover, there are no reports regarding the correlation between cervical disc degeneration and disc protrusion. Methods. Cervical disc degeneration and protrusion were prospectively measured using magnetic resonance imaging in 1211 relatively healthy volunteers. These included at least 100 males and 100 females in each decade of life between the 20s and the 70s. Cervical disc degeneration was defined according to the modified Pfirrmann classification system, and the amount of disc protrusion was evaluated using the anteroposterior diameter of disc protrusion on sagittal MR image. Results. Mild disc degeneration was very common, including 98.0% of both sexes in their 20s. The severity of cervical disc degeneration significantly increased with age in both sexes at every level. The disc degeneration predominantly occurred at C5\u2013C6 and C6\u2013C7. The difference between sexes was not significant except for individuals in their 50s. The average anteroposterior diameter of disc protrusion increased with aging, especially from the 20s to the 40s. The anteroposterior diameter of disc protrusion increased with a progression in the disc degeneration grade. Conclusion. Cervical disc degeneration and protrusion were frequently observed in healthy subjects even in their 20s and deteriorated with age. Cervical disc protrusion was significantly correlated with cervical disc degeneration, and spatial cervical disc protrusion was affected by biochemical degenerative changes as observed on MR images. Level of Evidence: 2\n\n==\n\nLiu 2015:\n\nTitle: Are the standard parameters of cervical spine alignment and range of motion related to age, sex, and cervical disc degeneration?\n\nAbstract: OBJECT\nThe aims of this study were 1) to establish the standard parameters of alignment and total and segmental range of motion (ROM) of the cervical spine in the asymptomatic population, and 2) to identify factors that influence cervical ROM and alignment.\n\n\nMETHODS\nThe authors measured 636 standard cervical lateral, flexion, and extension plain radiographs of 212 asymptomatic volunteers. The relationship between cervical alignment and total ROM was assessed with simple linear regression. Multivariate linear regression was used to determine the effect of the influential factors on cervical alignment and total and segmental ROM.\n\n\nRESULTS\nThe mean value for C2-7 cervical alignment was 21.40\u00b0 \u00b1 12.15\u00b0, and the mean value for total ROM was 63.59\u00b0 \u00b1 15.37\u00b0. Sex was a significant factor in cervical alignment, total ROM, and segmental ROM for C2-3 and C5-6 (p < 0.05). Age had a significant negative association with both the total ROM and all of the segmental ROM measurements (p < 0.05). Cervical disc degeneration at the level of interest had a significant negative association with C4-5, C5-6, and C6-7 ROM (p < 0.05).\n\n\nCONCLUSIONS\nCervical alignment in female subjects was 2.47\u00b0 lower than that in male subjects. Total ROM was 3.86\u00b0 greater in female than in male subjects and decreased 6.46\u00b0 for each decade of aging. Segmental ROM decreased 1.28\u00b0 for each decade of aging and 2.26\u00b0 for each category increase in disc degeneration at the level of interest.", "final_summary": "The relationship between cervical disc pathology and cervical posture is a complex and multifaceted issue, as evidenced by the diverse findings in the literature.\n\nC\u00e2mara-Souza (2018) found no significant relationship between cervical posture in the sagittal plane and temporomandibular disorders (TMD). In contrast, Laat (1998) found more segmental limitations and tender points in patients with TMD, indicating a correlation with cervical spine disorders.\n\nYoo (2009) found a significant correlation between active cervical range of motion (ROM) and changes in head and neck posture after continuous visual display terminal (VDT) work. This suggests that certain activities or work conditions could influence the relationship between cervical disc pathology and posture.\n\nSonnesen (2007) found that cervical column morphology was related to head posture and cranial base angle. The study examined both normal subjects and those with mandibular condylar hypoplasia, indicating a broad examination of factors related to cervical posture.\n\nTonetti (2004) provided a detailed morphological analysis of cervical discs in relation to traumatic and degenerative lesions, suggesting that the shape and function of the cervical disc could influence its relationship with cervical posture.\n\nRao (2002) discussed the pathophysiology, natural history, and clinical evaluation of neck pain, cervical radiculopathy, and cervical myelopathy. However, the abstract does not provide specific details on the relationship between cervical disc pathology and posture.\n\nNakashima (2015) found that cervical disc degeneration and protrusion were common even in healthy subjects and deteriorated with age. The study also found a significant correlation between cervical disc degeneration and disc protrusion, suggesting a potential link between disc pathology and changes in cervical posture.\n\nFinally, Liu (2015) found that cervical alignment and ROM were influenced by factors such as age, sex, and cervical disc degeneration. This suggests that these factors could also play a role in the relationship between cervical disc pathology and posture.\n\nIn conclusion, while some studies suggest a potential relationship between cervical disc pathology and cervical posture, the evidence is not conclusive, and other factors such as age, sex, and specific activities or work conditions may also play a significant role. Further research is needed to fully elucidate this complex relationship."}, {"query": "recent advances and therapeutic approache to Adult ADHD", "paper_list_string": "Knouse 2008:\n\nTitle: Recent developments in the psychosocial treatment of adult ADHD\n\nAbstract: Adult attention-deficit/hyperactivity disorder (ADHD) is an increasingly recognized Diagnostic and Statistical Manual of Mental Disorders (DSM)-IV psychiatric disorder associated with significant functional impairment in multiple domains. Although stimulant and other pharmacotherapy regimens have the most empirical support as treatments for ADHD in adults, many adults with the disorder continue to experience significant residual symptoms. In the present manuscript, we review the published studies examining group and individual psychosocial treatments for adult ADHD. We include a discussion of coaching interventions and how they differ from cognitive\u2013behavioral therapy. We conclude that the available data support the use of structured, skills-based psychosocial interventions as a viable treatment for adults with residual symptoms of ADHD. Common elements across the various treatment packages include psychoeducation, training in concrete skills (e.g., organization and planning strategies) and emphasis on outside practice and maintenance of these strategies in daily life. These treatments, however, require further study for replication, extension and refinement. Finally, we suggest future directions for the application of psychosocial treatments to the problems of adults with ADHD.\n\n==\n\nAntshel 2011:\n\nTitle: Advances in understanding and treating ADHD\n\nAbstract: Attention deficit hyperactivity disorder (ADHD) is a neurocognitive behavioral developmental disorder most commonly seen in childhood and adolescence, which often extends to the adult years. Relative to a decade ago, there has been extensive research into understanding the factors underlying ADHD, leading to far more treatment options available for both adolescents and adults with this disorder. Novel stimulant formulations have made it possible to tailor treatment to the duration of efficacy required by patients, and to help mitigate the potential for abuse, misuse and diversion. Several new non-stimulant options have also emerged in the past few years. Among these, cognitive behavioral interventions have proven popular in the treatment of adult ADHD, especially within the adult population who cannot or will not use medications, along with the many medication-treated patients who continue to show residual disability.\n\n==\n\nKnouse 2010:\n\nTitle: Current status of cognitive behavioral therapy for adult attention-deficit hyperactivity disorder.\n\nAbstract: Attention-deficit/hyperactivity disorder (ADHD) is a valid and impairing psychological disorder that persists into adulthood in a majority of cases and is associated with chronic functional impairment and increased rates of comorbidity. Cognitive behavioral therapy (CBT) approaches for this disorder have emerged recently, and available evidence from open and randomized controlled trials suggests that these approaches are promising in producing significant symptom reduction. A conceptual model of how CBT may work for ADHD is reviewed along with existing efficacy studies. A preliminary comparison of effect sizes across intervention packages suggests that targeted learning and practice of specific behavioral compensatory strategies may be a critical active ingredient in CBT for adult ADHD. The article concludes with a discussion of future directions and critical questions that must be addressed in this area of clinical research.\n\n==\n\nHorrigan 2001:\n\nTitle: Present and future pharmacotherapeutic options for adult attention deficit/hyperactivity disorder\n\nAbstract: Attention deficit/hyperactivity disorder (ADHD) is often a lifelong condition. When untreated or undertreated, it appears to have a deleterious impact upon the daily functioning of the majority of adults that were diagnosed with this condition during childhood. Effective treatment, under the best circumstances, is multi-modal. The recent MTA study staged by the United States government confirmed the primary role of psychostimulants for children with this condition. The findings from this study have been generalised to adults that also have ADHD, particularly in cases where there is a well-defined longitudinal history dating back to early childhood. Psychostimulants remain a viable first-choice strategy for adults with ADHD. There are idiosyncratic differences in response to the various psychostimulants for any given individual with ADHD. Furthermore, the emergence of long-acting, once daily psychostimulant medications is likely to improve the calibre of care for adults with ADHD. A number of alternative pharmacotherapies have been studied, or are being developed, for adults with ADHD. These pharmacotherapies include antidepressant medications that affect dopaminergic and noradrenergic bioavailability, as well as cholinergic agents. In addition, agents that manipulate histaminergic and glutaminergic receptors are being studied as possible non-stimulant alternatives in the management of adult ADHD. More information is needed before any definitive statements can be made concerning the feasibility and utility of these non-stimulant medication approaches.\n\n==\n\nBuoli 2016:\n\nTitle: Alternative pharmacological strategies for adult ADHD treatment: a systematic review\n\nAbstract: ABSTRACT Adult Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent psychiatric condition associated with high disability and frequent comorbidity. Current standard pharmacotherapy (methylphenidate and atomoxetine) improves ADHD symptoms in the short-term, but poor data were published about long-term treatment. In addition a number of patients present partial or no response to methylphenidate and atomoxetine. Research into the main database sources has been conducted to obtain an overview of alternative pharmacological approaches in adult ADHD patients. Among alternative compounds, amphetamines (mixed amphetamine salts and lisdexamfetamine) have the most robust evidence of efficacy, but they may be associated with serious side effects (e.g. psychotic symptoms or hypertension). Antidepressants, particularly those acting as noradrenaline or dopamine enhancers, have evidence of efficacy, but they should be avoided in patients with comorbid bipolar disorder. Finally metadoxine and lithium may be particularly suitable in case of comorbid alcohol misuse or bipolar disorder.\n\n==\n\nDodson 2005:\n\nTitle: Pharmacotherapy of adult ADHD.\n\nAbstract: Although attention deficit hyperactivity disorder (ADHD) has been officially recognized as persisting into adulthood for more than 25 years, only recently has the condition been studied in adults. There is great syndromatic continuity between childhood and adult ADHD, and thus much of the medication management of adults with ADHD can be based on the experience gained from treating children and adolescents. Stimulant medications remain the treatment of choice and are generally as effective in adults as they are in children. Several extended-release delivery systems that improve convenience and compliance have become available. Several second-line medications are also reviewed. The medications must be fine-tuned to the needs of the individual patient in regard to the dose and timing of dose that achieve optimal therapeutic benefit. Medication adjustment must be done by trial and error because no parameter yet identified predicts the molecule or dose that will provide optimal performance.\n\n==\n\nFullen 2020:\n\nTitle: Psychological Treatments in Adult ADHD: A Systematic Review\n\nAbstract: Attention deficit hyperactivity disorder (ADHD) is a common neurodevelopmental disorder, characterized by symptoms of inattention, hyperactivity and or impulsivity. First line treatment is medication; however, medication alone may not provide sufficient functional improvement for some patients, or be universally tolerated. A recent surge in research to treat ADHD using non-pharmacological interventions demands a comprehensive, systematic review of the literature. The aim of this review was to examine the evidence base for psychological treatments for ADHD management in adulthood. A systematic search of PsycINFO, MEDLINE, CINAHL, AMED, PubMed, and EMBASE was undertaken until January 2019 for peer-reviewed articles exploring psychological interventions for adults (18\u00a0years with no upper limit) diagnosed with ADHD. A total of 53 papers were identified for inclusion. Collectively, 92% of studies (employing various non-pharmacological interventions) found a variant of significant positive effect on either primary or secondary outcomes associated with ADHD. The strongest empirical support derived from Cognitive Behavioral Therapy interventions. In addition, findings indicated support for the effectiveness of Mindfulness, Dialectical Behavior Therapy and Neurofeedback. Other types of interventions also demonstrated effectiveness; however, support was limited due to lack of available research and methodological rigor. Psychological interventions should be considered a valid and useful addition to clinical practice. Implications and areas for future research are discussed.\n\n==\n\nPrince 2002:\n\nTitle: Pharmacotherapy of Adult ADHD\n\nAbstract: This chapter explains pharmacotherapy of adult Attention-Deficit Hyperactivity Disorder (ADHD). The aggregate literature supports the notion that pharmacotherapy provides an effective treatment for adults with ADHD. Effective pharmacological treatments for ADHD adults include the use of the psychostimulants and antidepressants, with unclear efficacy of cognitive enhancers. Structured psychotherapy may be effective when used adjunctly with medications. Groups focused on coping skills, support, and interpersonal psychotherapy may also be very useful for these adults. For adults considering advanced schooling, educational planning, and alterations in the school environment may be necessary. Further controlled investigations assessing the efficacy of single and combination agents for adults with ADHD are necessary, with careful attention to diagnostics, symptom ,and neuropsychological outcome, long-term tolerability and efficacy, and use in specific ADHD subgroups. The chapter concludes that further studies applying stringent diagnostic criteria and outcome methodology are necessary to enhance the range of pharmacotherapeutic options for adults with ADHD.", "final_summary": "The recent advances and therapeutic approaches to Adult Attention Deficit Hyperactivity Disorder (ADHD) have been extensively studied, with a focus on both pharmacological and psychosocial treatments. \n\nPharmacological treatments remain a primary approach, with stimulant medications being the treatment of choice (Dodson, 2005). Novel stimulant formulations have been developed to tailor treatment to the duration of efficacy required by patients and to mitigate potential misuse (Antshel, 2011). Horrigan (2001) also highlighted the role of psychostimulants, noting the emergence of long-acting, once-daily medications that improve care for adults with ADHD. \n\nAlternative pharmacotherapies have been explored, including antidepressants that affect dopaminergic and noradrenergic bioavailability, cholinergic agents, and agents that manipulate histaminergic and glutaminergic receptors (Horrigan, 2001). Buoli (2016) also discussed alternative compounds such as amphetamines and antidepressants, suggesting they may be particularly suitable in cases of comorbid conditions. \n\nIn addition to pharmacological treatments, psychosocial interventions have gained recognition. Knouse (2008) suggested that structured, skills-based psychosocial interventions could be a viable treatment for adults with residual symptoms of ADHD. Cognitive Behavioral Therapy (CBT) has emerged as a promising approach, with studies suggesting it may produce significant symptom reduction (Knouse, 2010). Fullen (2020) supported this, stating that the strongest empirical support derived from CBT interventions, but also noted the effectiveness of Mindfulness, Dialectical Behavior Therapy, and Neurofeedback. \n\nIn conclusion, the recent advances in therapeutic approaches to Adult ADHD encompass a range of pharmacological treatments, including stimulants and alternative compounds, as well as psychosocial interventions, particularly CBT. These findings highlight the importance of a multi-modal approach to treating Adult ADHD, tailored to the individual's needs and potential comorbid conditions (Prince, 2002)."}, {"query": "Does microbial activity affect the surface properties of jarosite precipitate", "paper_list_string": "Liu 2009:\n\nTitle: Study of formation of jarosite mediated by thiobacillus ferrooxidans in 9K medium\n\nAbstract: Thiobacillus ferrooxidans, occurring extensively in mine districts, have important effects on the oxidation of metal sulphide and the formation of jarosite. In the coal bio-desulphurization, jarosite mediated by microorganisms decreases the bacterial absorbability of nutrients, occupies the available sites of coal surfaces, ultimately results in residual sulfur, which cannot be removed from coal. The thiobacillus ferrooxidans cultivation conditions with minimal jarosite in 9K medium by varying the initial pH, the initial Fe2+ concentration and the applied potential were studied and determined. Experimental results show that the optimal combination giving the minimal jarosite precipitates (3.73 g/L) and the good growing activity of bacteria is the initial pH of 1.6\u20131.7 with the Fe2+ concentration of 9\u201310 g/L and the applied potential of \u22120.5 V for 7 hours. The results will be of significant importance for the further research on the bacterial cultivation and coal desulphurization.\n\n==\n\nSadowski 1999:\n\nTitle: Adhesion of microorganism cells and jarosite particles on the mineral surface\n\nAbstract: It has been know that during both bioleaching and biomodification of the mineral surface the microbial cell should be tenaciously adhered to the mineral surface. The main goal of this research was evaluate the effect of polysaccharides (dextrine) on both the bacterial cells and precipitated jarosite particles deposition onto the mineral surface. The determination of the free energy of solid surface was med. by means of the thin-layer wicking technique. It was found that the quartz particles which were covered by the dextrine film have value of \u03b3 LW =121.44 mN/m (without dextrin \u03b3 LW =62.18 mN/m). The adsorption of dextrin caused a decrease of the \u03b3 \u2212 component of the free energy from 171.90 to 123.19 mN/m. The treatment of both quartz and gold refractory ore by polysaccharides caused an increase of the adhesion of microbial cells and jarosite colloid particles. The deposition of jarosite on the surface of gold ore stopped by the dispersing reagent addition.\n\n==\n\nWeisener 2008:\n\nTitle: Microbial Dissolution of Silver Jarosite: Examining Its Trace Metal Behaviour in Reduced Environments\n\nAbstract: Iron sulfate minerals such as jarosite-group compounds (e.g., M Fe3(SO 4 ) 2 (OH) 6 ) can be of considerable environmental importance because of their ability to scavenge trace elements and thus contribute to some degree of metal cycling. Jarosite forms in low temperature hydrothermal, acidic, sulfate-rich environments often yielding a range of elemental substituted forms: plumbojarosite (M = Pb), argentojarosite (M = Ag), jarosite (M = K), natrojarosite (M = Na), hydroniumjarosite (M = H3O), and ammoniojarosite (M = NH4). Anthropogenic sources of jarosite are common in mine waste environments, most often associated with the waste products resulting from base metal recovery. Few studies have investigated the effect that dissimilatory metal reducing bacteria may have in the presence of these compounds following the onset of reducing conditions. Jarosite reactivity may differ systematically as a function of its chemical properties. For example, the incorporation of Ag in the mineral lattice may have inhibitory affects on the growth of microbial strains. In this study the reductive dissolution of argentojarosite (M = Ag) in the presence of Shewanella putrefaciens CN32 (10 9 cells ml 1 , pH 7.1) was examined. Using the silver (argento)jarosite, Ag Fe 3 (SO 4 ) 2 (OH) 6 , as a sole terminal electron acceptor we observed the reduction of structural Fe(III) and Ag(I) by CN32 through the release of Fe(II) ions to solution. Aqueous silver concentrations were below instrumental detection. Environmental SEM (ESEM) and TEM micrographs of the microbial clusters revealed the progressive heterogeneous nucleation of Ag(0) nanoparticles within cellular structures and also on adjacent mineral grains. The results of this study are the first presented for the anaerobic dissolution of silver jarosite. This has implications for understanding the processes leading to the mobility or retention of silver in mine waste and industrial landfill environments. It also provides insight into the microbial mechanisms of silver resistance and nanoparticle formation with potential applications for bioleaching and/or biotechnology.\n\n==\n\nSasaki 2006:\n\nTitle: FE-SEM Study of Microbially Formed Jarosites by Acidithiobacillus ferrooxidans\n\nAbstract: Morphological characterization of jarosite groups formed from Fe(III) biologically oxidized with different numbers of Acidithiobacillus ferrooxidans was conducted using FE-SEM. The higher population of A. ferrooxidans resulted in more distinct jarosite mineral shape, and stronger Raman intensities for potassium jarosite, ammoniojarosite and argentojarosite. The morphology of the jarosites might be dependent on iron-oxidizing activity of A. ferrooxidans. The technique was applied to identify jarosite compounds formed during microbially mediated dissolution of arsenopyrite by A. ferrooxidans. It is difficult to identify this jarosite compound by X-ray diffraction and Raman spectroscopy because amounts are typically low and the crystallization is poor in minerals formed by microbially mediated oxidation. However, FE-SEM image provided helpful information for identification of jarosite compounds. The results suggest that morphology would provide useful information for identification and history of jarosite minerals as geochemical samples.\n\n==\n\nDaoud 2006:\n\nTitle: Formation of jarosite during Fe2+ oxidation by Acidithiobacillus ferrooxidans\n\nAbstract: Abstract Jarosite precipitation is a very important phenomenon that is observed in many bacterial cultures. In many applications involving Acidithiobacillus ferrooxidans , like coal desulphurization and bioleaching, it is crucial to minimize jarosite formation in order to increase efficiency. The formation of jarosite during the oxidation of ferrous iron by free suspended cells of A. ferrooxidans was studied. The process was studied as a function of time, pH and temperature. The main parameter affecting the jarosite formation was pH. Several experiments yielded results showing oxidation rates as high as 0.181\u20130.194\u00a0g/L\u00a0h, with low jarosite precipitation of 0.0125\u20130.0209\u00a0g at conditions of pH 1.6\u20131.7 with an operating temperature of 35\u00a0\u00b0C.\n\n==\n\nGao 2019:\n\nTitle: Reductive dissolution of jarosite by a sulfate reducing bacterial community: Secondary mineralization and microflora development.\n\nAbstract: Jarosite is an iron-hydroxysulfate mineral commonly found in acid mine drainage (AMD). Given its strong adsorption capacity and its ability to co-precipitation with heavy metals, jarosite is considered a potent scavenger of contaminants in AMD-impacted environments. Sulfate-reducing bacteria (SRB) play an important role in the reductive dissolution of jarosite; however, the mechanism involved has yet to be elucidated. In this study, an indigenous SRB community enriched from the Dabaoshan mine area (Guangdong, China) was employed to explore the mechanism of the microbial reduction of jarosite. Different cultures, with or without dissolved sulfate and the physical separation of jarosite from bacteria by dialysis bags, were examined. Results indicate that the reduction of jarosite by SRB occurred via an indirect mechanism. In systems with dissolved sulfate, lactate was incompletely oxidized to acetate coupled with the reduction of SO42- to S2-, which subsequently reduced the Fe3+ in jarosite, forming secondary minerals including vivianite, mackinawite and pyrite. In systems without dissolved sulfate, jarosite dissolution occurred prior to reduction, and similar secondary minerals formed as well. Extracellular polymeric substances secreted by SRB appeared to facilitate the release of sulfate from jarosite. Structural sulfate in the solid phase of jarosite may not be available for SRB respiration. Although direct contact between SRB and jarosite is not necessary for mineral reduction, wrapping jarosite into dialysis bags suppressed the reduction to a certain extent. Microbial community composition differed in direct contact treatments and physical separation treatments. Physical separation of the SRB community from jarosite mineral supported the growth of Citrobacter, while Desulfosporosinus dominated in direct contact treatments.\n\n==\n\nJones 2014:\n\nTitle: Synthesis and properties of ternary (K, NH\u2084, H\u2083O)-jarosites precipitated from Acidithiobacillus ferrooxidans cultures in simulated bioleaching solutions.\n\nAbstract: The purpose of this study was to synthesize a series of solid solution jarosites by biological oxidation of ferrous iron at pH2.2-4.4 and ambient temperature in media containing mixtures of K(+) (0, 1, 4, 6, 12, 31 mM) and NH4(+) (6.1, 80, 160, 320 mM). The starting material was a liquid medium for Acidithiobacillus ferrooxidans comprised of 120 mM FeSO4 solution and mineral salts at pH2.2. Following inoculation with A. ferrooxidans, the cultures were incubated in shake flasks at 22\u00b0C. As bacteria oxidized ferrous iron, ferric iron hydrolyzed and precipitated as jarosite-group minerals (AFe3(SO4)2(OH)6) and/or schwertmannite (idealized formula Fe8O8(OH)6(SO4)\u00b7nH2O). The precipitates were characterized by X-ray diffraction (XRD), elemental analysis, and Munsell color. Schwertmannite was the dominant mineral product at low combinations of K(+) (\u2264 4 mM) and NH4(+) (\u2264 80 mM) in the media. At higher single or combined concentrations, yellowish jarosite phases were produced, and Munsell hue provided a sensitive means of detecting minor schwertmannite in the oxidation products. Although the hydrated ionic radii of K(+) and NH4(+) are similar, K(+) greatly facilitated the formation of a jarosite phase compared to NH4(+). Unit cell and cell volume calculations from refinements of the powder XRD patterns indicated that the jarosite phases produced were mostly ternary (K, NH4, H3O)-solid solutions that were also deficient in structural Fe, especially at low NH4 contents. Thus, ferric iron precipitation from the simulated bioleaching systems yielded solid solutions of jarosite with chemical compositions that were dependent on the relative concentrations of K(+) and NH4(+) in the synthesis media. No phase separations involving discrete, end-member K-jarosite or NH4-jarosite were detected in the un-aged precipitates.\n\n==\n\nGrishin 1988:\n\nTitle: Characterization of Jarosite Formed upon Bacterial Oxidation of Ferrous Sulfate in a Packed-Bed Reactor\n\nAbstract: A packed-bed bioreactor with activated-carbon particles as a carrier matrix material inoculated with Thiobacillus ferrooxidans was operated at a pH of 1.35 to 1.5 to convert ferrous sulfate to ferric sulfate. Despite the low operating pH, trace amounts of precipitates were produced in both the reactor and the oxidized effluent. X-ray diffraction and chemical analyses indicated that the precipitates were well-ordered potassium jarosite. The chemical analyses also revealed a relative deficiency of Fe and an excess of S in the reactor sample compared with the theoretical composition of potassium jarosite.", "final_summary": "Microbial activity has a significant impact on the surface properties of jarosite precipitate, a common mineral found in acid mine drainage. This relationship is complex and multifaceted, with various studies exploring different aspects of this interaction.\n\nLiu (2009) found that the bacteria Thiobacillus ferrooxidans, common in mining areas, influences the formation of jarosite. The study showed that the bacteria's activity could be manipulated to minimize jarosite formation, which is crucial for processes like coal desulphurization. Similarly, Daoud (2006) also found that jarosite formation during the oxidation of ferrous iron by A. ferrooxidans could be controlled by adjusting parameters like pH and temperature.\n\nSadowski (1999) explored the role of polysaccharides in the adhesion of bacterial cells and jarosite particles to mineral surfaces. The study found that treating mineral surfaces with polysaccharides increased the adhesion of both microbial cells and jarosite particles. This suggests that microbial activity can influence the distribution and adhesion of jarosite particles on mineral surfaces.\n\nWeisener (2008) and Gao (2019) both explored the role of bacteria in the dissolution of jarosite. Weisener (2008) found that bacteria could reduce structural Fe(III) and Ag(I) in silver jarosite, leading to the formation of Ag(0) nanoparticles. Gao (2019) found that sulfate-reducing bacteria could dissolve jarosite via an indirect mechanism, leading to the formation of secondary minerals.\n\nSasaki (2006) and Jones (2014) both explored the role of bacteria in the formation of jarosite. Sasaki (2006) found that the morphology of jarosite was dependent on the iron-oxidizing activity of A. ferrooxidans. Jones (2014) found that the precipitation of ferric iron from simulated bioleaching systems yielded solid solutions of jarosite with compositions dependent on the relative concentrations of K(+) and NH4(+) in the synthesis media.\n\nLastly, Grishin (1988) found that despite operating at a low pH, the bacterial oxidation of ferrous sulfate in a packed-bed reactor led to the formation of well-ordered potassium jarosite.\n\nIn conclusion, microbial activity plays a significant role in the formation, dissolution, and surface properties of jarosite precipitate. The interaction between microbes and jarosite is complex and can be influenced by various factors, including pH, temperature, and the presence of other compounds. Further research is needed to fully understand these interactions and their implications for processes like bioleaching and coal desulphurization."}, {"query": "Corruption and lack of economic reforms threaten Vietnam's economy. Corruption in Vietnam's government and economy leads to inefficient allocation of resources and deters foreign investment (Mai 2013).", "paper_list_string": "Salvador 2020:\n\nTitle: Foreign direct investment, corruption, and institutional reforms\n\nAbstract: Corruption impacts the competitive conditions among firms and the flow of foreign investment. Institutional reforms made for fighting against corruption are sometimes useless. We develop a model in which a corrupted government tries to set an optimal institutional level taking into account the cost of this policy on foreign investment, the benefit of a corrupted domestic firm and the benefit of local citizens. A political contribution is made by a corrupted lobby group in order to benefit from a lower institutional level. Our results suggest that the optimal institutional level depends on the degree of efficiency of firms and the level of corruption of the host government.\u00a0Key words: Corruption, Lobbying, Institutional reforms, Foreign direct investment.\u00a0JEL: F21, F30, K42\n\n==\n\nDang 2016:\n\nTitle: The impact of corruption on provincial development performance in Vietnam\n\nAbstract: Corruption has long been considered a national illness in Vietnam but progress in fighting corruption has been modest. In recent years, the Communist Party of Vietnam and the Government of Vietnam have strengthened their efforts to prevent and fight corruption. Despite strong anti-corruption measures being implemented at the national level, provincial authorities have shied away from tackling corruption. One of the reasons for this could be that it is not clear to provincial authorities if and how corruption is affecting local development economically and socially. This article demonstrates that corruption has a negative impact on private sector investment, employment and per capita income at the provincial level in Vietnam. However, corruption is found to have no significant impact on how income is distributed across provinces. The findings demonstrate that more effective anti-corruption measures are necessary to promote the private sector and improve household income. The study provides provincial leaders with empirical evidence and incentives for fighting corruption.\n\n==\n\nMalesky 2012:\n\nTitle: Foreign Investment and Bribery: A Firm-Level Analysis of Corruption in Vietnam\n\nAbstract: Among the concerns faced by countries pondering the costs and benefits of greater economic openness to international capital flows is the worry that new and powerful external actors will exert a corrupting influence on the domestic economy. In this paper, we use a novel empirical strategy, drawn from research in experimental psychology, to test the linkage between foreign direct investment (FDI) and corruption. The prevailing literature has produced confused and contradictory results on this vital relationship due to errors in their measurement of corruption which are correlated with FDI inflows. When a less biased operationalization is employed, we find clear evidence of corruption during both registration and procurement procedures in Vietnam. The prevalence of corruption, however, is not associated with inflows of FDI. On the contrary, one measure of economic openness appears to be the most important driver of reductions in Vietnamese corruption: the wave of domestic legislation, which accompanied the country's bilateral trade liberalization agreement with the United States (US-BTA), significantly reduced bribery during business registration.\n\n==\n\nPhuonga 2020:\n\nTitle: Corruption and long-term investment of businesses in Vietnam\n\nAbstract: Article history: Received: June 13 2020 Received in revised format: June 24 2020 Accepted: June 26 2020 Available online: June 26 2020 This paper investigates the effects of corruption and long-term investment of businesses in Vietnam using the General Least Square (GLS) estimation method for businesses in 63 provinces in Vietnam from 2016-2018. The results show that corruption was an important factor affecting the long-term investment decisions of Vietnamese enterprises. The ability to predict corruption of businesses can explain the phenomenon of part of the cash flow of businesses flowing out of production and business. Informal costs related to low-level administrative procedures act as \u201cgrease\u201d to help businesses reduce time costs, but when the total amount of unofficial expenses exceeds 10% of revenue of businesses, they become a burden for businesses and restrain them from making long-term investments. Corrupted public officials' behavior has led businesses to misallocate resources and prevent them from making long-term investments. The result shows that the East Asia paradox holds only for the case of informal costs related to administrative procedures in Vietnam. \u00a9 2020 by the authors; licensee Growing Science, Canada.\n\n==\n\nNguyen 2017:\n\nTitle: Tax corruption and private sector development in Vietnam\n\nAbstract: This article aims to examine the impact of tax corruption on private sector development in Vietnam. It is motivated by two separate but related considerations. First, despite the seriousness of the phenomenon of corruption, there is a paucity of rigorous empirical research of corruption, particularly tax corruption, in Vietnam. Secondly, ineffective control of corruption is viewed as a cause of Vietnam\u2019s recent total factor productivity (TFP) slowdown or its poor industrial policy, both of which may hamper Vietnam\u2019s progress as a low middle-income country. Without some understanding on the impact of tax corruption on the economy, it may not be possible to devise the most effective anti-corruption policy and measures. After a brief literature review that focuses on tax corruption, various conceptual issues relating to tax corruption are discussed and clarified. The extent of petty tax corruption in Vietnam is then discussed, followed by a review of findings and implications of recent studies on how tax corruption impacts on private sector development in Vietnam. Despite perceptions and evidence of widespread petty tax corruption, Vietnam ranks very highly both in terms of tax collection and tax effort.Not unexpectedly, the impact of tax corruption is mixed in the sense that empirical evidence lends credence to both 'sanding the wheels' and 'greasing the wheels' hypotheses. Finally, some broad policy recommendations for combating tax corruption are offered.\n\n==\n\nThao 2020:\n\nTitle: Current Situation of Corruption Offenses and Measures for Improvement of Anti-Corruption Effectiveness in Vietnam\u2019s Economy\n\nAbstract: Along with the promulgation of 2008 Law on Anti-Corruption, thanking to the comprehensive solutions and determinations of the whole governmental apparatus, there are signals of a positive change in the fight against corruption in Vietnam\u2019s market economy. However, compared to other countries around the world, the corruption in Vietnam is still a national problem. The number of corruption cases may decline, but the scale and severity has been increasing. Many cases has involved high-ranking officials in the government with more than 20 general officers in the armed forces to be sentenced. On the basis of analysing the current situation of corruption in recent years, the author hereby recommends some synchronous solutions to improve the effectiveness of anti-corruption.\n\n==\n\nPhuong 2017:\n\nTitle: Corruption in Vietnam: The current situation and proposed solutions\n\nAbstract: Abstract This chapter introduces an overview of corruption in Vietnam. Although the country\u2019s GDP has maintained a steady growth of around 5%\u20137% per year during 2011\u201315, corruption is widespread throughout Vietnam and remains a substantial issue. The chapter starts by defining corruption according to Vietnamese law and regulations. It then explains the three major characteristics of corruption as perceived in this country. The third section of the chapter lists the 12 acts of corruption and the current state of corruption in Vietnam. The chapter ends with recommendations and solutions to prevent corruption in Vietnam. The critical roles played by ethical government officials, managers, and employees to prevent and punish corruption and bribery, along with effective legislation, will create and contribute toward sustainable economic development in the country.\n\n==\n\nBaum:\n\nTitle: Tax corruption and private sector development in Vietnam *\n\nAbstract: This article aims to examine the impact of tax corruption on private sector development in Vietnam. It is motivated by two separate but related considerations. First, despite the seriousness of the phenomenon of corruption, there is a paucity of rigorous empirical research of corruption, particularly tax corruption, in Vietnam. Secondly, ineffective control of corruption is viewed as a cause of Vietnam\u2019s recent total factor productivity (TFP) slowdown or its poor industrial policy, both of which may hamper Vietnam\u2019s progress as a low middle-income country. Without some understanding on the impact of tax corruption on the economy, it may not be possible to devise the most effective anti-corruption policy and measures. After a brief literature review that focuses on tax corruption, various conceptual issues relating to tax corruption are discussed and clarified. The extent of petty tax corruption in Vietnam is then discussed, followed by a review of findings and implications of recent studies on how tax corruption impacts on private sector development in Vietnam. Despite perceptions and evidence of widespread petty tax corruption, Vietnam ranks very highly both in terms of tax collection and tax effort. Not unexpectedly, the impact of tax corruption is mixed in the sense that empirical evidence lends credence to both \u2018sanding the wheels\u2019 and \u2018greasing the wheels\u2019 hypotheses. Finally, some broad policy recommendations for combating tax corruption are offered.", "final_summary": "Corruption in Vietnam is a complex issue with implications for the country's economic landscape. Salvador (2020) suggests that corruption can influence the competitive conditions among firms, which could indirectly impact foreign investment. Dang (2016) provides evidence that corruption has a negative impact on private sector investment and per capita income at the provincial level in Vietnam, indicating a potential inefficiency in resource allocation.\n\nThe relationship between corruption and foreign direct investment (FDI) is nuanced. Malesky (2012) found no direct association between the prevalence of corruption and inflows of FDI in Vietnam. Instead, the study suggests that other factors, such as domestic legislation and trade liberalization, may play a significant role in business practices.\n\nPhuonga (2020) found that corruption influences the business decisions of Vietnamese enterprises, particularly in relation to long-term investments. This is echoed by Nguyen (2017) and Baum (year not provided), who both found that tax corruption impacts private sector development in Vietnam, although the specific effects vary.\n\nDespite efforts to combat corruption, it remains a significant issue in Vietnam. Thao (2020) notes that the scale and severity of corruption cases have been increasing, even involving high-ranking officials. However, the specific impact of this on the overall severity of corruption in Vietnam, as suggested by Phuong (2017), is not explicitly stated.\n\nIn conclusion, while corruption in Vietnam is a pervasive issue with potential implications for the country's economy, its specific impact on foreign investment, private sector development, and long-term investment decisions is complex and influenced by a variety of factors. Further research and effective anti-corruption measures are necessary to fully understand and address this issue."}, {"query": "long text classification", "paper_list_string": "Weijie 2021:\n\nTitle: Long Text Classification Based on BERT\n\nAbstract: Existing text classification algorithms generally have limitations in terms of text length and yield poor classification results for long texts. To address this problem, we propose a BERT-based long text classification method. First, we slice the long text and use BERT to encode the sliced clauses to obtain the local semantic information. Second, we use BiLSTM to fuse the local semantic information and adopt the attention mechanism to increase the weight of important clauses in the long text, so as to obtain the global semantic information. Finally, the global semantic information is input to the softmax layer for classification. Experimental results show that the proposed method achieves higher accuracy than commonly used models.\n\n==\n\nWeijie 2021:\n\nTitle: Long Text Classification Based on BERT\n\nAbstract: Existing text classification algorithms generally have limitations in terms of text length and yield poor classification results for long texts. To address this problem, we propose a BERT-based long text classification method. First, we slice the long text and use BERT to encode the sliced clauses to obtain the local semantic information. Second, we use BiLSTM to fuse the local semantic information and adopt the attention mechanism to increase the weight of important clauses in the long text, so as to obtain the global semantic information. Finally, the global semantic information is input to the softmax layer for classification. Experimental results show that the proposed method achieves higher accuracy than commonly used models.\n\n==\n\nYue 2022:\n\nTitle: Research on Long Text Classification Model Based on Multi-Feature Weighted Fusion\n\nAbstract: Text classification in the long-text domain has become a development challenge due to the significant increase in text data, complexity enhancement, and feature extraction of long texts in various domains of the Internet. A long text classification model based on multi-feature weighted fusion is proposed for the problems of contextual semantic relations, long-distance global relations, and multi-sense words in long text classification tasks. The BERT model is used to obtain feature representations containing global semantic and contextual feature information of text, convolutional neural networks to obtain features at different levels and combine attention mechanisms to obtain weighted local features, fuse global contextual features with weighted local features, and obtain classification results by equal-length convolutional pooling. The experimental results show that the proposed model outperforms other models in terms of accuracy, precision, recall, F1 value, etc., under the same data set conditions compared with traditional deep learning classification models, and it can be seen that the model has more obvious advantages in long text classification.\n\n==\n\nZang 2023:\n\nTitle: Long Text Multi-label Classification\n\nAbstract: In recent years, the evaluation of non-technical literacy such as sense of worth, outlook on life, and sustainable development concept, has become increasingly important in university education in China. They are often evaluated by questionnaires or interviews. However, the feedback texts from students often contain long paragraphs of students' opinion responses in Chinese, with long text, large amount of information and complex structure, which reduce the accuracy of long Chinese text classification. In this paper, we propose a multi-label classification model named MLformer, which uses the Longformer model to obtain feature representations containing information about the global semantic and local contextual features of the text to effectively classify students' views. The experimental results on our long text dataset in Chinese about engineering sustainability show that our method outperforms the typical deep learning classification model BERT in terms of accuracy, precision, and F1 score, thus, it can provide a powerful reference for students' opinion analysis and evaluation.\n\n==\n\nTang 2023:\n\nTitle: Research on multi-label long text classification algorithm based on transformer-LDA\n\nAbstract: Text classification is an important research area in the field of natural language processing. In view of the low efficiency of the traditional CNN and RNN algorithms for multi-label classification of long text due to timing and spatial displacement problems, this paper proposes a long text classification model with improved Transformer attention mechanism, and combines LDA topic classification algorithm to achieve multi-label classification of document length text. Firstly, the paper introduces the industry's solutions to the problem of multi-classification of long texts. Compared with traditional algorithms such as truncation method and pooling method, the PAPER proposes the LDA topic classification model combined with the improved Transformer-XL text classification model to extract text features with fine granularity, so as to classify texts with higher accuracy. Finally, comparative experiments show that the proposed solution has a significant improvement in P value, R value and F1 value compared with the traditional classification method in the field of long text multi-label classification.\n\n==\n\nFiok 2021:\n\nTitle: Revisiting Text Guide, a Truncation Method for Long Text Classification\n\nAbstract: The quality of text classification has greatly improved with the introduction of deep learning, and more recently, models using attention mechanism. However, to address the problem of classifying text instances that are longer than the length limit adopted by most of the best performing transformer models, the most common method is to naively truncate the text so that it meets the model limit. Researchers have proposed other approaches, but they do not appear to be popular, because of their high computational cost and implementation complexity. Recently, another method called Text Guide has been proposed, which allows for text truncation that outperforms the naive approach and simultaneously is less complex and costly than earlier proposed solutions. Our study revisits Text Guide by testing the influence of certain modifications on the method\u2019s performance. We found that some aspects of the method can be altered to further improve performance and confirmed several assumptions regarding the dependence of the method\u2019s quality on certain factors.\n\n==\n\nFiok 2021:\n\nTitle: Text Guide: Improving the Quality of Long Text Classification by a Text Selection Method Based on Feature Importance\n\nAbstract: The performance of text classification methods has improved greatly over the last decade for text instances of less than 512 tokens. This limit has been adopted by most state-of-the-research transformer models due to the high computational cost of analyzing longer text instances. To mitigate this problem and to improve classification for longer texts, researchers have sought to resolve the underlying causes of the computational cost and have proposed optimizations for the attention mechanism, which is the key element of every transformer model. In our study, we are not pursuing the ultimate goal of long text classification, i.e., the ability to analyze entire text instances at one time while preserving high performance at a reasonable computational cost. Instead, we propose a text truncation method called Text Guide, in which the original text length is reduced to a predefined limit in a manner that improves performance over naive and semi-naive approaches while preserving low computational costs. Text Guide benefits from the concept of feature importance, a notion from the explainable artificial intelligence domain. We demonstrate that Text Guide can be used to improve the performance of recent language models specifically designed for long text classification, such as Longformer. Moreover, we discovered that parameter optimization is the key to Text Guide performance and must be conducted before the method is deployed. Future experiments may reveal additional benefits provided by this new method.\n\n==\n\nHe 2011:\n\nTitle: Research on short text classification for web forum\n\nAbstract: The unique characteristic of short text makes short text classification quite different from traditional long text processing. The feature space of short text is so sparse, which makes it notoriously difficult to extract sufficient and effective features. In this paper, aiming to classify the short text on web forum accurately, a novel short-text-processing method based on semantic extension is introduced to enhance the content of the original short text, which effectively solves the problem of feature sparse. In addition, we put forward the concept of Key-Pattern (KP) and propose a new text feature representation approach based on KP, which extracts phrase with powerful semantic information as the text features. Traditional classifier model are applied to estimate the text's classification, experimental results show that the proposed method is effective to improve the accuracy and recall of short text classification.", "final_summary": "Long text classification is a complex task in the field of natural language processing, with various studies proposing different methods to tackle this challenge. \n\nWeijie (2021) proposed a BERT-based method that slices long text into smaller segments, encodes them using BERT, and then fuses the local semantic information using BiLSTM. An attention mechanism is also used to increase the weight of important clauses in the long text. This method reportedly achieves higher accuracy than commonly used models.\n\nYue (2022) introduced a long text classification model based on multi-feature weighted fusion. This model uses the BERT model to obtain feature representations and convolutional neural networks to obtain features at different levels. The model reportedly outperforms other models in terms of accuracy, precision, recall, and F1 value.\n\nZang (2023) proposed a multi-label classification model named MLformer, which uses the Longformer model to obtain feature representations. This model reportedly outperforms the typical deep learning classification model BERT in terms of accuracy, precision, and F1 score.\n\nTang (2023) proposed a long text classification model with an improved Transformer attention mechanism, combined with the LDA topic classification algorithm. This model reportedly has a significant improvement in P value, R value, and F1 value compared with the traditional classification method in the field of long text multi-label classification.\n\nFiok (2021) revisited the Text Guide method, a truncation method for long text classification. The study found that some aspects of the method can be altered to further improve performance. In another study, Fiok (2021) proposed a text truncation method called Text Guide, which reduces the original text length to a predefined limit in a manner that improves performance over naive and semi-naive approaches while preserving low computational costs.\n\nIn conclusion, various methods have been proposed to tackle the challenge of long text classification, with most of them leveraging the power of transformer models like BERT and Longformer. These methods often involve slicing the text into smaller segments, encoding them, and then fusing the local semantic information. Attention mechanisms and multi-feature weighted fusion are also commonly used to improve the performance of these models. However, the optimal method for long text classification may vary depending on the specific requirements and constraints of the task at hand."}, {"query": "Find a good introduction to quantum computing", "paper_list_string": "Hey 1999:\n\nTitle: Quantum computing: an introduction\n\nAbstract: The basic ideas of quantum computation are introduced by a brief discussion of Bennett (1973, 1982) and Fredkin's (1982, 1997) ideas of reversible computation. After some remarks about Deutsch's (1985) pioneering work on quantum complexity and Shor's (1996) factorisation algorithm, quantum logic gates, qubits and registers are discussed. The role of quantum entanglement is stressed and Grover's (1997) quantum search algorithm described in detail. The paper ends with a review of the current experimental status of quantum computers.\n\n==\n\nKaye 2006:\n\nTitle: An Introduction to Quantum Computing\n\nAbstract: Preface 1. Introduction and background 2. Linear algebra and the Dirac notation 3. Qubits and the framework of quantum mechanics 4. A quantum model of computation 5. Superdense coding and quantum teleportation 6. Introductory quantum algorithms 7. Algorithms with super-polynomial speed-up 8. Algorithms based on amplitude amplification 9. Quantum computational complexity theory and lower bounds 10. Quantum error correction Appendices Bibliography Index\n\n==\n\nPittenger 2000:\n\nTitle: An Introduction to Quantum Computing Algorithms\n\nAbstract: From the Publisher: \nThe purpose of this monograph is to provide the mathematically literate reader with an accessible introduction to the theory of quantum computing algorithms, one component of a fascinating and rapidly developing area which involves topics from physics, mathematics, and computer science. \nThe author briefly describes the historical context of quantum computing and provides the motivation, notation, and assumptions appropriate for quantum statics, a non-dynamical, finite dimensional model of quantum mechanics. This model is then used to define and illustrate quantum logic gates and representative subroutines required for quantum algorithms. A discussion of the basic algorithms of Simon and of Deutsch and Jozsa sets the stage for the presentation of Grover's search algorithm and Shor's factoring algorithm, key algorithms which crystallized interest in the practicality of quantum computers. A group theoretic abstraction of Shor's algorithms completes the discussion of algorithms. \nThe last third of the book briefly elaborates the need for error-correction capabilities and then traces the theory of quantum error-correcting codes from the earliest examples to an abstract formulation in Hilbert space. \nThis text is a good self-contained introductory resource for newcomers to the field of quantum computing algorithms, as well as a useful self-study guide for the more specialized scientist, mathematician, graduate student, or engineer. Readers interested in following the ongoing developments of quantum algorithms will benefit particularly from this presentation of the notation and basic theory.\n\n==\n\nLo 2002:\n\nTitle: Introduction to Quantum Computation Information\n\nAbstract: From the Publisher: \nThis book aims to provide a pedagogical introduction to the subjects of quantum information and computation. Topics include non-locality of quantum mechanics, quantum computation, quantum cryptography, quantum error correction, fault-tolerant quantum computation as well as some experimental aspects of quantum computation and quantum cryptography. Only knowledge of basic quantum mechanics is assumed. Whenever more advanced concepts and techniques are used, they are introduced carefully. This book is meant to be a self-contained overview. While basic concepts are discussed in detail, unnecessary technical details are excluded. It is well-suited for a wide audience ranging from physics graduate students to advanced researchers.\n\n==\n\nRieffel 2011:\n\nTitle: Quantum Computing: A Gentle Introduction\n\nAbstract: The combination of two of the twentieth centurys most influential and revolutionary scientific theories, information theory and quantum mechanics, gave rise to a radically new view of computing and information. Quantum information processing explores the implications of using quantum mechanics instead of classical mechanics to model information and its processing. Quantum computing is not about changing the physical substrate on which computation is done from classical to quantum but about changing the notion of computation itself, at the most basic level. The fundamental unit of computation is no longer the bit but the quantum bit or qubit. This comprehensive introduction to the field offers a thorough exposition of quantum computing and the underlying concepts of quantum physics, explaining all the relevant mathematics and offering numerous examples. With its careful development of concepts and thorough explanations, the book makes quantum computing accessible to students and professionals in mathematics, computer science, and engineering. A reader with no prior knowledge of quantum physics (but with sufficient knowledge of linear algebra) will be able to gain a fluent understanding by working through the book. The text covers the basic building blocks of quantum information processing, quantum bits and quantum gates, showing their relationship to the key quantum concepts of quantum measurement, quantum state transformation, and entanglement between quantum subsystems; it treats quantum algorithms, discussing notions of complexity and describing a number of simple algorithms as well as the most significant algorithms to date; and it explores entanglement and robust quantum computation, investigating such topics as quantifying entanglement, decoherence, quantum error correction, and fault tolerance.\n\n==\n\nNannicini 2017:\n\nTitle: An Introduction to Quantum Computing, Without the Physics\n\nAbstract: This paper is a gentle but rigorous introduction to quantum computing intended for discrete mathematicians. Starting from a small set of assumptions on the behavior of quantum computing devices, we analyze their main characteristics, stressing the differences with classical computers, and finally describe two well-known algorithms (Simon's algorithm and Grover's algorithm) using the formalism developed in previous sections. This paper does not touch on the physics of the devices, and therefore does not require any notion of quantum mechanics. Numerical examples on an implementation of Grover's algorithm using open-source software are provided.\n\n==\n\nChatterjee 2003:\n\nTitle: Introduction to Quantum Computation\n\nAbstract: This is an introductory review on the basic principles of quantum computation. Various important quantum logic gates and algorithms based on them are introduced. Quantum teleportation and decoherence are discussed briefly. Some problems, without solutions, are included.\n\n==\n\nChatterjee 2003:\n\nTitle: Introduction to Quantum Computation\n\nAbstract: This is an introductory review on the basic principles of quantum computation. Various important quantum logic gates and algorithms based on them are introduced. Quantum teleportation and decoherence are discussed briefly. Some problems, without solutions, are included.", "final_summary": "The collected papers provide a comprehensive introduction to quantum computing, each focusing on different aspects of the field. Hey (1999) introduces the basic ideas of quantum computation, discussing quantum logic gates, qubits, and registers, and emphasizing the role of quantum entanglement. Kaye (2006) provides a broader overview, covering topics from linear algebra and the Dirac notation to quantum computational complexity theory and quantum error correction.\n\nPittenger (2000) offers an accessible introduction to quantum computing algorithms, discussing key algorithms such as Grover's search algorithm and Shor's factoring algorithm. Lo (2002) provides a pedagogical introduction to quantum information and computation, covering topics from quantum mechanics to quantum cryptography and fault-tolerant quantum computation.\n\nRieffel (2011) offers a comprehensive introduction to quantum computing, explaining the underlying concepts of quantum physics and the relevant mathematics. Nannicini (2017) provides a rigorous introduction to quantum computing intended for discrete mathematicians, focusing on the behavior of quantum computing devices and analyzing their main characteristics.\n\nFinally, Chatterjee (2003) provides an introductory review on the basic principles of quantum computation, introducing important quantum logic gates and algorithms, and briefly discussing quantum teleportation and decoherence.\n\nIn conclusion, these papers collectively provide a thorough introduction to quantum computing, covering the basic principles, key algorithms, and important concepts such as quantum entanglement and quantum error correction. They offer a range of perspectives, making them suitable for readers with different backgrounds and interests."}, {"query": "what is good water governance?", "paper_list_string": "Xu 2018:\n\nTitle: Good Water Governance for the Sustainable Development of the Arid and Semi-arid Areas of Northwest China\n\nAbstract: Water resources are of great importance for the sustainable development of the Arid and Semi-arid Areas of Northwest China. The theory of good water governance could provide inspirations for properly dealing with the water challenges of these areas. The integrated water resources management, the environmental water flow protection and the public participation are three major requirements of the good water governance. There are three major challenges for promoting the good water governance. The first is related to the clarification of the responsibilities of the water governance. The second is about balancing the diversified water needs. The third is about the optimal approach for improving the public participation. Three policy choices are proposed for promoting the good water governance. The integration of the water governance system, which emphasizes the four principles and the role of the basin management, could be a significant way of clarifying the responsibilities. Improving the water use efficiency could help the water supply for the ecological environment. The capacity building should be enhanced for facilitating the public participation.\n\n==\n\nGrigg 2011:\n\nTitle: Water governance: from ideals to effective strategies\n\nAbstract: Integrated water solutions require effective governance, as well as appropriate technologies and management instruments. While decision scenarios vary across a range of water demands and scales, common patterns of governance are involved. The paper explains these patterns in terms of how policy, empowerment and control are applied in distinct ways in different water management scenarios. Principles of effective water governance emerge from the case discussions, and illustrate how decision makers can identify the actions needed for policy, empowerment and control as well as make progress even while other institutional arrangements continue to evolve.\n\n==\n\nTortajada 2010:\n\nTitle: Water Governance: Some Critical Issues\n\nAbstract: This paper presents an analysis of the issues discussed at a special international workshop on water governance. While it is generally accepted that good governance for the water sector is essential, it is also clear that its implementation requires qualitative and quantitative factors, which may vary from one country to another. In order to objectively assess the opportunities and constraints of implementing good water governance practices, a group of selected international experts were invited to address this complex issue.\n\n==\n\nLautze 2011:\n\nTitle: Putting the cart before the horse: Water governance and IWRM\n\nAbstract: Water governance has emerged as perhaps the most important topic of the international water community in the 21st century, and achieving \u201cgood\u201d water governance is now a focus of both policy discourse and innumerable development projects. Somewhat surprisingly in light of this attention, there is widespread confusion about the meaning of the term \u201cwater governance\u201d. This paper reviews the history of the term's use and misuse to reveal how the concept is frequently inflated to include issues that go well beyond governance. Further, it highlights how calls to improve water governance often espouse predetermined goals that should instead be the very function of water governance to define. To help overcome this confusion, the paper suggests a more refined definition of water governance and related qualities of good water governance that are consistent with broader notions of the concepts. In light of the substantial resources allocated in its name, this paper's findings show there is significant potential to strengthen efforts at improving water governance.\n\n==\n\nSolanes 2006:\n\nTitle: Water governance for development and sustainability\n\nAbstract: Abstract This document aims to identify characteristics of water institutions which promote the sustainable integration of water, both as a resource and as service, into socioeconomic development. As this does not depend only on formal institutional factors, such as legislation and organizational structure, there are also references to dynamic conditions, such as socioeconomic circumstances and the quality of the administration, summarized in the concept of governance, understood as the capability of a social system to mobilize energies, in a coherent manner, for the sustainable development of water resources. As human society becomes ever more complex and the intensity of human impact on natural resources becomes more severe, the need to integrate the different elements of water management becomes imperative. It is for this reason that effective water governance will be more and more closely linked to integrated water resources management. The specific objectives of this paper are: (i); to contribute to focusing the regional debate on those aspects of water institutions and macroeconomic policies which are particularly critical for Latin American and Caribbean countries; (ii); to promote the formulation of a regional position that genuinely reflects its situation, visions, aspirations and problems; (iii); to promote a critical and balanced analysis of legislation, regulatory frameworks and public policies for water resources management and provision of related public services; and (iv); to make available in English a summary of the water-related research carried out by the Division of Natural Resources and Infrastructure of the Economic Commission for Latin America and the Caribbean (ECLAC);.\n\n==\n\nFranks 2007:\n\nTitle: Water governance and poverty\n\nAbstract: This paper engages with policy on meeting development goals for water through interventions, which promote good governance. Addressing an under-researched area, we propose a new analytical framework for understanding water governance, not as a set of abstract principles, but as interlinked processes with variable practical outcomes for poor people. The framework is informed by theories of governance, institutions and structuration, empirical research and field insights. We apply the framework to a case in south-western Tanzania, and we identify a range of issues for further research, particularly relating to water access for the poor.\n\n==\n\nDukhovny 2011:\n\nTitle: Ways to improve water governance\n\nAbstract: Given the growing destabilizing factors (climate change demographic growth financial instability desertification) the water supply for future development of our planet will become even more an issue. In order to guarantee access to water for different countries regions and individuals it is necessary to elaborate and adopt for governance in general a body of legal financial institutional and economic tools and mechanisms that would become an indisputable set of rules as a kind of Bible for modern humankind and as a way to survive. Water governance should prevent any attempt to use water as a means of political or economic pressure or as a good like oil transformed into the subject of profiteering. Water is a holy thing and we all must take care of and save it so that everyone might have access to water in adequate quantity and quality. Copyright \u00a9 2011 John Wiley & Sons Ltd.\n\n==\n\nAraral 2013:\n\nTitle: Water Governance 2.0: A Review and Second Generation Research Agenda\n\nAbstract: Water scholars and practitioners generally agree that improving water governance is the key to addressing water insecurity in developing countries. We review the literature on water governance and argue for a second-generation research agenda, which pays more attention to the study of incentive structures, is multi and inter-disciplinary in orientation and with clear policy implications. We then illustrate how theories drawn from public economics, new institutional economics, political economy and public administration can help diagnose the challenges of integrated water resources management, improving efficiency of water utilities, privatization of utilities and public-private partnerships, water pricing reforms, virtual waters/water trading, among others. We conclude that these tools can help advance the second-generation research agenda on water governance.", "final_summary": "Good water governance is a complex concept that involves a range of principles and practices. Xu (2018) emphasizes the importance of integrated water resources management, environmental water flow protection, and public participation. These elements are crucial in dealing with water challenges and promoting sustainable development.\n\nGrigg (2011) further elaborates on the role of policy, empowerment, and control in effective water governance. These elements are applied in distinct ways in different water management scenarios, highlighting the need for adaptability in governance strategies.\n\nTortajada (2010) underscores the necessity of good governance in the water sector, noting that its implementation depends on both qualitative and quantitative factors that may vary across countries. This suggests that good water governance is not a one-size-fits-all concept but must be tailored to specific contexts.\n\nLautze (2011) calls for a more refined definition of water governance, arguing that the term is often inflated to include issues beyond governance. This highlights the need for clarity and precision in discussions and implementations of water governance.\n\nSolanes (2006) identifies characteristics of water institutions that promote sustainable integration of water into socioeconomic development. This underscores the role of governance in mobilizing energies for sustainable water resources development.\n\nFranks (2007) proposes a new analytical framework for understanding water governance as interlinked processes with variable outcomes for the poor. This perspective emphasizes the social equity dimension of water governance.\n\nDukhovny (2011) suggests that water governance should prevent the use of water as a means of political or economic pressure and should ensure access to water for all. This highlights the ethical and human rights dimensions of water governance.\n\nFinally, Araral (2013) advocates for a second-generation research agenda on water governance that focuses on incentive structures and is multidisciplinary in orientation with clear policy implications. This suggests that future research on water governance should be comprehensive, interdisciplinary, and policy-oriented.\n\nIn conclusion, good water governance is a multifaceted and dynamic concept that must be tailored to specific contexts and needs. It involves a balance of policy, control, and empowerment, and requires a clear understanding of responsibilities and the needs of all stakeholders, including the most vulnerable populations. It also requires ongoing research and adaptation to changing socioeconomic and environmental conditions (Xu, 2018; Grigg, 2011; Tortajada, 2010; Lautze, 2011; Solanes, 2006; Franks, 2007; Dukhovny, 2011; Araral, 2013)."}, {"query": "Emerging trends in Enterprise DevSecOps", "paper_list_string": "Rajapakse 2022:\n\nTitle: Collaborative Application Security Testing for DevSecOps: An Empirical Analysis of Challenges, Best Practices and Tool Support\n\nAbstract: DevSecOps is a software development paradigm that places a high emphasis on the culture of collaboration between developers (Dev), security (Sec) and operations (Ops) teams to deliver secure software continuously and rapidly. Adopting this paradigm effectively, therefore, requires an understanding of the challenges, best practices and available solutions for collaboration among these functional teams. However, collaborative aspects related to these teams have received very little empirical attention in the DevSecOps literature. Hence, we present a study focusing on a key security activity, Application Security Testing (AST), in which practitioners face difficulties performing collaborative work in a DevSecOps environment. Our study made novel use of 48 systematically selected webinars, technical talks and panel discussions as a data source to qualitatively analyse software practitioner discussions on the most recent trends and emerging solutions in this highly evolving field. We find that the lack of features that facilitate collaboration built into the AST tools themselves is a key tool-related challenge in DevSecOps. In addition, the lack of clarity related to role definitions, shared goals, and ownership also hinders Collaborative AST (CoAST). We also captured a range of best practices for collaboration (e.g., Shift-left security), emerging communication methods (e.g., ChatOps), and new team structures (e.g., hybrid teams) for CoAST. Finally, our study identified several requirements for new tool features and specific gap areas for future research to provide better support for CoAST in DevSecOps.\n\n==\n\nBrunnert 2015:\n\nTitle: Performance-oriented DevOps: A Research Agenda\n\nAbstract: DevOps is a trend towards a tighter integration between development (Dev) and operations (Ops) teams. The need for such an integration is driven by the requirement to continuously adapt enterprise applications (EAs) to changes in the business environment. As of today, DevOps concepts have been primarily introduced to ensure a constant flow of features and bug fixes into new releases from a functional perspective. In order to integrate a non-functional perspective into these DevOps concepts this report focuses on tools, activities, and processes to ensure one of the most important quality attributes of a software system, namely performance. \nPerformance describes system properties concerning its timeliness and use of resources. Common metrics are response time, throughput, and resource utilization. Performance goals for EAs are typically defined by setting upper and/or lower bounds for these metrics and specific business transactions. In order to ensure that such performance goals can be met, several activities are required during development and operation of these systems as well as during the transition from Dev to Ops. Activities during development are typically summarized by the term Software Performance Engineering (SPE), whereas activities during operations are called Application Performance Management (APM). SPE and APM were historically tackled independently from each other, but the newly emerging DevOps concepts require and enable a tighter integration between both activity streams. This report presents existing solutions to support this integration as well as open research challenges in this area.\n\n==\n\nMao 2020:\n\nTitle: Preliminary Findings about DevSecOps from Grey Literature\n\nAbstract: Context: Emerging from the agile culture, DevOps particularly emphasizes development and deployment speed to achieve rapid value delivery, which however brings some security risks to the software development process. DevSecOps is an extension of DevOps, which is considered as a means to intertwine development, operation and security. Some companies with security concerns begin to take DevSecOps into consideration when it comes to the application of DevOps. Objective: The goal of this study is to report the state-of-the-practice of DevSecOps as well as calling for academia to pay more attention to DevSecOps. Method: Using Google search engine to collect articles on DevSecOps, we conducted a Grey Literature Review (GLR) on the selected articles. Results: Whilst there exists three major software security risks in DevOps, the establishment of DevOps pipeline provides opportunities for software security activities. Based on the preliminary consensus that DevSecOps is an extension of DevOps, it is observed that the interpretations of DevSecOps can be classified into three core aspects, which are: DevSecOps capabilities, cultural enablers, and technological enablers. Furthermore, to materialize the interpretations into daily software production activities, the recommended DevSecOps practices we obtain from Grey Literature (GL) can be categorized in terms of process, infrastructure and collaboration. Conclusion: Although DevSecOps is getting increasing attention by industry, it is still in its infancy and needs to be promoted by both academia and industry.\n\n==\n\nMohan 2016:\n\nTitle: SecDevOps: Is It a Marketing Buzzword? - Mapping Research on Security in DevOps\n\nAbstract: DevOps is changing the way organizations develop and deploy applications and service customers. Many organizations want to apply DevOps, but they are concerned by the security aspects of the produced software. This has triggered the creation of the terms SecDevOps and DevSecOps. These terms refer to incorporating security practices in a DevOps environment by promoting the collaboration between the development teams, the operations teams, and the security teams. This paper surveys the literature from academia and industry to identify the main aspects of this trend. The main aspects that we found are: definition, security best practices, compliance, process automation, tools for SecDevOps, software configuration, team collaboration, availability of activity data and information secrecy. Although the number of relevant publications is low, we believe that the terms are not buzzwords, they imply important challenges that the security and software communities shall address to help organizations develop secure software while applying DevOps processes.\n\n==\n\nAlawneh 2022:\n\nTitle: Expanding DevSecOps Practices and Clarifying the Concepts within Kubernetes Ecosystem\n\nAbstract: DevSecOps principles and practices come with promising futures, which are related to integrating security by design within organizational processes. These include development, deployment, and operational management. For example, DevSecOps practices help securely speed up the processes of application delivery; resilience; elasticity; availability, and re-liability. Despite the promising future of DevSecOps, it comes with several challenges, and one of these is about establishing robust mechanisms for integrating security by design within the existing DevOps practices. This paper unifies and redefines DevSecOps practices, and then provides several real-life examples clarifying what it means to integrate security by design within each practice. Finally, it clarifies the concepts by illustrating the roles of DevSecOps practices in securing the Kubernetes ecosystem.\n\n==\n\nRajapakse 2022:\n\nTitle: Challenges and solutions when adopting DevSecOps: A systematic review\n\nAbstract: Abstract Context: DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge. Objective: This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future. Method: We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data. Results: We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied. Conclusions: We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.\n\n==\n\nRajapakse 2022:\n\nTitle: Challenges and solutions when adopting DevSecOps: A systematic review\n\nAbstract: Abstract Context: DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge. Objective: This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future. Method: We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data. Results: We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied. Conclusions: We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.\n\n==\n\nNadgowda 2021:\n\nTitle: tapiser\u00ed: Blueprint to modernize DevSecOps for real world\n\nAbstract: Micro-service application pattern has revolutionize the overall software delivery lifecycle. Modularization has allowed breaking monolithic application into independent components that can be developed faster and automation in CICD has enabled high velocity deployment of applications to the cloud. Such a modernization has mandated a need to put security at the center of the workflow from code to container, giving rise to the DevSecOps paradigms. Although effectiveness of the existing DevSecOps solutions is limited by lack of good development practices and narrow scope where it is applied for security analytic only around code hygiene, like vulnerability scanning, license auditing, etc. We discuss our survey on these challenges and highlight their security implications. In tapiser\u00ed we then present wider perspective to design a DevSecOps solution that addresses prevalent challenges around supply chain security, build security for micro-services, ensures integrity of the pipelines themselves and brings transparency and auditability to the process.", "final_summary": "Emerging trends in Enterprise DevSecOps are characterized by a shift towards collaboration, automation, and security integration. Rajapakse (2022) emphasizes the importance of collaboration in DevSecOps, highlighting the need for clear role definitions, shared goals, and ownership. The study also identifies the lack of collaborative features in Application Security Testing (AST) tools as a significant challenge.\n\nBrunnert (2015) discusses the integration of non-functional perspectives into DevSecOps, focusing on system properties such as response time, throughput, and resource utilization. The study suggests that the integration of Software Performance Engineering (SPE) and Application Performance Management (APM) is crucial in the DevOps concepts.\n\nMao (2020) and Mohan (2016) discuss the importance of incorporating security practices in a DevOps environment and the need for collaboration between development, operations, and security teams. They identify three core aspects: capabilities, cultural enablers, and technological enablers. They also highlight the need for academia and industry to promote the integration of security in DevOps practices.\n\nAlawneh (2022) discusses the promising future of integrating security by design within organizational processes in the DevSecOps practices. The study provides real-life examples of integrating security within each practice, particularly in securing the Kubernetes ecosystem.\n\nRajapakse (2022) provides a systematic review of the challenges and solutions in adopting DevSecOps. The study identifies 21 challenges and 31 specific solutions, emphasizing the need for automation and the balance between speed of delivery and security. The study also highlights the need for more research on automating traditionally manual security practices.\n\nLastly, Nadgowda (2021) presents a wider perspective on designing a DevSecOps solution that addresses challenges around supply chain security, build security for micro-services, and ensures the integrity of the pipelines. The study emphasizes the need for transparency and auditability in the process.\n\nIn conclusion, the emerging trends in Enterprise DevSecOps revolve around collaboration, automation, and security integration. The studies collectively highlight the need for clear role definitions, shared goals, and ownership, as well as the integration of non-functional perspectives and security by design within organizational processes. They also underscore the importance of academia and industry in promoting the integration of security in DevOps practices and the need for more research on automating traditionally manual security practices."}, {"query": "\"virginia woolf\" \"lower middle class\"", "paper_list_string": "Bailey 1999:\n\nTitle: White Collars, Gray Lives? The Lower Middle Class Revisited\n\nAbstract: The lower middle class has long had a bad press, for in common with other subaltern groups it has been more represented from without than within. Thus Victorian writers faced with the disquieting irruption of a new breed of petty bourgeois shop and office workers devised a parodic discourse of littleness, whose feminized tropes rendered the clerk as socially insignificant as the sequestered Victorian woman. George Grossmith's comic classic, Diary of a Nobody, pilloried the new social type in Mr. Pooter, whose smaller-than-life adventures stood for all that was ineffectual, pretentious, and banal in his class. Social commentators held the lower middle class responsible for the degeneration of civilization itself, stifled by their suburban respectability and addiction to mass culture. In Howard's End, E. M. Forster drew the clerk, Leonard Bast, with some sympathy but made him the book's major casualty, while belittling a class whose education was learned \u201cfrom the outside of books.\u201d In the interwar years the Marxist poet Christopher Caudwell likened the petty bourgeois world to \u201ca terrible stagnant marsh, all mud and bitterness, without even the saving grace of tragedy.\u201d George Orwell's fictional antihero from the same period, the insurance salesman George Bowling, characterizes the men of his class as \u201cTories, yes-men and bumsuckers.\u201d It is still hard to hide a certain relish in repeating such charges, for putting the boot in on the lower middle class has long been the intellectual's blood sport, an exorcism, so we are told, of the guilty secret so many of us share as closet petit bourgeois denying our own class origins.\n\n==\n\nFernald 2005:\n\nTitle: A Feminist Public Sphere? Virginia Woolf's Revisions of the Eighteenth Century\n\nAbstract: Since the historicist turn in scholarship on modernism, critics have focused on the ways modernist writers have marketed themselves. The modernist writers who expressed disdain for mass culture have been shown to have used and depended on the tools of advertising, marketing, professionalization, and self-promotion that was provided by mass culture. Thus, Peter McDonald argues that \u201cthe cultural divisions between the high-class reviews and the illustrated monthlies were not always as rigid as might be supposed,\u201d and Lawrence Rainey asserts that modernist publishing depended upon an intermediate economic stage, such as an early limited or subscription-based edition, during which a text\u2019s cultural value rose, potentially helping create demand for broader sales.1 When this sociohistorical approach focuses on a single writer\u2019s career, it shows how he or she fits into, or, more often, attempts to subvert or challenge the cultural categories of high, low, and middlebrow. Virginia Woolf was, of course, a woman, and thus something of an outsider in the London literary scene; she was also the daughter of Victorian man-of-letters and editor Leslie Stephen, and thus, the ultimate insider. This combination makes her career an ideal case for further study. As one recent critic puts it, \u201cWoolf entered public discourse by the side door.\u201d2\n\n==\n\nDaugherty 2006:\n\nTitle: \"You See You Kind of Belong to Us, and What You Do Matters Enormously\": Letters from Readers to Virginia Woolf\n\nAbstract: Writing to Virginia Woolf about Three Guineas from Springfield, MA in 1938, Agnes K. Potter says her comments are from \"the point of view of this ordinary reader\" (Snaith, \"Three Guineas Letters\" 97). Q. D. Leavis, however, would have us believe that Woolf did not have ordinary readers; Three Guineas \"is a conversation between [Woolf] and her friends\" (272). Woolf's careful and honest description of her class position leads to Leavis's assumption that all Woolf's readers must be of Woolf's own class. Anna Snaith's edition of the letters written to Woolf after the publication of Three Guineas proves that such a narrow construction of Woolf's readership is not accurate. (1) Similarly, Melba Cuddy-Keane's investigations of the readers \"spotting\" The Common Reader (Virginia Woolf 110-14) and of individual readers like John Farrelly (\"From Fan-Mail\" 3-32) succeed in \"break[ing] down categories that have identified high culture with high class\" and show that definitions of Woolf's readership must include respect for \"both the intellectual impulse and the intellectual accomplishment of non-privileged, non-specialist readers\" (\"Imbricated\" 5). Following Q. D. Leavis's lead, however, Jonathan Rose recently argues that Woolf's essay \"Middlebrow\" calls for cultural triage and asserts that modernists made literature difficult to make the common reader \"illiterate once again\" and to preserve \"a body of culture as the exclusive property of a coterie\" (394). He does caution, though, that \"no two individual reading histories [are] alike,\" that generalizations about readers, though not \"completely groundless,\" neglect the \"more complicated and ambiguous\" use of literacy, and that \"[t]he only workable method is to consult the readers themselves\" (367). Exactly. As Anna Snaith and Melba Cuddy-Keane discovered, one of the best ways to consult readers themselves is through their letters to Woolf. (2) Because, as Helen Waddell points out when she encloses someone else's letter about The Waves in her own note to Woolf, \"in spite of all that has already been said in print, there is something in the manuscript word and in the circumstances of the writing that makes it valuable\" (Letter 78). I discovered the same thing in the summer of 2001. At a session called \"Archives in the Age of Mechanical Reproduction\" during the Eleventh Annual Conference on Virginia Woolf at the University of Bangor in Wales, Bet Inglis, the retired Assistant Librarian in the Manuscripts Section at the University of Sussex, talked about the treasures in the Monks House Papers and casually mentioned that someone should take a look at the correspondence between Virginia Woolf and Elizabeth Bowen housed there. When I got to the Sussex archives the next week, planning to spend the entire summer on the drafts of early reviews written by Virginia Stephen, I thought I might take an afternoon to follow up on her suggestion. That digression quickly became an obsession, an archive-junkie race through the correspondence to Virginia Woolf located in Letters III of the Monks House Papers (SxMs 18). But what intrigued me, what I became addicted to, were not the letters from Elizabeth Bowen, engaging as they are. No, although the known correspondents in the authornamed files in Letters III certainly claimed some of my attention, what I could not stop reading and transcribing that crazed summer were the letters in a box tantalizingly called Correspondence of Various Persons re: Books, Articles. (3) Woolf's readers came alive as I read their letters, and I could not turn my back on them. \"[G]iving a voice ... to the silent reader\" (Carr), this collection of letters grows out of and builds on the fine work of Snaith, Cuddy-Keane, and Oldfield, increases the number of actual (as opposed to imagined) readers Woolf scholars can consult, and moves Woolf studies, particularly studies of her reception, another step closer to a full record of letters written to Virginia Woolf about her work and thus to a more accurate view of the \"far wider circle\" Woolf hoped to reach (L6 420). \u2026\n\n==\n\nSquier 1983:\n\nTitle: \"The London Scene\": Gender and Class in Virginia Woolf's London\n\nAbstract: Whether she thought it \"the most beautiful place on the face of the earth\" or \"the very devil,\" to Virginia Woolf the city of London was the focus for an intense, often ambivalent, lifelong scrutiny. Not only did she make her home there for nearly all of her fifty-nine years-first in the narrow streets of Kensington and then in the spacious squares of Bloomsbury-but she found it a powerfully evocative figure in the literary tradition within which she wrote. And one of the most powerful nonfiction representations of Virginia Woolf's response to London was the series of six essays which appeared a little over fifty years ago, from December 1931 to December 1932, in the magazine Good Housekeeping. As their titles indicate, the essays surveyed the highs and lows of the city: \"The Docks of London,\" \"Oxford Street Tide,\" \"Great Men's Houses,\" \"Abbeys and Cathedrals,\" \" 'This is the House of Commons,'\" and \"Portrait of a Londoner.\" Plotless, descriptive, slight as these essays seemed at the time to Woolf, to readers of today the \"London Scene\" essays are fascinating, for as I will show, they reveal Virginia Woolf's ambivalence about identity, social position, and access to material possessions, and they contain the strategies forged to accommodate her changing sense of self and social place without alienating the Good Housekeeping audience. Although the \"London Scene\" essays celebrate a conventionally modernist setting, the city, they are anything but conventionally modernist in their approach. At their best they subvert the often complacent genre of the urban travelogue to portray gender and class relations in the modern city.1 Woolf struggled with conflicting identifications in the \"London Scene\" essays, between insiders (men, the upper classes) and outsiders (women, the working classes), and she used a number of different\n\n==\n\nChilders 2009:\n\nTitle: Virginia Woolf on the Outside Looking Down: Reflections on the Class of Women\n\nAbstract: Sur la contradiction politique/esthetique et les limites du feminisme woolfien. Lecture de Three Guineas et la preface de Life as We Have Known It\n\n==\n\nBesnault-Levita 2012:\n\nTitle: Modernist Short Fiction by Women: The Liminal in Katherine Mansfield, Dorothy Richardson, May Sinclair and Virginia Woolf (review)\n\nAbstract: 214 publishers sought to emulate. Stephen Barkway\u2019s masterly, albeit brisk and businesslike, tour of Vita Sackville-West\u2019s 18-year association with the Press makes a similar point. While the best-selling novel The Edwardians (1930) earned much needed profits, signed limited editions of Sackville-West\u2019s poetry bolstered the firm\u2019s cultural capital. Barkway is a superb guide to the hard-boiled gentility that typified business dealings between Leonard and Sackville-West, severed shortly after Virginia\u2019s death by a poignant rejection letter. Southworth\u2019s contribution to the volume is strong but her characterizations of the \u201cworking-class voices\u201d (229) promoted by the Press prompts me to wonder how reliably she can parse British class distinctions. R. M. Fox, an Oxford educated son of a headmistress, was no more working class than George Orwell. Dealing with the Welsh poet Huw Menai, an ex-miner, Southworth provides a telling observation regarding the \u201cfaddishness of his London supporters, who dropped him when he failed to play up his colliery connections\u201d (224). The \u201cworking-class\u201d credentials paraded by the Hogarth Press were always tinged by an element of patrician patronage.4 It is laudable that these wide-ranging case studies should concentrate on the work of writers and artists who, for the most part, have rarely been examined in relation to the Hogarth Press. Southworth\u2019s intelligently edited collection successfully challenges and complicates received wisdom about the Woolfs as publishers. It also protests too much: Willis\u2019s detailed history of the Press remains indispensable.\n\n==\n\nCuddy\u2010Keane 2003:\n\nTitle: Virginia Woolf, the Intellectual, and the Public Sphere: Democratic highbrow: Woolf and the classless intellectual\n\nAbstract: \u201cWhen I use a word,\u201d Humpty Dumpty said, in rather a scornful tone, \u201cit means just what I choose it to mean \u2013 neither more nor less.\u201d \u201cThe question is,\u201d said Alice, \u201cwhether you can make words mean so many different things.\u201d \u201cThe question is,\u201d said Humpty Dumpty, \u201cwhich is to be master \u2013 that's all.\u201d Lewis Carroll, Alice in Wonderland It is also that the variations and confusions of meaning are not just faults in a system, or errors of feedback, or deficiencies of education. They are in many cases, in my terms, historical and contemporary substance. Indeed they have often, as variations, to be insisted upon, just because they embody different experiences and readings of experience, and this will continue to be true, in active relationships and conflicts, over and above the clarifying exercises of scholars or committees. What can really be contributed is not resolution but perhaps, at times, just that extra edge of consciousness. Raymond Williams, Keywords CULTURAL KEYWORDS To write about Virginia Woolf as a democratic highbrow is to invoke controversy. My approach challenges the notion of the modernist writer as aloof from the public and the idea of intellectuals as an elite; it resists identifications of the popular exclusively with the world of commodities and entertainment; it rejects the notion that declining standards must inevitably follow from \u201cthe mass.\u201d And, as I state in my introduction, my approach contests a number of prevailing constructions of \u201cVirginia Woolf.\n\n==\n\nCuddy\u2010Keane 2003:\n\nTitle: Virginia Woolf, the Intellectual, and the Public Sphere\n\nAbstract: Acknowledgments List of abbreviations Introduction: a wider sphere Part I. Cultural Contexts: 1. Democratic highbrow: Woolf and the classless intellectual 2. Woolf, English studies and the making of the (new) common reader Part II. Critical Practice: 3. Woolf and the theory and pedagogy of reading Postscript: intellectual work today Notes Bibliography Index.", "final_summary": "The collected papers provide a nuanced exploration of Virginia Woolf's relationship with the lower middle class. Bailey (1999) provides a broader context, discussing the general perception of the lower middle class in literature, which was often negative and dismissive. This sets the stage for understanding the unique position Woolf occupied as a writer.\n\nFernald (2005) and Daugherty (2006) both delve into Woolf's unique position as an insider and outsider in the London literary scene. Fernald (2005) highlights Woolf's dual role as a woman and the daughter of a Victorian man-of-letters, which allowed her to enter public discourse \"by the side door.\" Daugherty (2006) further explores this by examining the diverse readership of Woolf's work, which extended beyond her own class.\n\nSquier (1983) discusses Woolf's ambivalence about identity, social position, and material possessions in her response to London. The abstract of Childers (2009) does not provide enough information to determine its focus on Woolf's portrayal of class and gender.\n\nCuddy\u2010Keane (2003) challenges the notion of Woolf as an aloof modernist writer and instead presents her as a democratic highbrow. The abstract of Besnault-Levita (2012) does not provide enough information to determine its focus on Woolf's portrayal of the liminal in her short fiction.\n\nIn conclusion, these papers collectively suggest that Virginia Woolf had a complex relationship with the lower middle class, both in her personal life and in her literary work. She navigated this class boundary in a unique way, which allowed her to challenge prevailing notions and portray the lower middle class in a nuanced manner in her work."}, {"query": "How confidence do student paramedics feel when performing a lifting assessment?", "paper_list_string": "Anderson 2019:\n\nTitle: Paramedic student confidence, concerns, learning and experience with resuscitation decision-making and patient death: A pilot survey.\n\nAbstract: BACKGROUND\nAround the world, many paramedics are authorised to withhold or terminate resuscitation. Research indicates this can be a challenging part of their role. Little is known about graduating paramedic student confidence, concerns and learning in this area.\n\n\nMETHODS\nAn online cross-sectional survey of students nearing completion of a paramedic degree in New Zealand, including piloting of a newly-developed confidence scale.\n\n\nRESULTS\nSeventy-two participants reported varying exposure to termination of resuscitation and patient death. Participants felt most confident providing technical procedurally-based care and least confident with non-technical skills. Participants' greatest concerns included making 'the right call', supporting grieving family, controlling emotions and encountering conflict. Clinical exposure with supportive mentoring, clinical simulation, peer reflection and resolved personal experience with death, were considered most useful for professional development.\n\n\nCONCLUSIONS\nExposure to termination of resuscitation and management of the scene of a patient death is variable amongst graduating paramedics. Novice paramedics may benefit from opportunities to observe and rehearse non-technical skills including delivering death notification and communicating with bystanders and bereaved family. The Confidence with Resuscitation Decision-Making and Patient Death Scale has favourable psychometric properties and utility as an outcome measure for future research in this area.\n\n==\n\nSandy 2021:\n\nTitle: Paramedic students\u2019 confidence and satisfaction with clinical simulations of an emergency medical care programme in South Africa: A cross-sectional study\n\nAbstract: Background There has been an increase in the use of clinical simulations as instructional tools in healthcare education. This is because of their role in ensuring patients\u2019 safety and quality-care provision. Aim This study investigated the paramedic students\u2019 satisfaction and self-confidence in the clinical simulation of an emergency medical care programme. Setting The study was conducted at the Durban University of Technology in the KwaZulu-Natal Province of South Africa. The paramedic students\u2019 satisfaction and self-confidence in the clinical simulation of an emergency medical care programme were the focus of the study. Methods The study used a cross-sectional research design. A convenience sampling method was used to select the 83-paramedic students who participated in the study. Data were collected between July and September 2017 using a structured questionnaire. Descriptive statistics (frequencies and percentages and Spearman\u2019s rank-order correlation coefficient) and an inferential test, ordinal logistic regression analysis, were used for data analysis. Results High levels of paramedic students\u2019 satisfaction and self-confidence in simulation activities were reported. Generally, the paramedic students\u2019 demographics were associated with the satisfaction and self-confidence variables with p-values \u2264 0.04. Emergency medical care training undertaken by the paramedic students was significantly associated with self-confidence (p = 0.00). Conclusion Clinical simulation can bridge the theory-practice gap for paramedic students. It is a hands-on approach that promotes students learning of clinical skills through reflection.\n\n==\n\nHolmes 2017:\n\nTitle: Student Paramedic Anticipation, Confidence and Fears: Do Undergraduate Courses Prepare Student Paramedics for the Mental Health Challenges of the Profession?\n\nAbstract: Introduction This study explores the preparedness of undergraduate student paramedics for the mental health challenges of the paramedic profession from the perspective of course coordinators and their students. Methods Two surveys were developed and administered to course coordinators and students of the 16 undergraduate degree paramedicine courses across Australia and New Zealand. Sixteen course coordinators and 302 students responded. Results Results illustrate there was widespread recognition for the need to include preparation for the mental health challenges of the profession within undergraduate courses. Furthermore, most course coordinators and students had a preference for this topic to be taught using multiple teaching modes with particular preference for teaching the topic via discussion and activity based education. Teaching the topic as a standalone unit was supported by more than a third of course coordinators (43%) and a third of students (32%). Conclusion Six themes were identified as positive by anticipants: caring for people, high acuity work, diversity of work and patients, making a difference to patients and their families, using clinical skills and knowledge and engaging with the community. Students were most confident about communicating with patients and using clinical skills and knowledge. Students were least confident about clinical decision making and the most commonly cited fear was making a clinical mistake. A significant proportion of students (16%) feared for their personal mental wellbeing and 14% reported they were least confident about personal mental health within the profession.\n\n==\n\nRoss 2014:\n\nTitle: Perceptions of Student Paramedic Interpersonal Communication Competence: A Cross-Sectional Study\n\nAbstract: Introduction Interpersonal communication skills are essential to the healthcare practitioner aiding in high quality, effective and safe clinical practice. Effective communication exerts a positive influence on the patient's physical and emotional status resulting in better patient outcomes and satisfaction. By identifying strengths and weaknesses, self-assessment of interpersonal communication skills can be used as an intervention tool to inform future curriculum renewal. The objective of this study was to identify paramedic students\u2019 perceptions of their interpersonal communication competence. Methods Second year paramedic students from Monash University (Victoria) were invited to participate in a survey that asked them to record perceptions of their interpersonal communication skills using the Interpersonal Communication Competence Scale (ICCS). The ICCS is a 30-item unipolar questionnaire using a Likert scale ranging from 1 (almost never) to 5 (almost always). Mean and standard deviations (SD) were used to report results. Results Fifty-six second year paramedic students participated in the study. Participants were predominantly aged less than 25 years (85.7%) and male n=36 (64.3%). Students reported \u2018often\u2019 or \u2018almost always\u2019 for the items: \u2018I put myself in others\u2019 shoes\u2019, n=46 (82%), mean=3.98 (SD 0.59); and, \u2018I let others know that I understand what they say\u2019, n=45 (80%), mean=3.96 (SD 0.66). Students reported \u2018sometimes\u2019, \u2018often\u2019 or \u2018almost always\u2019, for the items: \u2018I have trouble convincing others to do what I want them to do\u2019, n=55 (98%), mean=3.5 (SD 0.63); and, \u2018My mind wanders during conversations\u2019, n=41 (73%), mean=3.05 (SD 0.88). Conclusion Preliminary results suggest that student paramedics self-report their interpersonal communication skills highly apart from areas related to assertiveness and listening skills. These results could be indicative of student age, personality or experience level and warrant further research with larger sample sizes.\n\n==\n\nWilliams 2015:\n\nTitle: Are paramedic students ready to be professional? An international comparison study.\n\nAbstract: INTRODUCTION\nThe last decade has seen rapid advancement in Australasian paramedic education, clinical practice, and research. Coupled with the movements towards national registration in Australia and New Zealand, these advancements contribute to the paramedic discipline gaining recognition as a health profession.\n\n\nAIM\nThe aim of this paper was to explore paramedic students' views on paramedic professionalism in Australia and New Zealand.\n\n\nMETHODS\nUsing a convenience sample of paramedic students from Whitireia New Zealand, Charles Sturt University and Monash University, attitudes towards paramedic professionalism were measured using the Professionalism at Work Questionnaire. The 77 item questionnaire uses a combination of binary and unipolar Likert scales (1\u2009=\u2009Strongly disagree/5\u2009=\u2009Strongly agree; Never\u2009=\u20091/Always\u2009=\u20095).\n\n\nRESULTS\nThere were 479 students who participated in the study from Charles Sturt University n\u2009=\u2009272 (56.8%), Monash University n\u2009=\u2009145 (30.3%) and Whitireia New Zealand n\u2009=\u200962 (12.9%). A number of items produced statistically significant differences P\u2009<\u20090.05 between universities, year levels and course type. These included: 'Allow my liking or dislike for patients to affect the way I approach them' and 'Discuss a bad job with family or friends outside work as a way of coping'.\n\n\nCONCLUSIONS\nThese results suggest that paramedic students are strong advocates of paramedic professionalism and support the need for regulation. Data also suggest that the next generation of paramedics can be the agents of change for the paramedic discipline as it attempts to achieve full professional status.\n\n==\n\nBoyle 2008:\n\nTitle: Ambulance clinical placements \u2013 A pilot study of students' experience\n\nAbstract: BackgroundUndergraduate paramedic students undertake clinical placements in a variety of locations. These placements are considered an essential element for paramedic pre-employment education. However, anecdotal evidence suggests some students have not had positive experiences on their emergency ambulance placements. The objective of this study was to identify the type of experiences had by students during ambulance clinical placements and to provide feedback to the ambulance services.MethodsIn this pilot study we employed a cross-sectional study methodology, using a convenience sample of undergraduate paramedic students available in semester one of 2007 to ascertain the students' views on their reception by on-road paramedics and their overall experience on emergency ambulance clinical placements. Ethics approval was granted.ResultsThere were 77 students who participated in the survey, 64% were females, with 92% of students < 25 years of age and 55% < 65 Kg in weight. There was a statistically significant difference in average height between the genders (Male 179 cm vs Female 168 cm, p < 0.001). Clinical instructors were available to 44% of students with 30% of students excluded from patient management. Thirty percent of students felt there was a lot of unproductive down time during the placement. Paramedics remarked to 40% of students that they doubted their ability to perform the physical role of a paramedic, of this group 36% were advised this more than once.ConclusionThis study demonstrates that for a small group of students, emergency ambulance clinical placements were not a positive experience clinically or educationally. Some qualified paramedics doubt if a number of female students can perform the physical role of a paramedic.\n\n==\n\nJensen 2016:\n\nTitle: A Survey to Determine Decision-Making Styles of Working Paramedics and Student Paramedics.\n\nAbstract: OBJECTIVE\nTwo major processes underlie human decision-making: experiential (intuitive) and rational (conscious) thinking. The predominant thinking process used by working paramedics and student paramedics to make clinical decisions is unknown.\n\n\nMETHODS\nA survey was administered to ground ambulance paramedics and to primary care paramedic students. The survey included demographic questions and the Rational Experiential Inventory-40, a validated psychometric tool involving 40 questions. Twenty questions evaluated each thinking style: 10 assessed preference and 10 assessed ability to use that style. Responses were provided on a five-point Likert scale, with higher scores indicating higher affinity for the style in question. Analysis included both descriptive statistics and t tests to evaluate differences in thinking style.\n\n\nRESULTS\nThe response rate was 88.4% (1172/1326). Paramedics (n=904) had a median age of 36 years (IQR 29-42) and most were male (69.5%) and primary or advanced care paramedics (PCP=55.5%; ACP=32.5%). Paramedic students (n=268) had a median age of 23 years (IQR 21-26), most were male (63.1%) and had completed high school (31.7%) or an undergraduate degree (25.4%) prior to paramedic training. Both groups scored their ability to use and favourability toward rational thinking significantly higher than experiential thinking. The mean score for rational thinking was 3.86/5 among paramedics and 3.97/5 among paramedic students (p<0.001). The mean score for experiential thinking was 3.41/5 among paramedics and 3.35/5 among paramedic students (p=0.06).\n\n\nCONCLUSION\nWorking paramedics and student paramedics prefer and perceive that they have the ability to use rational over experiential thinking. This information adds to our current knowledge on paramedic decision-making and is potentially important for developing continuing education and clinical support tools.\n\n==\n\nLowery 2005:\n\nTitle: Role of peer support and emotional expression on posttraumatic stress disorder in student paramedics.\n\nAbstract: This exploratory study contrasted and tested the predictive value of the reverse buffering hypothesis of social support and the information processing model of posttraumatic stress disorder (PTSD) in an investigation of trauma-related symptomatology (TRS) in a single sample of 42 student paramedics. Participants completed several anonymous self-report measures of PTSD symptomatology, peer social support, and attitude toward emotional expression. Regression-based path analyses did not support either theory of PTSD in this population. A path model of PTSD in student paramedics was subsequently developed, indicating that a direct relationship exists between duty-related trauma exposure, dysfunctional peer social support, and students' negative attitudes toward emotional expression. This new model accounted for 30% of the variance in student paramedics' TRS.", "final_summary": "The confidence of student paramedics when performing a lifting assessment is a multifaceted issue, with various studies exploring different aspects of this topic. \n\nAnderson (2019) found that paramedic students felt most confident providing technical procedurally-based care, but less confident with non-technical skills. This suggests that while students may feel confident in their ability to perform a lifting assessment, they may lack confidence in the interpersonal and decision-making aspects of the task. \n\nSandy (2021) reported high levels of student satisfaction and self-confidence in clinical simulation activities, indicating that simulated practice could enhance confidence in tasks such as lifting assessments. \n\nHolmes (2017) found that students were most confident about communicating with patients and using clinical skills, but least confident about clinical decision making. This could imply that while students may feel confident in the physical act of performing a lifting assessment, they may lack confidence in their ability to make decisions based on the assessment. \n\nRoss (2014) found that student paramedics self-reported high interpersonal communication skills, apart from areas related to assertiveness and listening skills. This could impact their confidence when communicating the results of a lifting assessment to patients or colleagues. \n\nWilliams (2015) found that paramedic students strongly advocated for professionalism, suggesting that they understand the importance of performing tasks such as lifting assessments professionally and competently. \n\nBoyle (2008) found that some students had negative experiences on emergency ambulance placements, with some qualified paramedics doubting their ability to perform the physical role of a paramedic. This could potentially impact student confidence in performing physically demanding tasks such as lifting assessments. \n\nJensen (2016) found that both working paramedics and student paramedics prefer and perceive that they have the ability to use rational over experiential thinking. This could influence their confidence in making decisions based on lifting assessments. \n\nFinally, Lowery (2005) found a direct relationship between trauma exposure, dysfunctional peer social support, and students' negative attitudes toward emotional expression. This suggests that the emotional and social aspects of the job could impact student confidence in various tasks, including lifting assessments. \n\nIn conclusion, while student paramedics may feel confident in the technical aspects of performing a lifting assessment, they may lack confidence in the decision-making and interpersonal aspects of the task. Factors such as negative placement experiences, lack of assertiveness, and emotional challenges could also impact their confidence. Further research is needed to explore these issues in more depth and develop strategies to enhance student confidence."}, {"query": "tay-sachs and sulfated glycolipid", "paper_list_string": "Hoffman 1977:\n\nTitle: GLYCOSPHINGOLIPIDS IN FETAL TAY\u2010SACHS DISEASE BRAIN AND LUNG CULTURES\n\nAbstract: Abstract\u2014 A study was undertaken of the glycosphingolipids in cell cultures derived from cerebellum of Tay\u2010Sachs disease fetal brain in order to determine the suitability of such cell strains as a model for Tay\u2010Sachs disease. The glycosphingolipids in the Tay\u2010Sachs disease cultured cerebellar cells were compared with those found in normal cultured cerebellar cells, normal and Tay\u2010Sachs cultured lung cells, and normal and Tay\u2010Sachs fetal brain. The glycolipids were separated by TLC, then analyzed by GLC of the trimethylsilyi derivatives of the methylglycosides of the sugar moieties. In the cultured cerebellar lines, the predominant gangliosides were GM2, GM3, and GD3. There was a 4\u2010fold increase of GM2 in the Tay\u2010Sachs as compared with the normal line. Only GM3 and GD3 gangliosides were found in the Tay\u2010Sachs and the normal fetal lung cell cultures. The major neutral glycosphingolipids in all of the cultured cells which were analyzed were glucosylceramide, lactosylceramide, digalactosyl\u2010glucosylceramide, and globoside. When the Tay\u2010Sachs cerebellar cells were labelled with [1\u201014C]gluco\u2010samine, some radioactivity was observed in the trihexosylceramide band, indicating the presence of a small amount of a galactosamine\u2010containing trihexosylceramide which may be asialo\u2010GM2 (GA2). The trihexosylceramide in Tay\u2010Sachs fetal brain was identified as GA2 by GLC. Both Tay\u2010Sachs and normal fetal brain gangliosides were more complex than those found in the cultured cells. Long chain fatty acids (C24:0 and C24;1) predominated in all of the glycosphingolipids of the Tay\u2010Sachs and the normal cultured cerebellar cells. In contrast, the glycosphingolipids of Tay\u2010Sachs and normal fetal brain contained mainly the shorter chain fatty acids (C16:0, C18:0, and C18:1). The cerebrosides in both the Tay\u2010Sachs and normal fetal brains were mainly glucosylceramide with only small amounts of the galactosylceramide which predominates in infant brain. Cultured cells from the fetal Tay\u2010Sachs disease\n\n==\n\nBerman 1963:\n\nTitle: STUDIES ON BRAIN LIPIDS IN TAY\u2010SACHS\u2019DISEASE\u2014I ISOLATION OF TWO SIALIC ACID\u2010FREE GLYCOLIPIDS *\n\nAbstract: A NUMBER of glycolipids (in addition to cerebrosides) containing sphingosine, fatty acids and sugars (glucose, galactose and/or hexosamine) have been isolated from spleen and erythrocyte stroma of various species (KLENK and DEBUCH, 1959). Although lipids of this composition have not been identified with certainty in brain tissue, there is nevertheless some indication that they might be present in small amounts (WEISS, 1956; KLENK et al., 1957; SVENNERHOLM and RAAL, 1961). Two such glycolipids have now been isolated from brain tissue of patients with Infantile Amaurotic Familial Idiocy (Tay-Sach's disease). The main biochemical abnormality which characterizes this disease is an accumulation of gangliosides (KLENK, 1939); the two new compounds which have now been identified in brain tissue of Tay-Sachs' disease belong to the same class of lipids. The isolation and chemical composition of these two glycolipids are described in the present communication. Some of this work has appeared in preliminary reports (GATT and BERMAN, 1961 ; BERMAN and GATT, 1962).\n\n==\n\nToma 2005:\n\nTitle: Impaired sulphated glycosaminoglycan metabolism in a patient with GM-2 gangliosidosis (Tay-Sachs disease)\n\nAbstract: SummaryAn abnormal urinary excretion of sulphated glycosaminoglycans in a patient with GM-2 gangliosidosis (Tay-Sachs disease) is described. Besides the accumulation of GM-2 ganglioside in liver and lack of hexosaminidase A, the patient shows an abnormal urinary excretion of an iduronic acid-rich low molecular weight heparan sulphate. Also, no dermatan sulphate could be detected in the urine, whereas this compound was the main sulphated glycosaminoglycan in the liver of the patient. Heparan sulphate was the main glycosaminoglycan of normal liver. The total amount of sulphated glycosaminoglycans in the urine and liver of the patient did not differ significantly from the amounts found in the liver and urine of normal subjects. Several plasma glycosidases have been assayed and the activities did not differ significantly from the values obtained for the plasma of normal subjects.\n\n==\n\nSandhoff 1969:\n\nTitle: Variation of \u03b2\u2010N\u2010acetylhexosaminidase\u2010pattern in Tay\u2010Sachs disease\n\nAbstract: Tay-Sachs disease is characterized by an accumulation of two types of glycosphingolipids, on the one hand ganglioside GM? (GalNAc-&1,4-(NeuNAc-&2,3) -Gal+1,4-Glc_13-1,1{2-N-acyl) sphingosine), and on the other its asialo residue (GalNAc-/3-1,4-Gal_P1,4GlcQ1,1(2-N-acyl)sphingosine) in nerve tissue [ l-31 . The disease is assumed to involve a lack of the enzyme which catabolizes the stored substances, similar to that present in other sphingolipidoses [4,5]. In a special case of Tay-Sachs disease with visceral storage of kidney globoside (GalNAc-P_1,3-Gal-/3-1,4-Gal-&l,4 GlcQ-1 ,l (2-N-acyl)sphingosine) a general lack of /3-Nacetylhexosaminidase activity was found [6] (fig. Id). The stored glycosphingolipids (ganglioside GM, and its asialo residue in nerve tissue, and kidney globoside in the visceral organs) in this case had in common a terminal /3-glycosidic-bound N-acetylgalactosamine, being partially hydrolized by PN-acetylhexosaminidase-preparations [6,7]. The same substances also accumulate in the various tissues of conventional Tay-Sachs cases, although the storage level of the asialo residue of ganglioside GM, and of the kidney globoside is found to be lower [6]. Accordingly, the lack of some particular /3-N-acetylhexosaminidase was discussed as one of the possible causes for storage in cases of conventional Tay-Sachs disease [6]. In the present study the f3-N-acetylhexosaminidase pattern of 4 cases of conventional Tay-Sachs disease is des cribed. In three of the four cases a lack of the PN-ace-\n\n==\n\nHoffman 1978:\n\nTitle: Fetal tay-sachs disease brain cells in culture: lack of turnover in [14C]glucosamine-labeled GM2\n\n\nAbstract: Abstract The glycosphingolipids in a cultured cell strain derived from Tay-Sachs disease fetal cerebellum were pulse-labeled with radioactive [ 14 C]glucosamine. The turnover of individual gangliosides in the cells were followed during a 10 day period. All of the gangliosides except G M2 had a buildup of counts which peaked between 24 and 96 h, then decreased. In contrast, the counts incorporated into G M2 did not decrease during the entire course of the experiment. The lack of turnover in G M2 ganglioside provides support for the use of these cultured brain cells as a model for Tay-Sachs disease.\n\n==\n\nYu 1983:\n\nTitle: Characterization of some minor gangliosides in Tay\u2014Sachs brains\n\nAbstract: Abstract The ganglioside distribution of Tay\u2014Sachs brain was re-examined in detail. In both the gray and white matter, the levels of lipid-bound sialic acid were increased 6- and 10-fold, respectively, over normal infant brain, and approximately 90% of the total ganglioside was G M2 . The level of G M2 was increased about 90 times in gray matter and 220 times in white matter in comparison with that in normal controls. The level of G D1a -GaINAc was increased 19 times and 10 times in gray and white matter, respectively. The concentration of G D2 was increased about 4-fold in Tay-S-Sachs white matter. In addition, the G M3 level was increased 2.7 and 3.5 times and the G D3 level 2 and 2.4 times over normal gray and white matter, respectively. However, the levels of other complex gangliosides such as G M1 , G D1a , G D1b , G T1b and G Q1b decreased remarkably. Since G M2 , G D2 , G D1a -GalNAc and a recently characterized ganglioside G M1b -GalNAc possess a common N-acetylgalactosaminyl terminal structure, their accumulation in Tay\u2014Sachs brains is therefore consistent with the known hexosaminidase A deficiency. However, the accumulation of hexosamine-free G M3 and G D3 is not. The in vitro incorporation of N-acetylgalactosamine into G M3 to form G M2 was examined in a rat brain microsomal fraction in the presence of large amounts of other glycolipids. Acidic glycolipids were slightly stimulating and then became increasingly inhibitory when the molar ratio of lipid to substrate G M3 exceeded 10 to 1. Neutral glycolipids and the phospholipid, phosphatidylcholine, were inhibitory at all levels tested. The data suggest that the accumulation of G M3 and G D3 in Tay\u2014Sachs brains could be due to an inhibition of N-acetylgalactosaminyl-transferas by high levels of glycolipids, and the inhibition is not due to chelation of the obligate divalent cation necessary for the activity of this enzyme. The inhibition of this enzyme may also be responsible for the decreased levels of other complex gangliosides.\n\n==\n\nPlatt 1997:\n\nTitle: Prevention of lysosomal storage in Tay-Sachs mice treated with N-butyldeoxynojirimycin.\n\nAbstract: The glycosphingolipid (GSL) lysosomal storage diseases result from the inheritance of defects in the genes encoding the enzymes required for catabolism of GSLs within lysosomes. A strategy for the treatment of these diseases, based on an inhibitor of GSL biosynthesis N-butyldeoxynojirimycin, was evaluated in a mouse model of Tay-Sachs disease. When Tay-Sachs mice were treated with N-butyldeoxynojirimycin, the accumulation of GM2 in the brain was prevented, with the number of storage neurons and the quantity of ganglioside stored per cell markedly reduced. Thus, limiting the biosynthesis of the substrate (GM2) for the defective enzyme (beta-hexosaminidase A) prevents GSL accumulation and the neuropathology associated with its lysosomal storage.\n\n==\n\nSchneck 1976:\n\nTitle: Glycolipids in cultured fetal Tay-Sachs disease cerebellar cells.\n\nAbstract: Tay-Sachs disease (TSD) is a fatal, genetically determined disorder of sphingoglycolipid metabolism, associated with the absence of the lysosomal enzyme B-D-N-acetylhexosaminidase A (Hex A) (1). There is a massive accumulation of GM2 ganglioside, and its asialo derivative GA2 in cells of the central nervous system. A cell culture which reproduces these biochemical parameters would permit one to measure the effect of enzyme replacement therapy under controlled conditions that are not easily attainable in vivo. Although skin fibroblasts cultured from TSD patients lack Hex A, these cultured cells do not accumulate GM2 ganglioside (2). Since TSD is a neuronal lipid storage disease, and since fetal TSD brain has the characteristic glycolipid patterns found in infant TSD brain, a cell strain from fetal TSD cerebellum was established and the glycolipid patterns were evaluated by TLC and GLC (3). The cells were labelled with 14C-glucosamine in order to compare the metabolic activity of the gangliosides over an extended time period. The cells were also transformed with the oncogenic DNA SV-40 virus (4) since we wished to establish a permanent cell line for the study of this disease.", "final_summary": "The research question \"Tay-Sachs and sulfated glycolipid\" delves into the intricate relationship between the genetic disorder Tay-Sachs and the metabolism of sulfated glycolipids. \n\nTay-Sachs disease is characterized by a deficiency in the enzyme \u03b2-N-acetylhexosaminidase A (Sandhoff, 1969). This deficiency leads to a significant accumulation of GM2 gangliosides in the brain, as observed in studies by Hoffman (1977) and Yu (1983). Specifically, Hoffman (1977) found a four-fold increase of GM2 in Tay-Sachs disease cultured cerebellar cells compared to normal cells, while Yu (1983) noted a 90 to 220-fold increase in GM2 levels in different areas of Tay-Sachs brains compared to normal controls.\n\nIn addition to the accumulation of GM2 gangliosides, Toma (2005) reported an abnormal urinary excretion of sulphated glycosaminoglycans in a patient with Tay-Sachs disease. This suggests a potential impairment in the metabolism of these compounds in individuals with Tay-Sachs disease. This metabolic impairment is further supported by Hoffman's (1978) observation of a lack of turnover in GM2 gangliosides in Tay-Sachs disease cells.\n\nPlatt (1997) proposed a potential strategy to address this metabolic impairment. By limiting the biosynthesis of GM2 using an inhibitor, the accumulation of GM2 and the associated neuropathology were prevented in a mouse model of Tay-Sachs disease.\n\nIn conclusion, the papers collectively suggest that Tay-Sachs disease, characterized by a deficiency in the enzyme \u03b2-N-acetylhexosaminidase A, leads to an accumulation of GM2 gangliosides and potentially impaired metabolism of sulfated glycolipids. This metabolic impairment, specifically the accumulation of GM2 gangliosides, may be a potential target for treatment strategies in Tay-Sachs disease (Platt, 1997). Further research is needed to fully understand the mechanisms underlying this impairment and to develop effective treatments for Tay-Sachs disease."}, {"query": "\u201cscreen addiction in middle or high school students\u201d", "paper_list_string": "Garc\u00eda-Santill\u00e1n 2021:\n\nTitle: Addiction to the Smartphone in High School Students: How It\u2019s in Daily Life?\n\nAbstract: Nowadays, the serious situation that affects the entire world goes beyond the social, cultural, economic problems and other conflicts that occur day by day. These were left aside to move to a global alert; we refer to the pandemic crisis that all the nations of the world are facing. Confinement forced people all over the world to stay at home; therefore, communications through electronic devices became very necessary. This study does not seek to analyze the pandemic crisis; its purpose is to analyze the use that students give to their mobile phone, to determine if this has generated addiction, in addition to identifying if use differs in men and in women. Participants were 184 high school students enrolled in a public sector institution in the Port of Veracruz, Mexico. To obtain the data, the SAS-CV test was used. This contains questions related to the profile of the respondent and 10 items in Likert format. It was distributed via electronic devices for their response. The data were statistically analyzed using polychoric correlation matrices and factor analysis with component extraction. The main findings demonstrate the obtaining of three components: physiological, dependence and distraction, which account for 68% of the total variance, and it was also shown that there are no differences by gender.\n\n==\n\nJohnson 2022:\n\nTitle: Is Screen Addiction a Problem Among Rural School-Going Adolescents? A Cross-Sectional Study in South Karnataka\n\nAbstract: Background: Media, including mobile phones, computers, and social media, is a dominant force in the lives of adolescents, and has now penetrated the rural areas. Screen addiction (SA) is a pathological compulsive use of screen-based devices. Objectives: To estimate SA and its associated factors among school-going adolescents residing in a rural area in South Karnataka. Methods: A cross-sectional study was conducted among adolescents (10-19 years) in 4 schools of Solur Hobli, Ramanagara district, using a structured questionnaire including physical activity, family, and academic stressors. SA was assessed using Tao\u2019s Diagnostic Criteria for Internet Addiction. Depression was screened using Physical Health Questionnaire-9. Chi-square test and logistic regression were done for factors associated with SA. Results: Of the 335 adolescents, 20% felt preoccupied with screens, 8% felt restless, moody, and irritable when not using them, 26% used screen devices as a way of escaping problems or relieving irritable mood, 12% had depression, and 3.9% had SA. SA was significantly higher among depressed subjects (adjusted odds ratio [AOR] = 5.18 [1.48-18.13], P = .010), males (AOR = 12.54 [1.46-109.9], P = .021), adolescents who felt that their parents fought frequently (AOR = 4.21 (1.17-15.09), P = 0.027), and adolescents who did not participate in sports (AOR = 3.82 (confidence interval: 1.04-14.06], P = 0.044). Conclusion: While proportion of rural students with SA is still low, we need to prevent this from developing into a public health issue by recognizing SA and depression among students and develop targeted interventions to manage the same, including increasing awareness among teachers and students about SA and its link with depression.\n\n==\n\nChoi 2021:\n\nTitle: Association Between Screen Overuse and Behavioral and Emotional Problems in Elementary School Children\n\nAbstract: Objectives This study identified the association between excessive exposure to screen media and behavioral and emotional problems in elementary school students. Methods A total of 331 parents of children aged 7\u201310 years were recruited from \u201cThe Kids Cohort for Understanding of Internet Addiction Risk Factors in Early Childhood (K-CURE)\u201d study. Children\u2019s demographics, household media ownership, screen time, and behavioral/emotional problems were assessed using a parental questionnaire. Children\u2019s behavior/emotional problems were measured using the Korean version the of Child Behavior Checklist (K-CBCL) score. Results The total K-CBCL score in the screen overuse group was 51.18\u00b19.55, significantly higher than 47.28\u00b110.09 in the control group (t=2.14, p=0.05). For each subscale, the externalization score (51.65\u00b110.14, 48.33\u00b18.97, respectively; t=2.02, p<0.05), social problem score (55.41\u00b16.11, 53.24\u00b15.19, respectively; t=2.27, p<0.05), and rule breaking behavior score (55.71\u00b16.11, 53.24\u00b15.19, respectively; t=2.27, p<0.05) were significantly higher in the screen overuse group than in the control group. In addition, the screen overuse group also had a significantly higher usage rate than the control group, even if limited to smartphones, not only on weekdays (3.56\u00b12.08, 1.87\u00b12.02, respectively; t=-4.597, p<0.001) but also weekends (1.62\u00b10.74, 1.19\u00b10.83, respectively; t=-3.14, p=0.003). Conclusion The study suggested that screen media overuse patterns in children in Korea are particularly relevant to the excessive use of smartphones and are related to higher risks of emotional and behavioral problems.\n\n==\n\nSasmaz 2014:\n\nTitle: Prevalence and risk factors of Internet addiction in high school students.\n\nAbstract: AIM\nIn this study, the prevalence and risk factors of Internet addiction in high school students was investigated.\n\n\nMATERIAL AND METHOD\nThis cross-sectional study was performed in the Mersin Province in 2012. The study sample consisted of students attending high school in the central district of Mersin. The data were summarized by descriptive statistics and compared by a binary logistic regression.\n\n\nRESULTS\nOur study population included 1156 students, among whom 609 (52.7%) were male. The mean age of the students was 16.1 \u00b1 0.9 years. Seventy-nine percent of the students had a computer at home, and 64.0% had a home Internet connection. In this study, 175 (15.1%) students were defined as Internet addicts. Whereas the addiction rate was 9.3% in girls, it was 20.4% in boys (P < 0.001). In this study, Internet addiction was found to have an independent relationship with gender, grade level, having a hobby, duration of daily computer use, depression and negative self-perception.\n\n\nCONCLUSION\nAccording to our study results, the prevalence of Internet addiction was high among high school students. We recommend preventing Internet addiction among adolescents by building a healthy living environment around them, controlling the computer and Internet use, promoting book reading and providing treatment to those with a psychological problem.\n\n==\n\nAhmadi 2014:\n\nTitle: Prevalence of Addiction to the Internet, Computer Games, DVD, and Video and Its Relationship to Anxiety and Depression in a Sample of Iranian High School Students\n\nAbstract: Objective: The objective of this study was to assess the prevalence of addiction to the Internet, computer games, DVD, and video and its relationship to anxiety and depression in a sample of Iranian high school students. Methods: In this cross-sectional study 1020 high school students (males and females) were selected randomly from different areas of Shiraz city in southern Iran. They were interviewed according to the Diagnostic and Statistical Manual of Mental Disorders, 4th ed (DSM-IV) criteria. Results: About 50% of the students were females, 277 students (27.2%) were studying in the first year of high school, 242 (23.7%) were in the second year, and others in the third year. The prevalence of anxiety was significantly higher in females than in males (p < 0.05). The prevalence of anxiety was lower among students of the third year (p < 0.05). The prevalence of depression was significantly higher in students with lower economic status defined as family monthly income. Internet dependence was seen only in 5 students. The prevalence of anxiety was significantly higher in the students who used internet for chatting, amusement, and reading news (p < 0.05). The prevalence of anxiety was significantly higher in students who were DVD or video CD dependents (p < 0.05). The students who used especial drugs or had especial diseases had higher rates of depression and anxiety (p < 0.05). Conclusion: Internet addiction may cause depression and anxiety in high school students. It seems necessary to develop an Internet addiction prevention program for adolescents taking into account the psychological factors such as depression and Internet use habits.\n\n==\n\nMohammadkhani 2017:\n\nTitle: Internet Addiction in High School Students and Its Relationship With the Symptoms of Mental Disorders\n\nAbstract: Received: 08 Nov. 2016 Accepted: 12 Mar. 2017\n\n==\n\nOng 2014:\n\nTitle: Internet addiction in young people.\n\nAbstract: In our technology-savvy population, mental health professionals are seeing an increasing trend of excessive Internet use or Internet addiction. Researchers in China, Taiwan and Korea have done extensive research in the field of Internet addiction. Screening instruments are available to identify the presence of Internet addiction and its extent. Internet addiction is frequently associated with mental illnesses such as anxiety, depression, conduct disorder and attention deficit hyperactivity disorder (ADHD). Treatment modalities include individual and group therapies, cognitive behavioural therapy (CBT), family therapy and psychotropic medications. A significant proportion of Singapore adolescents engaging in excessive Internet use are also diagnosed to have concomitant Internet addiction. Despite the presence of a variety of treatment options, future research in this area is needed to address its growing trend and to minimise its negative psychological and social impact on the individuals and their families.\n\n==\n\nLi 2022:\n\nTitle: Association of Parental Screen Addiction with Young Children\u2019s Screen Addiction: A Chain-Mediating Model\n\nAbstract: Preschool children are immersed in screen media, yet little study has been conducted on screen addiction among them. This study aimed to investigate the relationship between parental screen addiction and young children\u2019s screen addiction and to verify factors that mediate this relationship. A total of 477 parents of kindergarteners (3\u20136 years old) were recruited via six kindergartens in Henan province, China. They completed the Problematic Media Use Measure Short Form of Children, the Self-Rating Anxiety Scale, the Child\u2013Parent Relationship Scale, and the Parental Screen Addiction Scale. The results showed that the relationships between each pair of parental screen addiction, parental anxiety, and children\u2019s screen addiction were significantly positive, but the parent\u2013child relationship was negatively correlated with the other variables. Parental screen addiction can directly and indirectly affect children\u2019s screen addiction through parental anxiety and the parent\u2013child relationship. The findings contribute to the development of family protection mechanisms against screen addiction in children.", "final_summary": "Screen addiction, a pathological compulsive use of screen-based devices, is a growing concern among middle and high school students. The studies collectively suggest that screen addiction is prevalent and has significant psychological implications.\n\nGarc\u00eda-Santill\u00e1n (2021) conducted a study to analyze the use of mobile phones by high school students during the pandemic and determine if this has led to addiction. The study found no gender differences in phone use. Johnson (2022) found that 20% of rural adolescents felt preoccupied with screens, and 3.9% had screen addiction, which was significantly higher among depressed subjects, males, and those who did not participate in sports.\n\nChoi (2021) found that excessive exposure to screen media was associated with higher risks of emotional and behavioral problems in elementary school students. Sasmaz (2014) also found a high prevalence of internet addiction among high school students, which was independently related to gender, grade level, duration of daily computer use, depression, and negative self-perception.\n\nAhmadi (2014) found that internet addiction may cause depression and anxiety in high school students, suggesting the need for an internet addiction prevention program. Ong (2014) found that internet addiction was frequently associated with mental illnesses such as anxiety, depression, conduct disorder, and ADHD. Li (2022) found that parental screen addiction could directly and indirectly affect children's screen addiction through parental anxiety and the parent-child relationship.\n\nIn conclusion, these studies collectively suggest that screen addiction is a significant issue among middle and high school students, with potential psychological implications. The findings underscore the need for preventive measures and interventions to manage screen addiction among adolescents (Garc\u00eda-Santill\u00e1n, 2021; Johnson, 2022; Choi, 2021; Sasmaz, 2014; Ahmadi, 2014; Ong, 2014; Li, 2022)."}, {"query": "How do teachers' and scientists' understanding of effective teaching, effective science education, and education improvement differ?", "paper_list_string": "Taylor 2008:\n\nTitle: Creativity, inquiry, or accountability? Scientists' and teachers' perceptions of science education\n\nAbstract: Although there have been numerous studies that indicate the benefits of teachers and students working with scientists, there is little research that documents scientists' views of science education, science teacher preparation, and the goals of science education. Furthermore, little is known about how scientists' views of science education may differ from those held by science teachers. Through the use of semistructured interviews, the perceptions of 37 scientists from diverse science domains and 21 middle and high school science teachers were explored. Participating scientists expressed concerns about the variability in quality of teaching, programs, and resources available for science instruction. Scientists expressed a desire for teachers to have more experience conducting science research and developing their own critical thinking skills. When asked what goals are most important for science education, 40% of the scientists emphasized that teachers should make science fun and exciting for their students. Science teachers' perceptions of science education were compared with the scientists' perceptions. Thirty percent of the teachers agreed with the scientists that too much variability in program or instructional quality exists in science education. Seventy-six percent of the science teachers also thought there is a need to teach critical thinking skills, but more importantly there is a need to inspire creativity and a desire to learn science in students. Both teachers and scientists expressed concerns about how high-stakes accountability inhibits efforts to improve science education. \u00a9 2008 Wiley Periodicals, Inc. Sci Ed92:1058\u20131075, 2008\n\n==\n\nSchuster 2009:\n\nTitle: Scientists' teaching orientations in the context of teacher professional development\n\nAbstract: During the past decade science educators have taken steps to identify seminal structures and approaches of science teacher professional development. This literature, which is increasingly informing how and in which contexts professional developers design and implement programs, states that scientists' divergent research interests and knowledge about K-12 science teachers often limit their involvement in these programs. Conversely, concerns persist that there is very little empirical evidence to support these programmatic recommendations made by the science education literature. This embedded case study was bounded by the contextual similarities between seven professional development workshops with the goal of examining the relationship between scientists' views of teachers as professionals and the pedagogical orientations that the scientists used within these professional development contexts. Multiple methods were employed including systematic classroom observation by nine trained observers, as well as analysis of course materials, interviews, and questionnaires. Altogether, the teacher comments and observation data paint a picture of how the instructors implemented specific pedagogical orientations. Participants appear to have realistic views about the practical roles that research scientists can play in professional development. We explore how these findings merit attention and suggest alternatives to current practice and policy. \u00a9 2008 Wiley Periodicals, Inc. Sci Ed93:635\u2013655, 2009\n\n==\n\nUcar 2012:\n\nTitle: How Do Pre-Service Science Teachers\u2019 Views on Science, Scientists, and Science Teaching Change Over Time in a Science Teacher Training Program?\n\nAbstract: Every aspect of teaching, including the instructional method, the course content, and the types of assessments, is influenced by teachers\u2019 attitudes and beliefs. Teacher education programs play an important role in the development of beliefs regarding teaching and learning. The purpose of the study was to document pre-service teachers\u2019 views on science, scientists, and science teaching as well as the relations between these views and the offered courses over several years spent in an elementary science teacher training program. The sample consisted of 145 pre-service elementary science teachers who were being trained to teach general science to students in the 6th through 8th grades. The research design was a cross-sectional study. Three different instruments were used to collect the data, namely, the \u201cDraw a Scientist Test\u201d, \u201cDraw a Science Teacher Test\u201d, and \u201cStudents\u2019 Views about Science\u201d tests. The elementary science teacher training program influenced pre-service science teachers\u2019 views about science, scientists and science teaching to different degrees. The most pronounced impact of the program was on views about science teaching. Participants\u2019 impressions of science teaching changed from teacher-centered views to student-centered ones. In contrast, participants\u2019 views about scientists and science did not change much. This result could be interpreted as indicating that science teacher training programs do not change views about science and scientists but do change beliefs regarding teaching science.\n\n==\n\nHouseal 2014:\n\nTitle: Impact of a Student-Teacher-Scientist Partnership on Students' and Teachers' Content Knowledge, Attitudes toward Science, and Pedagogical Practices.\n\nAbstract: Engaging K-12 students in science-based inquiry is at the center of current science education reform efforts. Inquiry can best be taught through experiential, authentic science experiences, such as those provided by Student\u2013Teacher\u2013Scientist Partnerships (STSPs). However, very little is known about the impact of STSPs on teachers' and students' content knowledge growth or changes in their attitudes about science and scientists. This study addressed these two areas by examining an STSP called \u201cStudents, Teachers, and Rangers and Research Scientists\u201d (STaRRS). STaRRS was incorporated into the existing long-standing education program \u201cExpedition: Yellowstone!\u201d For teachers, a pre-test, intervention, post-test research design was used to assess changes and gains in content knowledge, attitudes, and pedagogical practices. A quasi-experimental, pre-test\u2013post-test, comparison group design was used to gauge gains in students' content knowledge and attitudes. Data analyses showed significant positive shifts in teachers' attitudes regarding science and scientists, and shifts in their pedagogical choices. Students showed significant content knowledge gains and increased positive attitudes regarding their perceptions of scientists. The findings indicate that STSPs might serve as a promising context for providing teachers and students with the sort of experiences that enhance their understandings of and about scientific inquiry, and improve their attitudes toward science and scientists. \u00a9 2013 Wiley Periodicals, Inc. J Res Sci Teach 51: 84\u2013115, 2014\n\n==\n\nCosta 2000:\n\nTitle: SCIENCE TEACHERS\u2019 AWARENESS OF FINDINGS FROM EDUCATION RESEARCH\n\nAbstract: In this paper, we report on a stnall-scale study designed to estimate science teachers' \nawareness of findings derived from research in science education and other branches of educational \nresearch. The study was conducted among experienced science teachers in Portugal who were \nfollowing advanced professional training programmes, usually leading to Masters' degrees in science \neducation. The results indicate that science teachers' knowledge of education research findings is \ngenerally very limited. What teachers regard as sound pedagogical knowledge is usually derived from \npersonal experience and 'common sense' and does tend not to be questioned by them as to its \ncompatibility with the results of research. The outcome of the study provides evidence of the \nexistence of a serious gap between research and the practice of science education. In the light of \nthese findings, the authors propose that to narrow this gap should be a major task to be addressed by \nresearchers and practitioners.\n\n==\n\nPilo 2012:\n\nTitle: Science Education and Teachers' Training: Research in Partnership.\n\nAbstract: International researches put to evidence a worrying decrease in science disciplines\u2019 roll in many countries, especially in European Community and a poor quality in scientific competences, as issues of TIMMS (trends in international mathematics and science study) and PISA (programme for international student assessment) have proved, together with a low interest concerning science knowledge of young people. According to our experience, the most important school-related factor in raising student achievement is the quality of the teacher. We investigated Italian pre-service teachers\u2019 scientific competences and their mind conceptions about teaching models/styles, students\u2019 learning, role of teacher and role of science in daily life. The aim of our research outlined in the present paper is to: (1) improve motivation, learning and pupils\u2019 attitudes in science education; (2) develop a critical thinking, stimulate intuition and creativity; and (3) increase scientific literacy in the community. We designed flexible materials, paths and courses within cooperation involving different institutions. The findings of this work concerning both pre-service and in-service teachers\u2019 training are: increased awareness about conceptual knots related to scientific concepts; more sensitiveness and attention to students\u2019 involvement; reflection about the effectiveness of daily school work-increased awareness about meta-cognition and cooperative learning.\n\n==\n\nLederman 1999:\n\nTitle: Teachers' Understanding of the Nature of Science and Classroom Practice: Factors That Facilitate or Impede the Relationship.\n\nAbstract: The purpose of this multiple case study was to investigate the relationship of teachers' understanding of the nature of science and classroom practice and to delineate factors that facilitate or impede a relationship. Five high school biology teachers, ranging in experience from 2 to 15 years, comprised the sample for this investigation. During one full academic year, multiple data sources were collected and included classroom observations, open-ended questionnaires, semistructured and structured interviews, and instructional plans and materials. In addition, students in each of the teachers' classrooms were interviewed with respect to their understanding of the nature of science. Using analytical induction, multiple data sources were analyzed independently and together to triangulate data while constructing teacher profiles. The results indicated that teachers' conceptions of science do not necessarily influence classroom practice. Of critical importance were teachers' level of experience, intentions, and perceptions of students. The results have importnat implications for teacher education as well as the successful implementation of current reforms. \u00a9 1999 John Wiley & Sons, Inc. J Res Sci Teach 36: 916\u2013929, 1999\n\n==\n\nLustick 2011:\n\nTitle: Experienced secondary science teachers' perceptions of effective professional development while pursuing National Board certification\n\nAbstract: Reports such as Rising Above the Gathering Storm and Europe Needs More Scientists exemplify the international community\u2019s concern about the state of public science education. With a lack of scientific literacy for all students and insufficient numbers of individuals choosing to pursue careers in science, the situation is seen as a threat to the sustainability of a robust national economy. Continuing professional development for science teachers is considered a key reform strategy for addressing the problem. As nations invest considerable funds towards improving the quality of science teachers (and consequently student learning) it is reasonable to explore science teacher perceptions regarding the most effective professional learning opportunities. This descriptive study surveyed 118 candidates for National Board certification in Adolescent and Young Adult Science from 42 states about their professional learning experiences. The survey intended to answer the question: \u2018How do candidates perceive the relative effectiveness of different professional learning experiences?\u2019 \u2018Effectiveness\u2019 here is defined as a professional development's ability to help a teacher foster improved student learning. It is important to note that the survey focused on the approaches to professional development rather than the content or goals of said experiences. Candidates' self\u2010reports reveal that developing science curriculum, reading scientific literature, and pursuing National Board certification are the three most productive approaches to professional development while education courses and in\u2010service workshops were identified as least effective. The results suggest that no one model of professional development explains the top rated activities.", "final_summary": "The understanding of effective teaching, science education, and education improvement varies significantly between teachers and scientists, as evidenced by the research papers. \n\nTaylor (2008) found that scientists expressed concerns about the variability in quality of teaching and desired for teachers to have more experience conducting science research. In contrast, teachers emphasized the need to inspire creativity and a desire to learn science in students. Schuster (2009) found that scientists' divergent research interests and knowledge about K-12 science teachers often limit their involvement in professional development programs. \n\nUcar (2012) found that science teacher training programs significantly influenced pre-service science teachers' views about science teaching, shifting from teacher-centered views to student-centered ones. However, their views about scientists and science did not change much. Houseal (2014) found that Student\u2013Teacher\u2013Scientist Partnerships (STSPs) enhanced teachers' and students' understandings of scientific inquiry and improved their attitudes toward science and scientists. \n\nCosta (2000) found that science teachers' knowledge of education research findings is generally very limited, and what they regard as sound pedagogical knowledge is usually derived from personal experience and 'common sense'. Pilo (2012) emphasized the importance of teacher quality in raising student achievement and found that both pre-service and in-service teachers' training increased awareness about conceptual knots related to scientific concepts and sensitivity to students' involvement. \n\nLederman (1999) found that teachers' conceptions of science do not necessarily influence classroom practice. Factors such as teachers' level of experience, intentions, and perceptions of students were of critical importance. Lustick (2011) found that developing science curriculum, reading scientific literature, and pursuing National Board certification were the most productive approaches to professional development for science teachers.\n\nIn conclusion, while there are shared concerns about the quality of science education, the perspectives of teachers and scientists diverge on the methods and goals of effective teaching and education improvement. This highlights the need for increased dialogue and collaboration between these two groups to bridge the gap and enhance the quality of science education (Taylor, 2008; Schuster, 2009; Ucar, 2012; Houseal, 2014; Costa, 2000; Pilo, 2012; Lederman, 1999; Lustick, 2011)."}, {"query": "multi-hop information retrieval  with large language models", "paper_list_string": "Awasthi 2022:\n\nTitle: Bootstrapping Multilingual Semantic Parsers using Large Language Models\n\nAbstract: Despite cross-lingual generalization demonstrated by pre-trained multilingual models, the translate-train paradigm of transferring English datasets across multiple languages remains to be a key mechanism for training task-specific multilingual models. However, for many low-resource languages, the availability of a reliable translation service entails significant amounts of costly human-annotated translation pairs. Further, translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models. For multilingual semantic parsing, we demonstrate the effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting. Through extensive comparisons on two public datasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show that our method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages. We study the key design choices that enable more effective multilingual data translation via prompted LLMs.\n\n==\n\nBellegarda 2000:\n\nTitle: Large vocabulary speech recognition with multispan statistical language models\n\nAbstract: Multispan language modeling refers to the integration of various constraints, both local and global, present in the language. It was recently proposed to capture global constraints through the use of latent semantic analysis, while taking local constraints into account via the usual n-gram approach. This has led to several families of data-driven, multispan language models for large vocabulary speech recognition. Because of the inherent complementarity in the two types of constraints, the multispan performance, as measured by perplexity, has been shown to compare favorably with the corresponding n-gram performance. The objective of this work is to characterize the behavior of such multispan modeling in actual recognition. Major implementation issues are addressed, including search integration and context scope selection. Experiments are conducted on a subset of the Wall Street Journal (WSJ) speaker-independent, 20000-word vocabulary, continuous speech task. Results show that, compared to standard n-gram, the multispan framework can lead to a reduction in average word error rate of over 20%. The paper concludes with a discussion of intrinsic multi-span tradeoffs, such as the influence of training data selection on the resulting performance.\n\n==\n\nZiems 2023:\n\nTitle: Large Language Models are Built-in Autoregressive Search Engines\n\nAbstract: Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.\n\n==\n\nHiemstra 2001:\n\nTitle: Using language models for information retrieval\n\nAbstract: Because of the world wide web, information retrieval systems are now used by millions of untrained users all over the world. The search engines that perform the information retrieval tasks, often retrieve thousands of potentially interesting documents to a query. The documents should be ranked in decreasing order of relevance in order to be useful to the user. This book describes a mathematical model of information retrieval based on the use of statistical language models. The approach uses simple document-based unigram models to compute for each document the probability that it generates the query. This probability is used to rank the documents. The study makes the following research contributions. \n \n * The development of a model that integrates term weighting, relevance feedback and structured queries. \n * The development of a model that supports multiple representations of a request or information need by integrating a statistical translation model. \n * The development of a model that supports multiple representations of a document, for instance by allowing proximity searches or searches for terms from a particular record field (e.g. a search for terms from the title). \n * A mathematical interpretation of stop word removal and stemming. \n * A mathematical interpretation of operators for mandatory terms, wildcards and synonyms. \n * A practical comparison of a language model-based retrieval system with similar systems that are based on well-established models and term weighting algorithms in a controlled experiment. \n * The application of the model to cross-language information retrieval and adaptive information filtering, and the evaluation of two prototype systems in a controlled experiment. \n \nExperimental results on three standard tasks show that the language model-based algorithms work as well as, or better than, today's top-performing retrieval algorithms. The standard tasks investigated are ad-hoc retrieval (when there are no previously retrieved documents to guide the search), retrospective relevance weighting (find the optimum model for a given set of relevant documents), and ad-hoc retrieval using manually formulated Boolean queries. The application to cross-language retrieval and adaptive filtering shows the practical use of respectively structured queries, and relevance feedback.\n\n==\n\nNing 2020:\n\nTitle: Multi-hop Interactive Cross-Modal Retrieval\n\nAbstract: Conventional representation learning based cross-modal retrieval approaches always represent the sentence with a global embedding feature, which easily neglects the local correlations between objects in the image and phrases in the sentence. In this paper, we present a novel Multi-hop Interactive Cross-modal Retrieval Model (MICRM), which interactively exploits the local correlations between images and words. We design a multi-hop interactive module to infer the high-order relevance between the image and the sentence. Experimental results on two benchmark datasets, MS-COCO and Flickr30K, demonstrate that our multi-hop interactive model performs significantly better than several competitive cross-modal retrieval methods.\n\n==\n\nLafferty 2001:\n\nTitle: Document language models, query models, and risk minimization for information retrieval\n\nAbstract: We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval. A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models. The Markov chain method has connections to algorithms from link analysis and social networks. The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio. Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data.\n\n==\n\nMisra 2023:\n\nTitle: Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks\n\nAbstract: Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random walks over structured knowledge graphs. Specifically, we use soft prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random walk paths that lead to the answer. Applying our methods on two T5 LMs shows substantial improvements over standard tuning approaches in answering questions that require 2-hop reasoning.\n\n==\n\nLafferty 2001:\n\nTitle: Document Language Models, Query Models, and Risk Minimization for Information Retrieval\n\nAbstract: We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval. A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models. The Markov chain method has connections to algorithms from link analysis and social networks. The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio. Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data.", "final_summary": "The research question \"multi-hop information retrieval with large language models\" has been addressed by several studies, each contributing unique insights into the field.\n\nAwasthi (2022) demonstrated the effectiveness of large language models (LLMs) in translating English datasets into multiple languages, outperforming traditional translate-train methods in 41 out of 50 languages. This suggests that LLMs can be used to enhance multilingual data translation, which could be beneficial for multi-hop information retrieval.\n\nBellegarda (2000) introduced multispan language modeling, which integrates local and global constraints in language. This approach led to a reduction in average word error rate by over 20%, indicating that multispan modeling could improve the accuracy of information retrieval.\n\nZiems (2023) found that LLMs can generate URLs for document retrieval when provided with a few {Query-URL} pairs as in-context demonstrations. This suggests that LLMs can be used as built-in search engines, potentially enhancing the efficiency of multi-hop information retrieval.\n\nHiemstra (2001) developed a mathematical model of information retrieval based on statistical language models. The model integrates term weighting, relevance feedback, and structured queries, and has been shown to perform as well as, or better than, other top-performing retrieval algorithms.\n\nNing (2020) proposed a Multi-hop Interactive Cross-modal Retrieval Model (MICRM) that exploits local correlations between images and words. This model outperformed several competitive cross-modal retrieval methods, suggesting that it could be useful for multi-hop information retrieval.\n\nLafferty (2001) presented a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. This method showed significant improvements over standard approaches, indicating its potential for enhancing multi-hop information retrieval.\n\nMisra (2023) proposed techniques that improve upon the limitation of pre-trained language models in multi-hop reasoning by relying on random walks over structured knowledge graphs and using soft prompts.\n\nIn conclusion, these studies collectively suggest that large language models, multispan language modeling, and innovative retrieval models can significantly enhance multi-hop information retrieval. However, further research is needed to fully explore the potential of these methods and to address any limitations or challenges that may arise."}, {"query": "criticism of Rona moss-morris", "paper_list_string": "Miller 1977:\n\nTitle: Conversations with Wright Morris : critical views and responses\n\nAbstract: This book is an attempt to approach the work of a leading American novelist from both sides of the looking-glass--from the opposite, but not necessarily opposing, points of view of the writer/creator and the reader/critic. In 1975, while the author was visiting professor of English at the University of Nebraska-Lincoln, several scholar-critics (among them John W. Aldridge, Wayne C. Booth, and David Madden) were invited to speak about his craft and artistic aims and principles and to record conversations with him about issues growing from their addresses. Since Morris is also an important photographer, facets of his achievement in this field were considered by Peter C. Bunnell. In addition to four conversations, three lectures, and a portfolio of twelve photographs, this volume includes an essay by Wright Morris and a bibliography compiled by Robert L. Boyce.\n\n==\n\nDavis 2000:\n\nTitle: Confessions of an Anacoluthon: Avital Ronell on Writing, Technology, Pedagogy, Politics.\n\nAbstract: In the introduction to a special issue of diacritics devoted to the work of Avital Ronell, Jonathan Culler writes that \"her books are like no others\"; that her sentences \"startle, irritate, illuminate\"; and that her work consti tutes \"one of the most remarkable critical oeuvres of our era.\" RonelPs writing is remarkable, in part, because of the unusual connections it makes, its determination to blur the distinctions between big thought and small talk, philosophy and rumor, literature and headline news?to blur, that is, the very divisions through which academia sustains itself. But Ronell's work is also remarkable in its style: her writing is characteristi cally tough, double entendre intended. It's difficult (because of its enormous scope and depth), and it's also gutsy, rough, edgy, and pushy, with a sort of streetwise candor. Indeed, Ronell herself identifies a kind of \"class struggle\" going down in her texts, a struggle involving her own various compulsions, denials, and voices?including the \"little hood lum,\" the \"high philosophical graduate student,\" and the \"more sophisti cated Parisian.\" Ronell notes, however, that the most discernible and continuous voice in her texts belongs to the \"wise-ass girl,\" an ancestor of the \"buffo\" and every bit an anacoluthon out to disrupt the \"smooth logic of accepted meaning or signification.\" This interruptive force? inasmuch as it does indeed \"startle, irritate, illuminate\"?takes a certain swipe at certitude, prompting rigorous hesitations that open the condi tions of possibility for what Ronell's works are always after: an ethics of decision in a postfoundational whirl(d). RonelPs rigorously deconstructive rereadings of everything?from the telephone, the television, and virtual reality to the Gulf War, AIDS, and Madame Bovary?take up that which has been \"marginalized, minoritized, evicted, persecuted, left out of the picture . . .feminized.\" Operating in the mode of \"irreverent reverence\" and in the service of a posthumanist ethical imperative, Ronell sets out to \"secure the space of academe as a sheltering place of unconditional hospitality for dissidence jac 20.2(2000)\n\n==\n\nMacCarthy 1994:\n\nTitle: William Morris: A Life for Our Time\n\nAbstract: Winner of the Wolfson History Prize, the essential biography of the father of the Arts and Crafts movement. The author, Fiona MacCarthy, is the curator of the National Portrait Gallery's 2014-15 exhibition Anarchy and Beauty: William Morris and His Legacy. \"One of the finest biographies ever published in this country\". (A. S. Byatt). Since his death in 1896, William Morris has come to be regarded as one of the giants of the Victorian era. But his genius was so many-sided and so profound that its full extent has rarely been grasped. Many people may find it hard to believe that the greatest English designer of his time, possibly of all time, could also be internationally renowned as a founder of the socialist movement, and could have been ranked as a poet together with Tennyson and Browning. With penetrating insight, Fiona MacCarthy has managed to encompass all the different facets of Morris' complex character, shedding light on his immense creative powers as artist and designer of furniture, fabrics, wallpaper, stained glass, tapestry and books, and as a poet, novelist and translator; his psychology and his emotional life; his frenetic activities as polemicist and reformer; and his remarkable circle of friends, literary, artistic and political, including Dante Gabriel Rossetti and Edward Burne-Jones. Fiona MacCarthy's skilful drawing together of these disparate elements makes for a comprehensive and compelling biography.\n\n==\n\nSlights 2001:\n\nTitle: Rape and the Romanticization of Shakespeare's Miranda\n\nAbstract: In 1981, Jean Elshtain issued a plea that political philosophy recognize female agency as a valid focus of study: \u201cThe feminist political thinker aims to transform her discipline as well as her social world in important ways. This necessitates locating the woman as subject of political and social inquiry, moving away from the abstracted, disembodied \u2018product\u2019 of social forces featured in much contemporary social science. This female subject, as the object of inquiry, must be approached as an active agent of a life-world of intense personalization and immediacy.\u201d 1 Twenty years later, I am taking up Elshtain\u2019s call in a literary context in order to suggest that the history of Tempest criticism stands as powerful proof that political criticism of Shakespearean drama has yet to devise a solid theoretical basis from which to approach female characters as dynamic participants in the fictional worlds of which they are constitutive members. Specifically, this paper seeks to account for, and to challenge, Miranda\u2019s exclusion from critical discourse. By exploring what happens when Miranda is treated merely as an emblem of a colonialist ruling class rather than understood as an active agent in the life-world of the play, my paper participates in a recent dialogue concerned with evaluating the role a rehabilitated notion of character might play in the development of an ethical\u2014and also historically aware\u2014criticism of Shakespearean drama. These days, \u201ccharacter criticism,\u201d an approach initiated in the eighteenth century and popularized in the early twentieth century by A. C. Bradley, is most often considered synonymous\n\n==\n\nSlights 2001:\n\nTitle: Rape and the Romanticization of Shakespeare's Miranda\n\nAbstract: In 1981, Jean Elshtain issued a plea that political philosophy recognize female agency as a valid focus of study: \u201cThe feminist political thinker aims to transform her discipline as well as her social world in important ways. This necessitates locating the woman as subject of political and social inquiry, moving away from the abstracted, disembodied \u2018product\u2019 of social forces featured in much contemporary social science. This female subject, as the object of inquiry, must be approached as an active agent of a life-world of intense personalization and immediacy.\u201d 1 Twenty years later, I am taking up Elshtain\u2019s call in a literary context in order to suggest that the history of Tempest criticism stands as powerful proof that political criticism of Shakespearean drama has yet to devise a solid theoretical basis from which to approach female characters as dynamic participants in the fictional worlds of which they are constitutive members. Specifically, this paper seeks to account for, and to challenge, Miranda\u2019s exclusion from critical discourse. By exploring what happens when Miranda is treated merely as an emblem of a colonialist ruling class rather than understood as an active agent in the life-world of the play, my paper participates in a recent dialogue concerned with evaluating the role a rehabilitated notion of character might play in the development of an ethical\u2014and also historically aware\u2014criticism of Shakespearean drama. These days, \u201ccharacter criticism,\u201d an approach initiated in the eighteenth century and popularized in the early twentieth century by A. C. Bradley, is most often considered synonymous\n\n==\n\nLeggott 2013:\n\nTitle: No Known Cure: The Comedy of Chris Morris\n\nAbstract: Acknowledgments Notes on Contributors Introduction: Why Bother? - James Leggott 1. Beyond Our Ken: Chris and Armando, Janet and John, and The Road To On the Hour - Ian Greaves 2. Rockarama Newsbanana: Chris Morris - Misbehaviour in Music Radio - Justin Lewis 3. Mocking the News: The Day Today and Brass Eye as Mockumentary News Satire - Craig Hight 4. 'Only This Can Make it a News': The Language of News Media - Dan North 5. Reporting the 'illegalisation of cake': Moral Panics and Brass Eye - Jeremy Collins 6. 'Complaints Were Received': Morris, Comedy, and Broadcasting in Context - Brett Mills 7. 'Handling' The Darkness: Chris Morris as Cultural Capital - Sam Friedman 8. 'Bashed Be Biff By All With Ears': Blue Jam and the Radio - Robert Dean and Richard J. Hand 9. Lost in Techno Trance: Dance Culture, Drugs and the Digital Image in Jam - Jamie Sexton 10. I remain, sir, Disgusted of America. 'Morriucci' and 9/11: 'An Absolute Atrocity Special' - David Walton 11. 'Well Futile': Nathan Barley and Post-Ironic Culture - Adam Whybray 12. 'Dad's Army Side to Terrorism': Chris Morris, Four Lions and Jihad Comedy - Sharon Lockyer 13. 'I don't wanna be dead! There's no future in it!': The US and UK critical reception of Four Lions - Russ Hunter 14. The helium of publicity: mass-mediated 'terrierism' - David Rolinson Appendix: List of Morris Works Index\n\n==\n\nAbram 2014:\n\nTitle: Staging the unsayable: debbie tucker green\u2019s political theatre\n\nAbstract: debbie tucker green is one of the most stylistically innovative and politically engaged playwrights at work in Britain today. Her prolific output is widely recognised in discussions of contemporary black British theatre, where she is often named alongside Kwame Kwei-Armah and Roy Williams as the leading playwrights of their generation. Moreover, she has become a figurehead for new British playwriting more broadly, as evinced by her inclusion in Aleks Sierz\u2019 Rewriting the Nation: British Theatre Today (2011). Yet this energetic acclaim was preceded by a period of critical questioning; early reviewers responded indignantly to her subversion of conventional plot structures, highly stylised use of language and assuredly sparse stage design. Many saw these features as a failure to fulfil the demands of the dramatic medium; some hesitated to use the term \u201cplay\u201d to classify her work. One critic, receiving her drama as an aural rather than visual or embodied experience, claimed that she would \u201cfind a happier home on [BBC] Radio 3\u201d (I. Johns). In another particularly piqued response, reviewer Dominic Cavendish parodied the demotic voice in which tucker green\u2019s characters speak: \u201cI don\u2019t do silence, innit\u201d (251). This invocation of silence \u2013 or, rather, its absence \u2013 is intriguing. Cavendish is right that tucker green\u2019s plays are marked by chaotic dialogue: incessant repetition, overlap and interruption. Yet, amidst this proliferation of speech, much remains unsaid. This article examines tucker\n\n==\n\nWilkes 2001:\n\nTitle: Women and Literature in Britain 1800\u20131900: Remaking the canon\n\nAbstract: In January 1870, a respected British weekly, the Athenaeum , discussed several now-forgotten novels, and gave them short shrift. These included Lady Betty , by Christabel Coleridge, and Herbert Tresham , by the Revd J. M. Neale. Christabel Coleridge's novel is criticised mainly for its plot, which the anonymous reviewer finds badly constructed, tedious and implausible, while the Revd Neale's book is faulted both for being \u2018pedantically written\u2019, and for telling an \u2018insignificant\u2019 tale which the reviewer believes is \u2018a mere pretext for thrusting the author's opinions down the reader's throat\u2019. The review identifies no redeeming features in Herbert Tresham , but acknowledges that Lady Betty does contain \u2018some pleasing writing\u2019, and therefore suggests that Christabel Coleridge is \u2018capable of something better\u2019. What is notable, however, is that Lady Betty's faults are ascribed to the writer's sex, while this is not the case with the Revd Neale's novel. The review of Christabel Coleridge's text begins: \u2018It scarcely needed the author's name affixed to the title page of \u201cLady Betty\u201d, to tell us it was the work of a lady\u2019 \u2013 and goes on to call the novel's story \u2018feeble and badly designed\u2019, as if this were the natural corollary of female authorship. By contrast, the pedantry and offensive didacticism of Herbert Tresham are not linked to the Revd Neale's sex, or even to his clerical status. These reviews represent a minor instance of a practice endemic to nineteenth-century literary criticism \u2013 the ascription of particular characteristics to writing on the basis of its author's sex. Although this happened to texts by writers of both sexes, the practice was more common in discussions of women's publications.", "final_summary": "I'm sorry, but none of the papers provided seem to contain any criticism or discussion of Rona Moss-Morris. Therefore, it's not possible to provide a summary addressing the research question."}, {"query": "Using the recent and updated references, What is the current state of human rights education in Turkey?", "paper_list_string": "\u00c7ay\u0131r 2011:\n\nTitle: \u2018No\u2010one respects them anyway\u2019: secondary school students\u2019 perceptions of human rights education in Turkey\n\nAbstract: The incorporation of compulsory courses on human rights into the secondary school curriculum in 1998 has been an important first step in developing respect for human rights and responsibilities among the younger generation in Turkey. Yet, these courses have many shortcomings in terms of materials, pedagogy and teacher attitudes. This paper explores Grades 7 and 8 (ages 13 and 14) students\u2019 experiences in Citizenship and Human Rights Education courses on the basis of qualitative data collected through focus group discussions in Ankara and Istanbul in the 2006\u20132007 academic year. The responses of the students indicate that these courses have had little impact in empowering students or in facilitating them to consider their own or others\u2019 human rights as an integral part of their lives. Rather, the students perceive the national and the global arena as characterized by mass human rights violations against which they feel powerless. The paper draws attention to the importance of a revised human rights education for students along with a global focus and appropriate methodology.\n\n==\n\nPayaslyoglu 1999:\n\nTitle: Awareness of and Support for Human Rights Among Turkish University Students\n\nAbstract: Human rights have a high place on the agenda of the world today. In the eyes of their defenders they are a sine qua non for the peace and the welfare of mankind and for democratic ideals. On the other hand the human rights records of many countries are still very poor and in some cases even scandalous. Thus the promotion and the reinforcement of human rights, their propagation, cultivation, and protection everywhere, depend upon the ceaseless efforts of all those concerned, including both official authorities and civil societies alike. As part of these efforts, studies on the attitudes of strategic social groups such as youth, women, and minorities with regard to human rights may provide some useful information and clues for both theoretical understanding and practical, preventive, and corrective purposes.' In view of this, the present study attempts to uncover certain facts concerning the awareness of and the support for human rights in one such strategic group in Turkey: university students.2\n\n==\n\n\u015een 2018:\n\nTitle: The Rise and Fall of Citizenship and Human Rights Education in Turkey.\n\nAbstract: Purpose: This article shows the effects of competing political forces on citizenship education in Turkey during the period of commitment to European Union (EU) accession (1999-2005). Methodology: It draws on textbooks, archival documents and interviews. Whilst Turkey had a history of civic education to promote a secular national ethos and identity, the post-Cold War democratisation movement encouraged the Turkish government in 1995 to attempt to internationalise civics by adding human rights themes. Findings: This effort occurred at a time when the hegemony of the secular nationalist establishment was challenged by the electoral rise of an Islamist party. Although Citizenship and Human Rights course suited the purposes of the secular nationalist establishment, after the EU recognised Turkey as a candidate in 1999, a new political Islamist government, elected in 2002, chose first to align the course with its ideology and later decided to repeal it. By exploring the evolution of the curriculum in a crucial period in which political power was switching from the ideology of secular nationalism to that of religious (Islamist) nationalism, the present study illustrates ways in which external and internal influences may affect citizenship education. In particular, it contributes to debates over the role of international agencies in curriculum change in citizenship education.\n\n==\n\n\u015een 2021:\n\nTitle: How are human rights presented in Turkey\u2019s textbooks? Development of an escapist model of human rights education (1950-2020)\n\nAbstract: ABSTRACT Drawing on a critical conceptualisation of human rights education (HRE), this study investigates how human rights were presented in a set of textbooks which were used in Turkey from 1950 to 2020. Findings show that the textbooks avoid human rights issues, struggles, campaigns, and activists by sustaining a de-politicised and de-contextualised narrative. They do not even refer to the Holocaust in explaining the historical development of human rights. These findings suggest that HRE gained a curricular status as a matter of window dressing, an \u2018escapist\u2019 model developed as a result. This model is characterised by a deliberate avoidance of anything that may be considered as \u2018political\u2019. This study concludes that placing socio-political issues at the centre of HRE and encouraging learners to critically reflect on discrepancies between claims and realities of human rights can improve the transformative powers of HRE in nationalist contexts with no strong tradition of rights struggle.\n\n==\n\nKaraman\u2010Kepenek\u00e7i 2005:\n\nTitle: Citizenship and Human Rights Education: A Comparison of Textbooks in Turkey and the United States\n\nAbstract: sues of today\u2019s world (Baehr, 1999; Buergental, 1995; Provost, 2002) and are essential to protect and improve human beings to ensure the national and world peace (Reardon, 2002). One of the important ways to secure human rights is known to be human rights education (Branson & Purta, 1982), which in turn is related to citizenship education. It is undoubtedly essential that human beings make use of the rights they own in order to be responsible citizens. However, making use of the rights primarily depends on humans knowing what rights they own. Are human beings aware and/or making use of the rights they have? It would not be so easy to answer this question as \u201cyes.\u201d Then we are faced with two questions: Should human rights be known by all human beings? Should human rights be utilized at the highest possible level? It is an obvious fact that this problem can be surmounted through education in this field through, briefly expressed, \u201ccitizenship and human rights education\u201d (G\u00fclmez, 2001; KaramanKepenekci, 1999a). There are a number of studies (Bajaj, 2004; Charles, 1991; Cogan, 1999; Goehring, Kurtz, & Rosenthal, 2000; Hornberg, 2002; Karaman-Kepenekci, 1999a; Lenhart & Savolainen, 2002; Lohrenscheit, 2002; Patrick, 1999) focused on citizenship and human rights education. Martin (1987) claims that for citizenship and human rights education to be successful, it is crucial that this education be applied effectively. A number of factors are shown to be important for an effective human rights education: democratic school and class climate (Balton, 1992; Bottery, 1999; Branson & Purta, 1982; Drubay, 1986; G\u00fclmez, 2001; Karaman-Kepenekci, 1999b; Pettman, 1984; Richardson, 1979), positive teacher behavior (Campos, 1989; Charles, 1991; Drubay, 1986; Karaman-Kepenekci, 1998; Pettman, 1984; UNESCO, 1969), contribution from other courses taught at school (Drubay, 1986; Karaman-Kepenekci, 2000; UNESCO, 1985; UNESCO, 1969; Vandenburg, 1984), and a separate course for citizenship and human rights education (Hornberg, 2002; Karaman-Kepenekci, 2000).\n\n==\n\nKepenekci 2005:\n\nTitle: A study of effectiveness of human rights education in Turkey\n\nAbstract: The aim of the research is to examine the effectiveness of Civics and Human Rights Education courses taught in primary schools in Turkey. The criteria for the effectiveness of the courses are determined as \u2018content\u2019, \u2018educational activities\u2019, \u2018teaching methods\u2019, \u2018educational materials\u2019, and \u2018evaluation of students\u2019. A total of 71 teachers teaching these courses participated in the study. The opinions of the teachers for the effectiveness of the courses were gathered by 12 open\u2010ended questions. Content analysis method was used to analyse the views of the teachers. To conclude, more than half of the teachers think that the courses are not effective due to having mainly an informative purpose.\n\n==\n\nMerey 2018:\n\nTitle: A Comparison of Human Rights Education in Social Studies Textbooks in Turkey and the United States\n\nAbstract: The aim of this study was to compare the level of allocation of human rights education issues in social studies textbooks in Turkey and the United States. For this aim, six social studies textbooks from both countries were examined. Textbooks were analyzed in terms of their level of \u201chuman rights education issues,\u201d in accordance with Karaman - Kepenekci\u2019s (1999) subcategories. A content analysis method was used to analyze the textbooks. As a result, it was observed that human rights education issues were included more in Turkish social studies textbooks. The study showed that Turkish and United States textbooks contained the \u201crights\u201d subcategory the most. While the intensity scores of the \u201cdemocracy,\u201d \u201cfreedom,\u201d and \u201cjudgment\u201d subcategories were listed the highest in both countries\u2019 textbooks, on the other hand, the intensity scores of the \u201ctolerance\u201d and \u201cpeace\u201d subcategories were placed last in the textbooks of both countries.\n\n==\n\nAslan 2008:\n\nTitle: Human rights education : a comparison of mother tongue textbooks in Turkey and France\n\nAbstract: Textbooks are major instructional tools playing an important role in education at all grades. The purpose of this study is to compare the level of allocation of human rights issues - rights, freedom, democracy, justice, tolerance and peace - in mother tongue textbooks in Turkey and France. For this aim, ten mother tongue textbooks from both countries were examined. Content analysis method was used to analyse the textbooks. As a result, it was observed that human rights issues are included more in Turkish textbooks when compared to French textbooks. It was found that in the textbooks of both countries the 'rights' subcategory is included the most while the 'justice' subcategory is included the least. While the intensity scores of the 'democracy' and 'freedom' subcategories were listed toward the very end in the textbooks of both countries, the 'democracy' subcategory occupies more places in French textbooks whereas the 'freedom' subcategory occupies more places in Turkish textbooks. Besides, the intensity scores for the subcategories of 'rights', 'peace' and 'tolerance' hold the first three places in the mother tongue textbooks of both countries.", "final_summary": "Human rights education (HRE) in Turkey has been a topic of considerable interest and debate in recent years. The studies collectively reveal a landscape marked by challenges.\n\n\u00c7ay\u0131r (2011) highlights that while HRE was incorporated into the secondary school curriculum in 1998, it has been met with several shortcomings, including inadequate materials, pedagogical issues, and teacher attitudes. The study found that these courses have had little impact on empowering students or enabling them to consider human rights as an integral part of their lives. \n\nPayaslyoglu (1999) underscores the importance of understanding the attitudes of strategic social groups, such as university students, towards human rights. However, the study does not provide specific findings on the state of HRE in Turkey.\n\n\u015een (2018) provides a historical perspective, showing how political forces have influenced HRE in Turkey. The study reveals that while attempts were made to internationalise civics by adding human rights themes in 1995, these efforts were later undermined by the rise of an Islamist party.\n\n\u015een (2021) further elaborates on the shortcomings of HRE in Turkey, arguing that textbooks avoid human rights issues, struggles, campaigns, and activists, resulting in a de-politicised and de-contextualised narrative. This 'escapist' model of HRE, as \u015een (2021) terms it, avoids anything that may be considered as 'political'.\n\nKepenekci (2005) emphasizes the importance of effective HRE, finding that more than half of the teachers surveyed believe that the current HRE courses in Turkey are not effective due to their primarily informative purpose.\n\nMerey (2018) and Aslan (2008) both compare HRE in Turkish textbooks with those in other countries, with Merey (2018) finding that human rights education issues are included more in Turkish social studies textbooks compared to those in the United States, and Aslan (2008) finding that human rights issues are included more in Turkish textbooks compared to French textbooks.\n\nIn conclusion, while HRE has been incorporated into the Turkish education system, its effectiveness and impact are questionable. The studies suggest that a more comprehensive and politically engaged approach to HRE is needed, one that goes beyond merely informative content and encourages critical reflection on human rights issues."}]