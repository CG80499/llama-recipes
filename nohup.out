/mnt/cg-experimental/llama-recipes/llama_finetuning.py:11: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.64s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  3.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.14s/it]
/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
--> Model lmsys/vicuna-13b-v1.5-16k

--> lmsys/vicuna-13b-v1.5-16k has 328.09472 Million params

trainable params: 3,276,800 || all params: 13,019,141,120 || trainable%: 0.025169095025525
--> Training Set Length = 150
--> Validation Set Length = 50
Training Epoch0:   0%|[34m          [0m| 0/150 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Training Epoch0:   1%|[34m          [0m| 1/150 [00:03<08:16,  3.33s/it]Training Epoch0:   1%|[34mâ–         [0m| 2/150 [00:04<05:27,  2.22s/it]Training Epoch0:   2%|[34mâ–         [0m| 3/150 [00:06<04:28,  1.83s/it]Training Epoch0:   3%|[34mâ–Ž         [0m| 4/150 [00:07<04:00,  1.65s/it]Training Epoch0:   3%|[34mâ–Ž         [0m| 5/150 [00:08<03:47,  1.57s/it]Training Epoch0:   4%|[34mâ–         [0m| 6/150 [00:10<03:39,  1.53s/it]Training Epoch0:   5%|[34mâ–         [0m| 7/150 [00:11<03:29,  1.47s/it]Training Epoch0:   5%|[34mâ–Œ         [0m| 8/150 [00:13<03:23,  1.43s/it]Training Epoch0:   6%|[34mâ–Œ         [0m| 9/150 [00:14<03:16,  1.40s/it]Training Epoch0:   7%|[34mâ–‹         [0m| 10/150 [00:15<03:12,  1.37s/it]Training Epoch0:   7%|[34mâ–‹         [0m| 11/150 [00:17<03:13,  1.39s/it]Training Epoch0:   8%|[34mâ–Š         [0m| 12/150 [00:18<03:10,  1.38s/it]Training Epoch0:   9%|[34mâ–Š         [0m| 13/150 [00:19<03:07,  1.37s/it]Training Epoch0:   9%|[34mâ–‰         [0m| 14/150 [00:21<03:03,  1.35s/it]Training Epoch0:  10%|[34mâ–ˆ         [0m| 15/150 [00:22<03:00,  1.33s/it]Training Epoch0:  11%|[34mâ–ˆ         [0m| 16/150 [00:23<02:57,  1.32s/it]Training Epoch0:  11%|[34mâ–ˆâ–        [0m| 17/150 [00:25<02:56,  1.33s/it]Training Epoch0:  12%|[34mâ–ˆâ–        [0m| 18/150 [00:26<02:57,  1.35s/it]Training Epoch0:  13%|[34mâ–ˆâ–Ž        [0m| 19/150 [00:27<02:56,  1.34s/it]Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Number of tokens in the example:  4
Training Epoch0:  13%|[34mâ–ˆâ–Ž        [0m| 19/150 [00:28<03:14,  1.49s/it]

 step 0 is completed and loss is 8.709982872009277

 step 1 is completed and loss is 7.923007965087891

 step 2 is completed and loss is 7.04954719543457

 step 3 is completed and loss is 6.405670642852783

 step 4 is completed and loss is 5.43271017074585

 step 5 is completed and loss is 4.086333274841309

 step 6 is completed and loss is 3.4508397579193115

 step 7 is completed and loss is 2.615060329437256

 step 8 is completed and loss is 1.6645731925964355

 step 9 is completed and loss is 1.1661345958709717

 step 10 is completed and loss is 0.8331237435340881

 step 11 is completed and loss is 0.4134867787361145

 step 12 is completed and loss is 0.134190633893013

 step 13 is completed and loss is 0.018055344000458717

 step 14 is completed and loss is 0.006737988442182541

 step 15 is completed and loss is 0.007097848691046238

 step 16 is completed and loss is 0.0018876809626817703

 step 17 is completed and loss is 0.002225890988484025

 step 18 is completed and loss is 0.0008004596456885338
Traceback (most recent call last):
  File "/mnt/cg-experimental/llama-recipes/llama_finetuning.py", line 253, in <module>
    fire.Fire(main)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/mnt/cg-experimental/llama-recipes/llama_finetuning.py", line 236, in main
    results = train(
  File "/mnt/cg-experimental/llama-recipes/utils/train_utils.py", line 92, in train
    loss = model(**batch).loss
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/peft/peft_model.py", line 931, in forward
    return self.base_model(
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 94, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 809, in forward
    outputs = self.model(
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 690, in forward
    layer_outputs = torch.utils.checkpoint.checkpoint(
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 249, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 107, in forward
    outputs = run_function(*args)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 686, in custom_forward
    return module(*inputs, past_key_value, output_attentions)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 413, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 310, in forward
    query_states = self.q_proj(hidden_states)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/peft/tuners/lora.py", line 1156, in forward
    output = lora_B(lora_A(dropout(x)))
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/llama_ft/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
